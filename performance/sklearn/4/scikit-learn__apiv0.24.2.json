{
  "schemaVersion": 1,
  "distribution": "",
  "package": "scikit-learn",
  "version": "",
  "modules": [],
  "classes": [
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation",
      "name": "AffinityPropagation",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/_pairwise@getter",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/_more_tags",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.",
      "docstring": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\ndamping : float, default=0.5\n    Damping factor (between 0.5 and 1) is the extent to\n    which the current value is maintained relative to\n    incoming values (weighted 1 - damping). This in order\n    to avoid numerical oscillations when updating these\n    values (messages).\n\nmax_iter : int, default=200\n    Maximum number of iterations.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\ncopy : bool, default=True\n    Make a copy of input data.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number\n    of exemplars, ie of clusters, is influenced by the input\n    preferences value. If the preferences are not passed as arguments,\n    they will be set to the median of the input similarities.\n\naffinity : {'euclidean', 'precomputed'}, default='euclidean'\n    Which affinity to use. At the moment 'precomputed' and\n    ``euclidean`` are supported. 'euclidean' uses the\n    negative squared euclidean distance between points.\n\nverbose : bool, default=False\n    Whether to be verbose.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nAttributes\n----------\ncluster_centers_indices_ : ndarray of shape (n_clusters,)\n    Indices of cluster centers.\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Cluster centers (if affinity != ``precomputed``).\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\naffinity_matrix_ : ndarray of shape (n_samples, n_samples)\n    Stores the affinity matrix used in ``fit``.\n\nn_iter_ : int\n    Number of iterations taken to converge.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nThe algorithmic complexity of affinity propagation is quadratic\nin the number of points.\n\nWhen ``fit`` does not converge, ``cluster_centers_`` becomes an empty\narray and all training samples will be labelled as ``-1``. In addition,\n``predict`` will then label every sample as ``-1``.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, ``fit`` will result in\na single cluster center and label ``0`` for every sample. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\n\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007\n\nExamples\n--------\n>>> from sklearn.cluster import AffinityPropagation\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AffinityPropagation(random_state=5).fit(X)\n>>> clustering\nAffinityPropagation(random_state=5)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])\n>>> clustering.predict([[0, 0], [4, 4]])\narray([0, 1])\n>>> clustering.cluster_centers_\narray([[1, 2],\n       [4, 2]])",
      "code": "class AffinityPropagation(ClusterMixin, BaseEstimator):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, default=0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, default=200\n        Maximum number of iterations.\n\n    convergence_iter : int, default=15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : bool, default=True\n        Make a copy of input data.\n\n    preference : array-like of shape (n_samples,) or float, default=None\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : {'euclidean', 'precomputed'}, default='euclidean'\n        Which affinity to use. At the moment 'precomputed' and\n        ``euclidean`` are supported. 'euclidean' uses the\n        negative squared euclidean distance between points.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    random_state : int, RandomState instance or None, default=0\n        Pseudo-random number generator to control the starting state.\n        Use an int for reproducible results across function calls.\n        See the :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.23\n            this parameter was previously hardcoded as 0.\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : ndarray of shape (n_clusters,)\n        Indices of cluster centers.\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    affinity_matrix_ : ndarray of shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n    array and all training samples will be labelled as ``-1``. In addition,\n    ``predict`` will then label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation(random_state=5).fit(X)\n    >>> clustering\n    AffinityPropagation(random_state=5)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, damping=.5, max_iter=200, convergence_iter=15,\n                 copy=True, preference=None, affinity='euclidean',\n                 verbose=False, random_state='warn'):\n\n        self.damping = damping\n        self.max_iter = max_iter\n        self.convergence_iter = convergence_iter\n        self.copy = copy\n        self.verbose = verbose\n        self.preference = preference\n        self.affinity = affinity\n        self.random_state = random_state\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute _pairwise was deprecated in \"  # type: ignore\n                \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def _pairwise(self):\n        return self.affinity == \"precomputed\"\n\n    def _more_tags(self):\n        return {'pairwise': self.affinity == 'precomputed'}\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        if self.affinity == \"precomputed\":\n            accept_sparse = False\n        else:\n            accept_sparse = 'csr'\n        X = self._validate_data(X, accept_sparse=accept_sparse)\n        if self.affinity == \"precomputed\":\n            self.affinity_matrix_ = X\n        elif self.affinity == \"euclidean\":\n            self.affinity_matrix_ = -euclidean_distances(X, squared=True)\n        else:\n            raise ValueError(\"Affinity must be 'precomputed' or \"\n                             \"'euclidean'. Got %s instead\"\n                             % str(self.affinity))\n\n        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \\\n            affinity_propagation(\n                self.affinity_matrix_, preference=self.preference,\n                max_iter=self.max_iter,\n                convergence_iter=self.convergence_iter, damping=self.damping,\n                copy=self.copy, verbose=self.verbose, return_n_iter=True,\n                random_state=self.random_state)\n\n        if self.affinity != \"precomputed\":\n            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        if not hasattr(self, \"cluster_centers_\"):\n            raise ValueError(\"Predict method is not supported when \"\n                             \"affinity='precomputed'.\")\n\n        if self.cluster_centers_.shape[0] > 0:\n            with config_context(assume_finite=True):\n                return pairwise_distances_argmin(X, self.cluster_centers_)\n        else:\n            warnings.warn(\"This model does not have any cluster centers \"\n                          \"because affinity propagation did not converge. \"\n                          \"Labeling every sample as '-1'.\", ConvergenceWarning)\n            return np.array([-1] * X.shape[0])\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Fit the clustering from features or affinity matrix, and return\n        cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)",
      "instance_attributes": [
        {
          "name": "damping",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "convergence_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "preference",
          "types": null
        },
        {
          "name": "affinity",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "random_state",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "affinity_matrix_",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "cluster_centers_indices_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cluster_centers_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering",
      "name": "AgglomerativeClustering",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__",
        "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit",
        "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int or None, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\naffinity : str or callable, default='euclidean'\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or \"precomputed\".\n    If linkage is \"ward\", only \"euclidean\" is accepted.\n    If \"precomputed\", a distance matrix (instead of a similarity matrix)\n    is needed as input for the fit method.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each sample the neighboring\n    samples following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    kneighbors_graph. Default is ``None``, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at ``n_clusters``. This is\n    useful to decrease computation time if the number of clusters is not\n    small compared to the number of samples. This option is useful only\n    when specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of observation. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - 'ward' minimizes the variance of the clusters being merged.\n    - 'average' uses the average of the distances of each observation of\n      the two sets.\n    - 'complete' or 'maximum' linkage uses the maximum distances between\n      all observations of the two sets.\n    - 'single' uses the minimum of the distances between all observations\n      of the two sets.\n\n    .. versionadded:: 0.20\n        Added the 'single' option\n\ndistance_threshold : float, default=None\n    The linkage distance threshold above which, clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : ndarray of shape (n_samples)\n    cluster labels for each point\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nchildren_ : array-like of shape (n_samples-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nExamples\n--------\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AgglomerativeClustering().fit(X)\n>>> clustering\nAgglomerativeClustering()\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])",
      "code": "class AgglomerativeClustering(ClusterMixin, BaseEstimator):\n    \"\"\"\n    Agglomerative Clustering\n\n    Recursively merges the pair of clusters that minimally increases\n    a given linkage distance.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    affinity : str or callable, default='euclidean'\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\".\n        If linkage is \"ward\", only \"euclidean\" is accepted.\n        If \"precomputed\", a distance matrix (instead of a similarity matrix)\n        is needed as input for the fit method.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like or callable, default=None\n        Connectivity matrix. Defines for each sample the neighboring\n        samples following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        kneighbors_graph. Default is ``None``, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at ``n_clusters``. This is\n        useful to decrease computation time if the number of clusters is not\n        small compared to the number of samples. This option is useful only\n        when specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of observation. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - 'ward' minimizes the variance of the clusters being merged.\n        - 'average' uses the average of the distances of each observation of\n          the two sets.\n        - 'complete' or 'maximum' linkage uses the maximum distances between\n          all observations of the two sets.\n        - 'single' uses the minimum of the distances between all observations\n          of the two sets.\n\n        .. versionadded:: 0.20\n            Added the 'single' option\n\n    distance_threshold : float, default=None\n        The linkage distance threshold above which, clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : ndarray of shape (n_samples)\n        cluster labels for each point\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    children_ : array-like of shape (n_samples-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AgglomerativeClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AgglomerativeClustering().fit(X)\n    >>> clustering\n    AgglomerativeClustering()\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', distance_threshold=None,\n                 compute_distances=False):\n        self.n_clusters = n_clusters\n        self.distance_threshold = distance_threshold\n        self.memory = memory\n        self.connectivity = connectivity\n        self.compute_full_tree = compute_full_tree\n        self.linkage = linkage\n        self.affinity = affinity\n        self.compute_distances = compute_distances\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator=self)\n        memory = check_memory(self.memory)\n\n        if self.n_clusters is not None and self.n_clusters <= 0:\n            raise ValueError(\"n_clusters should be an integer greater than 0.\"\n                             \" %s was provided.\" % str(self.n_clusters))\n\n        if not ((self.n_clusters is None) ^ (self.distance_threshold is None)):\n            raise ValueError(\"Exactly one of n_clusters and \"\n                             \"distance_threshold has to be set, and the other \"\n                             \"needs to be None.\")\n\n        if (self.distance_threshold is not None\n                and not self.compute_full_tree):\n            raise ValueError(\"compute_full_tree must be True if \"\n                             \"distance_threshold is set.\")\n\n        if self.linkage == \"ward\" and self.affinity != \"euclidean\":\n            raise ValueError(\"%s was provided as affinity. Ward can only \"\n                             \"work with euclidean distances.\" %\n                             (self.affinity, ))\n\n        if self.linkage not in _TREE_BUILDERS:\n            raise ValueError(\"Unknown linkage type %s. \"\n                             \"Valid options are %s\" % (self.linkage,\n                                                       _TREE_BUILDERS.keys()))\n        tree_builder = _TREE_BUILDERS[self.linkage]\n\n        connectivity = self.connectivity\n        if self.connectivity is not None:\n            if callable(self.connectivity):\n                connectivity = self.connectivity(X)\n            connectivity = check_array(\n                connectivity, accept_sparse=['csr', 'coo', 'lil'])\n\n        n_samples = len(X)\n        compute_full_tree = self.compute_full_tree\n        if self.connectivity is None:\n            compute_full_tree = True\n        if compute_full_tree == 'auto':\n            if self.distance_threshold is not None:\n                compute_full_tree = True\n            else:\n                # Early stopping is likely to give a speed up only for\n                # a large number of clusters. The actual threshold\n                # implemented here is heuristic\n                compute_full_tree = self.n_clusters < max(100, .02 * n_samples)\n        n_clusters = self.n_clusters\n        if compute_full_tree:\n            n_clusters = None\n\n        # Construct the tree\n        kwargs = {}\n        if self.linkage != 'ward':\n            kwargs['linkage'] = self.linkage\n            kwargs['affinity'] = self.affinity\n\n        distance_threshold = self.distance_threshold\n\n        return_distance = (\n            (distance_threshold is not None) or self.compute_distances\n        )\n\n        out = memory.cache(tree_builder)(X, connectivity=connectivity,\n                                         n_clusters=n_clusters,\n                                         return_distance=return_distance,\n                                         **kwargs)\n        (self.children_,\n         self.n_connected_components_,\n         self.n_leaves_,\n         parents) = out[:4]\n\n        if return_distance:\n            self.distances_ = out[-1]\n\n        if self.distance_threshold is not None:  # distance_threshold is used\n            self.n_clusters_ = np.count_nonzero(\n                self.distances_ >= distance_threshold) + 1\n        else:  # n_clusters is used\n            self.n_clusters_ = self.n_clusters\n\n        # Cut the tree\n        if compute_full_tree:\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_,\n                                   self.n_leaves_)\n        else:\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\n            # copy to avoid holding a reference on the original array\n            labels = np.copy(labels[:n_samples])\n            # Reassign cluster numbers\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)",
      "instance_attributes": [
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "distance_threshold",
          "types": null
        },
        {
          "name": "memory",
          "types": null
        },
        {
          "name": "connectivity",
          "types": null
        },
        {
          "name": "compute_full_tree",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "linkage",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "affinity",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "compute_distances",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "children_",
          "types": null
        },
        {
          "name": "n_connected_components_",
          "types": null
        },
        {
          "name": "n_leaves_",
          "types": null
        },
        {
          "name": "distances_",
          "types": null
        },
        {
          "name": "n_clusters_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration",
      "name": "FeatureAgglomeration",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration",
      "decorators": [],
      "superclasses": [
        "AgglomerativeClustering",
        "AgglomerationTransform"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__",
        "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit",
        "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit_predict@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\naffinity : str or callable, default='euclidean'\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or 'precomputed'.\n    If linkage is \"ward\", only \"euclidean\" is accepted.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each feature the neighboring\n    features following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    kneighbors_graph. Default is None, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at n_clusters. This is useful\n    to decrease computation time if the number of clusters is not small\n    compared to the number of features. This option is useful only when\n    specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of features. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - ward minimizes the variance of the clusters being merged.\n    - average uses the average of the distances of each feature of\n      the two sets.\n    - complete or maximum linkage uses the maximum distances between\n      all features of the two sets.\n    - single uses the minimum of the distances between all features\n      of the two sets.\n\npooling_func : callable, default=np.mean\n    This combines the values of agglomerated features into a single\n    value, and should accept an array of shape [M, N] and the keyword\n    argument `axis=1`, and reduce it to an array of size [M].\n\ndistance_threshold : float, default=None\n    The linkage distance threshold above which, clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : array-like of (n_features,)\n    cluster labels for each feature.\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nchildren_ : array-like of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_features`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_features` is a non-leaf\n    node and has children `children_[i - n_features]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_features + i`\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets, cluster\n>>> digits = datasets.load_digits()\n>>> images = digits.images\n>>> X = np.reshape(images, (len(images), -1))\n>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n>>> agglo.fit(X)\nFeatureAgglomeration(n_clusters=32)\n>>> X_reduced = agglo.transform(X)\n>>> X_reduced.shape\n(1797, 32)",
      "code": "class FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform):\n    \"\"\"Agglomerate features.\n\n    Similar to AgglomerativeClustering, but recursively merges features\n    instead of samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    affinity : str or callable, default='euclidean'\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or 'precomputed'.\n        If linkage is \"ward\", only \"euclidean\" is accepted.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like or callable, default=None\n        Connectivity matrix. Defines for each feature the neighboring\n        features following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        kneighbors_graph. Default is None, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at n_clusters. This is useful\n        to decrease computation time if the number of clusters is not small\n        compared to the number of features. This option is useful only when\n        specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of features. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - ward minimizes the variance of the clusters being merged.\n        - average uses the average of the distances of each feature of\n          the two sets.\n        - complete or maximum linkage uses the maximum distances between\n          all features of the two sets.\n        - single uses the minimum of the distances between all features\n          of the two sets.\n\n    pooling_func : callable, default=np.mean\n        This combines the values of agglomerated features into a single\n        value, and should accept an array of shape [M, N] and the keyword\n        argument `axis=1`, and reduce it to an array of size [M].\n\n    distance_threshold : float, default=None\n        The linkage distance threshold above which, clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array-like of (n_features,)\n        cluster labels for each feature.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    children_ : array-like of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_features`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_features` is a non-leaf\n        node and has children `children_[i - n_features]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_features + i`\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', pooling_func=np.mean,\n                 distance_threshold=None, compute_distances=False):\n        super().__init__(\n            n_clusters=n_clusters, memory=memory, connectivity=connectivity,\n            compute_full_tree=compute_full_tree, linkage=linkage,\n            affinity=affinity, distance_threshold=distance_threshold,\n            compute_distances=compute_distances)\n        self.pooling_func = pooling_func\n\n    def fit(self, X, y=None, **params):\n        \"\"\"Fit the hierarchical clustering on the data\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                ensure_min_features=2, estimator=self)\n        # save n_features_in_ attribute here to reset it after, because it will\n        # be overridden in AgglomerativeClustering since we passed it X.T.\n        n_features_in_ = self.n_features_in_\n        AgglomerativeClustering.fit(self, X.T, **params)\n        self.n_features_in_ = n_features_in_\n        return self\n\n    @property\n    def fit_predict(self):\n        raise AttributeError",
      "instance_attributes": [
        {
          "name": "pooling_func",
          "types": {
            "kind": "NamedType",
            "name": "Callable"
          }
        },
        {
          "name": "n_features_in_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering",
      "name": "SpectralBiclustering",
      "qname": "sklearn.cluster._bicluster.SpectralBiclustering",
      "decorators": [],
      "superclasses": [
        "BaseSpectral"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_check_parameters",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_fit",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_fit_best_piecewise",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_project_and_cluster"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.",
      "docstring": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.\n\nParameters\n----------\nn_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n    The number of row and column clusters in the checkerboard\n    structure.\n\nmethod : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n    Method of normalizing and converting singular vectors into\n    biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n    The authors recommend using 'log'. If the data is sparse,\n    however, log normalization will not work, which is why the\n    default is 'bistochastic'.\n\n    .. warning::\n       if `method='log'`, the data must be sparse.\n\nn_components : int, default=6\n    Number of singular vectors to check.\n\nn_best : int, default=3\n    Number of best singular vectors to which to project the data\n    for clustering.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', uses\n    :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', uses\n    `scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random'} or ndarray of (n_clusters, n_features),             default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    Row partition labels.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    Column partition labels.\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralBiclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_\narray([0, 1], dtype=int32)\n>>> clustering\nSpectralBiclustering(n_clusters=2, random_state=0)\n\nReferences\n----------\n\n* Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n  data: coclustering genes and conditions\n  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.",
      "code": "class SpectralBiclustering(BaseSpectral):\n    \"\"\"Spectral biclustering (Kluger, 2003).\n\n    Partitions rows and columns under the assumption that the data has\n    an underlying checkerboard structure. For instance, if there are\n    two row partitions and three column partitions, each row will\n    belong to three biclusters, and each column will belong to two\n    biclusters. The outer product of the corresponding row and column\n    label vectors gives this checkerboard structure.\n\n    Read more in the :ref:`User Guide <spectral_biclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n        The number of row and column clusters in the checkerboard\n        structure.\n\n    method : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n        Method of normalizing and converting singular vectors into\n        biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n        The authors recommend using 'log'. If the data is sparse,\n        however, log normalization will not work, which is why the\n        default is 'bistochastic'.\n\n        .. warning::\n           if `method='log'`, the data must be sparse.\n\n    n_components : int, default=6\n        Number of singular vectors to check.\n\n    n_best : int, default=3\n        Number of best singular vectors to which to project the data\n        for clustering.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', uses\n        :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', uses\n        `scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random'} or ndarray of (n_clusters, n_features), \\\n            default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        Row partition labels.\n\n    column_labels_ : array-like of shape (n_cols,)\n        Column partition labels.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralBiclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([0, 1], dtype=int32)\n    >>> clustering\n    SpectralBiclustering(n_clusters=2, random_state=0)\n\n    References\n    ----------\n\n    * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n      data: coclustering genes and conditions\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, method='bistochastic',\n                 n_components=6, n_best=3, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)\n        self.method = method\n        self.n_components = n_components\n        self.n_best = n_best\n\n    def _check_parameters(self):\n        super()._check_parameters()\n        legal_methods = ('bistochastic', 'scale', 'log')\n        if self.method not in legal_methods:\n            raise ValueError(\"Unknown method: '{0}'. method must be\"\n                             \" one of {1}.\".format(self.method, legal_methods))\n        try:\n            int(self.n_clusters)\n        except TypeError:\n            try:\n                r, c = self.n_clusters\n                int(r)\n                int(c)\n            except (ValueError, TypeError) as e:\n                raise ValueError(\"Incorrect parameter n_clusters has value:\"\n                                 \" {}. It should either be a single integer\"\n                                 \" or an iterable with two integers:\"\n                                 \" (n_row_clusters, n_column_clusters)\") from e\n        if self.n_components < 1:\n            raise ValueError(\"Parameter n_components must be greater than 0,\"\n                             \" but its value is {}\".format(self.n_components))\n        if self.n_best < 1:\n            raise ValueError(\"Parameter n_best must be greater than 0,\"\n                             \" but its value is {}\".format(self.n_best))\n        if self.n_best > self.n_components:\n            raise ValueError(\"n_best cannot be larger than\"\n                             \" n_components, but {} >  {}\"\n                             \"\".format(self.n_best, self.n_components))\n\n    def _fit(self, X):\n        n_sv = self.n_components\n        if self.method == 'bistochastic':\n            normalized_data = _bistochastic_normalize(X)\n            n_sv += 1\n        elif self.method == 'scale':\n            normalized_data, _, _ = _scale_normalize(X)\n            n_sv += 1\n        elif self.method == 'log':\n            normalized_data = _log_normalize(X)\n        n_discard = 0 if self.method == 'log' else 1\n        u, v = self._svd(normalized_data, n_sv, n_discard)\n        ut = u.T\n        vt = v.T\n\n        try:\n            n_row_clusters, n_col_clusters = self.n_clusters\n        except TypeError:\n            n_row_clusters = n_col_clusters = self.n_clusters\n\n        best_ut = self._fit_best_piecewise(ut, self.n_best,\n                                           n_row_clusters)\n\n        best_vt = self._fit_best_piecewise(vt, self.n_best,\n                                           n_col_clusters)\n\n        self.row_labels_ = self._project_and_cluster(X, best_vt.T,\n                                                     n_row_clusters)\n\n        self.column_labels_ = self._project_and_cluster(X.T, best_ut.T,\n                                                        n_col_clusters)\n\n        self.rows_ = np.vstack([self.row_labels_ == label\n                                for label in range(n_row_clusters)\n                                for _ in range(n_col_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == label\n                                   for _ in range(n_row_clusters)\n                                   for label in range(n_col_clusters)])\n\n    def _fit_best_piecewise(self, vectors, n_best, n_clusters):\n        \"\"\"Find the ``n_best`` vectors that are best approximated by piecewise\n        constant vectors.\n\n        The piecewise vectors are found by k-means; the best is chosen\n        according to Euclidean distance.\n\n        \"\"\"\n        def make_piecewise(v):\n            centroid, labels = self._k_means(v.reshape(-1, 1), n_clusters)\n            return centroid[labels].ravel()\n        piecewise_vectors = np.apply_along_axis(make_piecewise,\n                                                axis=1, arr=vectors)\n        dists = np.apply_along_axis(norm, axis=1,\n                                    arr=(vectors - piecewise_vectors))\n        result = vectors[np.argsort(dists)[:n_best]]\n        return result\n\n    def _project_and_cluster(self, data, vectors, n_clusters):\n        \"\"\"Project ``data`` to ``vectors`` and cluster the result.\"\"\"\n        projected = safe_sparse_dot(data, vectors)\n        _, labels = self._k_means(projected, n_clusters)\n        return labels",
      "instance_attributes": [
        {
          "name": "method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_components",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_best",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "row_labels_",
          "types": null
        },
        {
          "name": "column_labels_",
          "types": null
        },
        {
          "name": "rows_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "columns_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering",
      "name": "SpectralCoclustering",
      "qname": "sklearn.cluster._bicluster.SpectralCoclustering",
      "decorators": [],
      "superclasses": [
        "BaseSpectral"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__",
        "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/_fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.",
      "docstring": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.\n\nParameters\n----------\nn_clusters : int, default=3\n    The number of biclusters to find.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', use\n    :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', use\n    :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random', or ndarray of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    The bicluster label of each row.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    The bicluster label of each column.\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralCoclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_ #doctest: +SKIP\narray([0, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_ #doctest: +SKIP\narray([0, 0], dtype=int32)\n>>> clustering\nSpectralCoclustering(n_clusters=2, random_state=0)\n\nReferences\n----------\n\n* Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n  bipartite spectral graph partitioning\n  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.",
      "code": "class SpectralCoclustering(BaseSpectral):\n    \"\"\"Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=3\n        The number of biclusters to find.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random', or ndarray of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like of shape (n_cols,)\n        The bicluster label of each column.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_ #doctest: +SKIP\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_ #doctest: +SKIP\n    array([0, 0], dtype=int32)\n    >>> clustering\n    SpectralCoclustering(n_clusters=2, random_state=0)\n\n    References\n    ----------\n\n    * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n      bipartite spectral graph partitioning\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)\n\n    def _fit(self, X):\n        normalized_data, row_diag, col_diag = _scale_normalize(X)\n        n_sv = 1 + int(np.ceil(np.log2(self.n_clusters)))\n        u, v = self._svd(normalized_data, n_sv, n_discard=1)\n        z = np.vstack((row_diag[:, np.newaxis] * u,\n                       col_diag[:, np.newaxis] * v))\n\n        _, labels = self._k_means(z, self.n_clusters)\n\n        n_rows = X.shape[0]\n        self.row_labels_ = labels[:n_rows]\n        self.column_labels_ = labels[n_rows:]\n\n        self.rows_ = np.vstack([self.row_labels_ == c\n                                for c in range(self.n_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == c\n                                   for c in range(self.n_clusters)])",
      "instance_attributes": [
        {
          "name": "row_labels_",
          "types": null
        },
        {
          "name": "column_labels_",
          "types": null
        },
        {
          "name": "rows_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "columns_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch",
      "name": "Birch",
      "qname": "sklearn.cluster._birch.Birch",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._birch/Birch/__init__",
        "scikit-learn/sklearn.cluster._birch/Birch/fit",
        "scikit-learn/sklearn.cluster._birch/Birch/_fit",
        "scikit-learn/sklearn.cluster._birch/Birch/_get_leaves",
        "scikit-learn/sklearn.cluster._birch/Birch/partial_fit",
        "scikit-learn/sklearn.cluster._birch/Birch/_check_fit",
        "scikit-learn/sklearn.cluster._birch/Birch/predict",
        "scikit-learn/sklearn.cluster._birch/Birch/transform",
        "scikit-learn/sklearn.cluster._birch/Birch/_global_clustering"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16",
      "docstring": "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nthreshold : float, default=0.5\n    The radius of the subcluster obtained by merging a new sample and the\n    closest subcluster should be lesser than the threshold. Otherwise a new\n    subcluster is started. Setting this value to be very low promotes\n    splitting and vice-versa.\n\nbranching_factor : int, default=50\n    Maximum number of CF subclusters in each node. If a new samples enters\n    such that the number of subclusters exceed the branching_factor then\n    that node is split into two nodes with the subclusters redistributed\n    in each. The parent subcluster of that node is removed and two new\n    subclusters are added as parents of the 2 split nodes.\n\nn_clusters : int, instance of sklearn.cluster model, default=3\n    Number of clusters after the final clustering step, which treats the\n    subclusters from the leaves as new samples.\n\n    - `None` : the final clustering step is not performed and the\n      subclusters are returned as they are.\n\n    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n      is fit treating the subclusters as new samples and the initial data\n      is mapped to the label of the closest subcluster.\n\n    - `int` : the model fit is :class:`AgglomerativeClustering` with\n      `n_clusters` set to be equal to the int.\n\ncompute_labels : bool, default=True\n    Whether or not to compute labels for each fit.\n\ncopy : bool, default=True\n    Whether or not to make a copy of the given data. If set to False,\n    the initial data will be overwritten.\n\nAttributes\n----------\nroot_ : _CFNode\n    Root of the CFTree.\n\ndummy_leaf_ : _CFNode\n    Start pointer to all the leaves.\n\nsubcluster_centers_ : ndarray\n    Centroids of all subclusters read directly from the leaves.\n\nsubcluster_labels_ : ndarray\n    Labels assigned to the centroids of the subclusters after\n    they are clustered globally.\n\nlabels_ : ndarray of shape (n_samples,)\n    Array of labels assigned to the input data.\n    if partial_fit is used instead of fit, they are assigned to the\n    last batch of data.\n\nSee Also\n--------\nMiniBatchKMeans : Alternative implementation that does incremental updates\n    of the centers' positions using mini-batches.\n\nNotes\n-----\nThe tree data structure consists of nodes with each node consisting of\na number of subclusters. The maximum number of subclusters in a node\nis determined by the branching factor. Each subcluster maintains a\nlinear sum, squared sum and the number of samples in that subcluster.\nIn addition, each subcluster can also have a node as its child, if the\nsubcluster is not a member of a leaf node.\n\nFor a new point entering the root, it is merged with the subcluster closest\nto it and the linear sum, squared sum and the number of samples of that\nsubcluster are updated. This is done recursively till the properties of\nthe leaf node are updated.\n\nReferences\n----------\n* Tian Zhang, Raghu Ramakrishnan, Maron Livny\n  BIRCH: An efficient data clustering method for large databases.\n  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n* Roberto Perdisci\n  JBirch - Java implementation of BIRCH clustering algorithm\n  https://code.google.com/archive/p/jbirch\n\nExamples\n--------\n>>> from sklearn.cluster import Birch\n>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n>>> brc = Birch(n_clusters=None)\n>>> brc.fit(X)\nBirch(n_clusters=None)\n>>> brc.predict(X)\narray([0, 0, 0, 1, 1, 1])",
      "code": "class Birch(ClusterMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Implements the BIRCH clustering algorithm.\n\n    It is a memory-efficient, online-learning algorithm provided as an\n    alternative to :class:`MiniBatchKMeans`. It constructs a tree\n    data structure with the cluster centroids being read off the leaf.\n    These can be either the final cluster centroids or can be provided as input\n    to another clustering algorithm such as :class:`AgglomerativeClustering`.\n\n    Read more in the :ref:`User Guide <birch>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    threshold : float, default=0.5\n        The radius of the subcluster obtained by merging a new sample and the\n        closest subcluster should be lesser than the threshold. Otherwise a new\n        subcluster is started. Setting this value to be very low promotes\n        splitting and vice-versa.\n\n    branching_factor : int, default=50\n        Maximum number of CF subclusters in each node. If a new samples enters\n        such that the number of subclusters exceed the branching_factor then\n        that node is split into two nodes with the subclusters redistributed\n        in each. The parent subcluster of that node is removed and two new\n        subclusters are added as parents of the 2 split nodes.\n\n    n_clusters : int, instance of sklearn.cluster model, default=3\n        Number of clusters after the final clustering step, which treats the\n        subclusters from the leaves as new samples.\n\n        - `None` : the final clustering step is not performed and the\n          subclusters are returned as they are.\n\n        - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n          is fit treating the subclusters as new samples and the initial data\n          is mapped to the label of the closest subcluster.\n\n        - `int` : the model fit is :class:`AgglomerativeClustering` with\n          `n_clusters` set to be equal to the int.\n\n    compute_labels : bool, default=True\n        Whether or not to compute labels for each fit.\n\n    copy : bool, default=True\n        Whether or not to make a copy of the given data. If set to False,\n        the initial data will be overwritten.\n\n    Attributes\n    ----------\n    root_ : _CFNode\n        Root of the CFTree.\n\n    dummy_leaf_ : _CFNode\n        Start pointer to all the leaves.\n\n    subcluster_centers_ : ndarray\n        Centroids of all subclusters read directly from the leaves.\n\n    subcluster_labels_ : ndarray\n        Labels assigned to the centroids of the subclusters after\n        they are clustered globally.\n\n    labels_ : ndarray of shape (n_samples,)\n        Array of labels assigned to the input data.\n        if partial_fit is used instead of fit, they are assigned to the\n        last batch of data.\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative implementation that does incremental updates\n        of the centers' positions using mini-batches.\n\n    Notes\n    -----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch\n\n    Examples\n    --------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(n_clusters=None)\n    >>> brc.fit(X)\n    Birch(n_clusters=None)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, threshold=0.5, branching_factor=50, n_clusters=3,\n                 compute_labels=True, copy=True):\n        self.threshold = threshold\n        self.branching_factor = branching_factor\n        self.n_clusters = n_clusters\n        self.compute_labels = compute_labels\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Build a CF Tree for the input data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.fit_, self.partial_fit_ = True, False\n        return self._fit(X)\n\n    def _fit(self, X):\n        has_root = getattr(self, 'root_', None)\n        first_call = self.fit_ or (self.partial_fit_ and not has_root)\n\n        X = self._validate_data(X, accept_sparse='csr', copy=self.copy,\n                                reset=first_call)\n        threshold = self.threshold\n        branching_factor = self.branching_factor\n\n        if branching_factor <= 1:\n            raise ValueError(\"Branching_factor should be greater than one.\")\n        n_samples, n_features = X.shape\n\n        # If partial_fit is called for the first time or fit is called, we\n        # start a new tree.\n        if first_call:\n            # The first root is the leaf. Manipulate this object throughout.\n            self.root_ = _CFNode(threshold=threshold,\n                                 branching_factor=branching_factor,\n                                 is_leaf=True,\n                                 n_features=n_features)\n\n            # To enable getting back subclusters.\n            self.dummy_leaf_ = _CFNode(threshold=threshold,\n                                       branching_factor=branching_factor,\n                                       is_leaf=True, n_features=n_features)\n            self.dummy_leaf_.next_leaf_ = self.root_\n            self.root_.prev_leaf_ = self.dummy_leaf_\n\n        # Cannot vectorize. Enough to convince to use cython.\n        if not sparse.issparse(X):\n            iter_func = iter\n        else:\n            iter_func = _iterate_sparse_X\n\n        for sample in iter_func(X):\n            subcluster = _CFSubcluster(linear_sum=sample)\n            split = self.root_.insert_cf_subcluster(subcluster)\n\n            if split:\n                new_subcluster1, new_subcluster2 = _split_node(\n                    self.root_, threshold, branching_factor)\n                del self.root_\n                self.root_ = _CFNode(threshold=threshold,\n                                     branching_factor=branching_factor,\n                                     is_leaf=False,\n                                     n_features=n_features)\n                self.root_.append_subcluster(new_subcluster1)\n                self.root_.append_subcluster(new_subcluster2)\n\n        centroids = np.concatenate([\n            leaf.centroids_ for leaf in self._get_leaves()])\n        self.subcluster_centers_ = centroids\n\n        self._global_clustering(X)\n        return self\n\n    def _get_leaves(self):\n        \"\"\"\n        Retrieve the leaves of the CF Node.\n\n        Returns\n        -------\n        leaves : list of shape (n_leaves,)\n            List of the leaf nodes.\n        \"\"\"\n        leaf_ptr = self.dummy_leaf_.next_leaf_\n        leaves = []\n        while leaf_ptr is not None:\n            leaves.append(leaf_ptr)\n            leaf_ptr = leaf_ptr.next_leaf_\n        return leaves\n\n    def partial_fit(self, X=None, y=None):\n        \"\"\"\n        Online learning. Prevents rebuilding of CFTree from scratch.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), \\\n            default=None\n            Input data. If X is not provided, only the global clustering\n            step is done.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.partial_fit_, self.fit_ = True, False\n        if X is None:\n            # Perform just the final global clustering step.\n            self._global_clustering()\n            return self\n        else:\n            return self._fit(X)\n\n    def _check_fit(self, X):\n        check_is_fitted(self)\n\n        if (hasattr(self, 'subcluster_centers_') and\n                X.shape[1] != self.subcluster_centers_.shape[1]):\n            raise ValueError(\n                \"Training data and predicted data do \"\n                \"not have same number of features.\")\n\n    def predict(self, X):\n        \"\"\"\n        Predict data using the ``centroids_`` of subclusters.\n\n        Avoid computation of the row norms of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        labels : ndarray of shape(n_samples,)\n            Labelled data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n        kwargs = {'Y_norm_squared': self._subcluster_norms}\n\n        with config_context(assume_finite=True):\n            argmin = pairwise_distances_argmin(X, self.subcluster_centers_,\n                                               metric_kwargs=kwargs)\n        return self.subcluster_labels_[argmin]\n\n    def transform(self, X):\n        \"\"\"\n        Transform X into subcluster centroids dimension.\n\n        Each dimension represents the distance from the sample point to each\n        cluster centroid.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        self._validate_data(X, accept_sparse='csr', reset=False)\n        with config_context(assume_finite=True):\n            return euclidean_distances(X, self.subcluster_centers_)\n\n    def _global_clustering(self, X=None):\n        \"\"\"\n        Global clustering for the subclusters obtained after fitting\n        \"\"\"\n        clusterer = self.n_clusters\n        centroids = self.subcluster_centers_\n        compute_labels = (X is not None) and self.compute_labels\n\n        # Preprocessing for the global clustering.\n        not_enough_centroids = False\n        if isinstance(clusterer, numbers.Integral):\n            clusterer = AgglomerativeClustering(\n                n_clusters=self.n_clusters)\n            # There is no need to perform the global clustering step.\n            if len(centroids) < self.n_clusters:\n                not_enough_centroids = True\n        elif (clusterer is not None and not\n              hasattr(clusterer, 'fit_predict')):\n            raise ValueError(\"n_clusters should be an instance of \"\n                             \"ClusterMixin or an int\")\n\n        # To use in predict to avoid recalculation.\n        self._subcluster_norms = row_norms(\n            self.subcluster_centers_, squared=True)\n\n        if clusterer is None or not_enough_centroids:\n            self.subcluster_labels_ = np.arange(len(centroids))\n            if not_enough_centroids:\n                warnings.warn(\n                    \"Number of subclusters found (%d) by BIRCH is less \"\n                    \"than (%d). Decrease the threshold.\"\n                    % (len(centroids), self.n_clusters), ConvergenceWarning)\n        else:\n            # The global clustering step that clusters the subclusters of\n            # the leaves. It assumes the centroids of the subclusters as\n            # samples and finds the final centroids.\n            self.subcluster_labels_ = clusterer.fit_predict(\n                self.subcluster_centers_)\n\n        if compute_labels:\n            self.labels_ = self.predict(X)",
      "instance_attributes": [
        {
          "name": "threshold",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "branching_factor",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "compute_labels",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "fit_",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "partial_fit_",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "root_",
          "types": {
            "kind": "NamedType",
            "name": "_CFNode"
          }
        },
        {
          "name": "dummy_leaf_",
          "types": {
            "kind": "NamedType",
            "name": "_CFNode"
          }
        },
        {
          "name": "subcluster_centers_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_subcluster_norms",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "subcluster_labels_",
          "types": null
        },
        {
          "name": "labels_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN",
      "name": "DBSCAN",
      "qname": "sklearn.cluster._dbscan.DBSCAN",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__",
        "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit",
        "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.",
      "docstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : string, or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square. X may be a :term:`Glossary <sparse graph>`, in which\n    case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n    .. versionadded:: 0.17\n       metric *precomputed* to accept precomputed sparse matrix.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=None\n    The power of the Minkowski metric to be used to calculate distance\n    between points. If None, then ``p=2`` (equivalent to the Euclidean\n    distance).\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\ncore_sample_indices_ : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\ncomponents_ : ndarray of shape (n_core_samples, n_features)\n    Copy of each core sample found by training.\n\nlabels_ : ndarray of shape (n_samples)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples are given the label -1.\n\nExamples\n--------\n>>> from sklearn.cluster import DBSCAN\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 2], [2, 3],\n...               [8, 7], [8, 8], [25, 80]])\n>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n>>> clustering.labels_\narray([ 0,  0,  0,  1,  1, -1])\n>>> clustering\nDBSCAN(eps=3, min_samples=2)\n\nSee Also\n--------\nOPTICS : A similar clustering at multiple values of eps. Our implementation\n    is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:class:`cluster.OPTICS` provides a similar clustering with lower memory\nusage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\".\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.",
      "code": "class DBSCAN(ClusterMixin, BaseEstimator):\n    \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n\n    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n    Finds core samples of high density and expands clusters from them.\n    Good for data which contains clusters of similar density.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : string, or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`Glossary <sparse graph>`, in which\n        case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n        .. versionadded:: 0.17\n           metric *precomputed* to accept precomputed sparse matrix.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=None\n        The power of the Minkowski metric to be used to calculate distance\n        between points. If None, then ``p=2`` (equivalent to the Euclidean\n        distance).\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    core_sample_indices_ : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    components_ : ndarray of shape (n_core_samples, n_features)\n        Copy of each core sample found by training.\n\n    labels_ : ndarray of shape (n_samples)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)\n\n    See Also\n    --------\n    OPTICS : A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, eps=0.5, *, min_samples=5, metric='euclidean',\n                 metric_params=None, algorithm='auto', leaf_size=30, p=None,\n                 n_jobs=None):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.metric = metric\n        self.metric_params = metric_params\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.p = p\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr')\n\n        if not self.eps > 0.0:\n            raise ValueError(\"eps must be positive.\")\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        # Calculate neighborhood for all samples. This leaves the original\n        # point in, which needs to be considered later (i.e. point i is in the\n        # neighborhood of point i. While True, its useless information)\n        if self.metric == 'precomputed' and sparse.issparse(X):\n            # set the diagonal to explicit values, as a point is its own\n            # neighbor\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n                X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n\n        neighbors_model = NearestNeighbors(\n            radius=self.eps, algorithm=self.algorithm,\n            leaf_size=self.leaf_size, metric=self.metric,\n            metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n        neighbors_model.fit(X)\n        # This has worst case O(n^2) memory complexity\n        neighborhoods = neighbors_model.radius_neighbors(X,\n                                                         return_distance=False)\n\n        if sample_weight is None:\n            n_neighbors = np.array([len(neighbors)\n                                    for neighbors in neighborhoods])\n        else:\n            n_neighbors = np.array([np.sum(sample_weight[neighbors])\n                                    for neighbors in neighborhoods])\n\n        # Initially, all samples are noise.\n        labels = np.full(X.shape[0], -1, dtype=np.intp)\n\n        # A list of all core samples found.\n        core_samples = np.asarray(n_neighbors >= self.min_samples,\n                                  dtype=np.uint8)\n        dbscan_inner(core_samples, neighborhoods, labels)\n\n        self.core_sample_indices_ = np.where(core_samples)[0]\n        self.labels_ = labels\n\n        if len(self.core_sample_indices_):\n            # fix for scipy sparse indexing issue\n            self.components_ = X[self.core_sample_indices_].copy()\n        else:\n            # no core samples\n            self.components_ = np.empty((0, X.shape[1]))\n        return self\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.\n        \"\"\"\n        self.fit(X, sample_weight=sample_weight)\n        return self.labels_",
      "instance_attributes": [
        {
          "name": "eps",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_samples",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "metric",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "metric_params",
          "types": null
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "leaf_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "p",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "core_sample_indices_",
          "types": null
        },
        {
          "name": "labels_",
          "types": null
        },
        {
          "name": "components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans",
      "name": "KMeans",
      "qname": "sklearn.cluster._kmeans.KMeans",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_check_params",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_validate_center_shape",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_check_test_data",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_check_mkl_vcomp",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_init_centroids",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/fit",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/transform",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_transform",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/predict",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/score",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.",
      "docstring": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nprecompute_distances : {'auto', True, False}, default='auto'\n    Precompute distances (faster but takes more memory).\n\n    'auto' : do not precompute distances if n_samples * n_clusters > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n\n    True : always precompute distances.\n\n    False : never precompute distances.\n\n    .. deprecated:: 0.23\n        'precompute_distances' was deprecated in version 0.22 and will be\n        removed in 1.0 (renaming of 0.25). It has no effect.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nn_jobs : int, default=None\n    The number of OpenMP threads to use for the computation. Parallelism is\n    sample-wise on the main cython loop which assigns each sample to its\n    closest center.\n\n    ``None`` or ``-1`` means using all processors.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nalgorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n    The \"elkan\" variation is more efficient on data with well-defined\n    clusters, by using the triangle inequality. However it's more memory\n    intensive due to the allocation of an extra array of shape\n    (n_samples, n_clusters).\n\n    For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n    might change in the future for a better heuristic.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center.\n\nn_iter_ : int\n    Number of iterations run.\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n'How slow is the k-means method?' SoCG2006)\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])",
      "code": "class KMeans(TransformerMixin, ClusterMixin, BaseEstimator):\n    \"\"\"K-Means clustering.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm for a\n        single run.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    precompute_distances : {'auto', True, False}, default='auto'\n        Precompute distances (faster but takes more memory).\n\n        'auto' : do not precompute distances if n_samples * n_clusters > 12\n        million. This corresponds to about 100MB overhead per job using\n        double precision.\n\n        True : always precompute distances.\n\n        False : never precompute distances.\n\n        .. deprecated:: 0.23\n            'precompute_distances' was deprecated in version 0.22 and will be\n            removed in 1.0 (renaming of 0.25). It has no effect.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    n_jobs : int, default=None\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n        ``None`` or ``-1`` means using all processors.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n        The \"elkan\" variation is more efficient on data with well-defined\n        clusters, by using the triangle inequality. However it's more memory\n        intensive due to the allocation of an extra array of shape\n        (n_samples, n_clusters).\n\n        For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n        might change in the future for a better heuristic.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative online implementation that does incremental\n        updates of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n    'How slow is the k-means method?' SoCG2006)\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', n_init=10,\n                 max_iter=300, tol=1e-4, precompute_distances='deprecated',\n                 verbose=0, random_state=None, copy_x=True,\n                 n_jobs='deprecated', algorithm='auto'):\n\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.precompute_distances = precompute_distances\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n        self.copy_x = copy_x\n        self.n_jobs = n_jobs\n        self.algorithm = algorithm\n\n    def _check_params(self, X):\n        # precompute_distances\n        if self.precompute_distances != 'deprecated':\n            warnings.warn(\"'precompute_distances' was deprecated in version \"\n                          \"0.23 and will be removed in 1.0 (renaming of 0.25)\"\n                          \". It has no effect\", FutureWarning)\n\n        # n_jobs\n        if self.n_jobs != 'deprecated':\n            warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n                          \" removed in 1.0 (renaming of 0.25).\", FutureWarning)\n            self._n_threads = self.n_jobs\n        else:\n            self._n_threads = None\n        self._n_threads = _openmp_effective_n_threads(self._n_threads)\n\n        # n_init\n        if self.n_init <= 0:\n            raise ValueError(\n                f\"n_init should be > 0, got {self.n_init} instead.\")\n        self._n_init = self.n_init\n\n        # max_iter\n        if self.max_iter <= 0:\n            raise ValueError(\n                f\"max_iter should be > 0, got {self.max_iter} instead.\")\n\n        # n_clusters\n        if X.shape[0] < self.n_clusters:\n            raise ValueError(f\"n_samples={X.shape[0]} should be >= \"\n                             f\"n_clusters={self.n_clusters}.\")\n\n        # tol\n        self._tol = _tolerance(X, self.tol)\n\n        # algorithm\n        if self.algorithm not in (\"auto\", \"full\", \"elkan\"):\n            raise ValueError(f\"Algorithm must be 'auto', 'full' or 'elkan', \"\n                             f\"got {self.algorithm} instead.\")\n\n        self._algorithm = self.algorithm\n        if self._algorithm == \"auto\":\n            self._algorithm = \"full\" if self.n_clusters == 1 else \"elkan\"\n        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n            warnings.warn(\"algorithm='elkan' doesn't make sense for a single \"\n                          \"cluster. Using 'full' instead.\", RuntimeWarning)\n            self._algorithm = \"full\"\n\n        # init\n        if not (hasattr(self.init, '__array__') or callable(self.init)\n                or (isinstance(self.init, str)\n                    and self.init in [\"k-means++\", \"random\"])):\n            raise ValueError(\n                f\"init should be either 'k-means++', 'random', a ndarray or a \"\n                f\"callable, got '{self.init}' instead.\")\n\n        if hasattr(self.init, '__array__') and self._n_init != 1:\n            warnings.warn(\n                f\"Explicit initial center position passed: performing only\"\n                f\" one init in {self.__class__.__name__} instead of \"\n                f\"n_init={self._n_init}.\", RuntimeWarning, stacklevel=2)\n            self._n_init = 1\n\n    def _validate_center_shape(self, X, centers):\n        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n        if centers.shape[0] != self.n_clusters:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of clusters {self.n_clusters}.\")\n        if centers.shape[1] != X.shape[1]:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of features of the data {X.shape[1]}.\")\n\n    def _check_test_data(self, X):\n        X = self._validate_data(X, accept_sparse='csr', reset=False,\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n        return X\n\n    def _check_mkl_vcomp(self, X, n_samples):\n        \"\"\"Warns when vcomp and mkl are both present\"\"\"\n        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n        # cause a small memory leak when there are less chunks than the number\n        # of available threads. It only happens when the OpenMP library is\n        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n        if sp.issparse(X):\n            return\n\n        active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n        if active_threads < self._n_threads:\n            modules = threadpool_info()\n            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n            has_mkl = (\"mkl\", \"intel\") in [\n                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n                for module in modules]\n            if has_vcomp and has_mkl:\n                if not hasattr(self, \"batch_size\"):  # KMeans\n                    warnings.warn(\n                        f\"KMeans is known to have a memory leak on Windows \"\n                        f\"with MKL, when there are less chunks than available \"\n                        f\"threads. You can avoid it by setting the environment\"\n                        f\" variable OMP_NUM_THREADS={active_threads}.\")\n                else:  # MiniBatchKMeans\n                    warnings.warn(\n                        f\"MiniBatchKMeans is known to have a memory leak on \"\n                        f\"Windows with MKL, when there are less chunks than \"\n                        f\"available threads. You can prevent it by setting \"\n                        f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n                        f\"setting the environment variable \"\n                        f\"OMP_NUM_THREADS={active_threads}\")\n\n    def _init_centroids(self, X, x_squared_norms, init, random_state,\n                        init_size=None):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n                (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters\n\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n\n        if isinstance(init, str) and init == 'k-means++':\n            centers, _ = _kmeans_plusplus(X, n_clusters,\n                                          random_state=random_state,\n                                          x_squared_norms=x_squared_norms)\n        elif isinstance(init, str) and init == 'random':\n            seeds = random_state.permutation(n_samples)[:n_clusters]\n            centers = X[seeds]\n        elif hasattr(init, '__array__'):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(\n                centers, dtype=X.dtype, copy=False, order='C')\n            self._validate_center_shape(X, centers)\n\n        if sp.issparse(centers):\n            centers = centers.toarray()\n\n        return centers\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', copy=self.copy_x,\n                                accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if hasattr(init, '__array__'):\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"full\":\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n        else:\n            kmeans_single = _kmeans_single_elkan\n\n        best_inertia = None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X, x_squared_norms=x_squared_norms, init=init,\n                random_state=random_state)\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X, sample_weight, centers_init, max_iter=self.max_iter,\n                verbose=self.verbose, tol=self._tol,\n                x_squared_norms=x_squared_norms, n_threads=self._n_threads)\n\n            # determine if these results are the best so far\n            if best_inertia is None or inertia < best_inertia:\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning, stacklevel=2)\n\n        self.cluster_centers_ = best_centers\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n\n    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        # Currently, this just skips a copy of the data if it is not in\n        # np.array or CSR format already.\n        # XXX This skips _check_test_data, which may change the dtype;\n        # we should refactor the input validation.\n        return self.fit(X, sample_weight=sample_weight)._transform(X)\n\n    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)\n\n    def _transform(self, X):\n        \"\"\"Guts of transform method; no input validation.\"\"\"\n        return euclidean_distances(X, self.cluster_centers_)\n\n    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return _labels_inertia(X, sample_weight, x_squared_norms,\n                               self.cluster_centers_, self._n_threads)[0]\n\n    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return -_labels_inertia(X, sample_weight, x_squared_norms,\n                                self.cluster_centers_)[1]\n\n    def _more_tags(self):\n        return {\n            '_xfail_checks': {\n                'check_sample_weights_invariance':\n                'zero sample_weight is not equivalent to removing samples',\n            },\n        }",
      "instance_attributes": [
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "init",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "precompute_distances",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_init",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "copy_x",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "n_jobs",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "_n_threads",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "_n_init",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "_tol",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "_algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "cluster_centers_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "labels_",
          "types": null
        },
        {
          "name": "inertia_",
          "types": null
        },
        {
          "name": "n_iter_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans",
      "name": "MiniBatchKMeans",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans",
      "decorators": [],
      "superclasses": [
        "KMeans"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/counts_@getter",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/init_size_@getter",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/random_state_@getter",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/_check_params",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/_labels_inertia_minibatch",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.",
      "docstring": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nmax_iter : int, default=100\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion heuristics.\n\nbatch_size : int, default=100\n    Size of the mini batches.\n\nverbose : int, default=0\n    Verbosity mode.\n\ncompute_labels : bool, default=True\n    Compute label assignment and inertia for the complete dataset\n    once the minibatch optimization has converged in fit.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization and\n    random reassignment. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.0\n    Control early stopping based on the relative center changes as\n    measured by a smoothed, variance-normalized of the mean center\n    squared position changes. This early stopping heuristics is\n    closer to the one used for the batch variant of the algorithms\n    but induces a slight computational and memory overhead over the\n    inertia heuristic.\n\n    To disable convergence detection based on normalized center\n    change, set tol to 0.0 (default).\n\nmax_no_improvement : int, default=10\n    Control early stopping based on the consecutive number of mini\n    batches that does not yield an improvement on the smoothed inertia.\n\n    To disable convergence detection based on inertia, set\n    max_no_improvement to None.\n\ninit_size : int, default=None\n    Number of samples to randomly sample for speeding up the\n    initialization (sometimes at the expense of accuracy): the\n    only algorithm is initialized by running a batch KMeans on a\n    random subset of the data. This needs to be larger than n_clusters.\n\n    If `None`, `init_size= 3 * batch_size`.\n\nn_init : int, default=3\n    Number of random initializations that are tried.\n    In contrast to KMeans, the algorithm is only run once, using the\n    best of the ``n_init`` initializations as measured by inertia.\n\nreassignment_ratio : float, default=0.01\n    Control the fraction of the maximum number of counts for a\n    center to be reassigned. A higher value means that low count\n    centers are more easily reassigned, which means that the\n    model will take longer to converge, but should converge in a\n    better clustering.\n\nAttributes\n----------\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : int\n    Labels of each point (if compute_labels is set to True).\n\ninertia_ : float\n    The value of the inertia criterion associated with the chosen\n    partition (if compute_labels is set to True). The inertia is\n    defined as the sum of square distances of samples to their nearest\n    neighbor.\n\nn_iter_ : int\n    Number of batches processed.\n\ncounts_ : ndarray of shape (n_clusters,)\n    Weigth sum of each cluster.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26).\n\ninit_size_ : int\n    The effective number of samples used for the initialization.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26).\n\nSee Also\n--------\nKMeans : The classic implementation of the clustering method based on the\n    Lloyd's algorithm. It consumes the whole set of input data at each\n    iteration.\n\nNotes\n-----\nSee https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\nExamples\n--------\n>>> from sklearn.cluster import MiniBatchKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 0], [4, 4],\n...               [4, 5], [0, 1], [2, 2],\n...               [3, 2], [5, 5], [1, -1]])\n>>> # manually fit on batches\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6)\n>>> kmeans = kmeans.partial_fit(X[0:6,:])\n>>> kmeans = kmeans.partial_fit(X[6:12,:])\n>>> kmeans.cluster_centers_\narray([[2. , 1. ],\n       [3.5, 4.5]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([0, 1], dtype=int32)\n>>> # fit on the whole data\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6,\n...                          max_iter=10).fit(X)\n>>> kmeans.cluster_centers_\narray([[3.95918367, 2.40816327],\n       [1.12195122, 1.3902439 ]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([1, 0], dtype=int32)",
      "code": "class MiniBatchKMeans(KMeans):\n    \"\"\"\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=100\n        Size of the mini batches.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, `init_size= 3 * batch_size`.\n\n    n_init : int, default=3\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the\n        best of the ``n_init`` initializations as measured by inertia.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a\n        center to be reassigned. A higher value means that low count\n        centers are more easily reassigned, which means that the\n        model will take longer to converge, but should converge in a\n        better clustering.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : int\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition (if compute_labels is set to True). The inertia is\n        defined as the sum of square distances of samples to their nearest\n        neighbor.\n\n    n_iter_ : int\n        Number of batches processed.\n\n    counts_ : ndarray of shape (n_clusters,)\n        Weigth sum of each cluster.\n\n        .. deprecated:: 0.24\n           This attribute is deprecated in 0.24 and will be removed in\n           1.1 (renaming of 0.26).\n\n    init_size_ : int\n        The effective number of samples used for the initialization.\n\n        .. deprecated:: 0.24\n           This attribute is deprecated in 0.24 and will be removed in\n           1.1 (renaming of 0.26).\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6)\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[2. , 1. ],\n           [3.5, 4.5]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([0, 1], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10).fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.95918367, 2.40816327],\n           [1.12195122, 1.3902439 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', max_iter=100,\n                 batch_size=100, verbose=0, compute_labels=True,\n                 random_state=None, tol=0.0, max_no_improvement=10,\n                 init_size=None, n_init=3, reassignment_ratio=0.01):\n\n        super().__init__(\n            n_clusters=n_clusters, init=init, max_iter=max_iter,\n            verbose=verbose, random_state=random_state, tol=tol, n_init=n_init)\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n\n    @deprecated(\"The attribute 'counts_' is deprecated in 0.24\"  # type: ignore\n                \" and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def counts_(self):\n        return self._counts\n\n    @deprecated(\"The attribute 'init_size_' is deprecated in \"  # type: ignore\n                \"0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def init_size_(self):\n        return self._init_size\n\n    @deprecated(\"The attribute 'random_state_' is deprecated \"  # type: ignore\n                \"in 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def random_state_(self):\n        return getattr(self, \"_random_state\", None)\n\n    def _check_params(self, X):\n        super()._check_params(X)\n\n        # max_no_improvement\n        if self.max_no_improvement is not None and self.max_no_improvement < 0:\n            raise ValueError(\n                f\"max_no_improvement should be >= 0, got \"\n                f\"{self.max_no_improvement} instead.\")\n\n        # batch_size\n        if self.batch_size <= 0:\n            raise ValueError(\n                f\"batch_size should be > 0, got {self.batch_size} instead.\")\n\n        # init_size\n        if self.init_size is not None and self.init_size <= 0:\n            raise ValueError(\n                f\"init_size should be > 0, got {self.init_size} instead.\")\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self.batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                f\"init_size={self._init_size} should be larger than \"\n                f\"n_clusters={self.n_clusters}. Setting it to \"\n                f\"min(3*n_clusters, n_samples)\",\n                RuntimeWarning, stacklevel=2)\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                f\"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\")\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        n_samples, n_features = X.shape\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self.tol > 0.0:\n            tol = _tolerance(X, self.tol)\n\n            # using tol-based early stopping needs the allocation of a\n            # dedicated before which can be expensive for high dim data:\n            # hence we allocate it outside of the main loop\n            old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n        else:\n            tol = 0.0\n            # no need for the center buffer if tol-based early stopping is\n            # disabled\n            old_center_buffer = np.zeros(0, dtype=X.dtype)\n\n        distances = np.zeros(self.batch_size, dtype=X.dtype)\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n        n_iter = int(self.max_iter * n_batches)\n\n        self._check_mkl_vcomp(X, self.batch_size)\n\n        validation_indices = random_state.randint(0, n_samples,\n                                                  self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        x_squared_norms_valid = x_squared_norms[validation_indices]\n\n        # perform several inits with random sub-sets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(\"Init %d/%d with method: %s\"\n                      % (init_idx + 1, self._n_init, init))\n            weight_sums = np.zeros(self.n_clusters, dtype=sample_weight.dtype)\n\n            # TODO: once the `k_means` function works with sparse input we\n            # should refactor the following init to use it instead.\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans\n            cluster_centers = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size)\n\n            # Compute the label assignment on the init dataset\n            _mini_batch_step(\n                X_valid, sample_weight_valid,\n                x_squared_norms[validation_indices], cluster_centers,\n                weight_sums, old_center_buffer, False, distances=None,\n                verbose=self.verbose)\n\n            # Keep only the best cluster centers across independent inits on\n            # the common validation set\n            _, inertia = _labels_inertia(X_valid, sample_weight_valid,\n                                         x_squared_norms_valid,\n                                         cluster_centers)\n            if self.verbose:\n                print(\"Inertia for init %d/%d: %f\"\n                      % (init_idx + 1, self._n_init, inertia))\n            if best_inertia is None or inertia < best_inertia:\n                self.cluster_centers_ = cluster_centers\n                self._counts = weight_sums\n                best_inertia = inertia\n\n        # Empty context to be used inplace by the convergence check routine\n        convergence_context = {}\n\n        # Perform the iterative optimization until the final convergence\n        # criterion\n        for iteration_idx in range(n_iter):\n            # Sample a minibatch from the full dataset\n            minibatch_indices = random_state.randint(\n                0, n_samples, self.batch_size)\n\n            # Perform the actual update step on the minibatch data\n            batch_inertia, centers_squared_diff = _mini_batch_step(\n                X[minibatch_indices], sample_weight[minibatch_indices],\n                x_squared_norms[minibatch_indices],\n                self.cluster_centers_, self._counts,\n                old_center_buffer, tol > 0.0, distances=distances,\n                # Here we randomly choose whether to perform\n                # random reassignment: the choice is done as a function\n                # of the iteration index, and the minimum number of\n                # counts, in order to force this reassignment to happen\n                # every once in a while\n                random_reassign=((iteration_idx + 1)\n                                 % (10 + int(self._counts.min())) == 0),\n                random_state=random_state,\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose)\n\n            # Monitor convergence and do early stopping if necessary\n            if _mini_batch_convergence(\n                    self, iteration_idx, n_iter, tol, n_samples,\n                    centers_squared_diff, batch_inertia, convergence_context,\n                    verbose=self.verbose):\n                break\n\n        self.n_iter_ = iteration_idx + 1\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = \\\n                    self._labels_inertia_minibatch(X, sample_weight)\n\n        return self\n\n    def _labels_inertia_minibatch(self, X, sample_weight):\n        \"\"\"Compute labels and inertia using mini batches.\n\n        This is slightly slower than doing everything at once but prevents\n        memory errors / segfaults.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        sample_weight : array-like of shape (n_samples,)\n            The weights for each observation in X.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels for each point.\n\n        inertia : float\n            Sum of squared distances of points to nearest cluster.\n        \"\"\"\n        if self.verbose:\n            print('Computing label assignment and total inertia')\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        x_squared_norms = row_norms(X, squared=True)\n        slices = gen_batches(X.shape[0], self.batch_size)\n        results = [_labels_inertia(X[s], sample_weight[s], x_squared_norms[s],\n                                   self.cluster_centers_) for s in slices]\n        labels, inertia = zip(*results)\n        return np.hstack(labels), np.sum(inertia)\n\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Coordinates of the data points to cluster. It must be noted that\n            X will be copied if it is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        self\n        \"\"\"\n        is_first_call_to_partial_fit = not hasattr(self, 'cluster_centers_')\n\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False,\n                                reset=is_first_call_to_partial_fit)\n\n        self._random_state = getattr(self, \"_random_state\",\n                                     check_random_state(self.random_state))\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        x_squared_norms = row_norms(X, squared=True)\n\n        if is_first_call_to_partial_fit:\n            # this is the first call to partial_fit on this object\n            self._check_params(X)\n\n            # Validate init array\n            init = self.init\n            if hasattr(init, '__array__'):\n                init = check_array(init, dtype=X.dtype, copy=True, order='C')\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size)\n\n            self._counts = np.zeros(self.n_clusters,\n                                    dtype=sample_weight.dtype)\n            random_reassign = False\n            distances = None\n        else:\n            # The lower the minimum count is, the more we do random\n            # reassignment, however, we don't want to do random\n            # reassignment too often, to allow for building up counts\n            random_reassign = self._random_state.randint(\n                10 * (1 + self._counts.min())) == 0\n            distances = np.zeros(X.shape[0], dtype=X.dtype)\n\n        _mini_batch_step(X, sample_weight, x_squared_norms,\n                         self.cluster_centers_, self._counts,\n                         np.zeros(0, dtype=X.dtype), 0,\n                         random_reassign=random_reassign, distances=distances,\n                         random_state=self._random_state,\n                         reassignment_ratio=self.reassignment_ratio,\n                         verbose=self.verbose)\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia(\n                X, sample_weight, x_squared_norms, self.cluster_centers_)\n\n        return self\n\n    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._labels_inertia_minibatch(X, sample_weight)[0]\n\n    def _more_tags(self):\n        return {\n            '_xfail_checks': {\n                'check_sample_weights_invariance':\n                'zero sample_weight is not equivalent to removing samples',\n            }\n        }",
      "instance_attributes": [
        {
          "name": "max_no_improvement",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "batch_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "compute_labels",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "init_size",
          "types": null
        },
        {
          "name": "reassignment_ratio",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "_init_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cluster_centers_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_counts",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_iter_",
          "types": null
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "inertia_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_random_state",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift",
      "name": "MeanShift",
      "qname": "sklearn.cluster._mean_shift.MeanShift",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__",
        "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit",
        "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.",
      "docstring": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\nbandwidth : float, default=None\n    Bandwidth used in the RBF kernel.\n\n    If not given, the bandwidth is estimated using\n    sklearn.cluster.estimate_bandwidth; see the documentation for that\n    function for hints on scalability (see also the Notes, below).\n\nseeds : array-like of shape (n_samples, n_features), default=None\n    Seeds used to initialize kernels. If not set,\n    the seeds are calculated by clustering.get_bin_seeds\n    with bandwidth as the grid size and default values for\n    other parameters.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    The default value is False.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by computing\n    each of the n_init runs in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\nn_iter_ : int\n    Maximum number of iterations performed on each seed.\n\n    .. versionadded:: 0.22\n\nExamples\n--------\n>>> from sklearn.cluster import MeanShift\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = MeanShift(bandwidth=2).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering.predict([[0, 0], [5, 5]])\narray([1, 0])\n>>> clustering\nMeanShift(bandwidth=2)\n\nNotes\n-----\n\nScalability:\n\nBecause this implementation uses a flat kernel and\na Ball Tree to look up members of each kernel, the complexity will tend\ntowards O(T*n*log(n)) in lower dimensions, with n the number of samples\nand T the number of points. In higher dimensions the complexity will\ntend towards O(T*n^2).\n\nScalability can be boosted by using fewer seeds, for example by using\na higher value of min_bin_freq in the get_bin_seeds function.\n\nNote that the estimate_bandwidth function is much less scalable than the\nmean shift algorithm and will be the bottleneck if it is used.\n\nReferences\n----------\n\nDorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\nfeature space analysis\". IEEE Transactions on Pattern Analysis and\nMachine Intelligence. 2002. pp. 603-619.",
      "code": "class MeanShift(ClusterMixin, BaseEstimator):\n    \"\"\"Mean shift clustering using a flat kernel.\n\n    Mean shift clustering aims to discover \"blobs\" in a smooth density of\n    samples. It is a centroid-based algorithm, which works by updating\n    candidates for centroids to be the mean of the points within a given\n    region. These candidates are then filtered in a post-processing stage to\n    eliminate near-duplicates to form the final set of centroids.\n\n    Seeding is performed using a binning technique for scalability.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n    bandwidth : float, default=None\n        Bandwidth used in the RBF kernel.\n\n        If not given, the bandwidth is estimated using\n        sklearn.cluster.estimate_bandwidth; see the documentation for that\n        function for hints on scalability (see also the Notes, below).\n\n    seeds : array-like of shape (n_samples, n_features), default=None\n        Seeds used to initialize kernels. If not set,\n        the seeds are calculated by clustering.get_bin_seeds\n        with bandwidth as the grid size and default values for\n        other parameters.\n\n    bin_seeding : bool, default=False\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        The default value is False.\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, default=1\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds.\n\n    cluster_all : bool, default=True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by computing\n        each of the n_init runs in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    n_iter_ : int\n        Maximum number of iterations performed on each seed.\n\n        .. versionadded:: 0.22\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MeanShift\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = MeanShift(bandwidth=2).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering.predict([[0, 0], [5, 5]])\n    array([1, 0])\n    >>> clustering\n    MeanShift(bandwidth=2)\n\n    Notes\n    -----\n\n    Scalability:\n\n    Because this implementation uses a flat kernel and\n    a Ball Tree to look up members of each kernel, the complexity will tend\n    towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n    and T the number of points. In higher dimensions the complexity will\n    tend towards O(T*n^2).\n\n    Scalability can be boosted by using fewer seeds, for example by using\n    a higher value of min_bin_freq in the get_bin_seeds function.\n\n    Note that the estimate_bandwidth function is much less scalable than the\n    mean shift algorithm and will be the bottleneck if it is used.\n\n    References\n    ----------\n\n    Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n    feature space analysis\". IEEE Transactions on Pattern Analysis and\n    Machine Intelligence. 2002. pp. 603-619.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, bandwidth=None, seeds=None, bin_seeding=False,\n                 min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300):\n        self.bandwidth = bandwidth\n        self.seeds = seeds\n        self.bin_seeding = bin_seeding\n        self.cluster_all = cluster_all\n        self.min_bin_freq = min_bin_freq\n        self.n_jobs = n_jobs\n        self.max_iter = max_iter\n\n    def fit(self, X, y=None):\n        \"\"\"Perform clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to cluster.\n\n        y : Ignored\n\n        \"\"\"\n        X = self._validate_data(X)\n        bandwidth = self.bandwidth\n        if bandwidth is None:\n            bandwidth = estimate_bandwidth(X, n_jobs=self.n_jobs)\n        elif bandwidth <= 0:\n            raise ValueError(\"bandwidth needs to be greater than zero or None,\"\n                             \" got %f\" % bandwidth)\n\n        seeds = self.seeds\n        if seeds is None:\n            if self.bin_seeding:\n                seeds = get_bin_seeds(X, bandwidth, self.min_bin_freq)\n            else:\n                seeds = X\n        n_samples, n_features = X.shape\n        center_intensity_dict = {}\n\n        # We use n_jobs=1 because this will be used in nested calls under\n        # parallel calls to _mean_shift_single_seed so there is no need for\n        # for further parallelism.\n        nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n\n        # execute iterations on all seeds in parallel\n        all_res = Parallel(n_jobs=self.n_jobs)(\n            delayed(_mean_shift_single_seed)\n            (seed, X, nbrs, self.max_iter) for seed in seeds)\n        # copy results in a dictionary\n        for i in range(len(seeds)):\n            if all_res[i][1]:  # i.e. len(points_within) > 0\n                center_intensity_dict[all_res[i][0]] = all_res[i][1]\n\n        self.n_iter_ = max([x[2] for x in all_res])\n\n        if not center_intensity_dict:\n            # nothing near seeds\n            raise ValueError(\"No point was within bandwidth=%f of any seed.\"\n                             \" Try a different seeding strategy \\\n                             or increase the bandwidth.\"\n                             % bandwidth)\n\n        # POST PROCESSING: remove near duplicate points\n        # If the distance between two kernels is less than the bandwidth,\n        # then we have to remove one because it is a duplicate. Remove the\n        # one with fewer points.\n\n        sorted_by_intensity = sorted(center_intensity_dict.items(),\n                                     key=lambda tup: (tup[1], tup[0]),\n                                     reverse=True)\n        sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\n        unique = np.ones(len(sorted_centers), dtype=bool)\n        nbrs = NearestNeighbors(radius=bandwidth,\n                                n_jobs=self.n_jobs).fit(sorted_centers)\n        for i, center in enumerate(sorted_centers):\n            if unique[i]:\n                neighbor_idxs = nbrs.radius_neighbors([center],\n                                                      return_distance=False)[0]\n                unique[neighbor_idxs] = 0\n                unique[i] = 1  # leave the current point as unique\n        cluster_centers = sorted_centers[unique]\n\n        # ASSIGN LABELS: a point belongs to the cluster that it is closest to\n        nbrs = NearestNeighbors(n_neighbors=1,\n                                n_jobs=self.n_jobs).fit(cluster_centers)\n        labels = np.zeros(n_samples, dtype=int)\n        distances, idxs = nbrs.kneighbors(X)\n        if self.cluster_all:\n            labels = idxs.flatten()\n        else:\n            labels.fill(-1)\n            bool_selector = distances.flatten() <= bandwidth\n            labels[bool_selector] = idxs.flatten()[bool_selector]\n\n        self.cluster_centers_, self.labels_ = cluster_centers, labels\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        with config_context(assume_finite=True):\n            return pairwise_distances_argmin(X, self.cluster_centers_)",
      "instance_attributes": [
        {
          "name": "bandwidth",
          "types": null
        },
        {
          "name": "seeds",
          "types": null
        },
        {
          "name": "bin_seeding",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "cluster_all",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "min_bin_freq",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_iter_",
          "types": null
        },
        {
          "name": "cluster_centers_",
          "types": null
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/OPTICS",
      "name": "OPTICS",
      "qname": "sklearn.cluster._optics.OPTICS",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._optics/OPTICS/__init__",
        "scikit-learn/sklearn.cluster._optics/OPTICS/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.",
      "docstring": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nmin_samples : int > 1 or float between 0 and 1, default=5\n    The number of samples in a neighborhood for a point to be considered as\n    a core point. Also, up and down steep regions can't have more than\n    ``min_samples`` consecutive non-steep points. Expressed as an absolute\n    number or a fraction of the number of samples (rounded to be at least\n    2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncluster_method : str, default='xi'\n    The extraction method used to extract clusters using the calculated\n    reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\neps : float, default=None\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. By default it assumes the same value\n    as ``max_eps``.\n    Used only when ``cluster_method='dbscan'``.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n    Used only when ``cluster_method='xi'``.\n\npredecessor_correction : bool, default=True\n    Correct clusters according to the predecessors calculated by OPTICS\n    [2]_. This parameter has minimal effect on most datasets.\n    Used only when ``cluster_method='xi'``.\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n    Used only when ``cluster_method='xi'``.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method. (default)\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nlabels_ : ndarray of shape (n_samples,)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples and points which are not included in a leaf cluster\n    of ``cluster_hierarchy_`` are labeled as -1.\n\nreachability_ : ndarray of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\nordering_ : ndarray of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : ndarray of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : ndarray of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\ncluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to\n    ``(end, -start)`` (ascending) so that larger clusters encompassing\n    smaller clusters come after those smaller ones. Since ``labels_`` does\n    not reflect the hierarchy, usually\n    ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n    note that these indices are of the ``ordering_``, i.e.\n    ``X[ordering_][start:end + 1]`` form a cluster.\n    Only available when ``cluster_method='xi'``.\n\nSee Also\n--------\nDBSCAN : A similar clustering for a specified neighborhood radius (eps).\n    Our implementation is optimized for runtime.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n.. [2] Schubert, Erich, Michael Gertz.\n   \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n   the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\nExamples\n--------\n>>> from sklearn.cluster import OPTICS\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> clustering = OPTICS(min_samples=2).fit(X)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])",
      "code": "class OPTICS(ClusterMixin, BaseEstimator):\n    \"\"\"Estimate clustering structure from vector array.\n\n    OPTICS (Ordering Points To Identify the Clustering Structure), closely\n    related to DBSCAN, finds core sample of high density and expands clusters\n    from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n    neighborhood radius. Better suited for usage on large datasets than the\n    current sklearn implementation of DBSCAN.\n\n    Clusters are then extracted using a DBSCAN-like method\n    (cluster_method = 'dbscan') or an automatic\n    technique proposed in [1]_ (cluster_method = 'xi').\n\n    This implementation deviates from the original OPTICS by first performing\n    k-nearest-neighborhood searches on all points to identify core sizes, then\n    computing only the distances to unprocessed points when constructing the\n    cluster order. Note that we do not employ a heap to manage the expansion\n    candidates, so the time complexity will be O(n^2).\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    min_samples : int > 1 or float between 0 and 1, default=5\n        The number of samples in a neighborhood for a point to be considered as\n        a core point. Also, up and down steep regions can't have more than\n        ``min_samples`` consecutive non-steep points. Expressed as an absolute\n        number or a fraction of the number of samples (rounded to be at least\n        2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n    p : int, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The extraction method used to extract clusters using the calculated\n        reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\n    eps : float, default=None\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. By default it assumes the same value\n        as ``max_eps``.\n        Used only when ``cluster_method='dbscan'``.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n        Used only when ``cluster_method='xi'``.\n\n    predecessor_correction : bool, default=True\n        Correct clusters according to the predecessors calculated by OPTICS\n        [2]_. This parameter has minimal effect on most datasets.\n        Used only when ``cluster_method='xi'``.\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n        Used only when ``cluster_method='xi'``.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method. (default)\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n        affect the speed of the construction and query, as well as the memory\n        required to store the tree. The optimal value depends on the\n        nature of the problem.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples and points which are not included in a leaf cluster\n        of ``cluster_hierarchy_`` are labeled as -1.\n\n    reachability_ : ndarray of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    ordering_ : ndarray of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : ndarray of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : ndarray of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    cluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to\n        ``(end, -start)`` (ascending) so that larger clusters encompassing\n        smaller clusters come after those smaller ones. Since ``labels_`` does\n        not reflect the hierarchy, usually\n        ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n        note that these indices are of the ``ordering_``, i.e.\n        ``X[ordering_][start:end + 1]`` form a cluster.\n        Only available when ``cluster_method='xi'``.\n\n    See Also\n    --------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, min_samples=5, max_eps=np.inf, metric='minkowski',\n                 p=2, metric_params=None, cluster_method='xi', eps=None,\n                 xi=0.05, predecessor_correction=True, min_cluster_size=None,\n                 algorithm='auto', leaf_size=30, n_jobs=None):\n        self.max_eps = max_eps\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.algorithm = algorithm\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.leaf_size = leaf_size\n        self.cluster_method = cluster_method\n        self.eps = eps\n        self.xi = xi\n        self.predecessor_correction = predecessor_correction\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features), or \\\n                (n_samples, n_samples) if metric=\u2019precomputed\u2019\n            A feature array, or array of distances between samples if\n            metric='precomputed'.\n\n        y : ignored\n            Ignored.\n\n        Returns\n        -------\n        self : instance of OPTICS\n            The instance.\n        \"\"\"\n        X = self._validate_data(X, dtype=float)\n\n        if self.cluster_method not in ['dbscan', 'xi']:\n            raise ValueError(\"cluster_method should be one of\"\n                             \" 'dbscan' or 'xi' but is %s\" %\n                             self.cluster_method)\n\n        (self.ordering_, self.core_distances_, self.reachability_,\n         self.predecessor_) = compute_optics_graph(\n             X=X, min_samples=self.min_samples, algorithm=self.algorithm,\n             leaf_size=self.leaf_size, metric=self.metric,\n             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs,\n             max_eps=self.max_eps)\n\n        # Extract clusters from the calculated orders and reachability\n        if self.cluster_method == 'xi':\n            labels_, clusters_ = cluster_optics_xi(\n                reachability=self.reachability_,\n                predecessor=self.predecessor_,\n                ordering=self.ordering_,\n                min_samples=self.min_samples,\n                min_cluster_size=self.min_cluster_size,\n                xi=self.xi,\n                predecessor_correction=self.predecessor_correction)\n            self.cluster_hierarchy_ = clusters_\n        elif self.cluster_method == 'dbscan':\n            if self.eps is None:\n                eps = self.max_eps\n            else:\n                eps = self.eps\n\n            if eps > self.max_eps:\n                raise ValueError('Specify an epsilon smaller than %s. Got %s.'\n                                 % (self.max_eps, eps))\n\n            labels_ = cluster_optics_dbscan(\n                reachability=self.reachability_,\n                core_distances=self.core_distances_,\n                ordering=self.ordering_, eps=eps)\n\n        self.labels_ = labels_\n        return self",
      "instance_attributes": [
        {
          "name": "max_eps",
          "types": null
        },
        {
          "name": "min_samples",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_cluster_size",
          "types": null
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "metric",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "metric_params",
          "types": null
        },
        {
          "name": "p",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "leaf_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cluster_method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "eps",
          "types": null
        },
        {
          "name": "xi",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "predecessor_correction",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "ordering_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "core_distances_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "reachability_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "predecessor_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "cluster_hierarchy_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering",
      "name": "SpectralClustering",
      "qname": "sklearn.cluster._spectral.SpectralClustering",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/_more_tags",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/_pairwise@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.",
      "docstring": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=8\n    The dimension of the projection subspace.\n\neigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nn_components : int, default=n_clusters\n    Number of eigenvectors to use for the spectral embedding\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization of the\n    lobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and by\n    the K-Means initialization. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of n_init\n    consecutive runs in terms of inertia. Only used if\n    ``assign_labels='kmeans'``.\n\ngamma : float, default=1.0\n    Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n    Ignored for ``affinity='nearest_neighbors'``.\n\naffinity : str or callable, default='rbf'\n    How to construct the affinity matrix.\n     - 'nearest_neighbors': construct the affinity matrix by computing a\n       graph of nearest neighbors.\n     - 'rbf': construct the affinity matrix using a radial basis function\n       (RBF) kernel.\n     - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n       where larger values indicate greater similarity between instances.\n     - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n       of precomputed distances, and construct a binary affinity matrix\n       from the ``n_neighbors`` nearest neighbors of each instance.\n     - one of the kernels supported by\n       :func:`~sklearn.metrics.pairwise_kernels`.\n\n    Only kernels that produce similarity scores (non-negative values that\n    increase with similarity) should be used. This property is not checked\n    by the clustering algorithm.\n\nn_neighbors : int, default=10\n    Number of neighbors to use when constructing the affinity matrix using\n    the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when ``eigen_solver='arpack'``.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans'\n    The strategy for assigning labels in the embedding space. There are two\n    ways to assign labels after the Laplacian embedding. k-means is a\n    popular choice, but it can be sensitive to initialization.\n    Discretization is another approach which is less sensitive to random\n    initialization.\n\ndegree : float, default=3\n    Degree of the polynomial kernel. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Zero coefficient for polynomial and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict of str to any, default=None\n    Parameters (keyword arguments) and values for kernel passed as\n    callable object. Ignored by other kernels.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run when `affinity='nearest_neighbors'`\n    or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n    will be done in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\naffinity_matrix_ : array-like of shape (n_samples, n_samples)\n    Affinity matrix used for clustering. Available only after calling\n    ``fit``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralClustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralClustering(n_clusters=2,\n...         assign_labels='discretize',\n...         random_state=0).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering\nSpectralClustering(assign_labels='discretize', n_clusters=2,\n    random_state=0)\n\nNotes\n-----\nA distance matrix for which 0 indicates identical elements and high values\nindicate very dissimilar elements can be transformed into an affinity /\nsimilarity matrix that is well-suited for the algorithm by\napplying the Gaussian (aka RBF, heat) kernel::\n\n    np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\nwhere ``delta`` is a free parameter representing the width of the Gaussian\nkernel.\n\nAn alternative is to take a symmetric version of the k-nearest neighbors\nconnectivity matrix of the points.\n\nIf the pyamg package is installed, it is used: this greatly\nspeeds up computation.\n\nReferences\n----------\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf",
      "code": "class SpectralClustering(ClusterMixin, BaseEstimator):\n    \"\"\"Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex, or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster, such as when clusters are\n    nested circles on the 2D plane.\n\n    If the affinity matrix is the adjacency matrix of a graph, this method\n    can be used to find normalized graph cuts.\n\n    When calling ``fit``, an affinity matrix is constructed using either\n    a kernel function such the Gaussian (aka RBF) kernel with Euclidean\n    distance ``d(X, X)``::\n\n            np.exp(-gamma * d(X,X) ** 2)\n\n    or a k-nearest neighbors connectivity matrix.\n\n    Alternatively, a user-provided affinity matrix can be specified by\n    setting ``affinity='precomputed'``.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=8\n        The dimension of the projection subspace.\n\n    eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities. If None, then ``'arpack'`` is\n        used.\n\n    n_components : int, default=n_clusters\n        Number of eigenvectors to use for the spectral embedding\n\n    random_state : int, RandomState instance, default=None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and by\n        the K-Means initialization. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of n_init\n        consecutive runs in terms of inertia. Only used if\n        ``assign_labels='kmeans'``.\n\n    gamma : float, default=1.0\n        Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n        Ignored for ``affinity='nearest_neighbors'``.\n\n    affinity : str or callable, default='rbf'\n        How to construct the affinity matrix.\n         - 'nearest_neighbors': construct the affinity matrix by computing a\n           graph of nearest neighbors.\n         - 'rbf': construct the affinity matrix using a radial basis function\n           (RBF) kernel.\n         - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n           where larger values indicate greater similarity between instances.\n         - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n           of precomputed distances, and construct a binary affinity matrix\n           from the ``n_neighbors`` nearest neighbors of each instance.\n         - one of the kernels supported by\n           :func:`~sklearn.metrics.pairwise_kernels`.\n\n        Only kernels that produce similarity scores (non-negative values that\n        increase with similarity) should be used. This property is not checked\n        by the clustering algorithm.\n\n    n_neighbors : int, default=10\n        Number of neighbors to use when constructing the affinity matrix using\n        the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\n    eigen_tol : float, default=0.0\n        Stopping criterion for eigendecomposition of the Laplacian matrix\n        when ``eigen_solver='arpack'``.\n\n    assign_labels : {'kmeans', 'discretize'}, default='kmeans'\n        The strategy for assigning labels in the embedding space. There are two\n        ways to assign labels after the Laplacian embedding. k-means is a\n        popular choice, but it can be sensitive to initialization.\n        Discretization is another approach which is less sensitive to random\n        initialization.\n\n    degree : float, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict of str to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run when `affinity='nearest_neighbors'`\n        or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n        will be done in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    affinity_matrix_ : array-like of shape (n_samples, n_samples)\n        Affinity matrix used for clustering. Available only after calling\n        ``fit``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralClustering(n_clusters=2,\n    ...         assign_labels='discretize',\n    ...         random_state=0).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering\n    SpectralClustering(assign_labels='discretize', n_clusters=2,\n        random_state=0)\n\n    Notes\n    -----\n    A distance matrix for which 0 indicates identical elements and high values\n    indicate very dissimilar elements can be transformed into an affinity /\n    similarity matrix that is well-suited for the algorithm by\n    applying the Gaussian (aka RBF, heat) kernel::\n\n        np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\n    where ``delta`` is a free parameter representing the width of the Gaussian\n    kernel.\n\n    An alternative is to take a symmetric version of the k-nearest neighbors\n    connectivity matrix of the points.\n\n    If the pyamg package is installed, it is used: this greatly\n    speeds up computation.\n\n    References\n    ----------\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - Multiclass spectral clustering, 2003\n      Stella X. Yu, Jianbo Shi\n      https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, eigen_solver=None, n_components=None,\n                 random_state=None, n_init=10, gamma=1., affinity='rbf',\n                 n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans',\n                 degree=3, coef0=1, kernel_params=None, n_jobs=None,\n                 verbose=False):\n        self.n_clusters = n_clusters\n        self.eigen_solver = eigen_solver\n        self.n_components = n_components\n        self.random_state = random_state\n        self.n_init = n_init\n        self.gamma = gamma\n        self.affinity = affinity\n        self.n_neighbors = n_neighbors\n        self.eigen_tol = eigen_tol\n        self.assign_labels = assign_labels\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n    def fit(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                dtype=np.float64, ensure_min_samples=2)\n        allow_squared = self.affinity in [\"precomputed\",\n                                          \"precomputed_nearest_neighbors\"]\n        if X.shape[0] == X.shape[1] and not allow_squared:\n            warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n                          \"now constructs an affinity matrix from data. To use\"\n                          \" a custom affinity matrix, \"\n                          \"set ``affinity=precomputed``.\")\n\n        if self.affinity == 'nearest_neighbors':\n            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors,\n                                            include_self=True,\n                                            n_jobs=self.n_jobs)\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed_nearest_neighbors':\n            estimator = NearestNeighbors(n_neighbors=self.n_neighbors,\n                                         n_jobs=self.n_jobs,\n                                         metric=\"precomputed\").fit(X)\n            connectivity = estimator.kneighbors_graph(X=X, mode='connectivity')\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed':\n            self.affinity_matrix_ = X\n        else:\n            params = self.kernel_params\n            if params is None:\n                params = {}\n            if not callable(self.affinity):\n                params['gamma'] = self.gamma\n                params['degree'] = self.degree\n                params['coef0'] = self.coef0\n            self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n                                                     filter_params=True,\n                                                     **params)\n\n        random_state = check_random_state(self.random_state)\n        self.labels_ = spectral_clustering(self.affinity_matrix_,\n                                           n_clusters=self.n_clusters,\n                                           n_components=self.n_components,\n                                           eigen_solver=self.eigen_solver,\n                                           random_state=random_state,\n                                           n_init=self.n_init,\n                                           eigen_tol=self.eigen_tol,\n                                           assign_labels=self.assign_labels,\n                                           verbose=self.verbose)\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)\n\n    def _more_tags(self):\n        return {'pairwise': self.affinity in [\"precomputed\",\n                                              \"precomputed_nearest_neighbors\"]}\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute _pairwise was deprecated in \"  # type: ignore\n                \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def _pairwise(self):\n        return self.affinity in [\"precomputed\",\n                                 \"precomputed_nearest_neighbors\"]",
      "instance_attributes": [
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "eigen_solver",
          "types": null
        },
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "n_init",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "gamma",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "affinity",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_neighbors",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "eigen_tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "assign_labels",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "degree",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "coef0",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "kernel_params",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "affinity_matrix_",
          "types": null
        },
        {
          "name": "labels_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer",
      "name": "ColumnTransformer",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "_BaseComposition"
      ],
      "methods": [
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_transformers@getter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_transformers@setter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_iter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_transformers",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_column_callables",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_remainder",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/named_transformers_@getter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_feature_names",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_update_fitted_transformers",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_output",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_log_message",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_fit_transform",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_hstack",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_sk_visual_block_"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20",
      "docstring": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ntransformers : list of tuples\n    List of (name, transformer, columns) tuples specifying the\n    transformer objects to be applied to subsets of the data.\n\n    name : str\n        Like in Pipeline and FeatureUnion, this allows the transformer and\n        its parameters to be set using ``set_params`` and searched in grid\n        search.\n    transformer : {'drop', 'passthrough'} or estimator\n        Estimator must support :term:`fit` and :term:`transform`.\n        Special-cased strings 'drop' and 'passthrough' are accepted as\n        well, to indicate to drop the columns or to pass them through\n        untransformed, respectively.\n    columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n        Indexes the data on its second axis. Integers are interpreted as\n        positional columns, while strings can reference DataFrame columns\n        by name.  A scalar string or int should be used where\n        ``transformer`` expects X to be a 1d array-like (vector),\n        otherwise a 2d array will be passed to the transformer.\n        A callable is passed the input data `X` and can return any of the\n        above. To select multiple columns by name or dtype, you can use\n        :obj:`make_column_selector`.\n\nremainder : {'drop', 'passthrough'} or estimator, default='drop'\n    By default, only the specified columns in `transformers` are\n    transformed and combined in the output, and the non-specified\n    columns are dropped. (default of ``'drop'``).\n    By specifying ``remainder='passthrough'``, all remaining columns that\n    were not specified in `transformers` will be automatically passed\n    through. This subset of columns is concatenated with the output of\n    the transformers.\n    By setting ``remainder`` to be an estimator, the remaining\n    non-specified columns will use the ``remainder`` estimator. The\n    estimator must support :term:`fit` and :term:`transform`.\n    Note that using this feature requires that the DataFrame columns\n    input at :term:`fit` and :term:`transform` have identical order.\n\nsparse_threshold : float, default=0.3\n    If the output of the different transformers contains sparse matrices,\n    these will be stacked as a sparse matrix if the overall density is\n    lower than this value. Use ``sparse_threshold=0`` to always return\n    dense.  When the transformed output consists of all dense data, the\n    stacked result will be dense, and this keyword will be ignored.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ntransformer_weights : dict, default=None\n    Multiplicative weights for features per transformer. The output of the\n    transformer is multiplied by these weights. Keys are transformer names,\n    values the weights.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nAttributes\n----------\ntransformers_ : list\n    The collection of fitted transformers as tuples of\n    (name, fitted_transformer, column). `fitted_transformer` can be an\n    estimator, 'drop', or 'passthrough'. In case there were no columns\n    selected, this will be the unfitted transformer.\n    If there are remaining columns, the final element is a tuple of the\n    form:\n    ('remainder', transformer, remaining_columns) corresponding to the\n    ``remainder`` parameter. If there are remaining columns, then\n    ``len(transformers_)==len(transformers)+1``, otherwise\n    ``len(transformers_)==len(transformers)``.\n\nnamed_transformers_ : :class:`~sklearn.utils.Bunch`\n    Read-only attribute to access any transformer by given name.\n    Keys are transformer names and values are the fitted transformer\n    objects.\n\nsparse_output_ : bool\n    Boolean flag indicating whether the output of ``transform`` is a\n    sparse matrix or a dense numpy array, which depends on the output\n    of the individual transformers and the `sparse_threshold` keyword.\n\nNotes\n-----\nThe order of the columns in the transformed feature matrix follows the\norder of how the columns are specified in the `transformers` list.\nColumns of the original feature matrix that are not specified are\ndropped from the resulting transformed feature matrix, unless specified\nin the `passthrough` keyword. Those columns specified with `passthrough`\nare added at the right to the output of the transformers.\n\nSee Also\n--------\nmake_column_transformer : Convenience function for\n    combining the outputs of multiple transformer objects applied to\n    column subsets of the original feature space.\nmake_column_selector : Convenience function for selecting\n    columns based on datatype or the columns name with a regex pattern.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.preprocessing import Normalizer\n>>> ct = ColumnTransformer(\n...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n>>> X = np.array([[0., 1., 2., 2.],\n...               [1., 1., 0., 1.]])\n>>> # Normalizer scales each row of X to unit norm. A separate scaling\n>>> # is applied for the two first and two last elements of each\n>>> # row independently.\n>>> ct.fit_transform(X)\narray([[0. , 1. , 0.5, 0.5],\n       [0.5, 0.5, 0. , 1. ]])",
      "code": "class ColumnTransformer(TransformerMixin, _BaseComposition):\n    \"\"\"Applies transformers to columns of an array or pandas DataFrame.\n\n    This estimator allows different columns or column subsets of the input\n    to be transformed separately and the features generated by each transformer\n    will be concatenated to form a single feature space.\n    This is useful for heterogeneous or columnar data, to combine several\n    feature extraction mechanisms or transformations into a single transformer.\n\n    Read more in the :ref:`User Guide <column_transformer>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    transformers : list of tuples\n        List of (name, transformer, columns) tuples specifying the\n        transformer objects to be applied to subsets of the data.\n\n        name : str\n            Like in Pipeline and FeatureUnion, this allows the transformer and\n            its parameters to be set using ``set_params`` and searched in grid\n            search.\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns :  str, array-like of str, int, array-like of int, \\\n                array-like of bool, slice or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name.  A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n        Note that using this feature requires that the DataFrame columns\n        input at :term:`fit` and :term:`transform` have identical order.\n\n    sparse_threshold : float, default=0.3\n        If the output of the different transformers contains sparse matrices,\n        these will be stacked as a sparse matrix if the overall density is\n        lower than this value. Use ``sparse_threshold=0`` to always return\n        dense.  When the transformed output consists of all dense data, the\n        stacked result will be dense, and this keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    transformer_weights : dict, default=None\n        Multiplicative weights for features per transformer. The output of the\n        transformer is multiplied by these weights. Keys are transformer names,\n        values the weights.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    Attributes\n    ----------\n    transformers_ : list\n        The collection of fitted transformers as tuples of\n        (name, fitted_transformer, column). `fitted_transformer` can be an\n        estimator, 'drop', or 'passthrough'. In case there were no columns\n        selected, this will be the unfitted transformer.\n        If there are remaining columns, the final element is a tuple of the\n        form:\n        ('remainder', transformer, remaining_columns) corresponding to the\n        ``remainder`` parameter. If there are remaining columns, then\n        ``len(transformers_)==len(transformers)+1``, otherwise\n        ``len(transformers_)==len(transformers)``.\n\n    named_transformers_ : :class:`~sklearn.utils.Bunch`\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n    sparse_output_ : bool\n        Boolean flag indicating whether the output of ``transform`` is a\n        sparse matrix or a dense numpy array, which depends on the output\n        of the individual transformers and the `sparse_threshold` keyword.\n\n    Notes\n    -----\n    The order of the columns in the transformed feature matrix follows the\n    order of how the columns are specified in the `transformers` list.\n    Columns of the original feature matrix that are not specified are\n    dropped from the resulting transformed feature matrix, unless specified\n    in the `passthrough` keyword. Those columns specified with `passthrough`\n    are added at the right to the output of the transformers.\n\n    See Also\n    --------\n    make_column_transformer : Convenience function for\n        combining the outputs of multiple transformer objects applied to\n        column subsets of the original feature space.\n    make_column_selector : Convenience function for selecting\n        columns based on datatype or the columns name with a regex pattern.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    \"\"\"\n    _required_parameters = ['transformers']\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 transformers, *,\n                 remainder='drop',\n                 sparse_threshold=0.3,\n                 n_jobs=None,\n                 transformer_weights=None,\n                 verbose=False):\n        self.transformers = transformers\n        self.remainder = remainder\n        self.sparse_threshold = sparse_threshold\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose\n\n    @property\n    def _transformers(self):\n        \"\"\"\n        Internal list of transformer only containing the name and\n        transformers, dropping the columns. This is for the implementation\n        of get_params via BaseComposition._get_params which expects lists\n        of tuples of len 2.\n        \"\"\"\n        return [(name, trans) for name, trans, _ in self.transformers]\n\n    @_transformers.setter\n    def _transformers(self, value):\n        self.transformers = [\n            (name, trans, col) for ((name, trans), (_, _, col))\n            in zip(value, self.transformers)]\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `transformers` of the\n        `ColumnTransformer`.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        return self._get_params('_transformers', deep=deep)\n\n    def set_params(self, **kwargs):\n        \"\"\"Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``. Note that you\n        can directly set the parameters of the estimators contained in\n        `transformers` of `ColumnTransformer`.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._set_params('_transformers', **kwargs)\n        return self\n\n    def _iter(self, fitted=False, replace_strings=False):\n        \"\"\"\n        Generate (name, trans, column, weight) tuples.\n\n        If fitted=True, use the fitted transformers, else use the\n        user specified transformers updated with converted column names\n        and potentially appended with transformer for remainder.\n\n        \"\"\"\n        if fitted:\n            transformers = self.transformers_\n        else:\n            # interleave the validated column specifiers\n            transformers = [\n                (name, trans, column) for (name, trans, _), column\n                in zip(self.transformers, self._columns)\n            ]\n            # add transformer tuple for remainder\n            if self._remainder[2] is not None:\n                transformers = chain(transformers, [self._remainder])\n        get_weight = (self.transformer_weights or {}).get\n\n        for name, trans, column in transformers:\n            if replace_strings:\n                # replace 'passthrough' with identity transformer and\n                # skip in case of 'drop'\n                if trans == 'passthrough':\n                    trans = FunctionTransformer(\n                        accept_sparse=True, check_inverse=False\n                    )\n                elif trans == 'drop':\n                    continue\n                elif _is_empty_column_selection(column):\n                    continue\n\n            yield (name, trans, column, get_weight(name))\n\n    def _validate_transformers(self):\n        if not self.transformers:\n            return\n\n        names, transformers, _ = zip(*self.transformers)\n\n        # validate names\n        self._validate_names(names)\n\n        # validate estimators\n        for t in transformers:\n            if t in ('drop', 'passthrough'):\n                continue\n            if (not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not\n                    hasattr(t, \"transform\")):\n                raise TypeError(\"All estimators should implement fit and \"\n                                \"transform, or can be 'drop' or 'passthrough' \"\n                                \"specifiers. '%s' (type %s) doesn't.\" %\n                                (t, type(t)))\n\n    def _validate_column_callables(self, X):\n        \"\"\"\n        Converts callable column specifications.\n        \"\"\"\n        columns = []\n        for _, _, column in self.transformers:\n            if callable(column):\n                column = column(X)\n            columns.append(column)\n        self._columns = columns\n\n    def _validate_remainder(self, X):\n        \"\"\"\n        Validates ``remainder`` and defines ``_remainder`` targeting\n        the remaining columns.\n        \"\"\"\n        is_transformer = ((hasattr(self.remainder, \"fit\")\n                           or hasattr(self.remainder, \"fit_transform\"))\n                          and hasattr(self.remainder, \"transform\"))\n        if (self.remainder not in ('drop', 'passthrough')\n                and not is_transformer):\n            raise ValueError(\n                \"The remainder keyword needs to be one of 'drop', \"\n                \"'passthrough', or estimator. '%s' was passed instead\" %\n                self.remainder)\n\n        # Make it possible to check for reordered named columns on transform\n        self._has_str_cols = any(_determine_key_type(cols) == 'str'\n                                 for cols in self._columns)\n        if hasattr(X, 'columns'):\n            self._df_columns = X.columns\n\n        self._n_features = X.shape[1]\n        cols = []\n        for columns in self._columns:\n            cols.extend(_get_column_indices(X, columns))\n\n        remaining_idx = sorted(set(range(self._n_features)) - set(cols))\n        self._remainder = ('remainder', self.remainder, remaining_idx or None)\n\n    @property\n    def named_transformers_(self):\n        \"\"\"Access the fitted transformer by name.\n\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n        \"\"\"\n        # Use Bunch object to improve autocomplete\n        return Bunch(**{name: trans for name, trans, _\n                        in self.transformers_})\n\n    def get_feature_names(self):\n        \"\"\"Get feature names from all transformers.\n\n        Returns\n        -------\n        feature_names : list of strings\n            Names of the features produced by transform.\n        \"\"\"\n        check_is_fitted(self)\n        feature_names = []\n        for name, trans, column, _ in self._iter(fitted=True):\n            if trans == 'drop' or _is_empty_column_selection(column):\n                continue\n            if trans == 'passthrough':\n                if hasattr(self, '_df_columns'):\n                    if ((not isinstance(column, slice))\n                            and all(isinstance(col, str) for col in column)):\n                        feature_names.extend(column)\n                    else:\n                        feature_names.extend(self._df_columns[column])\n                else:\n                    indices = np.arange(self._n_features)\n                    feature_names.extend(['x%d' % i for i in indices[column]])\n                continue\n            if not hasattr(trans, 'get_feature_names'):\n                raise AttributeError(\"Transformer %s (type %s) does not \"\n                                     \"provide get_feature_names.\"\n                                     % (str(name), type(trans).__name__))\n            feature_names.extend([name + \"__\" + f for f in\n                                  trans.get_feature_names()])\n        return feature_names\n\n    def _update_fitted_transformers(self, transformers):\n        # transformers are fitted; excludes 'drop' cases\n        fitted_transformers = iter(transformers)\n        transformers_ = []\n\n        for name, old, column, _ in self._iter():\n            if old == 'drop':\n                trans = 'drop'\n            elif old == 'passthrough':\n                # FunctionTransformer is present in list of transformers,\n                # so get next transformer, but save original string\n                next(fitted_transformers)\n                trans = 'passthrough'\n            elif _is_empty_column_selection(column):\n                trans = old\n            else:\n                trans = next(fitted_transformers)\n            transformers_.append((name, trans, column))\n\n        # sanity check that transformers is exhausted\n        assert not list(fitted_transformers)\n        self.transformers_ = transformers_\n\n    def _validate_output(self, result):\n        \"\"\"\n        Ensure that the output of each transformer is 2D. Otherwise\n        hstack can raise an error or produce incorrect results.\n        \"\"\"\n        names = [name for name, _, _, _ in self._iter(fitted=True,\n                                                      replace_strings=True)]\n        for Xs, name in zip(result, names):\n            if not getattr(Xs, 'ndim', 0) == 2:\n                raise ValueError(\n                    \"The output of the '{0}' transformer should be 2D (scipy \"\n                    \"matrix, array, or pandas DataFrame).\".format(name))\n\n    def _log_message(self, name, idx, total):\n        if not self.verbose:\n            return None\n        return '(%d of %d) Processing %s' % (idx, total, name)\n\n    def _fit_transform(self, X, y, func, fitted=False):\n        \"\"\"\n        Private function to fit and/or transform on demand.\n\n        Return value (transformers and/or transformed X data) depends\n        on the passed function.\n        ``fitted=True`` ensures the fitted transformers are used.\n        \"\"\"\n        transformers = list(\n            self._iter(fitted=fitted, replace_strings=True))\n        try:\n            return Parallel(n_jobs=self.n_jobs)(\n                delayed(func)(\n                    transformer=clone(trans) if not fitted else trans,\n                    X=_safe_indexing(X, column, axis=1),\n                    y=y,\n                    weight=weight,\n                    message_clsname='ColumnTransformer',\n                    message=self._log_message(name, idx, len(transformers)))\n                for idx, (name, trans, column, weight) in enumerate(\n                        self._iter(fitted=fitted, replace_strings=True), 1))\n        except ValueError as e:\n            if \"Expected 2D array, got 1D array instead\" in str(e):\n                raise ValueError(_ERR_MSG_1DCOLUMN) from e\n            else:\n                raise\n\n    def fit(self, X, y=None):\n        \"\"\"Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,...), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        self : ColumnTransformer\n            This estimator\n\n        \"\"\"\n        # we use fit_transform to make sure to set sparse_output_ (for which we\n        # need the transformed data) to have consistent output type in predict\n        self.fit_transform(X, y=y)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        # TODO: this should be `feature_names_in_` when we start having it\n        if hasattr(X, \"columns\"):\n            self._feature_names_in = np.asarray(X.columns)\n        else:\n            self._feature_names_in = None\n        X = _check_X(X)\n        # set n_features_in_ attribute\n        self._check_n_features(X, reset=True)\n        self._validate_transformers()\n        self._validate_column_callables(X)\n        self._validate_remainder(X)\n\n        result = self._fit_transform(X, y, _fit_transform_one)\n\n        if not result:\n            self._update_fitted_transformers([])\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*result)\n\n        # determine if concatenated output will be sparse or not\n        if any(sparse.issparse(X) for X in Xs):\n            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)\n            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)\n                        else X.size for X in Xs)\n            density = nnz / total\n            self.sparse_output_ = density < self.sparse_threshold\n        else:\n            self.sparse_output_ = False\n\n        self._update_fitted_transformers(transformers)\n        self._validate_output(Xs)\n\n        return self._hstack(list(Xs))\n\n    def transform(self, X):\n        \"\"\"Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            The data to be transformed by subset.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        check_is_fitted(self)\n        X = _check_X(X)\n        if hasattr(X, \"columns\"):\n            X_feature_names = np.asarray(X.columns)\n        else:\n            X_feature_names = None\n\n        self._check_n_features(X, reset=False)\n        if (self._feature_names_in is not None and\n            X_feature_names is not None and\n                np.any(self._feature_names_in != X_feature_names)):\n            raise RuntimeError(\n                \"Given feature/column names do not match the ones for the \"\n                \"data given during fit.\"\n            )\n        Xs = self._fit_transform(X, None, _transform_one, fitted=True)\n        self._validate_output(Xs)\n\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        return self._hstack(list(Xs))\n\n    def _hstack(self, Xs):\n        \"\"\"Stacks Xs horizontally.\n\n        This allows subclasses to control the stacking behavior, while reusing\n        everything else from ColumnTransformer.\n\n        Parameters\n        ----------\n        Xs : list of {array-like, sparse matrix, dataframe}\n        \"\"\"\n        if self.sparse_output_:\n            try:\n                # since all columns should be numeric before stacking them\n                # in a sparse matrix, `check_array` is used for the\n                # dtype conversion if necessary.\n                converted_Xs = [check_array(X,\n                                            accept_sparse=True,\n                                            force_all_finite=False)\n                                for X in Xs]\n            except ValueError as e:\n                raise ValueError(\n                    \"For a sparse output, all columns should \"\n                    \"be a numeric or convertible to a numeric.\"\n                ) from e\n\n            return sparse.hstack(converted_Xs).tocsr()\n        else:\n            Xs = [f.toarray() if sparse.issparse(f) else f for f in Xs]\n            return np.hstack(Xs)\n\n    def _sk_visual_block_(self):\n        if isinstance(self.remainder, str) and self.remainder == 'drop':\n            transformers = self.transformers\n        elif hasattr(self, \"_remainder\"):\n            remainder_columns = self._remainder[2]\n            if hasattr(self, '_df_columns'):\n                remainder_columns = (\n                    self._df_columns[remainder_columns].tolist()\n                )\n            transformers = chain(self.transformers,\n                                 [('remainder', self.remainder,\n                                   remainder_columns)])\n        else:\n            transformers = chain(self.transformers,\n                                 [('remainder', self.remainder, '')])\n\n        names, transformers, name_details = zip(*transformers)\n        return _VisualBlock('parallel', transformers,\n                            names=names, name_details=name_details)",
      "instance_attributes": [
        {
          "name": "transformers",
          "types": null
        },
        {
          "name": "remainder",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "sparse_threshold",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "transformer_weights",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "_columns",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "_has_str_cols",
          "types": null
        },
        {
          "name": "_df_columns",
          "types": null
        },
        {
          "name": "_n_features",
          "types": null
        },
        {
          "name": "_remainder",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "transformers_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "_feature_names_in",
          "types": null
        },
        {
          "name": "sparse_output_",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector",
      "name": "make_column_selector",
      "qname": "sklearn.compose._column_transformer.make_column_selector",
      "decorators": [],
      "superclasses": [],
      "methods": [
        "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__",
        "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.",
      "docstring": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.\n\nParameters\n----------\npattern : str, default=None\n    Name of columns containing this regex pattern will be included. If\n    None, column selection will not be selected based on pattern.\n\ndtype_include : column dtype or list of column dtypes, default=None\n    A selection of dtypes to include. For more details, see\n    :meth:`pandas.DataFrame.select_dtypes`.\n\ndtype_exclude : column dtype or list of column dtypes, default=None\n    A selection of dtypes to exclude. For more details, see\n    :meth:`pandas.DataFrame.select_dtypes`.\n\nReturns\n-------\nselector : callable\n    Callable for column selection to be used by a\n    :class:`ColumnTransformer`.\n\nSee Also\n--------\nColumnTransformer : Class that allows combining the\n    outputs of multiple transformer objects used on column subsets\n    of the data into a single feature space.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> from sklearn.compose import make_column_selector\n>>> import pandas as pd  # doctest: +SKIP\n>>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n>>> ct = make_column_transformer(\n...       (StandardScaler(),\n...        make_column_selector(dtype_include=np.number)),  # rating\n...       (OneHotEncoder(),\n...        make_column_selector(dtype_include=object)))  # city\n>>> ct.fit_transform(X)  # doctest: +SKIP\narray([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n       [-1.50755672,  1.        ,  0.        ,  0.        ],\n       [-0.30151134,  0.        ,  1.        ,  0.        ],\n       [ 0.90453403,  0.        ,  0.        ,  1.        ]])",
      "code": "class make_column_selector:\n    \"\"\"Create a callable to select columns to be used with\n    :class:`ColumnTransformer`.\n\n    :func:`make_column_selector` can select columns based on datatype or the\n    columns name with a regex. When using multiple selection criteria, **all**\n    criteria must match for a column to be selected.\n\n    Parameters\n    ----------\n    pattern : str, default=None\n        Name of columns containing this regex pattern will be included. If\n        None, column selection will not be selected based on pattern.\n\n    dtype_include : column dtype or list of column dtypes, default=None\n        A selection of dtypes to include. For more details, see\n        :meth:`pandas.DataFrame.select_dtypes`.\n\n    dtype_exclude : column dtype or list of column dtypes, default=None\n        A selection of dtypes to exclude. For more details, see\n        :meth:`pandas.DataFrame.select_dtypes`.\n\n    Returns\n    -------\n    selector : callable\n        Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> from sklearn.compose import make_column_selector\n    >>> import pandas as pd  # doctest: +SKIP\n    >>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n    ...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n    >>> ct = make_column_transformer(\n    ...       (StandardScaler(),\n    ...        make_column_selector(dtype_include=np.number)),  # rating\n    ...       (OneHotEncoder(),\n    ...        make_column_selector(dtype_include=object)))  # city\n    >>> ct.fit_transform(X)  # doctest: +SKIP\n    array([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n           [-1.50755672,  1.        ,  0.        ,  0.        ],\n           [-0.30151134,  0.        ,  1.        ,  0.        ],\n           [ 0.90453403,  0.        ,  0.        ,  1.        ]])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, pattern=None, *, dtype_include=None,\n                 dtype_exclude=None):\n        self.pattern = pattern\n        self.dtype_include = dtype_include\n        self.dtype_exclude = dtype_exclude\n\n    def __call__(self, df):\n        \"\"\"Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n        Parameters\n        ----------\n        df : dataframe of shape (n_features, n_samples)\n            DataFrame to select columns from.\n        \"\"\"\n        if not hasattr(df, 'iloc'):\n            raise ValueError(\"make_column_selector can only be applied to \"\n                             \"pandas dataframes\")\n        df_row = df.iloc[:1]\n        if self.dtype_include is not None or self.dtype_exclude is not None:\n            df_row = df_row.select_dtypes(include=self.dtype_include,\n                                          exclude=self.dtype_exclude)\n        cols = df_row.columns\n        if self.pattern is not None:\n            cols = cols[cols.str.contains(self.pattern, regex=True)]\n        return cols.tolist()",
      "instance_attributes": [
        {
          "name": "pattern",
          "types": null
        },
        {
          "name": "dtype_include",
          "types": null
        },
        {
          "name": "dtype_exclude",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor",
      "name": "TransformedTargetRegressor",
      "qname": "sklearn.compose._target.TransformedTargetRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/_fit_transformer",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/_more_tags",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/n_features_in_@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20",
      "docstring": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nregressor : object, default=None\n    Regressor object such as derived from ``RegressorMixin``. This\n    regressor will automatically be cloned each time prior to fitting.\n    If regressor is ``None``, ``LinearRegression()`` is created and used.\n\ntransformer : object, default=None\n    Estimator object such as derived from ``TransformerMixin``. Cannot be\n    set at the same time as ``func`` and ``inverse_func``. If\n    ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\n    the transformer will be an identity transformer. Note that the\n    transformer will be cloned during fitting. Also, the transformer is\n    restricting ``y`` to be a numpy array.\n\nfunc : function, default=None\n    Function to apply to ``y`` before passing to ``fit``. Cannot be set at\n    the same time as ``transformer``. The function needs to return a\n    2-dimensional array. If ``func`` is ``None``, the function used will be\n    the identity function.\n\ninverse_func : function, default=None\n    Function to apply to the prediction of the regressor. Cannot be set at\n    the same time as ``transformer`` as well. The function needs to return\n    a 2-dimensional array. The inverse function is used to return\n    predictions to the same space of the original training labels.\n\ncheck_inverse : bool, default=True\n    Whether to check that ``transform`` followed by ``inverse_transform``\n    or ``func`` followed by ``inverse_func`` leads to the original targets.\n\nAttributes\n----------\nregressor_ : object\n    Fitted regressor.\n\ntransformer_ : object\n    Transformer used in ``fit`` and ``predict``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.compose import TransformedTargetRegressor\n>>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n...                                 func=np.log, inverse_func=np.exp)\n>>> X = np.arange(4).reshape(-1, 1)\n>>> y = np.exp(2 * X).ravel()\n>>> tt.fit(X, y)\nTransformedTargetRegressor(...)\n>>> tt.score(X, y)\n1.0\n>>> tt.regressor_.coef_\narray([2.])\n\nNotes\n-----\nInternally, the target ``y`` is always converted into a 2-dimensional array\nto be used by scikit-learn transformers. At the time of prediction, the\noutput will be reshaped to a have the same number of dimensions as ``y``.\n\nSee :ref:`examples/compose/plot_transformed_target.py\n<sphx_glr_auto_examples_compose_plot_transformed_target.py>`.",
      "code": "class TransformedTargetRegressor(RegressorMixin, BaseEstimator):\n    \"\"\"Meta-estimator to regress on a transformed target.\n\n    Useful for applying a non-linear transformation to the target ``y`` in\n    regression problems. This transformation can be given as a Transformer\n    such as the QuantileTransformer or as a function and its inverse such as\n    ``log`` and ``exp``.\n\n    The computation during ``fit`` is::\n\n        regressor.fit(X, func(y))\n\n    or::\n\n        regressor.fit(X, transformer.transform(y))\n\n    The computation during ``predict`` is::\n\n        inverse_func(regressor.predict(X))\n\n    or::\n\n        transformer.inverse_transform(regressor.predict(X))\n\n    Read more in the :ref:`User Guide <transformed_target_regressor>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    regressor : object, default=None\n        Regressor object such as derived from ``RegressorMixin``. This\n        regressor will automatically be cloned each time prior to fitting.\n        If regressor is ``None``, ``LinearRegression()`` is created and used.\n\n    transformer : object, default=None\n        Estimator object such as derived from ``TransformerMixin``. Cannot be\n        set at the same time as ``func`` and ``inverse_func``. If\n        ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\n        the transformer will be an identity transformer. Note that the\n        transformer will be cloned during fitting. Also, the transformer is\n        restricting ``y`` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to ``y`` before passing to ``fit``. Cannot be set at\n        the same time as ``transformer``. The function needs to return a\n        2-dimensional array. If ``func`` is ``None``, the function used will be\n        the identity function.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as ``transformer`` as well. The function needs to return\n        a 2-dimensional array. The inverse function is used to return\n        predictions to the same space of the original training labels.\n\n    check_inverse : bool, default=True\n        Whether to check that ``transform`` followed by ``inverse_transform``\n        or ``func`` followed by ``inverse_func`` leads to the original targets.\n\n    Attributes\n    ----------\n    regressor_ : object\n        Fitted regressor.\n\n    transformer_ : object\n        Transformer used in ``fit`` and ``predict``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.compose import TransformedTargetRegressor\n    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n    ...                                 func=np.log, inverse_func=np.exp)\n    >>> X = np.arange(4).reshape(-1, 1)\n    >>> y = np.exp(2 * X).ravel()\n    >>> tt.fit(X, y)\n    TransformedTargetRegressor(...)\n    >>> tt.score(X, y)\n    1.0\n    >>> tt.regressor_.coef_\n    array([2.])\n\n    Notes\n    -----\n    Internally, the target ``y`` is always converted into a 2-dimensional array\n    to be used by scikit-learn transformers. At the time of prediction, the\n    output will be reshaped to a have the same number of dimensions as ``y``.\n\n    See :ref:`examples/compose/plot_transformed_target.py\n    <sphx_glr_auto_examples_compose_plot_transformed_target.py>`.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, regressor=None, *, transformer=None,\n                 func=None, inverse_func=None, check_inverse=True):\n        self.regressor = regressor\n        self.transformer = transformer\n        self.func = func\n        self.inverse_func = inverse_func\n        self.check_inverse = check_inverse\n\n    def _fit_transformer(self, y):\n        \"\"\"Check transformer and fit transformer.\n\n        Create the default transformer, fit it and make additional inverse\n        check on a subset (optional).\n\n        \"\"\"\n        if (self.transformer is not None and\n                (self.func is not None or self.inverse_func is not None)):\n            raise ValueError(\"'transformer' and functions 'func'/\"\n                             \"'inverse_func' cannot both be set.\")\n        elif self.transformer is not None:\n            self.transformer_ = clone(self.transformer)\n        else:\n            if self.func is not None and self.inverse_func is None:\n                raise ValueError(\"When 'func' is provided, 'inverse_func' must\"\n                                 \" also be provided\")\n            self.transformer_ = FunctionTransformer(\n                func=self.func, inverse_func=self.inverse_func, validate=True,\n                check_inverse=self.check_inverse)\n        # XXX: sample_weight is not currently passed to the\n        # transformer. However, if transformer starts using sample_weight, the\n        # code should be modified accordingly. At the time to consider the\n        # sample_prop feature, it is also a good use case to be considered.\n        self.transformer_.fit(y)\n        if self.check_inverse:\n            idx_selected = slice(None, None, max(1, y.shape[0] // 10))\n            y_sel = _safe_indexing(y, idx_selected)\n            y_sel_t = self.transformer_.transform(y_sel)\n            if not np.allclose(y_sel,\n                               self.transformer_.inverse_transform(y_sel_t)):\n                warnings.warn(\"The provided functions or transformer are\"\n                              \" not strictly inverse of each other. If\"\n                              \" you are sure you want to proceed regardless\"\n                              \", set 'check_inverse=False'\", UserWarning)\n\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **fit_params : dict\n            Parameters passed to the ``fit`` method of the underlying\n            regressor.\n\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        y = check_array(y, accept_sparse=False, force_all_finite=True,\n                        ensure_2d=False, dtype='numeric')\n\n        # store the number of dimension of the target to predict an array of\n        # similar shape at predict\n        self._training_dim = y.ndim\n\n        # transformers are designed to modify X which is 2d dimensional, we\n        # need to modify y accordingly.\n        if y.ndim == 1:\n            y_2d = y.reshape(-1, 1)\n        else:\n            y_2d = y\n        self._fit_transformer(y_2d)\n\n        # transform y and convert back to 1d array if needed\n        y_trans = self.transformer_.transform(y_2d)\n        # FIXME: a FunctionTransformer can return a 1D array even when validate\n        # is set to True. Therefore, we need to check the number of dimension\n        # first.\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\n            y_trans = y_trans.squeeze(axis=1)\n\n        if self.regressor is None:\n            from ..linear_model import LinearRegression\n            self.regressor_ = LinearRegression()\n        else:\n            self.regressor_ = clone(self.regressor)\n\n        self.regressor_.fit(X, y_trans, **fit_params)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the base regressor, applying inverse.\n\n        The regressor is used to predict and the ``inverse_func`` or\n        ``inverse_transform`` is applied before returning the prediction.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y_hat : ndarray of shape (n_samples,)\n            Predicted values.\n\n        \"\"\"\n        check_is_fitted(self)\n        pred = self.regressor_.predict(X)\n        if pred.ndim == 1:\n            pred_trans = self.transformer_.inverse_transform(\n                pred.reshape(-1, 1))\n        else:\n            pred_trans = self.transformer_.inverse_transform(pred)\n        if (self._training_dim == 1 and\n                pred_trans.ndim == 2 and pred_trans.shape[1] == 1):\n            pred_trans = pred_trans.squeeze(axis=1)\n\n        return pred_trans\n\n    def _more_tags(self):\n        return {'poor_score': True, 'no_validation': True}\n\n    @property\n    def n_features_in_(self):\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() returns False the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n\n        return self.regressor_.n_features_in_",
      "instance_attributes": [
        {
          "name": "regressor",
          "types": null
        },
        {
          "name": "transformer",
          "types": null
        },
        {
          "name": "func",
          "types": null
        },
        {
          "name": "inverse_func",
          "types": null
        },
        {
          "name": "check_inverse",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "transformer_",
          "types": {
            "kind": "NamedType",
            "name": "Kernel"
          }
        },
        {
          "name": "_training_dim",
          "types": null
        },
        {
          "name": "regressor_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "LinearRegression"
              },
              {
                "kind": "NamedType",
                "name": "Kernel"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope",
      "name": "EllipticEnvelope",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope",
      "decorators": [],
      "superclasses": [
        "OutlierMixin",
        "MinCovDet"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.",
      "docstring": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, the support of robust location and covariance estimates\n    is computed, and a covariance estimate is recomputed from it,\n    without centering the data.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, the robust location and covariance are directly computed\n    with the FastMCD algorithm without additional treatment.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. If None, the minimum value of support_fraction will\n    be used within the algorithm: `[n_sample + n_features + 1] / 2`.\n    Range is (0, 1).\n\ncontamination : float, default=0.1\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. Range is (0, 0.5).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling\n    the data. Pass an int for reproducible results across multiple function\n    calls. See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated robust location.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated robust covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nsupport_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute the\n    robust estimates of location and shape.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores.\n    We have the relation: ``decision_function = score_samples - offset_``.\n    The offset depends on the contamination parameter and is defined in\n    such a way we obtain the expected number of outliers (samples with\n    decision function < 0) in training.\n\n    .. versionadded:: 0.20\n\nraw_location_ : ndarray of shape (n_features,)\n    The raw robust estimated location before correction and re-weighting.\n\nraw_covariance_ : ndarray of shape (n_features, n_features)\n    The raw robust estimated covariance before correction and re-weighting.\n\nraw_support_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the raw robust estimates of location and shape, before correction\n    and re-weighting.\n\ndist_ : ndarray of shape (n_samples,)\n    Mahalanobis distances of the training set (on which :meth:`fit` is\n    called) observations.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EllipticEnvelope\n>>> true_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n...                                                  cov=true_cov,\n...                                                  size=500)\n>>> cov = EllipticEnvelope(random_state=0).fit(X)\n>>> # predict returns 1 for an inlier and -1 for an outlier\n>>> cov.predict([[0, 0],\n...              [3, 3]])\narray([ 1, -1])\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n       [0.2535..., 0.3053...]])\n>>> cov.location_\narray([0.0813... , 0.0427...])\n\nSee Also\n--------\nEmpiricalCovariance, MinCovDet\n\nNotes\n-----\nOutlier detection from covariance estimation may break or not\nperform well in high-dimensional settings. In particular, one will\nalways take care to work with ``n_samples > n_features ** 2``.\n\nReferences\n----------\n.. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n   minimum covariance determinant estimator\" Technometrics 41(3), 212\n   (1999)",
      "code": "class EllipticEnvelope(OutlierMixin, MinCovDet):\n    \"\"\"An object for detecting outliers in a Gaussian distributed dataset.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of robust location and covariance estimates\n        is computed, and a covariance estimate is recomputed from it,\n        without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. If None, the minimum value of support_fraction will\n        be used within the algorithm: `[n_sample + n_features + 1] / 2`.\n        Range is (0, 1).\n\n    contamination : float, default=0.1\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Range is (0, 0.5).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling\n        the data. Pass an int for reproducible results across multiple function\n        calls. See :term: `Glossary <random_state>`.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute the\n        robust estimates of location and shape.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        The offset depends on the contamination parameter and is defined in\n        such a way we obtain the expected number of outliers (samples with\n        decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n\n    See Also\n    --------\n    EmpiricalCovariance, MinCovDet\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, contamination=0.1,\n                 random_state=None):\n        super().__init__(\n            store_precision=store_precision,\n            assume_centered=assume_centered,\n            support_fraction=support_fraction,\n            random_state=random_state)\n        self.contamination = contamination\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n        \"\"\"\n        super().fit(X)\n        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)\n        return self\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.\n        \"\"\"\n        check_is_fitted(self)\n        negative_mahal_dist = self.score_samples(X)\n        return negative_mahal_dist - self.offset_\n\n    def score_samples(self, X):\n        \"\"\"Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.\n        \"\"\"\n        check_is_fitted(self)\n        return -self.mahalanobis(X)\n\n    def predict(self, X):\n        \"\"\"\n        Predict the labels (1 inlier, -1 outlier) of X according to the\n        fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            Returns -1 for anomalies/outliers and +1 for inliers.\n        \"\"\"\n        X = check_array(X)\n        is_inlier = np.full(X.shape[0], -1, dtype=int)\n        values = self.decision_function(X)\n        is_inlier[values >= 0] = 1\n\n        return is_inlier\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)",
      "instance_attributes": [
        {
          "name": "contamination",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "offset_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance",
      "name": "EmpiricalCovariance",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance",
      "decorators": [],
      "superclasses": [
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/_set_covariance",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/get_precision",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.",
      "docstring": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specifies if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo-inverse matrix.\n    (stored only if store_precision is True)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EmpiricalCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> cov = EmpiricalCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7569..., 0.2818...],\n       [0.2818..., 0.3928...]])\n>>> cov.location_\narray([0.0622..., 0.0193...])",
      "code": "class EmpiricalCovariance(BaseEstimator):\n    \"\"\"Maximum likelihood covariance estimator\n\n    Read more in the :ref:`User Guide <covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specifies if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo-inverse matrix.\n        (stored only if store_precision is True)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EmpiricalCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> cov = EmpiricalCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7569..., 0.2818...],\n           [0.2818..., 0.3928...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n\n    def _set_covariance(self, covariance):\n        \"\"\"Saves the covariance and precision estimates\n\n        Storage is done accordingly to `self.store_precision`.\n        Precision stored only if invertible.\n\n        Parameters\n        ----------\n        covariance : array-like of shape (n_features, n_features)\n            Estimated covariance matrix to be stored, and from which precision\n            is computed.\n        \"\"\"\n        covariance = check_array(covariance)\n        # set covariance\n        self.covariance_ = covariance\n        # set precision\n        if self.store_precision:\n            self.precision_ = linalg.pinvh(covariance, check_finite=False)\n        else:\n            self.precision_ = None\n\n    def get_precision(self):\n        \"\"\"Getter for the precision matrix.\n\n        Returns\n        -------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.\n        \"\"\"\n        if self.store_precision:\n            precision = self.precision_\n        else:\n            precision = linalg.pinvh(self.covariance_, check_finite=False)\n        return precision\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the Maximum Likelihood Estimator covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n          Training data, where n_samples is the number of samples and\n          n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self._set_covariance(covariance)\n\n        return self\n\n    def score(self, X_test, y=None):\n        \"\"\"Computes the log-likelihood of a Gaussian data set with\n        `self.covariance_` as an estimator of its covariance matrix.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where n_samples is\n            the number of samples and n_features is the number of features.\n            X_test is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        res : float\n            The likelihood of the data set with `self.covariance_` as an\n            estimator of its covariance matrix.\n        \"\"\"\n        # compute empirical covariance of the test set\n        test_cov = empirical_covariance(\n            X_test - self.location_, assume_centered=True)\n        # compute log likelihood\n        res = log_likelihood(test_cov, self.get_precision())\n\n        return res\n\n    def error_norm(self, comp_cov, norm='frobenius', scaling=True,\n                   squared=True):\n        \"\"\"Computes the Mean Squared Error between two covariance estimators.\n        (In the sense of the Frobenius norm).\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.\n\n        Returns\n        -------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.\n        \"\"\"\n        # compute the error\n        error = comp_cov - self.covariance_\n        # compute the error norm\n        if norm == \"frobenius\":\n            squared_norm = np.sum(error ** 2)\n        elif norm == \"spectral\":\n            squared_norm = np.amax(linalg.svdvals(np.dot(error.T, error)))\n        else:\n            raise NotImplementedError(\n                \"Only spectral and frobenius norms are implemented\")\n        # optionally scale the error norm\n        if scaling:\n            squared_norm = squared_norm / error.shape[0]\n        # finally get either the squared norm or the norm\n        if squared:\n            result = squared_norm\n        else:\n            result = np.sqrt(squared_norm)\n\n        return result\n\n    def mahalanobis(self, X):\n        \"\"\"Computes the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.\n\n        Returns\n        -------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.\n        \"\"\"\n        precision = self.get_precision()\n        # compute mahalanobis distances\n        dist = pairwise_distances(X, self.location_[np.newaxis, :],\n                                  metric='mahalanobis', VI=precision)\n\n        return np.reshape(dist, (len(X),)) ** 2",
      "instance_attributes": [
        {
          "name": "store_precision",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "assume_centered",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "covariance_",
          "types": null
        },
        {
          "name": "precision_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso",
      "name": "GraphicalLasso",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLasso",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso",
      "docstring": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso\n\nParameters\n----------\nalpha : float, default=0.01\n    The regularization parameter: the higher alpha, the more\n    regularization, the sparser the inverse covariance.\n    Range is (0, inf].\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    The maximum number of iterations.\n\nverbose : bool, default=False\n    If verbose is True, the objective function and dual gap are\n    plotted at each iteration.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLasso\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLasso().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.049, 0.218, 0.019],\n       [0.049, 0.364, 0.017, 0.034],\n       [0.218, 0.017, 0.322, 0.093],\n       [0.019, 0.034, 0.093, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)\narray([0.073, 0.04 , 0.038, 0.143])\n\nSee Also\n--------\ngraphical_lasso, GraphicalLassoCV",
      "code": "class GraphicalLasso(EmpiricalCovariance):\n    \"\"\"Sparse inverse covariance estimation with an l1-penalized estimator.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLasso has been renamed to GraphicalLasso\n\n    Parameters\n    ----------\n    alpha : float, default=0.01\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n        Range is (0, inf].\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        The maximum number of iterations.\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and dual gap are\n        plotted at each iteration.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLasso\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLasso().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.049, 0.218, 0.019],\n           [0.049, 0.364, 0.017, 0.034],\n           [0.218, 0.017, 0.322, 0.093],\n           [0.019, 0.034, 0.093, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])\n\n    See Also\n    --------\n    graphical_lasso, GraphicalLassoCV\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, alpha=.01, *, mode='cd', tol=1e-4, enet_tol=1e-4,\n                 max_iter=100, verbose=False, assume_centered=False):\n        super().__init__(assume_centered=assume_centered)\n        self.alpha = alpha\n        self.mode = mode\n        self.tol = tol\n        self.enet_tol = enet_tol\n        self.max_iter = max_iter\n        self.verbose = verbose\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, ensure_min_samples=2,\n                                estimator=self)\n\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=self.verbose, return_n_iter=True)\n        return self",
      "instance_attributes": [
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "mode",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "enet_tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "covariance_",
          "types": null
        },
        {
          "name": "precision_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "tuple"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV",
      "name": "GraphicalLassoCV",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV",
      "decorators": [],
      "superclasses": [
        "GraphicalLasso"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/grid_scores_@getter",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/cv_alphas_@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV",
      "docstring": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV\n\nParameters\n----------\nalphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n    If an integer is given, it fixes the number of points on the\n    grids of alpha to be used. If a list is given, it gives the\n    grid to be used. See the notes in the class docstring for\n    more details. Range is (0, inf] when floats given.\n\nn_refinements : int, default=4\n    The number of times the grid is refined. Not used if explicit\n    values of alphas are passed. Range is [1, inf).\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.20\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    Maximum number of iterations.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where number of features is greater\n    than number of samples. Elsewhere prefer cd which is more numerically\n    stable.\n\nn_jobs : int, default=None\n    number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nverbose : bool, default=False\n    If verbose is True, the objective function and duality gap are\n    printed at each iteration.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated precision matrix (inverse covariance).\n\nalpha_ : float\n    Penalization parameter selected.\n\ncv_alphas_ : list of shape (n_alphas,), dtype=float\n    All penalization parameters explored.\n\n    .. deprecated:: 0.24\n        The `cv_alphas_` attribute is deprecated in version 0.24 in favor\n        of `cv_results_['alphas']` and will be removed in version\n        1.1 (renaming of 0.26).\n\ngrid_scores_ : ndarray of shape (n_alphas, n_folds)\n    Log-likelihood score on left-out data across folds.\n\n    .. deprecated:: 0.24\n        The `grid_scores_` attribute is deprecated in version 0.24 in favor\n        of `cv_results_` and will be removed in version\n        1.1 (renaming of 0.26).\n\ncv_results_ : dict of ndarrays\n    A dict with keys:\n\n    alphas : ndarray of shape (n_alphas,)\n        All penalization parameters explored.\n\n    split(k)_score : ndarray of shape (n_alphas,)\n        Log-likelihood score on left-out data across (k)th fold.\n\n    mean_score : ndarray of shape (n_alphas,)\n        Mean of scores over the folds.\n\n    std_score : ndarray of shape (n_alphas,)\n        Standard deviation of scores over the folds.\n\n    .. versionadded:: 0.24\n\nn_iter_ : int\n    Number of iterations run for the optimal alpha.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLassoCV\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLassoCV().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.051, 0.22 , 0.017],\n       [0.051, 0.364, 0.018, 0.036],\n       [0.22 , 0.018, 0.322, 0.094],\n       [0.017, 0.036, 0.094, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)\narray([0.073, 0.04 , 0.038, 0.143])\n\nSee Also\n--------\ngraphical_lasso, GraphicalLasso\n\nNotes\n-----\nThe search for the optimal penalization parameter (alpha) is done on an\niteratively refined grid: first the cross-validated scores on a grid are\ncomputed, then a new refined grid is centered around the maximum, and so\non.\n\nOne of the challenges which is faced here is that the solvers can\nfail to converge to a well-conditioned estimate. The corresponding\nvalues of alpha then come out as missing values, but the optimum may\nbe close to these missing values.",
      "code": "class GraphicalLassoCV(GraphicalLasso):\n    \"\"\"Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLassoCV has been renamed to GraphicalLassoCV\n\n    Parameters\n    ----------\n    alphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n        If an integer is given, it fixes the number of points on the\n        grids of alpha to be used. If a list is given, it gives the\n        grid to be used. See the notes in the class docstring for\n        more details. Range is (0, inf] when floats given.\n\n    n_refinements : int, default=4\n        The number of times the grid is refined. Not used if explicit\n        values of alphas are passed. Range is [1, inf).\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        Maximum number of iterations.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where number of features is greater\n        than number of samples. Elsewhere prefer cd which is more numerically\n        stable.\n\n    n_jobs : int, default=None\n        number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and duality gap are\n        printed at each iteration.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated precision matrix (inverse covariance).\n\n    alpha_ : float\n        Penalization parameter selected.\n\n    cv_alphas_ : list of shape (n_alphas,), dtype=float\n        All penalization parameters explored.\n\n        .. deprecated:: 0.24\n            The `cv_alphas_` attribute is deprecated in version 0.24 in favor\n            of `cv_results_['alphas']` and will be removed in version\n            1.1 (renaming of 0.26).\n\n    grid_scores_ : ndarray of shape (n_alphas, n_folds)\n        Log-likelihood score on left-out data across folds.\n\n        .. deprecated:: 0.24\n            The `grid_scores_` attribute is deprecated in version 0.24 in favor\n            of `cv_results_` and will be removed in version\n            1.1 (renaming of 0.26).\n\n    cv_results_ : dict of ndarrays\n        A dict with keys:\n\n        alphas : ndarray of shape (n_alphas,)\n            All penalization parameters explored.\n\n        split(k)_score : ndarray of shape (n_alphas,)\n            Log-likelihood score on left-out data across (k)th fold.\n\n        mean_score : ndarray of shape (n_alphas,)\n            Mean of scores over the folds.\n\n        std_score : ndarray of shape (n_alphas,)\n            Standard deviation of scores over the folds.\n\n        .. versionadded:: 0.24\n\n    n_iter_ : int\n        Number of iterations run for the optimal alpha.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])\n\n    See Also\n    --------\n    graphical_lasso, GraphicalLasso\n\n    Notes\n    -----\n    The search for the optimal penalization parameter (alpha) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of alpha then come out as missing values, but the optimum may\n    be close to these missing values.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, alphas=4, n_refinements=4, cv=None, tol=1e-4,\n                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=None,\n                 verbose=False, assume_centered=False):\n        super().__init__(\n            mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,\n            max_iter=max_iter, assume_centered=assume_centered)\n        self.alphas = alphas\n        self.n_refinements = n_refinements\n        self.cv = cv\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, estimator=self)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n\n        cv = check_cv(self.cv, y, classifier=False)\n\n        # List of (alpha, scores, covs)\n        path = list()\n        n_alphas = self.alphas\n        inner_verbose = max(0, self.verbose - 1)\n\n        if isinstance(n_alphas, Sequence):\n            alphas = self.alphas\n            n_refinements = 1\n        else:\n            n_refinements = self.n_refinements\n            alpha_1 = alpha_max(emp_cov)\n            alpha_0 = 1e-2 * alpha_1\n            alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1),\n                                 n_alphas)[::-1]\n\n        t0 = time.time()\n        for i in range(n_refinements):\n            with warnings.catch_warnings():\n                # No need to see the convergence warnings on this grid:\n                # they will always be points that will not converge\n                # during the cross-validation\n                warnings.simplefilter('ignore', ConvergenceWarning)\n                # Compute the cross-validated loss on the current grid\n\n                # NOTE: Warm-restarting graphical_lasso_path has been tried,\n                # and this did not allow to gain anything\n                # (same execution time with or without).\n                this_path = Parallel(\n                    n_jobs=self.n_jobs,\n                    verbose=self.verbose\n                )(delayed(graphical_lasso_path)(X[train], alphas=alphas,\n                                                X_test=X[test], mode=self.mode,\n                                                tol=self.tol,\n                                                enet_tol=self.enet_tol,\n                                                max_iter=int(.1 *\n                                                             self.max_iter),\n                                                verbose=inner_verbose)\n                  for train, test in cv.split(X, y))\n\n            # Little danse to transform the list in what we need\n            covs, _, scores = zip(*this_path)\n            covs = zip(*covs)\n            scores = zip(*scores)\n            path.extend(zip(alphas, scores, covs))\n            path = sorted(path, key=operator.itemgetter(0), reverse=True)\n\n            # Find the maximum (avoid using built in 'max' function to\n            # have a fully-reproducible selection of the smallest alpha\n            # in case of equality)\n            best_score = -np.inf\n            last_finite_idx = 0\n            for index, (alpha, scores, _) in enumerate(path):\n                this_score = np.mean(scores)\n                if this_score >= .1 / np.finfo(np.float64).eps:\n                    this_score = np.nan\n                if np.isfinite(this_score):\n                    last_finite_idx = index\n                if this_score >= best_score:\n                    best_score = this_score\n                    best_index = index\n\n            # Refine the grid\n            if best_index == 0:\n                # We do not need to go back: we have chosen\n                # the highest value of alpha for which there are\n                # non-zero coefficients\n                alpha_1 = path[0][0]\n                alpha_0 = path[1][0]\n            elif (best_index == last_finite_idx\n                    and not best_index == len(path) - 1):\n                # We have non-converged models on the upper bound of the\n                # grid, we need to refine the grid there\n                alpha_1 = path[best_index][0]\n                alpha_0 = path[best_index + 1][0]\n            elif best_index == len(path) - 1:\n                alpha_1 = path[best_index][0]\n                alpha_0 = 0.01 * path[best_index][0]\n            else:\n                alpha_1 = path[best_index - 1][0]\n                alpha_0 = path[best_index + 1][0]\n\n            if not isinstance(n_alphas, Sequence):\n                alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0),\n                                     n_alphas + 2)\n                alphas = alphas[1:-1]\n\n            if self.verbose and n_refinements > 1:\n                print('[GraphicalLassoCV] Done refinement % 2i out of'\n                      ' %i: % 3is' % (i + 1, n_refinements, time.time() - t0))\n\n        path = list(zip(*path))\n        grid_scores = list(path[1])\n        alphas = list(path[0])\n        # Finally, compute the score with alpha = 0\n        alphas.append(0)\n        grid_scores.append(cross_val_score(EmpiricalCovariance(), X,\n                                           cv=cv, n_jobs=self.n_jobs,\n                                           verbose=inner_verbose))\n        grid_scores = np.array(grid_scores)\n        self.cv_results_ = {'alphas': np.array(alphas)}\n        for i in range(grid_scores.shape[1]):\n            key = \"split{}_score\".format(i)\n            self.cv_results_[key] = grid_scores[:, i]\n\n        self.cv_results_[\"mean_score\"] = np.mean(grid_scores, axis=1)\n        self.cv_results_[\"std_score\"] = np.std(grid_scores, axis=1)\n\n        best_alpha = alphas[best_index]\n        self.alpha_ = best_alpha\n\n        # Finally fit the model with the selected alpha\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=inner_verbose, return_n_iter=True)\n        return self\n\n    # TODO: Remove in 1.1 when grid_scores_ is deprecated\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"The grid_scores_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_ and will be removed in version 1.1 (renaming of 0.26).\"\n    )\n    @property\n    def grid_scores_(self):\n        # remove 3 for mean_score, std_score, and alphas\n        n_alphas = len(self.cv_results_) - 3\n        return np.asarray(\n            [self.cv_results_[\"split{}_score\".format(i)]\n             for i in range(n_alphas)]).T\n\n    # TODO: Remove in 1.1 when cv_alphas_ is deprecated\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"The cv_alphas_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_['alpha'] and will be removed in version 1.1 \"\n        \"(renaming of 0.26).\"\n    )\n    @property\n    def cv_alphas_(self):\n        return self.cv_results_['alphas'].tolist()",
      "instance_attributes": [
        {
          "name": "alphas",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_refinements",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cv",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "cv_results_",
          "types": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "name": "alpha_",
          "types": null
        },
        {
          "name": "covariance_",
          "types": null
        },
        {
          "name": "precision_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "tuple"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet",
      "name": "MinCovDet",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__",
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit",
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance",
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.",
      "docstring": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, the support of the robust location and the covariance\n    estimates is computed, and a covariance estimate is recomputed from\n    it, without centering the data.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, the robust location and covariance are directly computed\n    with the FastMCD algorithm without additional treatment.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is None, which implies that the minimum\n    value of support_fraction will be used within the algorithm:\n    `(n_sample + n_features + 1) / 2`. The parameter must be in the range\n    (0, 1).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nraw_location_ : ndarray of shape (n_features,)\n    The raw robust estimated location before correction and re-weighting.\n\nraw_covariance_ : ndarray of shape (n_features, n_features)\n    The raw robust estimated covariance before correction and re-weighting.\n\nraw_support_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the raw robust estimates of location and shape, before correction\n    and re-weighting.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated robust location.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated robust covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nsupport_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the robust estimates of location and shape.\n\ndist_ : ndarray of shape (n_samples,)\n    Mahalanobis distances of the training set (on which :meth:`fit` is\n    called) observations.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import MinCovDet\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = MinCovDet(random_state=0).fit(X)\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n       [0.2535..., 0.3053...]])\n>>> cov.location_\narray([0.0813... , 0.0427...])\n\nReferences\n----------\n\n.. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n    J. Am Stat Ass, 79:871, 1984.\n.. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n    Estimator, 1999, American Statistical Association and the American\n    Society for Quality, TECHNOMETRICS\n.. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
      "code": "class MinCovDet(EmpiricalCovariance):\n    \"\"\"Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\n    The Minimum Covariance Determinant covariance estimator is to be applied\n    on Gaussian-distributed data, but could still be relevant on data\n    drawn from a unimodal, symmetric distribution. It is not meant to be used\n    with multi-modal data (the algorithm used to fit a MinCovDet object is\n    likely to fail in such a case).\n    One should consider projection pursuit methods to deal with multi-modal\n    datasets.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of the robust location and the covariance\n        estimates is computed, and a covariance estimate is recomputed from\n        it, without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. Default is None, which implies that the minimum\n        value of support_fraction will be used within the algorithm:\n        `(n_sample + n_features + 1) / 2`. The parameter must be in the range\n        (0, 1).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term: `Glossary <random_state>`.\n\n    Attributes\n    ----------\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the robust estimates of location and shape.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import MinCovDet\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = MinCovDet(random_state=0).fit(X)\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n\n    References\n    ----------\n\n    .. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n        J. Am Stat Ass, 79:871, 1984.\n    .. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n        Estimator, 1999, American Statistical Association and the American\n        Society for Quality, TECHNOMETRICS\n    .. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400\n    \"\"\"\n    _nonrobust_covariance = staticmethod(empirical_covariance)\n\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, random_state=None):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n        self.support_fraction = support_fraction\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator='MinCovDet')\n        random_state = check_random_state(self.random_state)\n        n_samples, n_features = X.shape\n        # check that the empirical covariance is full rank\n        if (linalg.svdvals(np.dot(X.T, X)) > 1e-8).sum() != n_features:\n            warnings.warn(\"The covariance matrix associated to your dataset \"\n                          \"is not full rank\")\n        # compute and store raw estimates\n        raw_location, raw_covariance, raw_support, raw_dist = fast_mcd(\n            X, support_fraction=self.support_fraction,\n            cov_computation_method=self._nonrobust_covariance,\n            random_state=random_state)\n        if self.assume_centered:\n            raw_location = np.zeros(n_features)\n            raw_covariance = self._nonrobust_covariance(X[raw_support],\n                                                        assume_centered=True)\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(raw_covariance)\n            raw_dist = np.sum(np.dot(X, precision) * X, 1)\n        self.raw_location_ = raw_location\n        self.raw_covariance_ = raw_covariance\n        self.raw_support_ = raw_support\n        self.location_ = raw_location\n        self.support_ = raw_support\n        self.dist_ = raw_dist\n        # obtain consistency at normal models\n        self.correct_covariance(X)\n        # re-weight estimator\n        self.reweight_covariance(X)\n\n        return self\n\n    def correct_covariance(self, data):\n        \"\"\"Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n\n        # Check that the covariance of the support data is not equal to 0.\n        # Otherwise self.dist_ = 0 and thus correction = 0.\n        n_samples = len(self.dist_)\n        n_support = np.sum(self.support_)\n        if n_support < n_samples and np.allclose(self.raw_covariance_, 0):\n            raise ValueError('The covariance matrix of the support data '\n                             'is equal to 0, try to increase support_fraction')\n        correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5)\n        covariance_corrected = self.raw_covariance_ * correction\n        self.dist_ /= correction\n        return covariance_corrected\n\n    def reweight_covariance(self, data):\n        \"\"\"Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n        n_samples, n_features = data.shape\n        mask = self.dist_ < chi2(n_features).isf(0.025)\n        if self.assume_centered:\n            location_reweighted = np.zeros(n_features)\n        else:\n            location_reweighted = data[mask].mean(0)\n        covariance_reweighted = self._nonrobust_covariance(\n            data[mask], assume_centered=self.assume_centered)\n        support_reweighted = np.zeros(n_samples, dtype=bool)\n        support_reweighted[mask] = True\n        self._set_covariance(covariance_reweighted)\n        self.location_ = location_reweighted\n        self.support_ = support_reweighted\n        X_centered = data - self.location_\n        self.dist_ = np.sum(\n            np.dot(X_centered, self.get_precision()) * X_centered, 1)\n        return location_reweighted, covariance_reweighted, support_reweighted",
      "instance_attributes": [
        {
          "name": "store_precision",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "assume_centered",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "support_fraction",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "raw_location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "raw_covariance_",
          "types": null
        },
        {
          "name": "raw_support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "dist_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf",
      "name": "LedoitWolf",
      "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__",
        "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split\n    during its Ledoit-Wolf estimation. This is purely a memory\n    optimization and does not affect results.\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nshrinkage_ : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import LedoitWolf\n>>> real_cov = np.array([[.4, .2],\n...                      [.2, .8]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=50)\n>>> cov = LedoitWolf().fit(X)\n>>> cov.covariance_\narray([[0.4406..., 0.1616...],\n       [0.1616..., 0.8022...]])\n>>> cov.location_\narray([ 0.0595... , -0.0075...])\n\nNotes\n-----\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\nand shrinkage is given by the Ledoit and Wolf formula (see References)\n\nReferences\n----------\n\"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\nLedoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\nFebruary 2004, pages 365-411.",
      "code": "class LedoitWolf(EmpiricalCovariance):\n    \"\"\"LedoitWolf Estimator\n\n    Ledoit-Wolf is a particular form of shrinkage, where the shrinkage\n    coefficient is computed using O. Ledoit and M. Wolf's formula as\n    described in \"A Well-Conditioned Estimator for Large-Dimensional\n    Covariance Matrices\", Ledoit and Wolf, Journal of Multivariate\n    Analysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split\n        during its Ledoit-Wolf estimation. This is purely a memory\n        optimization and does not affect results.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import LedoitWolf\n    >>> real_cov = np.array([[.4, .2],\n    ...                      [.2, .8]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=50)\n    >>> cov = LedoitWolf().fit(X)\n    >>> cov.covariance_\n    array([[0.4406..., 0.1616...],\n           [0.1616..., 0.8022...]])\n    >>> cov.location_\n    array([ 0.0595... , -0.0075...])\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the Ledoit and Wolf formula (see References)\n\n    References\n    ----------\n    \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\n    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\n    February 2004, pages 365-411.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 block_size=1000):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.block_size = block_size\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the Ledoit-Wolf shrunk covariance model according to the given\n        training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance, shrinkage = ledoit_wolf(X - self.location_,\n                                            assume_centered=True,\n                                            block_size=self.block_size)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self",
      "instance_attributes": [
        {
          "name": "block_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "shrinkage_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS",
      "name": "OAS",
      "qname": "sklearn.covariance._shrunk_covariance.OAS",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Oracle Approximating Shrinkage Estimator\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nOAS is a particular form of shrinkage described in\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the\narticle. In the original article, formula (23) states that 2/p is\nmultiplied by Trace(cov*cov) in both the numerator and denominator, but\nthis operation is omitted because for a large p, the value of 2/p is\nso small that it doesn't affect the value of the estimator.",
      "docstring": "Oracle Approximating Shrinkage Estimator\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nOAS is a particular form of shrinkage described in\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the\narticle. In the original article, formula (23) states that 2/p is\nmultiplied by Trace(cov*cov) in both the numerator and denominator, but\nthis operation is omitted because for a large p, the value of 2/p is\nso small that it doesn't affect the value of the estimator.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data will be centered before computation.\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nshrinkage_ : float\n  coefficient in the convex combination used for the computation\n  of the shrunk estimate. Range is [0, 1].\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import OAS\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> oas = OAS().fit(X)\n>>> oas.covariance_\narray([[0.7533..., 0.2763...],\n       [0.2763..., 0.3964...]])\n>>> oas.precision_\narray([[ 1.7833..., -1.2431... ],\n       [-1.2431...,  3.3889...]])\n>>> oas.shrinkage_\n0.0195...\n\nNotes\n-----\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\nand shrinkage is given by the OAS formula (see References)\n\nReferences\n----------\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.",
      "code": "class OAS(EmpiricalCovariance):\n    \"\"\"Oracle Approximating Shrinkage Estimator\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    OAS is a particular form of shrinkage described in\n    \"Shrinkage Algorithms for MMSE Covariance Estimation\"\n    Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\n    The formula used here does not correspond to the one given in the\n    article. In the original article, formula (23) states that 2/p is\n    multiplied by Trace(cov*cov) in both the numerator and denominator, but\n    this operation is omitted because for a large p, the value of 2/p is\n    so small that it doesn't affect the value of the estimator.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n      coefficient in the convex combination used for the computation\n      of the shrunk estimate. Range is [0, 1].\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    0.0195...\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the OAS formula (see References)\n\n    References\n    ----------\n    \"Shrinkage Algorithms for MMSE Covariance Estimation\"\n    Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n    \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the Oracle Approximating Shrinkage covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n\n        covariance, shrinkage = oas(X - self.location_, assume_centered=True)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self",
      "instance_attributes": [
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "shrinkage_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance",
      "name": "ShrunkCovariance",
      "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__",
        "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data will be centered before computation.\n\nshrinkage : float, default=0.1\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import ShrunkCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = ShrunkCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7387..., 0.2536...],\n       [0.2536..., 0.4110...]])\n>>> cov.location_\narray([0.0622..., 0.0193...])\n\nNotes\n-----\nThe regularized covariance is given by:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "class ShrunkCovariance(EmpiricalCovariance):\n    \"\"\"Covariance estimator with shrinkage\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data will be centered before computation.\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ShrunkCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = ShrunkCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7387..., 0.2536...],\n           [0.2536..., 0.4110...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])\n\n    Notes\n    -----\n    The regularized covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 shrinkage=0.1):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.shrinkage = shrinkage\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the shrunk covariance model according to the given training data\n        and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid a potential\n        # matrix inversion when setting the precision\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        covariance = shrunk_covariance(covariance, self.shrinkage)\n        self._set_covariance(covariance)\n\n        return self",
      "instance_attributes": [
        {
          "name": "shrinkage",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA",
      "name": "CCA",
      "qname": "sklearn.cross_decomposition._pls.CCA",
      "decorators": [],
      "superclasses": [
        "_PLS"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.",
      "docstring": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nmax_iter : int, default=500\n    the maximum number of iterations of the power method.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import CCA\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> cca = CCA(n_components=1)\n>>> cca.fit(X, Y)\nCCA(n_components=1)\n>>> X_c, Y_c = cca.transform(X, Y)\n\nSee Also\n--------\nPLSCanonical\nPLSSVD",
      "code": "class CCA(_PLS):\n    \"\"\"Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    max_iter : int, default=500\n        the maximum number of iterations of the power method.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n        .. deprecated:: 0.24\n           `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n        .. deprecated:: 0.24\n           `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_features, n_targets)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_`.\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import CCA\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> cca = CCA(n_components=1)\n    >>> cca.fit(X, Y)\n    CCA(n_components=1)\n    >>> X_c, Y_c = cca.transform(X, Y)\n\n    See Also\n    --------\n    PLSCanonical\n    PLSSVD\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(n_components=n_components, scale=scale,\n                         deflation_mode=\"canonical\", mode=\"B\",\n                         algorithm=\"nipals\", max_iter=max_iter, tol=tol,\n                         copy=copy)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical",
      "name": "PLSCanonical",
      "qname": "sklearn.cross_decomposition._pls.PLSCanonical",
      "decorators": [],
      "superclasses": [
        "_PLS"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nalgorithm : {'nipals', 'svd'}, default='nipals'\n    The algorithm used to estimate the first singular vectors of the\n    cross-covariance matrix. 'nipals' uses the power method while 'svd'\n    will compute the whole SVD.\n\nmax_iter : int, default=500\n    the maximum number of iterations of the power method when\n    `algorithm='nipals'`. Ignored otherwise.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component. Empty if `algorithm='svd'`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import PLSCanonical\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> plsca = PLSCanonical(n_components=2)\n>>> plsca.fit(X, Y)\nPLSCanonical()\n>>> X_c, Y_c = plsca.transform(X, Y)\n\nSee Also\n--------\nCCA\nPLSSVD",
      "code": "class PLSCanonical(_PLS):\n    \"\"\"Partial Least Squares transformer and regressor.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    algorithm : {'nipals', 'svd'}, default='nipals'\n        The algorithm used to estimate the first singular vectors of the\n        cross-covariance matrix. 'nipals' uses the power method while 'svd'\n        will compute the whole SVD.\n\n    max_iter : int, default=500\n        the maximum number of iterations of the power method when\n        `algorithm='nipals'`. Ignored otherwise.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n        .. deprecated:: 0.24\n           `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n        .. deprecated:: 0.24\n           `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_features, n_targets)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_`.\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component. Empty if `algorithm='svd'`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, Y)\n    PLSCanonical()\n    >>> X_c, Y_c = plsca.transform(X, Y)\n\n    See Also\n    --------\n    CCA\n    PLSSVD\n    \"\"\"\n    # This implementation provides the same results that the \"plspm\" package\n    # provided in the R language (R-project), using the function plsca(X, Y).\n    # Results are equal or collinear with the function\n    # ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The\n    # difference relies in the fact that mixOmics implementation does not\n    # exactly implement the Wold algorithm since it does not normalize\n    # y_weights to one.\n\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, algorithm=\"nipals\",\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"canonical\", mode=\"A\",\n            algorithm=algorithm,\n            max_iter=max_iter, tol=tol, copy=copy)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression",
      "name": "PLSRegression",
      "qname": "sklearn.cross_decomposition._pls.PLSRegression",
      "decorators": [],
      "superclasses": [
        "_PLS"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nmax_iter : int, default=500\n    The maximum number of iterations of the power method when\n    `algorithm='nipals'`. Ignored otherwise.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import PLSRegression\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> pls2 = PLSRegression(n_components=2)\n>>> pls2.fit(X, Y)\nPLSRegression()\n>>> Y_pred = pls2.predict(X)",
      "code": "class PLSRegression(_PLS):\n    \"\"\"PLS regression\n\n    PLSRegression is also known as PLS2 or PLS1, depending on the number of\n    targets.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    max_iter : int, default=500\n        The maximum number of iterations of the power method when\n        `algorithm='nipals'`. Ignored otherwise.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_features, n_targets)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_`.\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSRegression\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> pls2 = PLSRegression(n_components=2)\n    >>> pls2.fit(X, Y)\n    PLSRegression()\n    >>> Y_pred = pls2.predict(X)\n    \"\"\"\n\n    # This implementation provides the same results that 3 PLS packages\n    # provided in the R language (R-project):\n    #     - \"mixOmics\" with function pls(X, Y, mode = \"regression\")\n    #     - \"plspm \" with function plsreg2(X, Y)\n    #     - \"pls\" with function oscorespls.fit(X, Y)\n\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"regression\", mode=\"A\",\n            algorithm='nipals', max_iter=max_iter,\n            tol=tol, copy=copy)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD",
      "name": "PLSSVD",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_scores_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_scores_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_mean_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_mean_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_std_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_std_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    The number of components to keep. Should be in `[1,\n    min(n_samples, n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the SVD of the cross-covariance matrix.\n    Used to project `X` in `transform`.\n\ny_weights_ : ndarray of (n_targets, n_components)\n    The right singular vectors of the SVD of the cross-covariance matrix.\n    Used to project `X` in `transform`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cross_decomposition import PLSSVD\n>>> X = np.array([[0., 0., 1.],\n...               [1., 0., 0.],\n...               [2., 2., 2.],\n...               [2., 5., 4.]])\n>>> Y = np.array([[0.1, -0.2],\n...               [0.9, 1.1],\n...               [6.2, 5.9],\n...               [11.9, 12.3]])\n>>> pls = PLSSVD(n_components=2).fit(X, Y)\n>>> X_c, Y_c = pls.transform(X, Y)\n>>> X_c.shape, Y_c.shape\n((4, 2), (4, 2))\n\nSee Also\n--------\nPLSCanonical\nCCA",
      "code": "class PLSSVD(TransformerMixin, BaseEstimator):\n    \"\"\"Partial Least Square SVD.\n\n    This transformer simply performs a SVD on the crosscovariance matrix X'Y.\n    It is able to project both the training data `X` and the targets `Y`. The\n    training data X is projected on the left singular vectors, while the\n    targets are projected on the right singular vectors.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        The number of components to keep. Should be in `[1,\n        min(n_samples, n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the SVD of the cross-covariance matrix.\n        Used to project `X` in `transform`.\n\n    y_weights_ : ndarray of (n_targets, n_components)\n        The right singular vectors of the SVD of the cross-covariance matrix.\n        Used to project `X` in `transform`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n        .. deprecated:: 0.24\n           `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n        .. deprecated:: 0.24\n           `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cross_decomposition import PLSSVD\n    >>> X = np.array([[0., 0., 1.],\n    ...               [1., 0., 0.],\n    ...               [2., 2., 2.],\n    ...               [2., 5., 4.]])\n    >>> Y = np.array([[0.1, -0.2],\n    ...               [0.9, 1.1],\n    ...               [6.2, 5.9],\n    ...               [11.9, 12.3]])\n    >>> pls = PLSSVD(n_components=2).fit(X, Y)\n    >>> X_c, Y_c = pls.transform(X, Y)\n    >>> X_c.shape, Y_c.shape\n    ((4, 2), (4, 2))\n\n    See Also\n    --------\n    PLSCanonical\n    CCA\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, copy=True):\n        self.n_components = n_components\n        self.scale = scale\n        self.copy = copy\n\n    def fit(self, X, Y):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Targets.\n        \"\"\"\n        check_consistent_length(X, Y)\n        X = self._validate_data(X, dtype=np.float64, copy=self.copy,\n                                ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)\n        # This matrix rank is at most min(n_samples, n_features, n_targets) so\n        # n_components cannot be bigger than that.\n        n_components = self.n_components\n        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])\n        if not 1 <= n_components <= rank_upper_bound:\n            # TODO: raise an error in 1.1\n            warnings.warn(\n                f\"As of version 0.24, n_components({n_components}) should be \"\n                f\"in [1, min(n_features, n_samples, n_targets)] = \"\n                f\"[1, {rank_upper_bound}]. \"\n                f\"n_components={rank_upper_bound} will be used instead. \"\n                f\"In version 1.1 (renaming of 0.26), an error will be raised.\",\n                FutureWarning\n            )\n            n_components = rank_upper_bound\n\n        X, Y, self._x_mean, self._y_mean, self._x_std, self._y_std = (\n            _center_scale_xy(X, Y, self.scale))\n\n        # Compute SVD of cross-covariance matrix\n        C = np.dot(X.T, Y)\n        U, s, Vt = svd(C, full_matrices=False)\n        U = U[:, :n_components]\n        Vt = Vt[:n_components]\n        U, Vt = svd_flip(U, Vt)\n        V = Vt.T\n\n        self._x_scores = np.dot(X, U)  # TODO: remove in 1.1\n        self._y_scores = np.dot(Y, V)  # TODO: remove in 1.1\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self\n\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"Attribute x_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X) on \"\n        \"the training data instead.\"\n    )\n    @property\n    def x_scores_(self):\n        return self._x_scores\n\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"Attribute y_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X, Y) \"\n        \"on the training data instead.\"\n    )\n    @property\n    def y_scores_(self):\n        return self._y_scores\n\n    @deprecated(  # type: ignore\n        \"Attribute x_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_mean_(self):\n        return self._x_mean\n\n    @deprecated(  # type: ignore\n        \"Attribute y_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_mean_(self):\n        return self._y_mean\n\n    @deprecated(  # type: ignore\n        \"Attribute x_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_std_(self):\n        return self._x_std\n\n    @deprecated(  # type: ignore\n        \"Attribute y_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_std_(self):\n        return self._y_std\n\n    def transform(self, X, Y=None):\n        \"\"\"\n        Apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to be transformed.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, dtype=np.float64)\n        Xr = (X - self._x_mean) / self._x_std\n        x_scores = np.dot(Xr, self.x_weights_)\n        if Y is not None:\n            Y = check_array(Y, ensure_2d=False, dtype=np.float64)\n            if Y.ndim == 1:\n                Y = Y.reshape(-1, 1)\n            Yr = (Y - self._y_mean) / self._y_std\n            y_scores = np.dot(Yr, self.y_weights_)\n            return x_scores, y_scores\n        return x_scores\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "scale",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "_x_mean",
          "types": null
        },
        {
          "name": "_y_mean",
          "types": null
        },
        {
          "name": "_x_std",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_y_std",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_x_scores",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_y_scores",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "x_weights_",
          "types": null
        },
        {
          "name": "y_weights_",
          "types": null
        }
      ]
    }
  ],
  "functions": [
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/damping",
          "name": "damping",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.damping",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "Damping factor (between 0.5 and 1) is the extent to\nwhich the current value is maintained relative to\nincoming values (weighted 1 - damping). This in order\nto avoid numerical oscillations when updating these\nvalues (messages)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/convergence_iter",
          "name": "convergence_iter",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.convergence_iter",
          "default_value": "15",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "15",
            "description": "Number of iterations with no change in the number\nof estimated clusters that stops the convergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Make a copy of input data."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/preference",
          "name": "preference",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.preference",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or float",
            "default_value": "None",
            "description": "Preferences for each point - points with larger values of\npreferences are more likely to be chosen as exemplars. The number\nof exemplars, ie of clusters, is influenced by the input\npreferences value. If the preferences are not passed as arguments,\nthey will be set to the median of the input similarities."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples,)"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'euclidean', 'precomputed'}",
            "default_value": "'euclidean'",
            "description": "Which affinity to use. At the moment 'precomputed' and\n``euclidean`` are supported. 'euclidean' uses the\nnegative squared euclidean distance between points."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "euclidean",
              "precomputed"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to be verbose."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.random_state",
          "default_value": "'warn'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "0",
            "description": "Pseudo-random number generator to control the starting state.\nUse an int for reproducible results across function calls.\nSee the :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.23\n    this parameter was previously hardcoded as 0."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, damping=.5, max_iter=200, convergence_iter=15,\n                 copy=True, preference=None, affinity='euclidean',\n                 verbose=False, random_state='warn'):\n\n        self.damping = damping\n        self.max_iter = max_iter\n        self.convergence_iter = convergence_iter\n        self.copy = copy\n        self.verbose = verbose\n        self.preference = preference\n        self.affinity = affinity\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit",
      "name": "fit",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse feature matrix\nis provided, it will be converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the clustering from features, or affinity matrix.",
      "docstring": "Fit the clustering from features, or affinity matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse feature matrix\n    is provided, it will be converted into a sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        if self.affinity == \"precomputed\":\n            accept_sparse = False\n        else:\n            accept_sparse = 'csr'\n        X = self._validate_data(X, accept_sparse=accept_sparse)\n        if self.affinity == \"precomputed\":\n            self.affinity_matrix_ = X\n        elif self.affinity == \"euclidean\":\n            self.affinity_matrix_ = -euclidean_distances(X, squared=True)\n        else:\n            raise ValueError(\"Affinity must be 'precomputed' or \"\n                             \"'euclidean'. Got %s instead\"\n                             % str(self.affinity))\n\n        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \\\n            affinity_propagation(\n                self.affinity_matrix_, preference=self.preference,\n                max_iter=self.max_iter,\n                convergence_iter=self.convergence_iter, damping=self.damping,\n                copy=self.copy, verbose=self.verbose, return_n_iter=True,\n                random_state=self.random_state)\n\n        if self.affinity != \"precomputed\":\n            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse feature matrix\nis provided, it will be converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the clustering from features or affinity matrix, and return\ncluster labels.",
      "docstring": "Fit the clustering from features or affinity matrix, and return\ncluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse feature matrix\n    is provided, it will be converted into a sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def fit_predict(self, X, y=None):\n        \"\"\"Fit the clustering from features or affinity matrix, and return\n        cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict",
      "name": "predict",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        if not hasattr(self, \"cluster_centers_\"):\n            raise ValueError(\"Predict method is not supported when \"\n                             \"affinity='precomputed'.\")\n\n        if self.cluster_centers_.shape[0] > 0:\n            with config_context(assume_finite=True):\n                return pairwise_distances_argmin(X, self.cluster_centers_)\n        else:\n            warnings.warn(\"This model does not have any cluster centers \"\n                          \"because affinity propagation did not converge. \"\n                          \"Labeling every sample as '-1'.\", ConvergenceWarning)\n            return np.array([-1] * X.shape[0])"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation",
      "name": "affinity_propagation",
      "qname": "sklearn.cluster._affinity_propagation.affinity_propagation",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/S",
          "name": "S",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.S",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "Matrix of similarities between points."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_samples)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/preference",
          "name": "preference",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.preference",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or float",
            "default_value": "None",
            "description": "Preferences for each point - points with larger values of\npreferences are more likely to be chosen as exemplars. The number of\nexemplars, i.e. of clusters, is influenced by the input preferences\nvalue. If the preferences are not passed as arguments, they will be\nset to the median of the input similarities (resulting in a moderate\nnumber of clusters). For a smaller amount of clusters, this can be set\nto the minimum value of the similarities."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples,)"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/convergence_iter",
          "name": "convergence_iter",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.convergence_iter",
          "default_value": "15",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "15",
            "description": "Number of iterations with no change in the number\nof estimated clusters that stops the convergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/damping",
          "name": "damping",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.damping",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "Damping factor between 0.5 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/copy",
          "name": "copy",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If copy is False, the affinity matrix is modified inplace by the\nalgorithm, for memory efficiency."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "The verbosity level."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.random_state",
          "default_value": "'warn'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "0",
            "description": "Pseudo-random number generator to control the starting state.\nUse an int for reproducible results across function calls.\nSee the :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.23\n    this parameter was previously hardcoded as 0."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.",
      "docstring": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\n\nS : array-like of shape (n_samples, n_samples)\n    Matrix of similarities between points.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number of\n    exemplars, i.e. of clusters, is influenced by the input preferences\n    value. If the preferences are not passed as arguments, they will be\n    set to the median of the input similarities (resulting in a moderate\n    number of clusters). For a smaller amount of clusters, this can be set\n    to the minimum value of the similarities.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\nmax_iter : int, default=200\n    Maximum number of iterations\n\ndamping : float, default=0.5\n    Damping factor between 0.5 and 1.\n\ncopy : bool, default=True\n    If copy is False, the affinity matrix is modified inplace by the\n    algorithm, for memory efficiency.\n\nverbose : bool, default=False\n    The verbosity level.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nReturns\n-------\n\ncluster_centers_indices : ndarray of shape (n_clusters,)\n    Index of clusters centers.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to True.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nWhen the algorithm does not converge, it returns an empty array as\n``cluster_center_indices`` and ``-1`` as label for each training sample.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, a single cluster center\nand label ``0`` for every sample will be returned. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007",
      "code": "@_deprecate_positional_args\ndef affinity_propagation(S, *, preference=None, convergence_iter=15,\n                         max_iter=200, damping=0.5, copy=True, verbose=False,\n                         return_n_iter=False, random_state='warn'):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like of shape (n_samples, n_samples)\n        Matrix of similarities between points.\n\n    preference : array-like of shape (n_samples,) or float, default=None\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, default=15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, default=200\n        Maximum number of iterations\n\n    damping : float, default=0.5\n        Damping factor between 0.5 and 1.\n\n    copy : bool, default=True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency.\n\n    verbose : bool, default=False\n        The verbosity level.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    random_state : int, RandomState instance or None, default=0\n        Pseudo-random number generator to control the starting state.\n        Use an int for reproducible results across function calls.\n        See the :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.23\n            this parameter was previously hardcoded as 0.\n\n    Returns\n    -------\n\n    cluster_centers_indices : ndarray of shape (n_clusters,)\n        Index of clusters centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    When the algorithm does not converge, it returns an empty array as\n    ``cluster_center_indices`` and ``-1`` as label for each training sample.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    \"\"\"\n    S = as_float_array(S, copy=copy)\n    n_samples = S.shape[0]\n\n    if S.shape[0] != S.shape[1]:\n        raise ValueError(\"S must be a square array (shape=%s)\" % repr(S.shape))\n\n    if preference is None:\n        preference = np.median(S)\n    if damping < 0.5 or damping >= 1:\n        raise ValueError('damping must be >= 0.5 and < 1')\n\n    preference = np.array(preference)\n\n    if (n_samples == 1 or\n            _equal_similarities_and_preferences(S, preference)):\n        # It makes no sense to run the algorithm in this case, so return 1 or\n        # n_samples clusters, depending on preferences\n        warnings.warn(\"All samples have mutually equal similarities. \"\n                      \"Returning arbitrary cluster center(s).\")\n        if preference.flat[0] >= S.flat[n_samples - 1]:\n            return ((np.arange(n_samples), np.arange(n_samples), 0)\n                    if return_n_iter\n                    else (np.arange(n_samples), np.arange(n_samples)))\n        else:\n            return ((np.array([0]), np.array([0] * n_samples), 0)\n                    if return_n_iter\n                    else (np.array([0]), np.array([0] * n_samples)))\n\n    if random_state == 'warn':\n        warnings.warn(\n            \"'random_state' has been introduced in 0.23. It will be set to \"\n            \"None starting from 1.0 (renaming of 0.25) which means that \"\n            \"results will differ at every function call. Set 'random_state' \"\n            \"to None to silence this warning, or to 0 to keep the behavior of \"\n            \"versions <0.23.\",\n            FutureWarning\n        )\n        random_state = 0\n    random_state = check_random_state(random_state)\n\n    # Place preference on the diagonal of S\n    S.flat[::(n_samples + 1)] = preference\n\n    A = np.zeros((n_samples, n_samples))\n    R = np.zeros((n_samples, n_samples))  # Initialize messages\n    # Intermediate results\n    tmp = np.zeros((n_samples, n_samples))\n\n    # Remove degeneracies\n    S += ((np.finfo(S.dtype).eps * S + np.finfo(S.dtype).tiny * 100) *\n          random_state.randn(n_samples, n_samples))\n\n    # Execute parallel affinity propagation updates\n    e = np.zeros((n_samples, convergence_iter))\n\n    ind = np.arange(n_samples)\n\n    for it in range(max_iter):\n        # tmp = A + S; compute responsibilities\n        np.add(A, S, tmp)\n        I = np.argmax(tmp, axis=1)\n        Y = tmp[ind, I]  # np.max(A + S, axis=1)\n        tmp[ind, I] = -np.inf\n        Y2 = np.max(tmp, axis=1)\n\n        # tmp = Rnew\n        np.subtract(S, Y[:, None], tmp)\n        tmp[ind, I] = S[ind, I] - Y2\n\n        # Damping\n        tmp *= 1 - damping\n        R *= damping\n        R += tmp\n\n        # tmp = Rp; compute availabilities\n        np.maximum(R, 0, tmp)\n        tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]\n\n        # tmp = -Anew\n        tmp -= np.sum(tmp, axis=0)\n        dA = np.diag(tmp).copy()\n        tmp.clip(0, np.inf, tmp)\n        tmp.flat[::n_samples + 1] = dA\n\n        # Damping\n        tmp *= 1 - damping\n        A *= damping\n        A -= tmp\n\n        # Check for convergence\n        E = (np.diag(A) + np.diag(R)) > 0\n        e[:, it % convergence_iter] = E\n        K = np.sum(E, axis=0)\n\n        if it >= convergence_iter:\n            se = np.sum(e, axis=1)\n            unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                           != n_samples)\n            if (not unconverged and (K > 0)) or (it == max_iter):\n                never_converged = False\n                if verbose:\n                    print(\"Converged after %d iterations.\" % it)\n                break\n    else:\n        never_converged = True\n        if verbose:\n            print(\"Did not converge\")\n\n    I = np.flatnonzero(E)\n    K = I.size  # Identify exemplars\n\n    if K > 0 and not never_converged:\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)  # Identify clusters\n        # Refine the final set of exemplars and clusters and return results\n        for k in range(K):\n            ii = np.where(c == k)[0]\n            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))\n            I[k] = ii[j]\n\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)\n        labels = I[c]\n        # Reduce labels to a sorted, gapless, list\n        cluster_centers_indices = np.unique(labels)\n        labels = np.searchsorted(cluster_centers_indices, labels)\n    else:\n        warnings.warn(\"Affinity propagation did not converge, this model \"\n                      \"will not have any cluster centers.\", ConvergenceWarning)\n        labels = np.array([-1] * n_samples)\n        cluster_centers_indices = []\n\n    if return_n_iter:\n        return cluster_centers_indices, labels, it + 1\n    else:\n        return cluster_centers_indices, labels"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.n_clusters",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "2",
            "description": "The number of clusters to find. It must be ``None`` if\n``distance_threshold`` is not ``None``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'euclidean'",
            "description": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n\"manhattan\", \"cosine\", or \"precomputed\".\nIf linkage is \"ward\", only \"euclidean\" is accepted.\nIf \"precomputed\", a distance matrix (instead of a similarity matrix)\nis needed as input for the fit method."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/memory",
          "name": "memory",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.memory",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or object with the joblib.Memory interface",
            "default_value": "None",
            "description": "Used to cache the output of the computation of the tree.\nBy default, no caching is done. If a string is given, it is the\npath to the caching directory."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "object with the joblib.Memory interface"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.connectivity",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like or callable",
            "default_value": "None",
            "description": "Connectivity matrix. Defines for each sample the neighboring\nsamples following a given structure of the data.\nThis can be a connectivity matrix itself or a callable that transforms\nthe data into a connectivity matrix, such as derived from\nkneighbors_graph. Default is ``None``, i.e, the\nhierarchical clustering algorithm is unstructured."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/compute_full_tree",
          "name": "compute_full_tree",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.compute_full_tree",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or bool",
            "default_value": "'auto'",
            "description": "Stop early the construction of the tree at ``n_clusters``. This is\nuseful to decrease computation time if the number of clusters is not\nsmall compared to the number of samples. This option is useful only\nwhen specifying a connectivity matrix. Note also that when varying the\nnumber of clusters and using caching, it may be advantageous to compute\nthe full tree. It must be ``True`` if ``distance_threshold`` is not\n``None``. By default `compute_full_tree` is \"auto\", which is equivalent\nto `True` when `distance_threshold` is not `None` or that `n_clusters`\nis inferior to the maximum between 100 or `0.02 * n_samples`.\nOtherwise, \"auto\" is equivalent to `False`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/linkage",
          "name": "linkage",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.linkage",
          "default_value": "'ward'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ward', 'complete', 'average', 'single'}",
            "default_value": "'ward'",
            "description": "Which linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of observation. The algorithm will merge\nthe pairs of cluster that minimize this criterion.\n\n- 'ward' minimizes the variance of the clusters being merged.\n- 'average' uses the average of the distances of each observation of\n  the two sets.\n- 'complete' or 'maximum' linkage uses the maximum distances between\n  all observations of the two sets.\n- 'single' uses the minimum of the distances between all observations\n  of the two sets.\n\n.. versionadded:: 0.20\n    Added the 'single' option"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "complete",
              "average",
              "single",
              "ward"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/distance_threshold",
          "name": "distance_threshold",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.distance_threshold",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The linkage distance threshold above which, clusters will not be\nmerged. If not ``None``, ``n_clusters`` must be ``None`` and\n``compute_full_tree`` must be ``True``.\n\n.. versionadded:: 0.21"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/compute_distances",
          "name": "compute_distances",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.compute_distances",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Computes distances between clusters even if `distance_threshold` is not\nused. This can be used to make dendrogram visualization, but introduces\na computational and memory overhead.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', distance_threshold=None,\n                 compute_distances=False):\n        self.n_clusters = n_clusters\n        self.distance_threshold = distance_threshold\n        self.memory = memory\n        self.connectivity = connectivity\n        self.compute_full_tree = compute_full_tree\n        self.linkage = linkage\n        self.affinity = affinity\n        self.compute_distances = compute_distances"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit",
      "name": "fit",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, shape (n_samples, n_features) or (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``affinity='precomputed'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the hierarchical clustering from features, or distance matrix.",
      "docstring": "Fit the hierarchical clustering from features, or distance matrix.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``affinity='precomputed'``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator=self)\n        memory = check_memory(self.memory)\n\n        if self.n_clusters is not None and self.n_clusters <= 0:\n            raise ValueError(\"n_clusters should be an integer greater than 0.\"\n                             \" %s was provided.\" % str(self.n_clusters))\n\n        if not ((self.n_clusters is None) ^ (self.distance_threshold is None)):\n            raise ValueError(\"Exactly one of n_clusters and \"\n                             \"distance_threshold has to be set, and the other \"\n                             \"needs to be None.\")\n\n        if (self.distance_threshold is not None\n                and not self.compute_full_tree):\n            raise ValueError(\"compute_full_tree must be True if \"\n                             \"distance_threshold is set.\")\n\n        if self.linkage == \"ward\" and self.affinity != \"euclidean\":\n            raise ValueError(\"%s was provided as affinity. Ward can only \"\n                             \"work with euclidean distances.\" %\n                             (self.affinity, ))\n\n        if self.linkage not in _TREE_BUILDERS:\n            raise ValueError(\"Unknown linkage type %s. \"\n                             \"Valid options are %s\" % (self.linkage,\n                                                       _TREE_BUILDERS.keys()))\n        tree_builder = _TREE_BUILDERS[self.linkage]\n\n        connectivity = self.connectivity\n        if self.connectivity is not None:\n            if callable(self.connectivity):\n                connectivity = self.connectivity(X)\n            connectivity = check_array(\n                connectivity, accept_sparse=['csr', 'coo', 'lil'])\n\n        n_samples = len(X)\n        compute_full_tree = self.compute_full_tree\n        if self.connectivity is None:\n            compute_full_tree = True\n        if compute_full_tree == 'auto':\n            if self.distance_threshold is not None:\n                compute_full_tree = True\n            else:\n                # Early stopping is likely to give a speed up only for\n                # a large number of clusters. The actual threshold\n                # implemented here is heuristic\n                compute_full_tree = self.n_clusters < max(100, .02 * n_samples)\n        n_clusters = self.n_clusters\n        if compute_full_tree:\n            n_clusters = None\n\n        # Construct the tree\n        kwargs = {}\n        if self.linkage != 'ward':\n            kwargs['linkage'] = self.linkage\n            kwargs['affinity'] = self.affinity\n\n        distance_threshold = self.distance_threshold\n\n        return_distance = (\n            (distance_threshold is not None) or self.compute_distances\n        )\n\n        out = memory.cache(tree_builder)(X, connectivity=connectivity,\n                                         n_clusters=n_clusters,\n                                         return_distance=return_distance,\n                                         **kwargs)\n        (self.children_,\n         self.n_connected_components_,\n         self.n_leaves_,\n         parents) = out[:4]\n\n        if return_distance:\n            self.distances_ = out[-1]\n\n        if self.distance_threshold is not None:  # distance_threshold is used\n            self.n_clusters_ = np.count_nonzero(\n                self.distances_ >= distance_threshold) + 1\n        else:  # n_clusters is used\n            self.n_clusters_ = self.n_clusters\n\n        # Cut the tree\n        if compute_full_tree:\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_,\n                                   self.n_leaves_)\n        else:\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\n            # copy to avoid holding a reference on the original array\n            labels = np.copy(labels[:n_samples])\n            # Reassign cluster numbers\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``affinity='precomputed'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features) or (n_samples, n_samples)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the hierarchical clustering from features or distance matrix,\nand return cluster labels.",
      "docstring": "Fit the hierarchical clustering from features or distance matrix,\nand return cluster labels.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``affinity='precomputed'``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def fit_predict(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.n_clusters",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of clusters to find. It must be ``None`` if\n``distance_threshold`` is not ``None``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'euclidean'",
            "description": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n\"manhattan\", \"cosine\", or 'precomputed'.\nIf linkage is \"ward\", only \"euclidean\" is accepted."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/memory",
          "name": "memory",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.memory",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or object with the joblib.Memory interface",
            "default_value": "None",
            "description": "Used to cache the output of the computation of the tree.\nBy default, no caching is done. If a string is given, it is the\npath to the caching directory."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "object with the joblib.Memory interface"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.connectivity",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like or callable",
            "default_value": "None",
            "description": "Connectivity matrix. Defines for each feature the neighboring\nfeatures following a given structure of the data.\nThis can be a connectivity matrix itself or a callable that transforms\nthe data into a connectivity matrix, such as derived from\nkneighbors_graph. Default is None, i.e, the\nhierarchical clustering algorithm is unstructured."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/compute_full_tree",
          "name": "compute_full_tree",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.compute_full_tree",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or bool",
            "default_value": "'auto'",
            "description": "Stop early the construction of the tree at n_clusters. This is useful\nto decrease computation time if the number of clusters is not small\ncompared to the number of features. This option is useful only when\nspecifying a connectivity matrix. Note also that when varying the\nnumber of clusters and using caching, it may be advantageous to compute\nthe full tree. It must be ``True`` if ``distance_threshold`` is not\n``None``. By default `compute_full_tree` is \"auto\", which is equivalent\nto `True` when `distance_threshold` is not `None` or that `n_clusters`\nis inferior to the maximum between 100 or `0.02 * n_samples`.\nOtherwise, \"auto\" is equivalent to `False`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/linkage",
          "name": "linkage",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.linkage",
          "default_value": "'ward'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ward', 'complete', 'average', 'single'}",
            "default_value": "'ward'",
            "description": "Which linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of features. The algorithm will merge\nthe pairs of cluster that minimize this criterion.\n\n- ward minimizes the variance of the clusters being merged.\n- average uses the average of the distances of each feature of\n  the two sets.\n- complete or maximum linkage uses the maximum distances between\n  all features of the two sets.\n- single uses the minimum of the distances between all features\n  of the two sets."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "complete",
              "average",
              "single",
              "ward"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/pooling_func",
          "name": "pooling_func",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.pooling_func",
          "default_value": "np.mean",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "np.mean",
            "description": "This combines the values of agglomerated features into a single\nvalue, and should accept an array of shape [M, N] and the keyword\nargument `axis=1`, and reduce it to an array of size [M]."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/distance_threshold",
          "name": "distance_threshold",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.distance_threshold",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The linkage distance threshold above which, clusters will not be\nmerged. If not ``None``, ``n_clusters`` must be ``None`` and\n``compute_full_tree`` must be ``True``.\n\n.. versionadded:: 0.21"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/compute_distances",
          "name": "compute_distances",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.compute_distances",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Computes distances between clusters even if `distance_threshold` is not\nused. This can be used to make dendrogram visualization, but introduces\na computational and memory overhead.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', pooling_func=np.mean,\n                 distance_threshold=None, compute_distances=False):\n        super().__init__(\n            n_clusters=n_clusters, memory=memory, connectivity=connectivity,\n            compute_full_tree=compute_full_tree, linkage=linkage,\n            affinity=affinity, distance_threshold=distance_threshold,\n            compute_distances=compute_distances)\n        self.pooling_func = pooling_func"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit",
      "name": "fit",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/params",
          "name": "params",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the hierarchical clustering on the data",
      "docstring": "Fit the hierarchical clustering on the data\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, **params):\n        \"\"\"Fit the hierarchical clustering on the data\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                ensure_min_features=2, estimator=self)\n        # save n_features_in_ attribute here to reset it after, because it will\n        # be overridden in AgglomerativeClustering since we passed it X.T.\n        n_features_in_ = self.n_features_in_\n        AgglomerativeClustering.fit(self, X.T, **params)\n        self.n_features_in_ = n_features_in_\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit_predict@getter",
      "name": "fit_predict",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit_predict",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit_predict@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def fit_predict(self):\n        raise AttributeError"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree",
      "name": "linkage_tree",
      "qname": "sklearn.cluster._agglomerative.linkage_tree",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "feature matrix representing n_samples samples to be clustered"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.connectivity",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "sparse matrix",
            "default_value": "None",
            "description": "connectivity matrix. Defines for each sample the neighboring samples\nfollowing a given structure of the data. The matrix is assumed to\nbe symmetric and only the upper triangular half is used.\nDefault is None, i.e, the Ward algorithm is unstructured."
          },
          "type": {
            "kind": "NamedType",
            "name": "sparse matrix"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.n_clusters",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Stop early the construction of the tree at n_clusters. This is\nuseful to decrease computation time if the number of clusters is\nnot small compared to the number of samples. In this case, the\ncomplete tree is not computed, thus the 'children' output is of\nlimited use, and the 'parents' output should rather be used.\nThis option is valid only when specifying a connectivity matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/linkage",
          "name": "linkage",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.linkage",
          "default_value": "'complete'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{\"average\", \"complete\", \"single\"}",
            "default_value": "\"complete\"",
            "description": "Which linkage criteria to use. The linkage criterion determines which\ndistance to use between sets of observation.\n    - average uses the average of the distances of each observation of\n      the two sets\n    - complete or maximum linkage uses the maximum distances between\n      all observations of the two sets.\n    - single uses the minimum of the distances between all observations\n      of the two sets."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "average",
              "single",
              "complete"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "\"euclidean\".",
            "description": "which metric to use. Can be \"euclidean\", \"manhattan\", or any\ndistance know to paired distance (see metric.pairwise)"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/return_distance",
          "name": "return_distance",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.return_distance",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "whether or not to return the distances between the clusters."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Linkage agglomerative clustering based on a Feature matrix.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Linkage agglomerative clustering based on a Feature matrix.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    feature matrix representing n_samples samples to be clustered\n\nconnectivity : sparse matrix, default=None\n    connectivity matrix. Defines for each sample the neighboring samples\n    following a given structure of the data. The matrix is assumed to\n    be symmetric and only the upper triangular half is used.\n    Default is None, i.e, the Ward algorithm is unstructured.\n\nn_clusters : int, default=None\n    Stop early the construction of the tree at n_clusters. This is\n    useful to decrease computation time if the number of clusters is\n    not small compared to the number of samples. In this case, the\n    complete tree is not computed, thus the 'children' output is of\n    limited use, and the 'parents' output should rather be used.\n    This option is valid only when specifying a connectivity matrix.\n\nlinkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n    Which linkage criteria to use. The linkage criterion determines which\n    distance to use between sets of observation.\n        - average uses the average of the distances of each observation of\n          the two sets\n        - complete or maximum linkage uses the maximum distances between\n          all observations of the two sets.\n        - single uses the minimum of the distances between all observations\n          of the two sets.\n\naffinity : str or callable, default=\"euclidean\".\n    which metric to use. Can be \"euclidean\", \"manhattan\", or any\n    distance know to paired distance (see metric.pairwise)\n\nreturn_distance : bool, default=False\n    whether or not to return the distances between the clusters.\n\nReturns\n-------\nchildren : ndarray of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\nn_connected_components : int\n    The number of connected components in the graph.\n\nn_leaves : int\n    The number of leaves in the tree.\n\nparents : ndarray of shape (n_nodes, ) or None\n    The parent of each node. Only returned when a connectivity matrix\n    is specified, elsewhere 'None' is returned.\n\ndistances : ndarray of shape (n_nodes-1,)\n    Returned when return_distance is set to True.\n\n    distances[i] refers to the distance between children[i][0] and\n    children[i][1] when they are merged.\n\nSee Also\n--------\nward_tree : Hierarchical clustering with ward linkage.",
      "code": "def linkage_tree(X, connectivity=None, n_clusters=None, linkage='complete',\n                 affinity=\"euclidean\", return_distance=False):\n    \"\"\"Linkage agglomerative clustering based on a Feature matrix.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        feature matrix representing n_samples samples to be clustered\n\n    connectivity : sparse matrix, default=None\n        connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int, default=None\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the 'children' output is of\n        limited use, and the 'parents' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    linkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n        Which linkage criteria to use. The linkage criterion determines which\n        distance to use between sets of observation.\n            - average uses the average of the distances of each observation of\n              the two sets\n            - complete or maximum linkage uses the maximum distances between\n              all observations of the two sets.\n            - single uses the minimum of the distances between all observations\n              of the two sets.\n\n    affinity : str or callable, default=\"euclidean\".\n        which metric to use. Can be \"euclidean\", \"manhattan\", or any\n        distance know to paired distance (see metric.pairwise)\n\n    return_distance : bool, default=False\n        whether or not to return the distances between the clusters.\n\n    Returns\n    -------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : ndarray of shape (n_nodes, ) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Returned when return_distance is set to True.\n\n        distances[i] refers to the distance between children[i][0] and\n        children[i][1] when they are merged.\n\n    See Also\n    --------\n    ward_tree : Hierarchical clustering with ward linkage.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = np.reshape(X, (-1, 1))\n    n_samples, n_features = X.shape\n\n    linkage_choices = {'complete': _hierarchical.max_merge,\n                       'average': _hierarchical.average_merge,\n                       'single': None}  # Single linkage is handled differently\n    try:\n        join_func = linkage_choices[linkage]\n    except KeyError as e:\n        raise ValueError(\n            'Unknown linkage option, linkage should be one '\n            'of %s, but %s was given' % (linkage_choices.keys(), linkage)\n        ) from e\n\n    if affinity == 'cosine' and np.any(~np.any(X, axis=1)):\n        raise ValueError(\n            'Cosine affinity cannot be used when X contains zero vectors')\n\n    if connectivity is None:\n        from scipy.cluster import hierarchy  # imports PIL\n\n        if n_clusters is not None:\n            warnings.warn('Partial build of the tree is implemented '\n                          'only for structured clustering (i.e. with '\n                          'explicit connectivity). The algorithm '\n                          'will build the full tree and only '\n                          'retain the lower branches required '\n                          'for the specified number of clusters',\n                          stacklevel=2)\n\n        if affinity == 'precomputed':\n            # for the linkage function of hierarchy to work on precomputed\n            # data, provide as first argument an ndarray of the shape returned\n            # by sklearn.metrics.pairwise_distances.\n            if X.shape[0] != X.shape[1]:\n                raise ValueError(\n                    'Distance matrix should be square, '\n                    'Got matrix of shape {X.shape}'\n                )\n            i, j = np.triu_indices(X.shape[0], k=1)\n            X = X[i, j]\n        elif affinity == 'l2':\n            # Translate to something understood by scipy\n            affinity = 'euclidean'\n        elif affinity in ('l1', 'manhattan'):\n            affinity = 'cityblock'\n        elif callable(affinity):\n            X = affinity(X)\n            i, j = np.triu_indices(X.shape[0], k=1)\n            X = X[i, j]\n        if (linkage == 'single'\n                and affinity != 'precomputed'\n                and not callable(affinity)\n                and affinity in METRIC_MAPPING):\n\n            # We need the fast cythonized metric from neighbors\n            dist_metric = DistanceMetric.get_metric(affinity)\n\n            # The Cython routines used require contiguous arrays\n            X = np.ascontiguousarray(X, dtype=np.double)\n\n            mst = _hierarchical.mst_linkage_core(X, dist_metric)\n            # Sort edges of the min_spanning_tree by weight\n            mst = mst[np.argsort(mst.T[2], kind='mergesort'), :]\n\n            # Convert edge list into standard hierarchical clustering format\n            out = _hierarchical.single_linkage_label(mst)\n        else:\n            out = hierarchy.linkage(X, method=linkage, metric=affinity)\n        children_ = out[:, :2].astype(int, copy=False)\n\n        if return_distance:\n            distances = out[:, 2]\n            return children_, 1, n_samples, None, distances\n        return children_, 1, n_samples, None\n\n    connectivity, n_connected_components = _fix_connectivity(\n                                                X, connectivity,\n                                                affinity=affinity)\n    connectivity = connectivity.tocoo()\n    # Put the diagonal to zero\n    diag_mask = (connectivity.row != connectivity.col)\n    connectivity.row = connectivity.row[diag_mask]\n    connectivity.col = connectivity.col[diag_mask]\n    connectivity.data = connectivity.data[diag_mask]\n    del diag_mask\n\n    if affinity == 'precomputed':\n        distances = X[connectivity.row, connectivity.col].astype(\n            'float64', **_astype_copy_false(X))\n    else:\n        # FIXME We compute all the distances, while we could have only computed\n        # the \"interesting\" distances\n        distances = paired_distances(X[connectivity.row],\n                                     X[connectivity.col],\n                                     metric=affinity)\n    connectivity.data = distances\n\n    if n_clusters is None:\n        n_nodes = 2 * n_samples - 1\n    else:\n        assert n_clusters <= n_samples\n        n_nodes = 2 * n_samples - n_clusters\n\n    if linkage == 'single':\n        return _single_linkage_tree(connectivity, n_samples, n_nodes,\n                                    n_clusters, n_connected_components,\n                                    return_distance)\n\n    if return_distance:\n        distances = np.empty(n_nodes - n_samples)\n    # create inertia heap and connection matrix\n    A = np.empty(n_nodes, dtype=object)\n    inertia = list()\n\n    # LIL seems to the best format to access the rows quickly,\n    # without the numpy overhead of slicing CSR indices and data.\n    connectivity = connectivity.tolil()\n    # We are storing the graph in a list of IntFloatDict\n    for ind, (data, row) in enumerate(zip(connectivity.data,\n                                          connectivity.rows)):\n        A[ind] = IntFloatDict(np.asarray(row, dtype=np.intp),\n                              np.asarray(data, dtype=np.float64))\n        # We keep only the upper triangular for the heap\n        # Generator expressions are faster than arrays on the following\n        inertia.extend(_hierarchical.WeightedEdge(d, ind, r)\n                       for r, d in zip(row, data) if r < ind)\n    del connectivity\n\n    heapify(inertia)\n\n    # prepare the main fields\n    parent = np.arange(n_nodes, dtype=np.intp)\n    used_node = np.ones(n_nodes, dtype=np.intp)\n    children = []\n\n    # recursive merge loop\n    for k in range(n_samples, n_nodes):\n        # identify the merge\n        while True:\n            edge = heappop(inertia)\n            if used_node[edge.a] and used_node[edge.b]:\n                break\n        i = edge.a\n        j = edge.b\n\n        if return_distance:\n            # store distances\n            distances[k - n_samples] = edge.weight\n\n        parent[i] = parent[j] = k\n        children.append((i, j))\n        # Keep track of the number of elements per cluster\n        n_i = used_node[i]\n        n_j = used_node[j]\n        used_node[k] = n_i + n_j\n        used_node[i] = used_node[j] = False\n\n        # update the structure matrix A and the inertia matrix\n        # a clever 'min', or 'max' operation between A[i] and A[j]\n        coord_col = join_func(A[i], A[j], used_node, n_i, n_j)\n        for col, d in coord_col:\n            A[col].append(k, d)\n            # Here we use the information from coord_col (containing the\n            # distances) to update the heap\n            heappush(inertia, _hierarchical.WeightedEdge(d, k, col))\n        A[k] = coord_col\n        # Clear A[i] and A[j] to save memory\n        A[i] = A[j] = 0\n\n    # Separate leaves in children (empty lists up to now)\n    n_leaves = n_samples\n\n    # # return numpy array for efficient caching\n    children = np.array(children)[:, ::-1]\n\n    if return_distance:\n        return children, n_connected_components, n_leaves, parent, distances\n    return children, n_connected_components, n_leaves, parent"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree",
      "name": "ward_tree",
      "qname": "sklearn.cluster._agglomerative.ward_tree",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.ward_tree.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "feature matrix representing n_samples samples to be clustered"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.ward_tree.connectivity",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "sparse matrix",
            "default_value": "None",
            "description": "connectivity matrix. Defines for each sample the neighboring samples\nfollowing a given structure of the data. The matrix is assumed to\nbe symmetric and only the upper triangular half is used.\nDefault is None, i.e, the Ward algorithm is unstructured."
          },
          "type": {
            "kind": "NamedType",
            "name": "sparse matrix"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.ward_tree.n_clusters",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Stop early the construction of the tree at n_clusters. This is\nuseful to decrease computation time if the number of clusters is\nnot small compared to the number of samples. In this case, the\ncomplete tree is not computed, thus the 'children' output is of\nlimited use, and the 'parents' output should rather be used.\nThis option is valid only when specifying a connectivity matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/return_distance",
          "name": "return_distance",
          "qname": "sklearn.cluster._agglomerative.ward_tree.return_distance",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "None",
            "description": "If True, return the distance between the clusters."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Ward clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases\nwithin-cluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Ward clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases\nwithin-cluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    feature matrix representing n_samples samples to be clustered\n\nconnectivity : sparse matrix, default=None\n    connectivity matrix. Defines for each sample the neighboring samples\n    following a given structure of the data. The matrix is assumed to\n    be symmetric and only the upper triangular half is used.\n    Default is None, i.e, the Ward algorithm is unstructured.\n\nn_clusters : int, default=None\n    Stop early the construction of the tree at n_clusters. This is\n    useful to decrease computation time if the number of clusters is\n    not small compared to the number of samples. In this case, the\n    complete tree is not computed, thus the 'children' output is of\n    limited use, and the 'parents' output should rather be used.\n    This option is valid only when specifying a connectivity matrix.\n\nreturn_distance : bool, default=None\n    If True, return the distance between the clusters.\n\nReturns\n-------\nchildren : ndarray of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\nn_connected_components : int\n    The number of connected components in the graph.\n\nn_leaves : int\n    The number of leaves in the tree\n\nparents : ndarray of shape (n_nodes,) or None\n    The parent of each node. Only returned when a connectivity matrix\n    is specified, elsewhere 'None' is returned.\n\ndistances : ndarray of shape (n_nodes-1,)\n    Only returned if return_distance is set to True (for compatibility).\n    The distances between the centers of the nodes. `distances[i]`\n    corresponds to a weighted euclidean distance between\n    the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n    leaves of the tree, then `distances[i]` is their unweighted euclidean\n    distance. Distances are updated in the following way\n    (from scipy.hierarchy.linkage):\n\n    The new entry :math:`d(u,v)` is computed as follows,\n\n    .. math::\n\n       d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                           {T}d(v,s)^2\n                    + \\frac{|v|+|t|}\n                           {T}d(v,t)^2\n                    - \\frac{|v|}\n                           {T}d(s,t)^2}\n\n    where :math:`u` is the newly joined cluster consisting of\n    clusters :math:`s` and :math:`t`, :math:`v` is an unused\n    cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n    :math:`|*|` is the cardinality of its argument. This is also\n    known as the incremental algorithm.",
      "code": "@_deprecate_positional_args\ndef ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False):\n    \"\"\"Ward clustering based on a Feature matrix.\n\n    Recursively merges the pair of clusters that minimally increases\n    within-cluster variance.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        feature matrix representing n_samples samples to be clustered\n\n    connectivity : sparse matrix, default=None\n        connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int, default=None\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the 'children' output is of\n        limited use, and the 'parents' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    return_distance : bool, default=None\n        If True, return the distance between the clusters.\n\n    Returns\n    -------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree\n\n    parents : ndarray of shape (n_nodes,) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Only returned if return_distance is set to True (for compatibility).\n        The distances between the centers of the nodes. `distances[i]`\n        corresponds to a weighted euclidean distance between\n        the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n        leaves of the tree, then `distances[i]` is their unweighted euclidean\n        distance. Distances are updated in the following way\n        (from scipy.hierarchy.linkage):\n\n        The new entry :math:`d(u,v)` is computed as follows,\n\n        .. math::\n\n           d(u,v) = \\\\sqrt{\\\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\\\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\\\frac{|v|}\n                               {T}d(s,t)^2}\n\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = np.reshape(X, (-1, 1))\n    n_samples, n_features = X.shape\n\n    if connectivity is None:\n        from scipy.cluster import hierarchy  # imports PIL\n\n        if n_clusters is not None:\n            warnings.warn('Partial build of the tree is implemented '\n                          'only for structured clustering (i.e. with '\n                          'explicit connectivity). The algorithm '\n                          'will build the full tree and only '\n                          'retain the lower branches required '\n                          'for the specified number of clusters',\n                          stacklevel=2)\n        X = np.require(X, requirements=\"W\")\n        out = hierarchy.ward(X)\n        children_ = out[:, :2].astype(np.intp)\n\n        if return_distance:\n            distances = out[:, 2]\n            return children_, 1, n_samples, None, distances\n        else:\n            return children_, 1, n_samples, None\n\n    connectivity, n_connected_components = _fix_connectivity(\n                                                X, connectivity,\n                                                affinity='euclidean')\n    if n_clusters is None:\n        n_nodes = 2 * n_samples - 1\n    else:\n        if n_clusters > n_samples:\n            raise ValueError('Cannot provide more clusters than samples. '\n                             '%i n_clusters was asked, and there are %i '\n                             'samples.' % (n_clusters, n_samples))\n        n_nodes = 2 * n_samples - n_clusters\n\n    # create inertia matrix\n    coord_row = []\n    coord_col = []\n    A = []\n    for ind, row in enumerate(connectivity.rows):\n        A.append(row)\n        # We keep only the upper triangular for the moments\n        # Generator expressions are faster than arrays on the following\n        row = [i for i in row if i < ind]\n        coord_row.extend(len(row) * [ind, ])\n        coord_col.extend(row)\n\n    coord_row = np.array(coord_row, dtype=np.intp, order='C')\n    coord_col = np.array(coord_col, dtype=np.intp, order='C')\n\n    # build moments as a list\n    moments_1 = np.zeros(n_nodes, order='C')\n    moments_1[:n_samples] = 1\n    moments_2 = np.zeros((n_nodes, n_features), order='C')\n    moments_2[:n_samples] = X\n    inertia = np.empty(len(coord_row), dtype=np.float64, order='C')\n    _hierarchical.compute_ward_dist(moments_1, moments_2, coord_row, coord_col,\n                                    inertia)\n    inertia = list(zip(inertia, coord_row, coord_col))\n    heapify(inertia)\n\n    # prepare the main fields\n    parent = np.arange(n_nodes, dtype=np.intp)\n    used_node = np.ones(n_nodes, dtype=bool)\n    children = []\n    if return_distance:\n        distances = np.empty(n_nodes - n_samples)\n\n    not_visited = np.empty(n_nodes, dtype=np.int8, order='C')\n\n    # recursive merge loop\n    for k in range(n_samples, n_nodes):\n        # identify the merge\n        while True:\n            inert, i, j = heappop(inertia)\n            if used_node[i] and used_node[j]:\n                break\n        parent[i], parent[j] = k, k\n        children.append((i, j))\n        used_node[i] = used_node[j] = False\n        if return_distance:  # store inertia value\n            distances[k - n_samples] = inert\n\n        # update the moments\n        moments_1[k] = moments_1[i] + moments_1[j]\n        moments_2[k] = moments_2[i] + moments_2[j]\n\n        # update the structure matrix A and the inertia matrix\n        coord_col = []\n        not_visited.fill(1)\n        not_visited[k] = 0\n        _hierarchical._get_parents(A[i], coord_col, parent, not_visited)\n        _hierarchical._get_parents(A[j], coord_col, parent, not_visited)\n        # List comprehension is faster than a for loop\n        [A[col].append(k) for col in coord_col]\n        A.append(coord_col)\n        coord_col = np.array(coord_col, dtype=np.intp, order='C')\n        coord_row = np.empty(coord_col.shape, dtype=np.intp, order='C')\n        coord_row.fill(k)\n        n_additions = len(coord_row)\n        ini = np.empty(n_additions, dtype=np.float64, order='C')\n\n        _hierarchical.compute_ward_dist(moments_1, moments_2,\n                                        coord_row, coord_col, ini)\n\n        # List comprehension is faster than a for loop\n        [heappush(inertia, (ini[idx], k, coord_col[idx]))\n            for idx in range(n_additions)]\n\n    # Separate leaves in children (empty lists up to now)\n    n_leaves = n_samples\n    # sort children to get consistent output with unstructured version\n    children = [c[::-1] for c in children]\n    children = np.array(children)  # return numpy array for efficient caching\n\n    if return_distance:\n        # 2 is scaling factor to compare w/ unstructured version\n        distances = np.sqrt(2. * distances)\n        return children, n_connected_components, n_leaves, parent, distances\n    else:\n        return children, n_connected_components, n_leaves, parent"
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_clusters",
          "default_value": "3",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or tuple (n_row_clusters, n_column_clusters)",
            "default_value": "3",
            "description": "The number of row and column clusters in the checkerboard\nstructure."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "tuple (n_row_clusters, n_column_clusters)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/method",
          "name": "method",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.method",
          "default_value": "'bistochastic'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'bistochastic', 'scale', 'log'}",
            "default_value": "'bistochastic'",
            "description": "Method of normalizing and converting singular vectors into\nbiclusters. May be one of 'scale', 'bistochastic', or 'log'.\nThe authors recommend using 'log'. If the data is sparse,\nhowever, log normalization will not work, which is why the\ndefault is 'bistochastic'.\n\n.. warning::\n   if `method='log'`, the data must be sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "scale",
              "bistochastic",
              "log"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_components",
          "default_value": "6",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "6",
            "description": "Number of singular vectors to check."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_best",
          "name": "n_best",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_best",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of best singular vectors to which to project the data\nfor clustering."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/svd_method",
          "name": "svd_method",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.svd_method",
          "default_value": "'randomized'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'randomized', 'arpack'}",
            "default_value": "'randomized'",
            "description": "Selects the algorithm for finding singular vectors. May be\n'randomized' or 'arpack'. If 'randomized', uses\n:func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\nfor large matrices. If 'arpack', uses\n`scipy.sparse.linalg.svds`, which is more accurate, but\npossibly slower in some cases."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "arpack",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_svd_vecs",
          "name": "n_svd_vecs",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_svd_vecs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of vectors to use in calculating the SVD. Corresponds\nto `ncv` when `svd_method=arpack` and `n_oversamples` when\n`svd_method` is 'randomized`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/mini_batch",
          "name": "mini_batch",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.mini_batch",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use mini-batch k-means, which is faster but may get\ndifferent results."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'} or ndarray of (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization of k-means algorithm; defaults to\n'k-means++'."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "ndarray of (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of random initializations that are tried with the\nk-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is\nchosen and the algorithm runs once. Otherwise, the algorithm\nis run for each initialization and the best solution chosen."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by breaking\ndown the pairwise matrix into n_jobs even slices and computing them in\nparallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "Used for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, method='bistochastic',\n                 n_components=6, n_best=3, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)\n        self.method = method\n        self.n_components = n_components\n        self.n_best = n_best"
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_clusters",
          "default_value": "3",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "The number of biclusters to find."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/svd_method",
          "name": "svd_method",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.svd_method",
          "default_value": "'randomized'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'randomized', 'arpack'}",
            "default_value": "'randomized'",
            "description": "Selects the algorithm for finding singular vectors. May be\n'randomized' or 'arpack'. If 'randomized', use\n:func:`sklearn.utils.extmath.randomized_svd`, which may be faster\nfor large matrices. If 'arpack', use\n:func:`scipy.sparse.linalg.svds`, which is more accurate, but\npossibly slower in some cases."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "arpack",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_svd_vecs",
          "name": "n_svd_vecs",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_svd_vecs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of vectors to use in calculating the SVD. Corresponds\nto `ncv` when `svd_method=arpack` and `n_oversamples` when\n`svd_method` is 'randomized`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/mini_batch",
          "name": "mini_batch",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.mini_batch",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use mini-batch k-means, which is faster but may get\ndifferent results."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random', or ndarray of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization of k-means algorithm; defaults to\n'k-means++'."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "{'k-means++'"
              },
              {
                "kind": "NamedType",
                "name": "'random'"
              },
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of random initializations that are tried with the\nk-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is\nchosen and the algorithm runs once. Otherwise, the algorithm\nis run for each initialization and the best solution chosen."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by breaking\ndown the pairwise matrix into n_jobs even slices and computing them in\nparallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "Used for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._birch.Birch.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/threshold",
          "name": "threshold",
          "qname": "sklearn.cluster._birch.Birch.__init__.threshold",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The radius of the subcluster obtained by merging a new sample and the\nclosest subcluster should be lesser than the threshold. Otherwise a new\nsubcluster is started. Setting this value to be very low promotes\nsplitting and vice-versa."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/branching_factor",
          "name": "branching_factor",
          "qname": "sklearn.cluster._birch.Birch.__init__.branching_factor",
          "default_value": "50",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "50",
            "description": "Maximum number of CF subclusters in each node. If a new samples enters\nsuch that the number of subclusters exceed the branching_factor then\nthat node is split into two nodes with the subclusters redistributed\nin each. The parent subcluster of that node is removed and two new\nsubclusters are added as parents of the 2 split nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._birch.Birch.__init__.n_clusters",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, instance of sklearn.cluster model",
            "default_value": "3",
            "description": "Number of clusters after the final clustering step, which treats the\nsubclusters from the leaves as new samples.\n\n- `None` : the final clustering step is not performed and the\n  subclusters are returned as they are.\n\n- :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n  is fit treating the subclusters as new samples and the initial data\n  is mapped to the label of the closest subcluster.\n\n- `int` : the model fit is :class:`AgglomerativeClustering` with\n  `n_clusters` set to be equal to the int."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "instance of sklearn.cluster model"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/compute_labels",
          "name": "compute_labels",
          "qname": "sklearn.cluster._birch.Birch.__init__.compute_labels",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to compute labels for each fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cluster._birch.Birch.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to make a copy of the given data. If set to False,\nthe initial data will be overwritten."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, threshold=0.5, branching_factor=50, n_clusters=3,\n                 compute_labels=True, copy=True):\n        self.threshold = threshold\n        self.branching_factor = branching_factor\n        self.n_clusters = n_clusters\n        self.compute_labels = compute_labels\n        self.copy = copy"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/fit",
      "name": "fit",
      "qname": "sklearn.cluster._birch.Birch.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._birch.Birch.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Build a CF Tree for the input data.",
      "docstring": "Build a CF Tree for the input data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself\n    Fitted estimator.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"\n        Build a CF Tree for the input data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.fit_, self.partial_fit_ = True, False\n        return self._fit(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.cluster._birch.Birch.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.partial_fit.X",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "None",
            "description": "Input data. If X is not provided, only the global clustering\nstep is done."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit/y",
          "name": "y",
          "qname": "sklearn.cluster._birch.Birch.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Online learning. Prevents rebuilding of CFTree from scratch.",
      "docstring": "Online learning. Prevents rebuilding of CFTree from scratch.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features),             default=None\n    Input data. If X is not provided, only the global clustering\n    step is done.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself\n    Fitted estimator.",
      "code": "    def partial_fit(self, X=None, y=None):\n        \"\"\"\n        Online learning. Prevents rebuilding of CFTree from scratch.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), \\\n            default=None\n            Input data. If X is not provided, only the global clustering\n            step is done.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.partial_fit_, self.fit_ = True, False\n        if X is None:\n            # Perform just the final global clustering step.\n            self._global_clustering()\n            return self\n        else:\n            return self._fit(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/predict",
      "name": "predict",
      "qname": "sklearn.cluster._birch.Birch.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict data using the ``centroids_`` of subclusters.\n\nAvoid computation of the row norms of X.",
      "docstring": "Predict data using the ``centroids_`` of subclusters.\n\nAvoid computation of the row norms of X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nlabels : ndarray of shape(n_samples,)\n    Labelled data.",
      "code": "    def predict(self, X):\n        \"\"\"\n        Predict data using the ``centroids_`` of subclusters.\n\n        Avoid computation of the row norms of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        labels : ndarray of shape(n_samples,)\n            Labelled data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n        kwargs = {'Y_norm_squared': self._subcluster_norms}\n\n        with config_context(assume_finite=True):\n            argmin = pairwise_distances_argmin(X, self.subcluster_centers_,\n                                               metric_kwargs=kwargs)\n        return self.subcluster_labels_[argmin]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/transform",
      "name": "transform",
      "qname": "sklearn.cluster._birch.Birch.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/transform/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/transform/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each\ncluster centroid.",
      "docstring": "Transform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each\ncluster centroid.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nX_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n    Transformed data.",
      "code": "    def transform(self, X):\n        \"\"\"\n        Transform X into subcluster centroids dimension.\n\n        Each dimension represents the distance from the sample point to each\n        cluster centroid.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        self._validate_data(X, accept_sparse='csr', reset=False)\n        with config_context(assume_finite=True):\n            return euclidean_distances(X, self.subcluster_centers_)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._dbscan.DBSCAN.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/eps",
          "name": "eps",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.eps",
          "default_value": "0.5",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The maximum distance between two samples for one to be considered\nas in the neighborhood of the other. This is not a maximum bound\non the distances of points within a cluster. This is the most\nimportant DBSCAN parameter to choose appropriately for your data set\nand distance function."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.min_samples",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "The number of samples (or total weight) in a neighborhood for a point\nto be considered as a core point. This includes the point itself."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/metric",
          "name": "metric",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.metric",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string, or callable",
            "default_value": "'euclidean'",
            "description": "The metric to use when calculating distance between instances in a\nfeature array. If metric is a string or callable, it must be one of\nthe options allowed by :func:`sklearn.metrics.pairwise_distances` for\nits metric parameter.\nIf metric is \"precomputed\", X is assumed to be a distance matrix and\nmust be square. X may be a :term:`Glossary <sparse graph>`, in which\ncase only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n.. versionadded:: 0.17\n   metric *precomputed* to accept precomputed sparse matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "string"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.metric_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "The algorithm to be used by the NearestNeighbors module\nto compute pointwise distances and find nearest neighbors.\nSee NearestNeighbors module documentation for details."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ball_tree",
              "kd_tree",
              "auto",
              "brute"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.leaf_size",
          "default_value": "30",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to BallTree or cKDTree. This can affect the speed\nof the construction and query, as well as the memory required\nto store the tree. The optimal value depends\non the nature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/p",
          "name": "p",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.p",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The power of the Minkowski metric to be used to calculate distance\nbetween points. If None, then ``p=2`` (equivalent to the Euclidean\ndistance)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, eps=0.5, *, min_samples=5, metric='euclidean',\n                 metric_params=None, algorithm='auto', leaf_size=30, p=None,\n                 n_jobs=None):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.metric = metric\n        self.metric_params = metric_params\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.p = p\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit",
      "name": "fit",
      "qname": "sklearn.cluster._dbscan.DBSCAN.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``metric='precomputed'``. If a sparse matrix is provided, it will\nbe converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with a\nnegative weight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform DBSCAN clustering from features, or distance matrix.",
      "docstring": "Perform DBSCAN clustering from features, or distance matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``metric='precomputed'``. If a sparse matrix is provided, it will\n    be converted into a sparse ``csr_matrix``.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with a\n    negative weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr')\n\n        if not self.eps > 0.0:\n            raise ValueError(\"eps must be positive.\")\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        # Calculate neighborhood for all samples. This leaves the original\n        # point in, which needs to be considered later (i.e. point i is in the\n        # neighborhood of point i. While True, its useless information)\n        if self.metric == 'precomputed' and sparse.issparse(X):\n            # set the diagonal to explicit values, as a point is its own\n            # neighbor\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n                X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n\n        neighbors_model = NearestNeighbors(\n            radius=self.eps, algorithm=self.algorithm,\n            leaf_size=self.leaf_size, metric=self.metric,\n            metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n        neighbors_model.fit(X)\n        # This has worst case O(n^2) memory complexity\n        neighborhoods = neighbors_model.radius_neighbors(X,\n                                                         return_distance=False)\n\n        if sample_weight is None:\n            n_neighbors = np.array([len(neighbors)\n                                    for neighbors in neighborhoods])\n        else:\n            n_neighbors = np.array([np.sum(sample_weight[neighbors])\n                                    for neighbors in neighborhoods])\n\n        # Initially, all samples are noise.\n        labels = np.full(X.shape[0], -1, dtype=np.intp)\n\n        # A list of all core samples found.\n        core_samples = np.asarray(n_neighbors >= self.min_samples,\n                                  dtype=np.uint8)\n        dbscan_inner(core_samples, neighborhoods, labels)\n\n        self.core_sample_indices_ = np.where(core_samples)[0]\n        self.labels_ = labels\n\n        if len(self.core_sample_indices_):\n            # fix for scipy sparse indexing issue\n            self.components_ = X[self.core_sample_indices_].copy()\n        else:\n            # no core samples\n            self.components_ = np.empty((0, X.shape[1]))\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``metric='precomputed'``. If a sparse matrix is provided, it will\nbe converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with a\nnegative weight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform DBSCAN clustering from features or distance matrix,\nand return cluster labels.",
      "docstring": "Perform DBSCAN clustering from features or distance matrix,\nand return cluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``metric='precomputed'``. If a sparse matrix is provided, it will\n    be converted into a sparse ``csr_matrix``.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with a\n    negative weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels. Noisy samples are given the label -1.",
      "code": "    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.\n        \"\"\"\n        self.fit(X, sample_weight=sample_weight)\n        return self.labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/dbscan",
      "name": "dbscan",
      "qname": "sklearn.cluster._dbscan.dbscan",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/X",
          "name": "X",
          "qname": "sklearn.cluster._dbscan.dbscan.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)",
            "default_value": "",
            "description": "A feature array, or array of distances between samples if\n``metric='precomputed'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/eps",
          "name": "eps",
          "qname": "sklearn.cluster._dbscan.dbscan.eps",
          "default_value": "0.5",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The maximum distance between two samples for one to be considered\nas in the neighborhood of the other. This is not a maximum bound\non the distances of points within a cluster. This is the most\nimportant DBSCAN parameter to choose appropriately for your data set\nand distance function."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._dbscan.dbscan.min_samples",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "The number of samples (or total weight) in a neighborhood for a point\nto be considered as a core point. This includes the point itself."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/metric",
          "name": "metric",
          "qname": "sklearn.cluster._dbscan.dbscan.metric",
          "default_value": "'minkowski'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'minkowski'",
            "description": "The metric to use when calculating distance between instances in a\nfeature array. If metric is a string or callable, it must be one of\nthe options allowed by :func:`sklearn.metrics.pairwise_distances` for\nits metric parameter.\nIf metric is \"precomputed\", X is assumed to be a distance matrix and\nmust be square during fit.\nX may be a :term:`sparse graph <sparse graph>`,\nin which case only \"nonzero\" elements may be considered neighbors."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._dbscan.dbscan.metric_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._dbscan.dbscan.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "The algorithm to be used by the NearestNeighbors module\nto compute pointwise distances and find nearest neighbors.\nSee NearestNeighbors module documentation for details."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ball_tree",
              "kd_tree",
              "auto",
              "brute"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._dbscan.dbscan.leaf_size",
          "default_value": "30",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to BallTree or cKDTree. This can affect the speed\nof the construction and query, as well as the memory required\nto store the tree. The optimal value depends\non the nature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/p",
          "name": "p",
          "qname": "sklearn.cluster._dbscan.dbscan.p",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "2",
            "description": "The power of the Minkowski metric to be used to calculate distance\nbetween points."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._dbscan.dbscan.sample_weight",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with negative\nweight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._dbscan.dbscan.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search. ``None`` means\n1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\nusing all processors. See :term:`Glossary <n_jobs>` for more details.\nIf precomputed distance are used, parallel execution is not available\nand thus n_jobs will have no effect."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.",
      "docstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\nX : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)\n    A feature array, or array of distances between samples if\n    ``metric='precomputed'``.\n\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : str or callable, default='minkowski'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit.\n    X may be a :term:`sparse graph <sparse graph>`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=2\n    The power of the Minkowski metric to be used to calculate distance\n    between points.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with negative\n    weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search. ``None`` means\n    1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n    using all processors. See :term:`Glossary <n_jobs>` for more details.\n    If precomputed distance are used, parallel execution is not available\n    and thus n_jobs will have no effect.\n\nReturns\n-------\ncore_samples : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.  Noisy samples are given the label -1.\n\nSee Also\n--------\nDBSCAN : An estimator interface for this clustering algorithm.\nOPTICS : A similar estimator interface clustering at multiple values of\n    eps. Our implementation is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:func:`cluster.optics <sklearn.cluster.optics>` provides a similar\nclustering with lower memory usage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\".\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.",
      "code": "@_deprecate_positional_args\ndef dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski',\n           metric_params=None, algorithm='auto', leaf_size=30, p=2,\n           sample_weight=None, n_jobs=None):\n    \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or \\\n            (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric='precomputed'``.\n\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : str or callable, default='minkowski'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit.\n        X may be a :term:`sparse graph <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=2\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weight of each sample, such that a sample with a weight of at least\n        ``min_samples`` is by itself a core sample; a sample with negative\n        weight may inhibit its eps-neighbor from being core.\n        Note that weights are absolute, and default to 1.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. ``None`` means\n        1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n        using all processors. See :term:`Glossary <n_jobs>` for more details.\n        If precomputed distance are used, parallel execution is not available\n        and thus n_jobs will have no effect.\n\n    Returns\n    -------\n    core_samples : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    See Also\n    --------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :func:`cluster.optics <sklearn.cluster.optics>` provides a similar\n    clustering with lower memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    \"\"\"\n\n    est = DBSCAN(eps=eps, min_samples=min_samples, metric=metric,\n                 metric_params=metric_params, algorithm=algorithm,\n                 leaf_size=leaf_size, p=p, n_jobs=n_jobs)\n    est.fit(X, sample_weight=sample_weight)\n    return est.core_sample_indices_, est.labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._kmeans.KMeans.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.n_clusters",
          "default_value": "8",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "8",
            "description": "The number of clusters to form as well as the number of\ncentroids to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization:\n\n'k-means++' : selects initial cluster centers for k-mean\nclustering in a smart way to speed up convergence. See section\nNotes in k_init for more details.\n\n'random': choose `n_clusters` observations (rows) at random from data\nfor the initial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features)\nand gives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a\nrandom state and return an initialization."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of\nn_init consecutive runs in terms of inertia."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations of the k-means algorithm for a\nsingle run."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Relative tolerance with regards to Frobenius norm of the difference\nin the cluster centers of two consecutive iterations to declare\nconvergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/precompute_distances",
          "name": "precompute_distances",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.precompute_distances",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', True, False}",
            "default_value": "'auto'",
            "description": "Precompute distances (faster but takes more memory).\n\n'auto' : do not precompute distances if n_samples * n_clusters > 12\nmillion. This corresponds to about 100MB overhead per job using\ndouble precision.\n\nTrue : always precompute distances.\n\nFalse : never precompute distances.\n\n.. deprecated:: 0.23\n    'precompute_distances' was deprecated in version 0.22 and will be\n    removed in 1.0 (renaming of 0.25). It has no effect."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity mode."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization. Use\nan int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/copy_x",
          "name": "copy_x",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.copy_x",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "When pre-computing distances it is more numerically accurate to center\nthe data first. If copy_x is True (default), then the original data is\nnot modified. If False, the original data is modified, and put back\nbefore the function returns, but small numerical differences may be\nintroduced by subtracting and then adding the data mean. Note that if\nthe original data is not C-contiguous, a copy will be made even if\ncopy_x is False. If the original data is sparse, but not in CSR format,\na copy will be made even if copy_x is False."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its\nclosest center.\n\n``None`` or ``-1`` means using all processors.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"full\", \"elkan\"}",
            "default_value": "\"auto\"",
            "description": "K-means algorithm to use. The classical EM-style algorithm is \"full\".\nThe \"elkan\" variation is more efficient on data with well-defined\nclusters, by using the triangle inequality. However it's more memory\nintensive due to the allocation of an extra array of shape\n(n_samples, n_clusters).\n\nFor now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\nmight change in the future for a better heuristic.\n\n.. versionchanged:: 0.18\n    Added Elkan algorithm"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "full",
              "auto",
              "elkan"
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', n_init=10,\n                 max_iter=300, tol=1e-4, precompute_distances='deprecated',\n                 verbose=0, random_state=None, copy_x=True,\n                 n_jobs='deprecated', algorithm='auto'):\n\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.precompute_distances = precompute_distances\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n        self.copy_x = copy_x\n        self.n_jobs = n_jobs\n        self.algorithm = algorithm"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit",
      "name": "fit",
      "qname": "sklearn.cluster._kmeans.KMeans.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training instances to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory\ncopy if the given data is not C-contiguous.\nIf a sparse matrix is passed, a copy will be made if it's not in\nCSR format."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute k-means clustering.",
      "docstring": "Compute k-means clustering.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory\n    copy if the given data is not C-contiguous.\n    If a sparse matrix is passed, a copy will be made if it's not in\n    CSR format.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself\n    Fitted estimator.",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', copy=self.copy_x,\n                                accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if hasattr(init, '__array__'):\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"full\":\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n        else:\n            kmeans_single = _kmeans_single_elkan\n\n        best_inertia = None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X, x_squared_norms=x_squared_norms, init=init,\n                random_state=random_state)\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X, sample_weight, centers_init, max_iter=self.max_iter,\n                verbose=self.verbose, tol=self._tol,\n                x_squared_norms=x_squared_norms, n_threads=self._n_threads)\n\n            # determine if these results are the best so far\n            if best_inertia is None or inertia < best_inertia:\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning, stacklevel=2)\n\n        self.cluster_centers_ = best_centers\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._kmeans.KMeans.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to transform."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).",
      "docstring": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.cluster._kmeans.KMeans.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to transform."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.",
      "docstring": "Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_clusters)\n    X transformed in the new space.",
      "code": "    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        # Currently, this just skips a copy of the data if it is not in\n        # np.array or CSR format already.\n        # XXX This skips _check_test_data, which may change the dtype;\n        # we should refactor the input validation.\n        return self.fit(X, sample_weight=sample_weight)._transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict",
      "name": "predict",
      "qname": "sklearn.cluster._kmeans.KMeans.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return _labels_inertia(X, sample_weight, x_squared_norms,\n                               self.cluster_centers_, self._n_threads)[0]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score",
      "name": "score",
      "qname": "sklearn.cluster._kmeans.KMeans.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.score.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Opposite of the value of X on the K-means objective.",
      "docstring": "Opposite of the value of X on the K-means objective.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nscore : float\n    Opposite of the value of X on the K-means objective.",
      "code": "    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return -_labels_inertia(X, sample_weight, x_squared_norms,\n                                self.cluster_centers_)[1]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/transform",
      "name": "transform",
      "qname": "sklearn.cluster._kmeans.KMeans.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/transform/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/transform/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to transform."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters. Note that even if X is sparse, the array returned by\n`transform` will typically be dense.",
      "docstring": "Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters. Note that even if X is sparse, the array returned by\n`transform` will typically be dense.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_clusters)\n    X transformed in the new space.",
      "code": "    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.n_clusters",
          "default_value": "8",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "8",
            "description": "The number of clusters to form as well as the number of\ncentroids to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization:\n\n'k-means++' : selects initial cluster centers for k-mean\nclustering in a smart way to speed up convergence. See section\nNotes in k_init for more details.\n\n'random': choose `n_clusters` observations (rows) at random from data\nfor the initial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features)\nand gives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a\nrandom state and return an initialization."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum number of iterations over the complete dataset before\nstopping independently of any early stopping criterion heuristics."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/batch_size",
          "name": "batch_size",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.batch_size",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Size of the mini batches."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity mode."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/compute_labels",
          "name": "compute_labels",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.compute_labels",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Compute label assignment and inertia for the complete dataset\nonce the minibatch optimization has converged in fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization and\nrandom reassignment. Use an int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Control early stopping based on the relative center changes as\nmeasured by a smoothed, variance-normalized of the mean center\nsquared position changes. This early stopping heuristics is\ncloser to the one used for the batch variant of the algorithms\nbut induces a slight computational and memory overhead over the\ninertia heuristic.\n\nTo disable convergence detection based on normalized center\nchange, set tol to 0.0 (default)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/max_no_improvement",
          "name": "max_no_improvement",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.max_no_improvement",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Control early stopping based on the consecutive number of mini\nbatches that does not yield an improvement on the smoothed inertia.\n\nTo disable convergence detection based on inertia, set\nmax_no_improvement to None."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/init_size",
          "name": "init_size",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.init_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of samples to randomly sample for speeding up the\ninitialization (sometimes at the expense of accuracy): the\nonly algorithm is initialized by running a batch KMeans on a\nrandom subset of the data. This needs to be larger than n_clusters.\n\nIf `None`, `init_size= 3 * batch_size`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.n_init",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of random initializations that are tried.\nIn contrast to KMeans, the algorithm is only run once, using the\nbest of the ``n_init`` initializations as measured by inertia."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/reassignment_ratio",
          "name": "reassignment_ratio",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.reassignment_ratio",
          "default_value": "0.01",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "Control the fraction of the maximum number of counts for a\ncenter to be reassigned. A higher value means that low count\ncenters are more easily reassigned, which means that the\nmodel will take longer to converge, but should converge in a\nbetter clustering."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', max_iter=100,\n                 batch_size=100, verbose=0, compute_labels=True,\n                 random_state=None, tol=0.0, max_no_improvement=10,\n                 init_size=None, n_init=3, reassignment_ratio=0.01):\n\n        super().__init__(\n            n_clusters=n_clusters, init=init, max_iter=max_iter,\n            verbose=verbose, random_state=random_state, tol=tol, n_init=n_init)\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/counts_@getter",
      "name": "counts_",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.counts_",
      "decorators": [
        "deprecated(\"The attribute 'counts_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/counts_@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.counts_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'counts_' is deprecated in 0.24\"  # type: ignore\n                \" and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def counts_(self):\n        return self._counts"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit",
      "name": "fit",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training instances to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory copy\nif the given data is not C-contiguous."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None).\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the centroids on X by chunking it into mini-batches.",
      "docstring": "Compute the centroids on X by chunking it into mini-batches.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory copy\n    if the given data is not C-contiguous.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        n_samples, n_features = X.shape\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self.tol > 0.0:\n            tol = _tolerance(X, self.tol)\n\n            # using tol-based early stopping needs the allocation of a\n            # dedicated before which can be expensive for high dim data:\n            # hence we allocate it outside of the main loop\n            old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n        else:\n            tol = 0.0\n            # no need for the center buffer if tol-based early stopping is\n            # disabled\n            old_center_buffer = np.zeros(0, dtype=X.dtype)\n\n        distances = np.zeros(self.batch_size, dtype=X.dtype)\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n        n_iter = int(self.max_iter * n_batches)\n\n        self._check_mkl_vcomp(X, self.batch_size)\n\n        validation_indices = random_state.randint(0, n_samples,\n                                                  self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        x_squared_norms_valid = x_squared_norms[validation_indices]\n\n        # perform several inits with random sub-sets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(\"Init %d/%d with method: %s\"\n                      % (init_idx + 1, self._n_init, init))\n            weight_sums = np.zeros(self.n_clusters, dtype=sample_weight.dtype)\n\n            # TODO: once the `k_means` function works with sparse input we\n            # should refactor the following init to use it instead.\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans\n            cluster_centers = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size)\n\n            # Compute the label assignment on the init dataset\n            _mini_batch_step(\n                X_valid, sample_weight_valid,\n                x_squared_norms[validation_indices], cluster_centers,\n                weight_sums, old_center_buffer, False, distances=None,\n                verbose=self.verbose)\n\n            # Keep only the best cluster centers across independent inits on\n            # the common validation set\n            _, inertia = _labels_inertia(X_valid, sample_weight_valid,\n                                         x_squared_norms_valid,\n                                         cluster_centers)\n            if self.verbose:\n                print(\"Inertia for init %d/%d: %f\"\n                      % (init_idx + 1, self._n_init, inertia))\n            if best_inertia is None or inertia < best_inertia:\n                self.cluster_centers_ = cluster_centers\n                self._counts = weight_sums\n                best_inertia = inertia\n\n        # Empty context to be used inplace by the convergence check routine\n        convergence_context = {}\n\n        # Perform the iterative optimization until the final convergence\n        # criterion\n        for iteration_idx in range(n_iter):\n            # Sample a minibatch from the full dataset\n            minibatch_indices = random_state.randint(\n                0, n_samples, self.batch_size)\n\n            # Perform the actual update step on the minibatch data\n            batch_inertia, centers_squared_diff = _mini_batch_step(\n                X[minibatch_indices], sample_weight[minibatch_indices],\n                x_squared_norms[minibatch_indices],\n                self.cluster_centers_, self._counts,\n                old_center_buffer, tol > 0.0, distances=distances,\n                # Here we randomly choose whether to perform\n                # random reassignment: the choice is done as a function\n                # of the iteration index, and the minimum number of\n                # counts, in order to force this reassignment to happen\n                # every once in a while\n                random_reassign=((iteration_idx + 1)\n                                 % (10 + int(self._counts.min())) == 0),\n                random_state=random_state,\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose)\n\n            # Monitor convergence and do early stopping if necessary\n            if _mini_batch_convergence(\n                    self, iteration_idx, n_iter, tol, n_samples,\n                    centers_squared_diff, batch_inertia, convergence_context,\n                    verbose=self.verbose):\n                break\n\n        self.n_iter_ = iteration_idx + 1\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = \\\n                    self._labels_inertia_minibatch(X, sample_weight)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/init_size_@getter",
      "name": "init_size_",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.init_size_",
      "decorators": [
        "deprecated(\"The attribute 'init_size_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/init_size_@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.init_size_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'init_size_' is deprecated in \"  # type: ignore\n                \"0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def init_size_(self):\n        return self._init_size"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Coordinates of the data points to cluster. It must be noted that\nX will be copied if it is not C-contiguous."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Update k means estimate on a single mini-batch X.",
      "docstring": "Update k means estimate on a single mini-batch X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Coordinates of the data points to cluster. It must be noted that\n    X will be copied if it is not C-contiguous.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\nReturns\n-------\nself",
      "code": "    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Coordinates of the data points to cluster. It must be noted that\n            X will be copied if it is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        self\n        \"\"\"\n        is_first_call_to_partial_fit = not hasattr(self, 'cluster_centers_')\n\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False,\n                                reset=is_first_call_to_partial_fit)\n\n        self._random_state = getattr(self, \"_random_state\",\n                                     check_random_state(self.random_state))\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        x_squared_norms = row_norms(X, squared=True)\n\n        if is_first_call_to_partial_fit:\n            # this is the first call to partial_fit on this object\n            self._check_params(X)\n\n            # Validate init array\n            init = self.init\n            if hasattr(init, '__array__'):\n                init = check_array(init, dtype=X.dtype, copy=True, order='C')\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size)\n\n            self._counts = np.zeros(self.n_clusters,\n                                    dtype=sample_weight.dtype)\n            random_reassign = False\n            distances = None\n        else:\n            # The lower the minimum count is, the more we do random\n            # reassignment, however, we don't want to do random\n            # reassignment too often, to allow for building up counts\n            random_reassign = self._random_state.randint(\n                10 * (1 + self._counts.min())) == 0\n            distances = np.zeros(X.shape[0], dtype=X.dtype)\n\n        _mini_batch_step(X, sample_weight, x_squared_norms,\n                         self.cluster_centers_, self._counts,\n                         np.zeros(0, dtype=X.dtype), 0,\n                         random_reassign=random_reassign, distances=distances,\n                         random_state=self._random_state,\n                         reassignment_ratio=self.reassignment_ratio,\n                         verbose=self.verbose)\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia(\n                X, sample_weight, x_squared_norms, self.cluster_centers_)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict",
      "name": "predict",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._labels_inertia_minibatch(X, sample_weight)[0]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/random_state_@getter",
      "name": "random_state_",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.random_state_",
      "decorators": [
        "deprecated(\"The attribute 'random_state_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/random_state_@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.random_state_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'random_state_' is deprecated \"  # type: ignore\n                \"in 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def random_state_(self):\n        return getattr(self, \"_random_state\", None)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/k_means",
      "name": "k_means",
      "qname": "sklearn.cluster._kmeans.k_means",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.k_means.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The observations to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory copy\nif the given data is not C-contiguous."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.k_means.n_clusters",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "The number of clusters to form as well as the number of\ncentroids to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.k_means.sample_weight",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/init",
          "name": "init",
          "qname": "sklearn.cluster._kmeans.k_means.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization:\n\n'k-means++' : selects initial cluster centers for k-mean\nclustering in a smart way to speed up convergence. See section\nNotes in k_init for more details.\n\n'random': choose `n_clusters` observations (rows) at random from data\nfor the initial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features)\nand gives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a\nrandom state and return an initialization."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/precompute_distances",
          "name": "precompute_distances",
          "qname": "sklearn.cluster._kmeans.k_means.precompute_distances",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', True, False}",
            "default_value": "",
            "description": "Precompute distances (faster but takes more memory).\n\n'auto' : do not precompute distances if n_samples * n_clusters > 12\nmillion. This corresponds to about 100MB overhead per job using\ndouble precision.\n\nTrue : always precompute distances\n\nFalse : never precompute distances\n\n.. deprecated:: 0.23\n    'precompute_distances' was deprecated in version 0.23 and will be\n    removed in 1.0 (renaming of 0.25). It has no effect."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._kmeans.k_means.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of\nn_init consecutive runs in terms of inertia."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._kmeans.k_means.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations of the k-means algorithm to run."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._kmeans.k_means.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Verbosity mode."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/tol",
          "name": "tol",
          "qname": "sklearn.cluster._kmeans.k_means.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Relative tolerance with regards to Frobenius norm of the difference\nin the cluster centers of two consecutive iterations to declare\nconvergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.k_means.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization. Use\nan int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/copy_x",
          "name": "copy_x",
          "qname": "sklearn.cluster._kmeans.k_means.copy_x",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "When pre-computing distances it is more numerically accurate to center\nthe data first. If copy_x is True (default), then the original data is\nnot modified. If False, the original data is modified, and put back\nbefore the function returns, but small numerical differences may be\nintroduced by subtracting and then adding the data mean. Note that if\nthe original data is not C-contiguous, a copy will be made even if\ncopy_x is False. If the original data is sparse, but not in CSR format,\na copy will be made even if copy_x is False."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._kmeans.k_means.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its\nclosest center.\n\n``None`` or ``-1`` means using all processors.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._kmeans.k_means.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"full\", \"elkan\"}",
            "default_value": "\"auto\"",
            "description": "K-means algorithm to use. The classical EM-style algorithm is \"full\".\nThe \"elkan\" variation is more efficient on data with well-defined\nclusters, by using the triangle inequality. However it's more memory\nintensive due to the allocation of an extra array of shape\n(n_samples, n_clusters).\n\nFor now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\nmight change in the future for a better heuristic."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "full",
              "auto",
              "elkan"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.cluster._kmeans.k_means.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "K-means clustering algorithm.\n\nRead more in the :ref:`User Guide <k_means>`.",
      "docstring": "K-means clustering algorithm.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The observations to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory copy\n    if the given data is not C-contiguous.\n\nn_clusters : int\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nprecompute_distances : {'auto', True, False}\n    Precompute distances (faster but takes more memory).\n\n    'auto' : do not precompute distances if n_samples * n_clusters > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n\n    True : always precompute distances\n\n    False : never precompute distances\n\n    .. deprecated:: 0.23\n        'precompute_distances' was deprecated in version 0.23 and will be\n        removed in 1.0 (renaming of 0.25). It has no effect.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm to run.\n\nverbose : bool, default=False\n    Verbosity mode.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nn_jobs : int, default=None\n    The number of OpenMP threads to use for the computation. Parallelism is\n    sample-wise on the main cython loop which assigns each sample to its\n    closest center.\n\n    ``None`` or ``-1`` means using all processors.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nalgorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n    The \"elkan\" variation is more efficient on data with well-defined\n    clusters, by using the triangle inequality. However it's more memory\n    intensive due to the allocation of an extra array of shape\n    (n_samples, n_clusters).\n\n    For now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\n    might change in the future for a better heuristic.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncentroid : ndarray of shape (n_clusters, n_features)\n    Centroids found at the last iteration of k-means.\n\nlabel : ndarray of shape (n_samples,)\n    label[i] is the code or index of the centroid the\n    i'th observation is closest to.\n\ninertia : float\n    The final value of the inertia criterion (sum of squared distances to\n    the closest centroid for all observations in the training set).\n\nbest_n_iter : int\n    Number of iterations corresponding to the best results.\n    Returned only if `return_n_iter` is set to True.",
      "code": "@_deprecate_positional_args\ndef k_means(X, n_clusters, *, sample_weight=None, init='k-means++',\n            precompute_distances='deprecated', n_init=10, max_iter=300,\n            verbose=False, tol=1e-4, random_state=None, copy_x=True,\n            n_jobs='deprecated', algorithm=\"auto\", return_n_iter=False):\n    \"\"\"K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in X. If None, all observations\n        are assigned equal weight.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    precompute_distances : {'auto', True, False}\n        Precompute distances (faster but takes more memory).\n\n        'auto' : do not precompute distances if n_samples * n_clusters > 12\n        million. This corresponds to about 100MB overhead per job using\n        double precision.\n\n        True : always precompute distances\n\n        False : never precompute distances\n\n        .. deprecated:: 0.23\n            'precompute_distances' was deprecated in version 0.23 and will be\n            removed in 1.0 (renaming of 0.25). It has no effect.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    n_jobs : int, default=None\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n        ``None`` or ``-1`` means using all processors.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n        The \"elkan\" variation is more efficient on data with well-defined\n        clusters, by using the triangle inequality. However it's more memory\n        intensive due to the allocation of an extra array of shape\n        (n_samples, n_clusters).\n\n        For now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\n        might change in the future for a better heuristic.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n    \"\"\"\n    est = KMeans(\n        n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter,\n        verbose=verbose, precompute_distances=precompute_distances, tol=tol,\n        random_state=random_state, copy_x=copy_x, n_jobs=n_jobs,\n        algorithm=algorithm\n    ).fit(X, sample_weight=sample_weight)\n    if return_n_iter:\n        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n    else:\n        return est.cluster_centers_, est.labels_, est.inertia_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus",
      "name": "kmeans_plusplus",
      "qname": "sklearn.cluster._kmeans.kmeans_plusplus",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data to pick seeds from."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.n_clusters",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "The number of centroids to initialize"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/x_squared_norms",
          "name": "x_squared_norms",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.x_squared_norms",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Squared Euclidean norm of each data point."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or RandomState instance",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization. Pass\nan int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/n_local_trials",
          "name": "n_local_trials",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.n_local_trials",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of seeding trials for each center (except the first),\nof which the one reducing inertia the most is greedily chosen.\nSet to None to make the number of trials depend logarithmically\non the number of seeds (2+log(k))."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Init n_clusters seeds according to k-means++\n\n.. versionadded:: 0.24",
      "docstring": "Init n_clusters seeds according to k-means++\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to pick seeds from.\n\nn_clusters : int\n    The number of centroids to initialize\n\nx_squared_norms : array-like of shape (n_samples,), default=None\n    Squared Euclidean norm of each data point.\n\nrandom_state : int or RandomState instance, default=None\n    Determines random number generation for centroid initialization. Pass\n    an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_local_trials : int, default=None\n    The number of seeding trials for each center (except the first),\n    of which the one reducing inertia the most is greedily chosen.\n    Set to None to make the number of trials depend logarithmically\n    on the number of seeds (2+log(k)).\n\nReturns\n-------\ncenters : ndarray of shape (n_clusters, n_features)\n    The inital centers for k-means.\n\nindices : ndarray of shape (n_clusters,)\n    The index location of the chosen centers in the data array X. For a\n    given index and center, X[index] = center.\n\nNotes\n-----\nSelects initial cluster centers for k-mean clustering in a smart way\nto speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n\"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\non Discrete algorithms. 2007\n\nExamples\n--------\n\n>>> from sklearn.cluster import kmeans_plusplus\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n>>> centers\narray([[10,  4],\n       [ 1,  0]])\n>>> indices\narray([4, 2])",
      "code": "def kmeans_plusplus(X, n_clusters, *, x_squared_norms=None,\n                    random_state=None, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds from.\n\n    n_clusters : int\n        The number of centroids to initialize\n\n    x_squared_norms : array-like of shape (n_samples,), default=None\n        Squared Euclidean norm of each data point.\n\n    random_state : int or RandomState instance, default=None\n        Determines random number generation for centroid initialization. Pass\n        an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)).\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The inital centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  4],\n           [ 1,  0]])\n    >>> indices\n    array([4, 2])\n    \"\"\"\n\n    # Check data\n    check_array(X, accept_sparse='csr',\n                dtype=[np.float64, np.float32])\n\n    if X.shape[0] < n_clusters:\n        raise ValueError(f\"n_samples={X.shape[0]} should be >= \"\n                         f\"n_clusters={n_clusters}.\")\n\n    # Check parameters\n    if x_squared_norms is None:\n        x_squared_norms = row_norms(X, squared=True)\n    else:\n        x_squared_norms = check_array(x_squared_norms,\n                                      dtype=X.dtype,\n                                      ensure_2d=False)\n\n    if x_squared_norms.shape[0] != X.shape[0]:\n        raise ValueError(\n            f\"The length of x_squared_norms {x_squared_norms.shape[0]} should \"\n            f\"be equal to the length of n_samples {X.shape[0]}.\")\n\n    if n_local_trials is not None and n_local_trials < 1:\n        raise ValueError(\n            f\"n_local_trials is set to {n_local_trials} but should be an \"\n            f\"integer value greater than zero.\")\n\n    random_state = check_random_state(random_state)\n\n    # Call private k-means++\n    centers, indices = _kmeans_plusplus(X, n_clusters, x_squared_norms,\n                                        random_state, n_local_trials)\n\n    return centers, indices"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._mean_shift.MeanShift.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/bandwidth",
          "name": "bandwidth",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.bandwidth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Bandwidth used in the RBF kernel.\n\nIf not given, the bandwidth is estimated using\nsklearn.cluster.estimate_bandwidth; see the documentation for that\nfunction for hints on scalability (see also the Notes, below)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/seeds",
          "name": "seeds",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.seeds",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "None",
            "description": "Seeds used to initialize kernels. If not set,\nthe seeds are calculated by clustering.get_bin_seeds\nwith bandwidth as the grid size and default values for\nother parameters."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/bin_seeding",
          "name": "bin_seeding",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.bin_seeding",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If true, initial kernel locations are not locations of all\npoints, but rather the location of the discretized version of\npoints, where points are binned onto a grid whose coarseness\ncorresponds to the bandwidth. Setting this option to True will speed\nup the algorithm because fewer seeds will be initialized.\nThe default value is False.\nIgnored if seeds argument is not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/min_bin_freq",
          "name": "min_bin_freq",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.min_bin_freq",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "To speed up the algorithm, accept only those bins with at least\nmin_bin_freq points as seeds."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/cluster_all",
          "name": "cluster_all",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.cluster_all",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If true, then all points are clustered, even those orphans that are\nnot within any kernel. Orphans are assigned to the nearest kernel.\nIf false, then orphans are given cluster label -1."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by computing\neach of the n_init runs in parallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations, per seed point before the clustering\noperation terminates (for that seed point), if has not converged yet.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, bandwidth=None, seeds=None, bin_seeding=False,\n                 min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300):\n        self.bandwidth = bandwidth\n        self.seeds = seeds\n        self.bin_seeding = bin_seeding\n        self.cluster_all = cluster_all\n        self.min_bin_freq = min_bin_freq\n        self.n_jobs = n_jobs\n        self.max_iter = max_iter"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit",
      "name": "fit",
      "qname": "sklearn.cluster._mean_shift.MeanShift.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._mean_shift.MeanShift.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.MeanShift.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Samples to cluster."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._mean_shift.MeanShift.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform clustering.",
      "docstring": "Perform clustering.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Samples to cluster.\n\ny : Ignored",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Perform clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to cluster.\n\n        y : Ignored\n\n        \"\"\"\n        X = self._validate_data(X)\n        bandwidth = self.bandwidth\n        if bandwidth is None:\n            bandwidth = estimate_bandwidth(X, n_jobs=self.n_jobs)\n        elif bandwidth <= 0:\n            raise ValueError(\"bandwidth needs to be greater than zero or None,\"\n                             \" got %f\" % bandwidth)\n\n        seeds = self.seeds\n        if seeds is None:\n            if self.bin_seeding:\n                seeds = get_bin_seeds(X, bandwidth, self.min_bin_freq)\n            else:\n                seeds = X\n        n_samples, n_features = X.shape\n        center_intensity_dict = {}\n\n        # We use n_jobs=1 because this will be used in nested calls under\n        # parallel calls to _mean_shift_single_seed so there is no need for\n        # for further parallelism.\n        nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n\n        # execute iterations on all seeds in parallel\n        all_res = Parallel(n_jobs=self.n_jobs)(\n            delayed(_mean_shift_single_seed)\n            (seed, X, nbrs, self.max_iter) for seed in seeds)\n        # copy results in a dictionary\n        for i in range(len(seeds)):\n            if all_res[i][1]:  # i.e. len(points_within) > 0\n                center_intensity_dict[all_res[i][0]] = all_res[i][1]\n\n        self.n_iter_ = max([x[2] for x in all_res])\n\n        if not center_intensity_dict:\n            # nothing near seeds\n            raise ValueError(\"No point was within bandwidth=%f of any seed.\"\n                             \" Try a different seeding strategy \\\n                             or increase the bandwidth.\"\n                             % bandwidth)\n\n        # POST PROCESSING: remove near duplicate points\n        # If the distance between two kernels is less than the bandwidth,\n        # then we have to remove one because it is a duplicate. Remove the\n        # one with fewer points.\n\n        sorted_by_intensity = sorted(center_intensity_dict.items(),\n                                     key=lambda tup: (tup[1], tup[0]),\n                                     reverse=True)\n        sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\n        unique = np.ones(len(sorted_centers), dtype=bool)\n        nbrs = NearestNeighbors(radius=bandwidth,\n                                n_jobs=self.n_jobs).fit(sorted_centers)\n        for i, center in enumerate(sorted_centers):\n            if unique[i]:\n                neighbor_idxs = nbrs.radius_neighbors([center],\n                                                      return_distance=False)[0]\n                unique[neighbor_idxs] = 0\n                unique[i] = 1  # leave the current point as unique\n        cluster_centers = sorted_centers[unique]\n\n        # ASSIGN LABELS: a point belongs to the cluster that it is closest to\n        nbrs = NearestNeighbors(n_neighbors=1,\n                                n_jobs=self.n_jobs).fit(cluster_centers)\n        labels = np.zeros(n_samples, dtype=int)\n        distances, idxs = nbrs.kneighbors(X)\n        if self.cluster_all:\n            labels = idxs.flatten()\n        else:\n            labels.fill(-1)\n            bool_selector = distances.flatten() <= bandwidth\n            labels[bool_selector] = idxs.flatten()[bool_selector]\n\n        self.cluster_centers_, self.labels_ = cluster_centers, labels\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict",
      "name": "predict",
      "qname": "sklearn.cluster._mean_shift.MeanShift.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._mean_shift.MeanShift.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.MeanShift.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        with config_context(assume_finite=True):\n            return pairwise_distances_argmin(X, self.cluster_centers_)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth",
      "name": "estimate_bandwidth",
      "qname": "sklearn.cluster._mean_shift.estimate_bandwidth",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input points."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/quantile",
          "name": "quantile",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.quantile",
          "default_value": "0.3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.3",
            "description": "should be between [0, 1]\n0.5 means that the median of all pairwise distances is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/n_samples",
          "name": "n_samples",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.n_samples",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of samples to use. If not given, all samples are used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.random_state",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "The generator used to randomly select the samples from input points\nfor bandwidth estimation. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Estimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it's wise to set that parameter to a small value.",
      "docstring": "Estimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it's wise to set that parameter to a small value.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input points.\n\nquantile : float, default=0.3\n    should be between [0, 1]\n    0.5 means that the median of all pairwise distances is used.\n\nn_samples : int, default=None\n    The number of samples to use. If not given, all samples are used.\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to randomly select the samples from input points\n    for bandwidth estimation. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nbandwidth : float\n    The bandwidth parameter.",
      "code": "@_deprecate_positional_args\ndef estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0,\n                       n_jobs=None):\n    \"\"\"Estimate the bandwidth to use with the mean-shift algorithm.\n\n    That this function takes time at least quadratic in n_samples. For large\n    datasets, it's wise to set that parameter to a small value.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input points.\n\n    quantile : float, default=0.3\n        should be between [0, 1]\n        0.5 means that the median of all pairwise distances is used.\n\n    n_samples : int, default=None\n        The number of samples to use. If not given, all samples are used.\n\n    random_state : int, RandomState instance, default=None\n        The generator used to randomly select the samples from input points\n        for bandwidth estimation. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    bandwidth : float\n        The bandwidth parameter.\n    \"\"\"\n    X = check_array(X)\n\n    random_state = check_random_state(random_state)\n    if n_samples is not None:\n        idx = random_state.permutation(X.shape[0])[:n_samples]\n        X = X[idx]\n    n_neighbors = int(X.shape[0] * quantile)\n    if n_neighbors < 1:  # cannot fit NearestNeighbors with n_neighbors = 0\n        n_neighbors = 1\n    nbrs = NearestNeighbors(n_neighbors=n_neighbors,\n                            n_jobs=n_jobs)\n    nbrs.fit(X)\n\n    bandwidth = 0.\n    for batch in gen_batches(len(X), 500):\n        d, _ = nbrs.kneighbors(X[batch, :], return_distance=True)\n        bandwidth += np.max(d, axis=1).sum()\n\n    return bandwidth / X.shape[0]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds",
      "name": "get_bin_seeds",
      "qname": "sklearn.cluster._mean_shift.get_bin_seeds",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.get_bin_seeds.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input points, the same points that will be used in mean_shift."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds/bin_size",
          "name": "bin_size",
          "qname": "sklearn.cluster._mean_shift.get_bin_seeds.bin_size",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "",
            "description": "Controls the coarseness of the binning. Smaller values lead\nto more seeding (which is computationally more expensive). If you're\nnot sure how to set this, set it to the value of the bandwidth used\nin clustering.mean_shift."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds/min_bin_freq",
          "name": "min_bin_freq",
          "qname": "sklearn.cluster._mean_shift.get_bin_seeds.min_bin_freq",
          "default_value": "1",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "Only bins with at least min_bin_freq will be selected as seeds.\nRaising this value decreases the number of seeds found, which\nmakes mean_shift computationally cheaper."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are\nspaced bin_size apart, and then choosing those bins with at least\nmin_bin_freq points.",
      "docstring": "Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are\nspaced bin_size apart, and then choosing those bins with at least\nmin_bin_freq points.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Input points, the same points that will be used in mean_shift.\n\nbin_size : float\n    Controls the coarseness of the binning. Smaller values lead\n    to more seeding (which is computationally more expensive). If you're\n    not sure how to set this, set it to the value of the bandwidth used\n    in clustering.mean_shift.\n\nmin_bin_freq : int, default=1\n    Only bins with at least min_bin_freq will be selected as seeds.\n    Raising this value decreases the number of seeds found, which\n    makes mean_shift computationally cheaper.\n\nReturns\n-------\nbin_seeds : array-like of shape (n_samples, n_features)\n    Points used as initial kernel positions in clustering.mean_shift.",
      "code": "def get_bin_seeds(X, bin_size, min_bin_freq=1):\n    \"\"\"Finds seeds for mean_shift.\n\n    Finds seeds by first binning data onto a grid whose lines are\n    spaced bin_size apart, and then choosing those bins with at least\n    min_bin_freq points.\n\n    Parameters\n    ----------\n\n    X : array-like of shape (n_samples, n_features)\n        Input points, the same points that will be used in mean_shift.\n\n    bin_size : float\n        Controls the coarseness of the binning. Smaller values lead\n        to more seeding (which is computationally more expensive). If you're\n        not sure how to set this, set it to the value of the bandwidth used\n        in clustering.mean_shift.\n\n    min_bin_freq : int, default=1\n        Only bins with at least min_bin_freq will be selected as seeds.\n        Raising this value decreases the number of seeds found, which\n        makes mean_shift computationally cheaper.\n\n    Returns\n    -------\n    bin_seeds : array-like of shape (n_samples, n_features)\n        Points used as initial kernel positions in clustering.mean_shift.\n    \"\"\"\n    if bin_size == 0:\n        return X\n\n    # Bin points\n    bin_sizes = defaultdict(int)\n    for point in X:\n        binned_point = np.round(point / bin_size)\n        bin_sizes[tuple(binned_point)] += 1\n\n    # Select only those bins as seeds which have enough members\n    bin_seeds = np.array([point for point, freq in bin_sizes.items() if\n                          freq >= min_bin_freq], dtype=np.float32)\n    if len(bin_seeds) == len(X):\n        warnings.warn(\"Binning data failed with provided bin_size=%f,\"\n                      \" using data points as seeds.\" % bin_size)\n        return X\n    bin_seeds = bin_seeds * bin_size\n    return bin_seeds"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift",
      "name": "mean_shift",
      "qname": "sklearn.cluster._mean_shift.mean_shift",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.mean_shift.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/bandwidth",
          "name": "bandwidth",
          "qname": "sklearn.cluster._mean_shift.mean_shift.bandwidth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Kernel bandwidth.\n\nIf bandwidth is not given, it is determined using a heuristic based on\nthe median of all pairwise distances. This will take quadratic time in\nthe number of samples. The sklearn.cluster.estimate_bandwidth function\ncan be used to do this more efficiently."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/seeds",
          "name": "seeds",
          "qname": "sklearn.cluster._mean_shift.mean_shift.seeds",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_seeds, n_features) or None",
            "default_value": "",
            "description": "Point used as initial kernel locations. If None and bin_seeding=False,\neach data point is used as a seed. If None and bin_seeding=True,\nsee bin_seeding."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_seeds, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/bin_seeding",
          "name": "bin_seeding",
          "qname": "sklearn.cluster._mean_shift.mean_shift.bin_seeding",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If true, initial kernel locations are not locations of all\npoints, but rather the location of the discretized version of\npoints, where points are binned onto a grid whose coarseness\ncorresponds to the bandwidth. Setting this option to True will speed\nup the algorithm because fewer seeds will be initialized.\nIgnored if seeds argument is not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/min_bin_freq",
          "name": "min_bin_freq",
          "qname": "sklearn.cluster._mean_shift.mean_shift.min_bin_freq",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "To speed up the algorithm, accept only those bins with at least\nmin_bin_freq points as seeds."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/cluster_all",
          "name": "cluster_all",
          "qname": "sklearn.cluster._mean_shift.mean_shift.cluster_all",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If true, then all points are clustered, even those orphans that are\nnot within any kernel. Orphans are assigned to the nearest kernel.\nIf false, then orphans are given cluster label -1."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._mean_shift.mean_shift.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations, per seed point before the clustering\noperation terminates (for that seed point), if has not converged yet."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._mean_shift.mean_shift.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by computing\neach of the n_init runs in parallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionadded:: 0.17\n   Parallel Execution using *n_jobs*."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform mean shift clustering of data using a flat kernel.\n\nRead more in the :ref:`User Guide <mean_shift>`.",
      "docstring": "Perform mean shift clustering of data using a flat kernel.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\nbandwidth : float, default=None\n    Kernel bandwidth.\n\n    If bandwidth is not given, it is determined using a heuristic based on\n    the median of all pairwise distances. This will take quadratic time in\n    the number of samples. The sklearn.cluster.estimate_bandwidth function\n    can be used to do this more efficiently.\n\nseeds : array-like of shape (n_seeds, n_features) or None\n    Point used as initial kernel locations. If None and bin_seeding=False,\n    each data point is used as a seed. If None and bin_seeding=True,\n    see bin_seeding.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by computing\n    each of the n_init runs in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.17\n       Parallel Execution using *n_jobs*.\n\nReturns\n-------\n\ncluster_centers : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_mean_shift.py\n<sphx_glr_auto_examples_cluster_plot_mean_shift.py>`.",
      "code": "@_deprecate_positional_args\ndef mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False,\n               min_bin_freq=1, cluster_all=True, max_iter=300,\n               n_jobs=None):\n    \"\"\"Perform mean shift clustering of data using a flat kernel.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    bandwidth : float, default=None\n        Kernel bandwidth.\n\n        If bandwidth is not given, it is determined using a heuristic based on\n        the median of all pairwise distances. This will take quadratic time in\n        the number of samples. The sklearn.cluster.estimate_bandwidth function\n        can be used to do this more efficiently.\n\n    seeds : array-like of shape (n_seeds, n_features) or None\n        Point used as initial kernel locations. If None and bin_seeding=False,\n        each data point is used as a seed. If None and bin_seeding=True,\n        see bin_seeding.\n\n    bin_seeding : bool, default=False\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, default=1\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds.\n\n    cluster_all : bool, default=True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by computing\n        each of the n_init runs in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.17\n           Parallel Execution using *n_jobs*.\n\n    Returns\n    -------\n\n    cluster_centers : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_mean_shift.py\n    <sphx_glr_auto_examples_cluster_plot_mean_shift.py>`.\n\n    \"\"\"\n    model = MeanShift(bandwidth=bandwidth, seeds=seeds,\n                      min_bin_freq=min_bin_freq,\n                      bin_seeding=bin_seeding,\n                      cluster_all=cluster_all, n_jobs=n_jobs,\n                      max_iter=max_iter).fit(X)\n    return model.cluster_centers_, model.labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._optics.OPTICS.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.min_samples",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "5",
            "description": "The number of samples in a neighborhood for a point to be considered as\na core point. Also, up and down steep regions can't have more than\n``min_samples`` consecutive non-steep points. Expressed as an absolute\nnumber or a fraction of the number of samples (rounded to be at least\n2)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/max_eps",
          "name": "max_eps",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.max_eps",
          "default_value": "np.inf",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "np.inf",
            "description": "The maximum distance between two samples for one to be considered as\nin the neighborhood of the other. Default value of ``np.inf`` will\nidentify clusters across all scales; reducing ``max_eps`` will result\nin shorter run times."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/metric",
          "name": "metric",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.metric",
          "default_value": "'minkowski'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'minkowski'",
            "description": "Metric to use for distance computation. Any metric from scikit-learn\nor scipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each\npair of instances (rows) and the resulting value recorded. The callable\nshould take two arrays as input and return one value indicating the\ndistance between them. This works for Scipy's metrics, but is less\nefficient than passing the metric name as a string. If metric is\n\"precomputed\", X is assumed to be a distance matrix and must be square.\n\nValid values for metric are:\n\n- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n  'manhattan']\n\n- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n  'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n  'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n  'yule']\n\nSee the documentation for scipy.spatial.distance for details on these\nmetrics."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/p",
          "name": "p",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.p",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Parameter for the Minkowski metric from\n:class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.metric_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/cluster_method",
          "name": "cluster_method",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.cluster_method",
          "default_value": "'xi'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "'xi'",
            "description": "The extraction method used to extract clusters using the calculated\nreachability and ordering. Possible values are \"xi\" and \"dbscan\"."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/eps",
          "name": "eps",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.eps",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The maximum distance between two samples for one to be considered as\nin the neighborhood of the other. By default it assumes the same value\nas ``max_eps``.\nUsed only when ``cluster_method='dbscan'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/xi",
          "name": "xi",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.xi",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float between 0 and 1",
            "default_value": "0.05",
            "description": "Determines the minimum steepness on the reachability plot that\nconstitutes a cluster boundary. For example, an upwards point in the\nreachability plot is defined by the ratio from one point to its\nsuccessor being at most 1-xi.\nUsed only when ``cluster_method='xi'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float between 0 and 1"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/predecessor_correction",
          "name": "predecessor_correction",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.predecessor_correction",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Correct clusters according to the predecessors calculated by OPTICS\n[2]_. This parameter has minimal effect on most datasets.\nUsed only when ``cluster_method='xi'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/min_cluster_size",
          "name": "min_cluster_size",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.min_cluster_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "None",
            "description": "Minimum number of samples in an OPTICS cluster, expressed as an\nabsolute number or a fraction of the number of samples (rounded to be\nat least 2). If ``None``, the value of ``min_samples`` is used instead.\nUsed only when ``cluster_method='xi'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm\n  based on the values passed to :meth:`fit` method. (default)\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ball_tree",
              "kd_tree",
              "auto",
              "brute"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.leaf_size",
          "default_value": "30",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\naffect the speed of the construction and query, as well as the memory\nrequired to store the tree. The optimal value depends on the\nnature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, min_samples=5, max_eps=np.inf, metric='minkowski',\n                 p=2, metric_params=None, cluster_method='xi', eps=None,\n                 xi=0.05, predecessor_correction=True, min_cluster_size=None,\n                 algorithm='auto', leaf_size=30, n_jobs=None):\n        self.max_eps = max_eps\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.algorithm = algorithm\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.leaf_size = leaf_size\n        self.cluster_method = cluster_method\n        self.eps = eps\n        self.xi = xi\n        self.predecessor_correction = predecessor_correction\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit",
      "name": "fit",
      "qname": "sklearn.cluster._optics.OPTICS.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._optics.OPTICS.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._optics.OPTICS.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric=\u2019precomputed\u2019",
            "default_value": "",
            "description": "A feature array, or array of distances between samples if\nmetric='precomputed'."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples) if metric=\u2019precomputed\u2019"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._optics.OPTICS.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ignored",
            "default_value": "",
            "description": "Ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and\nperforms initial clustering using ``max_eps`` distance specified at\nOPTICS object instantiation.",
      "docstring": "Perform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and\nperforms initial clustering using ``max_eps`` distance specified at\nOPTICS object instantiation.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric=\u2019precomputed\u2019\n    A feature array, or array of distances between samples if\n    metric='precomputed'.\n\ny : ignored\n    Ignored.\n\nReturns\n-------\nself : instance of OPTICS\n    The instance.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features), or \\\n                (n_samples, n_samples) if metric=\u2019precomputed\u2019\n            A feature array, or array of distances between samples if\n            metric='precomputed'.\n\n        y : ignored\n            Ignored.\n\n        Returns\n        -------\n        self : instance of OPTICS\n            The instance.\n        \"\"\"\n        X = self._validate_data(X, dtype=float)\n\n        if self.cluster_method not in ['dbscan', 'xi']:\n            raise ValueError(\"cluster_method should be one of\"\n                             \" 'dbscan' or 'xi' but is %s\" %\n                             self.cluster_method)\n\n        (self.ordering_, self.core_distances_, self.reachability_,\n         self.predecessor_) = compute_optics_graph(\n             X=X, min_samples=self.min_samples, algorithm=self.algorithm,\n             leaf_size=self.leaf_size, metric=self.metric,\n             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs,\n             max_eps=self.max_eps)\n\n        # Extract clusters from the calculated orders and reachability\n        if self.cluster_method == 'xi':\n            labels_, clusters_ = cluster_optics_xi(\n                reachability=self.reachability_,\n                predecessor=self.predecessor_,\n                ordering=self.ordering_,\n                min_samples=self.min_samples,\n                min_cluster_size=self.min_cluster_size,\n                xi=self.xi,\n                predecessor_correction=self.predecessor_correction)\n            self.cluster_hierarchy_ = clusters_\n        elif self.cluster_method == 'dbscan':\n            if self.eps is None:\n                eps = self.max_eps\n            else:\n                eps = self.eps\n\n            if eps > self.max_eps:\n                raise ValueError('Specify an epsilon smaller than %s. Got %s.'\n                                 % (self.max_eps, eps))\n\n            labels_ = cluster_optics_dbscan(\n                reachability=self.reachability_,\n                core_distances=self.core_distances_,\n                ordering=self.ordering_, eps=eps)\n\n        self.labels_ = labels_\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan",
      "name": "cluster_optics_dbscan",
      "qname": "sklearn.cluster._optics.cluster_optics_dbscan",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/reachability",
          "name": "reachability",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.reachability",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_samples,)",
            "default_value": "",
            "description": "Reachability distances calculated by OPTICS (``reachability_``)"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/core_distances",
          "name": "core_distances",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.core_distances",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_samples,)",
            "default_value": "",
            "description": "Distances at which points become core (``core_distances_``)"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/ordering",
          "name": "ordering",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.ordering",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_samples,)",
            "default_value": "",
            "description": "OPTICS ordered point indices (``ordering_``)"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/eps",
          "name": "eps",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.eps",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "",
            "description": "DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\nwill be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\nto one another."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Performs DBSCAN extraction for an arbitrary epsilon.\n\nExtracting the clusters runs in linear time. Note that this results in\n``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\nsimilar settings and ``eps``, only if ``eps`` is close to ``max_eps``.",
      "docstring": "Performs DBSCAN extraction for an arbitrary epsilon.\n\nExtracting the clusters runs in linear time. Note that this results in\n``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\nsimilar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n\nParameters\n----------\nreachability : array of shape (n_samples,)\n    Reachability distances calculated by OPTICS (``reachability_``)\n\ncore_distances : array of shape (n_samples,)\n    Distances at which points become core (``core_distances_``)\n\nordering : array of shape (n_samples,)\n    OPTICS ordered point indices (``ordering_``)\n\neps : float\n    DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n    will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n    to one another.\n\nReturns\n-------\nlabels_ : array of shape (n_samples,)\n    The estimated labels.",
      "code": "@_deprecate_positional_args\ndef cluster_optics_dbscan(*, reachability, core_distances, ordering, eps):\n    \"\"\"Performs DBSCAN extraction for an arbitrary epsilon.\n\n    Extracting the clusters runs in linear time. Note that this results in\n    ``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\n    similar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n\n    Parameters\n    ----------\n    reachability : array of shape (n_samples,)\n        Reachability distances calculated by OPTICS (``reachability_``)\n\n    core_distances : array of shape (n_samples,)\n        Distances at which points become core (``core_distances_``)\n\n    ordering : array of shape (n_samples,)\n        OPTICS ordered point indices (``ordering_``)\n\n    eps : float\n        DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n        will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n        to one another.\n\n    Returns\n    -------\n    labels_ : array of shape (n_samples,)\n        The estimated labels.\n\n    \"\"\"\n    n_samples = len(core_distances)\n    labels = np.zeros(n_samples, dtype=int)\n\n    far_reach = reachability > eps\n    near_core = core_distances <= eps\n    labels[ordering] = np.cumsum(far_reach[ordering] & near_core[ordering]) - 1\n    labels[far_reach & ~near_core] = -1\n    return labels"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi",
      "name": "cluster_optics_xi",
      "qname": "sklearn.cluster._optics.cluster_optics_xi",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/reachability",
          "name": "reachability",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.reachability",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "Reachability distances calculated by OPTICS (`reachability_`)"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/predecessor",
          "name": "predecessor",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.predecessor",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "Predecessors calculated by OPTICS."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/ordering",
          "name": "ordering",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.ordering",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "OPTICS ordered point indices (`ordering_`)"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.min_samples",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "",
            "description": "The same as the min_samples given to OPTICS. Up and down steep regions\ncan't have more then ``min_samples`` consecutive non-steep points.\nExpressed as an absolute number or a fraction of the number of samples\n(rounded to be at least 2)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/min_cluster_size",
          "name": "min_cluster_size",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.min_cluster_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "None",
            "description": "Minimum number of samples in an OPTICS cluster, expressed as an\nabsolute number or a fraction of the number of samples (rounded to be\nat least 2). If ``None``, the value of ``min_samples`` is used instead."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/xi",
          "name": "xi",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.xi",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float between 0 and 1",
            "default_value": "0.05",
            "description": "Determines the minimum steepness on the reachability plot that\nconstitutes a cluster boundary. For example, an upwards point in the\nreachability plot is defined by the ratio from one point to its\nsuccessor being at most 1-xi."
          },
          "type": {
            "kind": "NamedType",
            "name": "float between 0 and 1"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/predecessor_correction",
          "name": "predecessor_correction",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.predecessor_correction",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Correct clusters based on the calculated predecessors."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Automatically extract clusters according to the Xi-steep method.",
      "docstring": "Automatically extract clusters according to the Xi-steep method.\n\nParameters\n----------\nreachability : ndarray of shape (n_samples,)\n    Reachability distances calculated by OPTICS (`reachability_`)\n\npredecessor : ndarray of shape (n_samples,)\n    Predecessors calculated by OPTICS.\n\nordering : ndarray of shape (n_samples,)\n    OPTICS ordered point indices (`ordering_`)\n\nmin_samples : int > 1 or float between 0 and 1\n    The same as the min_samples given to OPTICS. Up and down steep regions\n    can't have more then ``min_samples`` consecutive non-steep points.\n    Expressed as an absolute number or a fraction of the number of samples\n    (rounded to be at least 2).\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n\npredecessor_correction : bool, default=True\n    Correct clusters based on the calculated predecessors.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    The labels assigned to samples. Points which are not included\n    in any cluster are labeled as -1.\n\nclusters : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to ``(end,\n    -start)`` (ascending) so that larger clusters encompassing smaller\n    clusters come after such nested smaller clusters. Since ``labels`` does\n    not reflect the hierarchy, usually ``len(clusters) >\n    np.unique(labels)``.",
      "code": "def cluster_optics_xi(*, reachability, predecessor, ordering, min_samples,\n                      min_cluster_size=None, xi=0.05,\n                      predecessor_correction=True):\n    \"\"\"Automatically extract clusters according to the Xi-steep method.\n\n    Parameters\n    ----------\n    reachability : ndarray of shape (n_samples,)\n        Reachability distances calculated by OPTICS (`reachability_`)\n\n    predecessor : ndarray of shape (n_samples,)\n        Predecessors calculated by OPTICS.\n\n    ordering : ndarray of shape (n_samples,)\n        OPTICS ordered point indices (`ordering_`)\n\n    min_samples : int > 1 or float between 0 and 1\n        The same as the min_samples given to OPTICS. Up and down steep regions\n        can't have more then ``min_samples`` consecutive non-steep points.\n        Expressed as an absolute number or a fraction of the number of samples\n        (rounded to be at least 2).\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n\n    predecessor_correction : bool, default=True\n        Correct clusters based on the calculated predecessors.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        The labels assigned to samples. Points which are not included\n        in any cluster are labeled as -1.\n\n    clusters : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to ``(end,\n        -start)`` (ascending) so that larger clusters encompassing smaller\n        clusters come after such nested smaller clusters. Since ``labels`` does\n        not reflect the hierarchy, usually ``len(clusters) >\n        np.unique(labels)``.\n    \"\"\"\n    n_samples = len(reachability)\n    _validate_size(min_samples, n_samples, 'min_samples')\n    if min_samples <= 1:\n        min_samples = max(2, int(min_samples * n_samples))\n    if min_cluster_size is None:\n        min_cluster_size = min_samples\n    _validate_size(min_cluster_size, n_samples, 'min_cluster_size')\n    if min_cluster_size <= 1:\n        min_cluster_size = max(2, int(min_cluster_size * n_samples))\n\n    clusters = _xi_cluster(reachability[ordering], predecessor[ordering],\n                           ordering, xi,\n                           min_samples, min_cluster_size,\n                           predecessor_correction)\n    labels = _extract_xi_labels(ordering, clusters)\n    return labels, clusters"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph",
      "name": "compute_optics_graph",
      "qname": "sklearn.cluster._optics.compute_optics_graph",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/X",
          "name": "X",
          "qname": "sklearn.cluster._optics.compute_optics_graph.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=\u2019precomputed\u2019.",
            "default_value": "",
            "description": "A feature array, or array of distances between samples if\nmetric='precomputed'"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples) if metric=\u2019precomputed\u2019."
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._optics.compute_optics_graph.min_samples",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "",
            "description": "The number of samples in a neighborhood for a point to be considered\nas a core point. Expressed as an absolute number or a fraction of the\nnumber of samples (rounded to be at least 2)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/max_eps",
          "name": "max_eps",
          "qname": "sklearn.cluster._optics.compute_optics_graph.max_eps",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "np.inf",
            "description": "The maximum distance between two samples for one to be considered as\nin the neighborhood of the other. Default value of ``np.inf`` will\nidentify clusters across all scales; reducing ``max_eps`` will result\nin shorter run times."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/metric",
          "name": "metric",
          "qname": "sklearn.cluster._optics.compute_optics_graph.metric",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'minkowski'",
            "description": "Metric to use for distance computation. Any metric from scikit-learn\nor scipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each\npair of instances (rows) and the resulting value recorded. The callable\nshould take two arrays as input and return one value indicating the\ndistance between them. This works for Scipy's metrics, but is less\nefficient than passing the metric name as a string. If metric is\n\"precomputed\", X is assumed to be a distance matrix and must be square.\n\nValid values for metric are:\n\n- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n  'manhattan']\n\n- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n  'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n  'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n  'yule']\n\nSee the documentation for scipy.spatial.distance for details on these\nmetrics."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/p",
          "name": "p",
          "qname": "sklearn.cluster._optics.compute_optics_graph.p",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Parameter for the Minkowski metric from\n:class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._optics.compute_optics_graph.metric_params",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._optics.compute_optics_graph.algorithm",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm\n  based on the values passed to :meth:`fit` method. (default)\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ball_tree",
              "kd_tree",
              "auto",
              "brute"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._optics.compute_optics_graph.leaf_size",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\naffect the speed of the construction and query, as well as the memory\nrequired to store the tree. The optimal value depends on the\nnature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._optics.compute_optics_graph.n_jobs",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Computes the OPTICS reachability graph.\n\nRead more in the :ref:`User Guide <optics>`.",
      "docstring": "Computes the OPTICS reachability graph.\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=\u2019precomputed\u2019.\n    A feature array, or array of distances between samples if\n    metric='precomputed'\n\nmin_samples : int > 1 or float between 0 and 1\n    The number of samples in a neighborhood for a point to be considered\n    as a core point. Expressed as an absolute number or a fraction of the\n    number of samples (rounded to be at least 2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method. (default)\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nordering_ : array of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : array of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\nreachability_ : array of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : array of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.",
      "code": "@_deprecate_positional_args\ndef compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params,\n                         algorithm, leaf_size, n_jobs):\n    \"\"\"Computes the OPTICS reachability graph.\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features), or \\\n            (n_samples, n_samples) if metric=\u2019precomputed\u2019.\n        A feature array, or array of distances between samples if\n        metric='precomputed'\n\n    min_samples : int > 1 or float between 0 and 1\n        The number of samples in a neighborhood for a point to be considered\n        as a core point. Expressed as an absolute number or a fraction of the\n        number of samples (rounded to be at least 2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n    p : int, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method. (default)\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n        affect the speed of the construction and query, as well as the memory\n        required to store the tree. The optimal value depends on the\n        nature of the problem.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    ordering_ : array of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : array of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    reachability_ : array of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : array of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n    \"\"\"\n    n_samples = X.shape[0]\n    _validate_size(min_samples, n_samples, 'min_samples')\n    if min_samples <= 1:\n        min_samples = max(2, int(min_samples * n_samples))\n\n    # Start all points as 'unprocessed' ##\n    reachability_ = np.empty(n_samples)\n    reachability_.fill(np.inf)\n    predecessor_ = np.empty(n_samples, dtype=int)\n    predecessor_.fill(-1)\n\n    nbrs = NearestNeighbors(n_neighbors=min_samples,\n                            algorithm=algorithm,\n                            leaf_size=leaf_size,\n                            metric=metric,\n                            metric_params=metric_params,\n                            p=p,\n                            n_jobs=n_jobs)\n\n    nbrs.fit(X)\n    # Here we first do a kNN query for each point, this differs from\n    # the original OPTICS that only used epsilon range queries.\n    # TODO: handle working_memory somehow?\n    core_distances_ = _compute_core_distances_(X=X, neighbors=nbrs,\n                                               min_samples=min_samples,\n                                               working_memory=None)\n    # OPTICS puts an upper limit on these, use inf for undefined.\n    core_distances_[core_distances_ > max_eps] = np.inf\n    np.around(core_distances_,\n              decimals=np.finfo(core_distances_.dtype).precision,\n              out=core_distances_)\n\n    # Main OPTICS loop. Not parallelizable. The order that entries are\n    # written to the 'ordering_' list is important!\n    # Note that this implementation is O(n^2) theoretically, but\n    # supposedly with very low constant factors.\n    processed = np.zeros(X.shape[0], dtype=bool)\n    ordering = np.zeros(X.shape[0], dtype=int)\n    for ordering_idx in range(X.shape[0]):\n        # Choose next based on smallest reachability distance\n        # (And prefer smaller ids on ties, possibly np.inf!)\n        index = np.where(processed == 0)[0]\n        point = index[np.argmin(reachability_[index])]\n\n        processed[point] = True\n        ordering[ordering_idx] = point\n        if core_distances_[point] != np.inf:\n            _set_reach_dist(core_distances_=core_distances_,\n                            reachability_=reachability_,\n                            predecessor_=predecessor_,\n                            point_index=point,\n                            processed=processed, X=X, nbrs=nbrs,\n                            metric=metric, metric_params=metric_params,\n                            p=p, max_eps=max_eps)\n    if np.all(np.isinf(reachability_)):\n        warnings.warn(\"All reachability values are inf. Set a larger\"\n                      \" max_eps or all data will be considered outliers.\",\n                      UserWarning)\n    return ordering, core_distances_, reachability_, predecessor_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._spectral.SpectralClustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_clusters",
          "default_value": "8",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "8",
            "description": "The dimension of the projection subspace."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/eigen_solver",
          "name": "eigen_solver",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.eigen_solver",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'arpack', 'lobpcg', 'amg'}",
            "default_value": "None",
            "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg\nto be installed. It can be faster on very large, sparse problems,\nbut may also lead to instabilities. If None, then ``'arpack'`` is\nused."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "amg",
              "lobpcg",
              "arpack"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_components",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "n_clusters",
            "description": "Number of eigenvectors to use for the spectral embedding"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "A pseudo random number generator used for the initialization of the\nlobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and by\nthe K-Means initialization. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of n_init\nconsecutive runs in terms of inertia. Only used if\n``assign_labels='kmeans'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/gamma",
          "name": "gamma",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.gamma",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\nIgnored for ``affinity='nearest_neighbors'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.affinity",
          "default_value": "'rbf'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'rbf'",
            "description": "How to construct the affinity matrix.\n - 'nearest_neighbors': construct the affinity matrix by computing a\n   graph of nearest neighbors.\n - 'rbf': construct the affinity matrix using a radial basis function\n   (RBF) kernel.\n - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n   where larger values indicate greater similarity between instances.\n - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n   of precomputed distances, and construct a binary affinity matrix\n   from the ``n_neighbors`` nearest neighbors of each instance.\n - one of the kernels supported by\n   :func:`~sklearn.metrics.pairwise_kernels`.\n\nOnly kernels that produce similarity scores (non-negative values that\nincrease with similarity) should be used. This property is not checked\nby the clustering algorithm."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_neighbors",
          "name": "n_neighbors",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_neighbors",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of neighbors to use when constructing the affinity matrix using\nthe nearest neighbors method. Ignored for ``affinity='rbf'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/eigen_tol",
          "name": "eigen_tol",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.eigen_tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Stopping criterion for eigendecomposition of the Laplacian matrix\nwhen ``eigen_solver='arpack'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/assign_labels",
          "name": "assign_labels",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.assign_labels",
          "default_value": "'kmeans'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'kmeans', 'discretize'}",
            "default_value": "'kmeans'",
            "description": "The strategy for assigning labels in the embedding space. There are two\nways to assign labels after the Laplacian embedding. k-means is a\npopular choice, but it can be sensitive to initialization.\nDiscretization is another approach which is less sensitive to random\ninitialization."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "kmeans",
              "discretize"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/degree",
          "name": "degree",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.degree",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "3",
            "description": "Degree of the polynomial kernel. Ignored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/coef0",
          "name": "coef0",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.coef0",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1",
            "description": "Zero coefficient for polynomial and sigmoid kernels.\nIgnored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/kernel_params",
          "name": "kernel_params",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.kernel_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict of str to any",
            "default_value": "None",
            "description": "Parameters (keyword arguments) and values for kernel passed as\ncallable object. Ignored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict of str to any"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run when `affinity='nearest_neighbors'`\nor `affinity='precomputed_nearest_neighbors'`. The neighbors search\nwill be done in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Verbosity mode.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, eigen_solver=None, n_components=None,\n                 random_state=None, n_init=10, gamma=1., affinity='rbf',\n                 n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans',\n                 degree=3, coef0=1, kernel_params=None, n_jobs=None,\n                 verbose=False):\n        self.n_clusters = n_clusters\n        self.eigen_solver = eigen_solver\n        self.n_components = n_components\n        self.random_state = random_state\n        self.n_init = n_init\n        self.gamma = gamma\n        self.affinity = affinity\n        self.n_neighbors = n_neighbors\n        self.eigen_tol = eigen_tol\n        self.assign_labels = assign_labels\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.n_jobs = n_jobs\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit",
      "name": "fit",
      "qname": "sklearn.cluster._spectral.SpectralClustering.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, similarities / affinities between\ninstances if ``affinity='precomputed'``, or distances between\ninstances if ``affinity='precomputed_nearest_neighbors``. If a\nsparse matrix is provided in a format other than ``csr_matrix``,\n``csc_matrix``, or ``coo_matrix``, it will be converted into a\nsparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform spectral clustering from features, or affinity matrix.",
      "docstring": "Perform spectral clustering from features, or affinity matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, similarities / affinities between\n    instances if ``affinity='precomputed'``, or distances between\n    instances if ``affinity='precomputed_nearest_neighbors``. If a\n    sparse matrix is provided in a format other than ``csr_matrix``,\n    ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n    sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                dtype=np.float64, ensure_min_samples=2)\n        allow_squared = self.affinity in [\"precomputed\",\n                                          \"precomputed_nearest_neighbors\"]\n        if X.shape[0] == X.shape[1] and not allow_squared:\n            warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n                          \"now constructs an affinity matrix from data. To use\"\n                          \" a custom affinity matrix, \"\n                          \"set ``affinity=precomputed``.\")\n\n        if self.affinity == 'nearest_neighbors':\n            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors,\n                                            include_self=True,\n                                            n_jobs=self.n_jobs)\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed_nearest_neighbors':\n            estimator = NearestNeighbors(n_neighbors=self.n_neighbors,\n                                         n_jobs=self.n_jobs,\n                                         metric=\"precomputed\").fit(X)\n            connectivity = estimator.kneighbors_graph(X=X, mode='connectivity')\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed':\n            self.affinity_matrix_ = X\n        else:\n            params = self.kernel_params\n            if params is None:\n                params = {}\n            if not callable(self.affinity):\n                params['gamma'] = self.gamma\n                params['degree'] = self.degree\n                params['coef0'] = self.coef0\n            self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n                                                     filter_params=True,\n                                                     **params)\n\n        random_state = check_random_state(self.random_state)\n        self.labels_ = spectral_clustering(self.affinity_matrix_,\n                                           n_clusters=self.n_clusters,\n                                           n_components=self.n_components,\n                                           eigen_solver=self.eigen_solver,\n                                           random_state=random_state,\n                                           n_init=self.n_init,\n                                           eigen_tol=self.eigen_tol,\n                                           assign_labels=self.assign_labels,\n                                           verbose=self.verbose)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, similarities / affinities between\ninstances if ``affinity='precomputed'``, or distances between\ninstances if ``affinity='precomputed_nearest_neighbors``. If a\nsparse matrix is provided in a format other than ``csr_matrix``,\n``csc_matrix``, or ``coo_matrix``, it will be converted into a\nsparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform spectral clustering from features, or affinity matrix,\nand return cluster labels.",
      "docstring": "Perform spectral clustering from features, or affinity matrix,\nand return cluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, similarities / affinities between\n    instances if ``affinity='precomputed'``, or distances between\n    instances if ``affinity='precomputed_nearest_neighbors``. If a\n    sparse matrix is provided in a format other than ``csr_matrix``,\n    ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n    sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def fit_predict(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering",
      "name": "spectral_clustering",
      "qname": "sklearn.cluster._spectral.spectral_clustering",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._spectral.spectral_clustering.affinity",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "The affinity matrix describing the relationship of the samples to\nembed. **Must be symmetric**.\n\nPossible examples:\n  - adjacency matrix of a graph,\n  - heat kernel of the pairwise distance matrix of the samples,\n  - symmetric k-nearest neighbours connectivity matrix of the samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._spectral.spectral_clustering.n_clusters",
          "default_value": "8",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of clusters to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/n_components",
          "name": "n_components",
          "qname": "sklearn.cluster._spectral.spectral_clustering.n_components",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "n_clusters",
            "description": "Number of eigenvectors to use for the spectral embedding"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/eigen_solver",
          "name": "eigen_solver",
          "qname": "sklearn.cluster._spectral.spectral_clustering.eigen_solver",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{None, 'arpack', 'lobpcg', or 'amg'}",
            "default_value": "",
            "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg\nto be installed. It can be faster on very large, sparse problems,\nbut may also lead to instabilities. If None, then ``'arpack'`` is\nused."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "amg",
              "lobpcg",
              "arpack"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._spectral.spectral_clustering.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "A pseudo random number generator used for the initialization of the\nlobpcg eigenvectors decomposition when eigen_solver == 'amg' and by\nthe K-Means initialization. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._spectral.spectral_clustering.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of n_init\nconsecutive runs in terms of inertia. Only used if\n``assign_labels='kmeans'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/eigen_tol",
          "name": "eigen_tol",
          "qname": "sklearn.cluster._spectral.spectral_clustering.eigen_tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Stopping criterion for eigendecomposition of the Laplacian matrix\nwhen using arpack eigen_solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/assign_labels",
          "name": "assign_labels",
          "qname": "sklearn.cluster._spectral.spectral_clustering.assign_labels",
          "default_value": "'kmeans'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'kmeans', 'discretize'}",
            "default_value": "'kmeans'",
            "description": "The strategy to use to assign labels in the embedding\nspace.  There are two ways to assign labels after the Laplacian\nembedding.  k-means can be applied and is a popular choice. But it can\nalso be sensitive to initialization. Discretization is another\napproach which is less sensitive to random initialization. See\nthe 'Multiclass spectral clustering' paper referenced below for\nmore details on the discretization approach."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "kmeans",
              "discretize"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._spectral.spectral_clustering.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Verbosity mode.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance, when clusters are\nnested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.",
      "docstring": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance, when clusters are\nnested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\naffinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n    The affinity matrix describing the relationship of the samples to\n    embed. **Must be symmetric**.\n\n    Possible examples:\n      - adjacency matrix of a graph,\n      - heat kernel of the pairwise distance matrix of the samples,\n      - symmetric k-nearest neighbours connectivity matrix of the samples.\n\nn_clusters : int, default=None\n    Number of clusters to extract.\n\nn_components : int, default=n_clusters\n    Number of eigenvectors to use for the spectral embedding\n\neigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization of the\n    lobpcg eigenvectors decomposition when eigen_solver == 'amg' and by\n    the K-Means initialization. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of n_init\n    consecutive runs in terms of inertia. Only used if\n    ``assign_labels='kmeans'``.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when using arpack eigen_solver.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans'\n    The strategy to use to assign labels in the embedding\n    space.  There are two ways to assign labels after the Laplacian\n    embedding.  k-means can be applied and is a popular choice. But it can\n    also be sensitive to initialization. Discretization is another\n    approach which is less sensitive to random initialization. See\n    the 'Multiclass spectral clustering' paper referenced below for\n    more details on the discretization approach.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nlabels : array of integers, shape: n_samples\n    The labels of the clusters.\n\nReferences\n----------\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\nNotes\n-----\nThe graph should contain only one connect component, elsewhere\nthe results make little sense.\n\nThis algorithm solves the normalized cut for k=2: it is a\nnormalized spectral clustering.",
      "code": "@_deprecate_positional_args\ndef spectral_clustering(affinity, *, n_clusters=8, n_components=None,\n                        eigen_solver=None, random_state=None, n_init=10,\n                        eigen_tol=0.0, assign_labels='kmeans',\n                        verbose=False):\n    \"\"\"Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster. For instance, when clusters are\n    nested circles on the 2D plane.\n\n    If affinity is the adjacency matrix of a graph, this method can be\n    used to find normalized graph cuts.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    affinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n        The affinity matrix describing the relationship of the samples to\n        embed. **Must be symmetric**.\n\n        Possible examples:\n          - adjacency matrix of a graph,\n          - heat kernel of the pairwise distance matrix of the samples,\n          - symmetric k-nearest neighbours connectivity matrix of the samples.\n\n    n_clusters : int, default=None\n        Number of clusters to extract.\n\n    n_components : int, default=n_clusters\n        Number of eigenvectors to use for the spectral embedding\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities. If None, then ``'arpack'`` is\n        used.\n\n    random_state : int, RandomState instance, default=None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors decomposition when eigen_solver == 'amg' and by\n        the K-Means initialization. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of n_init\n        consecutive runs in terms of inertia. Only used if\n        ``assign_labels='kmeans'``.\n\n    eigen_tol : float, default=0.0\n        Stopping criterion for eigendecomposition of the Laplacian matrix\n        when using arpack eigen_solver.\n\n    assign_labels : {'kmeans', 'discretize'}, default='kmeans'\n        The strategy to use to assign labels in the embedding\n        space.  There are two ways to assign labels after the Laplacian\n        embedding.  k-means can be applied and is a popular choice. But it can\n        also be sensitive to initialization. Discretization is another\n        approach which is less sensitive to random initialization. See\n        the 'Multiclass spectral clustering' paper referenced below for\n        more details on the discretization approach.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    labels : array of integers, shape: n_samples\n        The labels of the clusters.\n\n    References\n    ----------\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - Multiclass spectral clustering, 2003\n      Stella X. Yu, Jianbo Shi\n      https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\n    Notes\n    -----\n    The graph should contain only one connect component, elsewhere\n    the results make little sense.\n\n    This algorithm solves the normalized cut for k=2: it is a\n    normalized spectral clustering.\n    \"\"\"\n    if assign_labels not in ('kmeans', 'discretize'):\n        raise ValueError(\"The 'assign_labels' parameter should be \"\n                         \"'kmeans' or 'discretize', but '%s' was given\"\n                         % assign_labels)\n\n    random_state = check_random_state(random_state)\n    n_components = n_clusters if n_components is None else n_components\n\n    # The first eigenvector is constant only for fully connected graphs\n    # and should be kept for spectral clustering (drop_first = False)\n    # See spectral_embedding documentation.\n    maps = spectral_embedding(affinity, n_components=n_components,\n                              eigen_solver=eigen_solver,\n                              random_state=random_state,\n                              eigen_tol=eigen_tol, drop_first=False)\n    if verbose:\n        print(f'Computing label assignment using {assign_labels}')\n\n    if assign_labels == 'kmeans':\n        _, labels, _ = k_means(maps, n_clusters, random_state=random_state,\n                               n_init=n_init, verbose=verbose)\n    else:\n        labels = discretize(maps, random_state=random_state)\n\n    return labels"
    },
    {
      "id": "scikit-learn/sklearn.cluster.setup/configuration",
      "name": "configuration",
      "qname": "sklearn.cluster.setup.configuration",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster.setup/configuration/parent_package",
          "name": "parent_package",
          "qname": "sklearn.cluster.setup.configuration.parent_package",
          "default_value": "''",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster.setup/configuration/top_path",
          "name": "top_path",
          "qname": "sklearn.cluster.setup.configuration.top_path",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def configuration(parent_package='', top_path=None):\n    from numpy.distutils.misc_util import Configuration\n\n    libraries = []\n    if os.name == 'posix':\n        libraries.append('m')\n\n    config = Configuration('cluster', parent_package, top_path)\n\n    config.add_extension('_dbscan_inner',\n                         sources=['_dbscan_inner.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         language=\"c++\")\n\n    config.add_extension('_hierarchical_fast',\n                         sources=['_hierarchical_fast.pyx'],\n                         language=\"c++\",\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_k_means_fast',\n                         sources=['_k_means_fast.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_k_means_lloyd',\n                         sources=['_k_means_lloyd.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_k_means_elkan',\n                         sources=['_k_means_elkan.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_subpackage('tests')\n\n    return config"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__",
      "name": "__init__",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/transformers",
          "name": "transformers",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.transformers",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "list of tuples",
            "default_value": "",
            "description": "List of (name, transformer, columns) tuples specifying the\ntransformer objects to be applied to subsets of the data.\n\nname : str\n    Like in Pipeline and FeatureUnion, this allows the transformer and\n    its parameters to be set using ``set_params`` and searched in grid\n    search.\ntransformer : {'drop', 'passthrough'} or estimator\n    Estimator must support :term:`fit` and :term:`transform`.\n    Special-cased strings 'drop' and 'passthrough' are accepted as\n    well, to indicate to drop the columns or to pass them through\n    untransformed, respectively.\ncolumns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n    Indexes the data on its second axis. Integers are interpreted as\n    positional columns, while strings can reference DataFrame columns\n    by name.  A scalar string or int should be used where\n    ``transformer`` expects X to be a 1d array-like (vector),\n    otherwise a 2d array will be passed to the transformer.\n    A callable is passed the input data `X` and can return any of the\n    above. To select multiple columns by name or dtype, you can use\n    :obj:`make_column_selector`."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of tuples"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/remainder",
          "name": "remainder",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.remainder",
          "default_value": "'drop'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'drop', 'passthrough'} or estimator",
            "default_value": "'drop'",
            "description": "By default, only the specified columns in `transformers` are\ntransformed and combined in the output, and the non-specified\ncolumns are dropped. (default of ``'drop'``).\nBy specifying ``remainder='passthrough'``, all remaining columns that\nwere not specified in `transformers` will be automatically passed\nthrough. This subset of columns is concatenated with the output of\nthe transformers.\nBy setting ``remainder`` to be an estimator, the remaining\nnon-specified columns will use the ``remainder`` estimator. The\nestimator must support :term:`fit` and :term:`transform`.\nNote that using this feature requires that the DataFrame columns\ninput at :term:`fit` and :term:`transform` have identical order."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "drop",
                  "passthrough"
                ]
              },
              {
                "kind": "NamedType",
                "name": "estimator"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/sparse_threshold",
          "name": "sparse_threshold",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.sparse_threshold",
          "default_value": "0.3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.3",
            "description": "If the output of the different transformers contains sparse matrices,\nthese will be stacked as a sparse matrix if the overall density is\nlower than this value. Use ``sparse_threshold=0`` to always return\ndense.  When the transformed output consists of all dense data, the\nstacked result will be dense, and this keyword will be ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/transformer_weights",
          "name": "transformer_weights",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.transformer_weights",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Multiplicative weights for features per transformer. The output of the\ntransformer is multiplied by these weights. Keys are transformer names,\nvalues the weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the time elapsed while fitting each transformer will be\nprinted as it is completed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 transformers, *,\n                 remainder='drop',\n                 sparse_threshold=0.3,\n                 n_jobs=None,\n                 transformer_weights=None,\n                 verbose=False):\n        self.transformers = transformers\n        self.remainder = remainder\n        self.sparse_threshold = sparse_threshold\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit",
      "name": "fit",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit/X",
          "name": "X",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, dataframe} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data, of which specified subsets are used to fit the\ntransformers."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit/y",
          "name": "y",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,...)",
            "default_value": "None",
            "description": "Targets for supervised learning."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,...)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit all transformers using X.",
      "docstring": "Fit all transformers using X.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    Input data, of which specified subsets are used to fit the\n    transformers.\n\ny : array-like of shape (n_samples,...), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nself : ColumnTransformer\n    This estimator",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,...), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        self : ColumnTransformer\n            This estimator\n\n        \"\"\"\n        # we use fit_transform to make sure to set sparse_output_ (for which we\n        # need the transformed data) to have consistent output type in predict\n        self.fit_transform(X, y=y)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform/X",
          "name": "X",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, dataframe} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data, of which specified subsets are used to fit the\ntransformers."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform/y",
          "name": "y",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Targets for supervised learning."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit all transformers, transform the data and concatenate results.",
      "docstring": "Fit all transformers, transform the data and concatenate results.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    Input data, of which specified subsets are used to fit the\n    transformers.\n\ny : array-like of shape (n_samples,), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nX_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers. If\n    any result is a sparse matrix, everything will be converted to\n    sparse matrices.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        # TODO: this should be `feature_names_in_` when we start having it\n        if hasattr(X, \"columns\"):\n            self._feature_names_in = np.asarray(X.columns)\n        else:\n            self._feature_names_in = None\n        X = _check_X(X)\n        # set n_features_in_ attribute\n        self._check_n_features(X, reset=True)\n        self._validate_transformers()\n        self._validate_column_callables(X)\n        self._validate_remainder(X)\n\n        result = self._fit_transform(X, y, _fit_transform_one)\n\n        if not result:\n            self._update_fitted_transformers([])\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*result)\n\n        # determine if concatenated output will be sparse or not\n        if any(sparse.issparse(X) for X in Xs):\n            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)\n            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)\n                        else X.size for X in Xs)\n            density = nnz / total\n            self.sparse_output_ = density < self.sparse_threshold\n        else:\n            self.sparse_output_ = False\n\n        self._update_fitted_transformers(transformers)\n        self._validate_output(Xs)\n\n        return self._hstack(list(Xs))"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_feature_names",
      "name": "get_feature_names",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_feature_names",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_feature_names/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_feature_names.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Get feature names from all transformers.",
      "docstring": "Get feature names from all transformers.\n\nReturns\n-------\nfeature_names : list of strings\n    Names of the features produced by transform.",
      "code": "    def get_feature_names(self):\n        \"\"\"Get feature names from all transformers.\n\n        Returns\n        -------\n        feature_names : list of strings\n            Names of the features produced by transform.\n        \"\"\"\n        check_is_fitted(self)\n        feature_names = []\n        for name, trans, column, _ in self._iter(fitted=True):\n            if trans == 'drop' or _is_empty_column_selection(column):\n                continue\n            if trans == 'passthrough':\n                if hasattr(self, '_df_columns'):\n                    if ((not isinstance(column, slice))\n                            and all(isinstance(col, str) for col in column)):\n                        feature_names.extend(column)\n                    else:\n                        feature_names.extend(self._df_columns[column])\n                else:\n                    indices = np.arange(self._n_features)\n                    feature_names.extend(['x%d' % i for i in indices[column]])\n                continue\n            if not hasattr(trans, 'get_feature_names'):\n                raise AttributeError(\"Transformer %s (type %s) does not \"\n                                     \"provide get_feature_names.\"\n                                     % (str(name), type(trans).__name__))\n            feature_names.extend([name + \"__\" + f for f in\n                                  trans.get_feature_names()])\n        return feature_names"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params",
      "name": "get_params",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_params",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_params.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params/deep",
          "name": "deep",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_params.deep",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformers` of the\n`ColumnTransformer`.",
      "docstring": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformers` of the\n`ColumnTransformer`.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values.",
      "code": "    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `transformers` of the\n        `ColumnTransformer`.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        return self._get_params('_transformers', deep=deep)"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/named_transformers_@getter",
      "name": "named_transformers_",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.named_transformers_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/named_transformers_@getter/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.named_transformers_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Access the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name.\nKeys are transformer names and values are the fitted transformer\nobjects.",
      "docstring": "Access the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name.\nKeys are transformer names and values are the fitted transformer\nobjects.",
      "code": "    @property\n    def named_transformers_(self):\n        \"\"\"Access the fitted transformer by name.\n\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n        \"\"\"\n        # Use Bunch object to improve autocomplete\n        return Bunch(**{name: trans for name, trans, _\n                        in self.transformers_})"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params",
      "name": "set_params",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.set_params",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.set_params.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params/kwargs",
          "name": "kwargs",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.set_params.kwargs",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that you\ncan directly set the parameters of the estimators contained in\n`transformers` of `ColumnTransformer`.",
      "docstring": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that you\ncan directly set the parameters of the estimators contained in\n`transformers` of `ColumnTransformer`.\n\nReturns\n-------\nself",
      "code": "    def set_params(self, **kwargs):\n        \"\"\"Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``. Note that you\n        can directly set the parameters of the estimators contained in\n        `transformers` of `ColumnTransformer`.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._set_params('_transformers', **kwargs)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform",
      "name": "transform",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform/X",
          "name": "X",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, dataframe} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data to be transformed by subset."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X separately by each transformer, concatenate results.",
      "docstring": "Transform X separately by each transformer, concatenate results.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    The data to be transformed by subset.\n\nReturns\n-------\nX_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers. If\n    any result is a sparse matrix, everything will be converted to\n    sparse matrices.",
      "code": "    def transform(self, X):\n        \"\"\"Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            The data to be transformed by subset.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        check_is_fitted(self)\n        X = _check_X(X)\n        if hasattr(X, \"columns\"):\n            X_feature_names = np.asarray(X.columns)\n        else:\n            X_feature_names = None\n\n        self._check_n_features(X, reset=False)\n        if (self._feature_names_in is not None and\n            X_feature_names is not None and\n                np.any(self._feature_names_in != X_feature_names)):\n            raise RuntimeError(\n                \"Given feature/column names do not match the ones for the \"\n                \"data given during fit.\"\n            )\n        Xs = self._fit_transform(X, None, _transform_one, fitted=True)\n        self._validate_output(Xs)\n\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        return self._hstack(list(Xs))"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__",
      "name": "__call__",
      "qname": "sklearn.compose._column_transformer.make_column_selector.__call__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__call__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__/df",
          "name": "df",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__call__.df",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "dataframe of shape (n_features, n_samples)",
            "default_value": "",
            "description": "DataFrame to select columns from."
          },
          "type": {
            "kind": "NamedType",
            "name": "dataframe of shape (n_features, n_samples)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Callable for column selection to be used by a\n:class:`ColumnTransformer`.",
      "docstring": "Callable for column selection to be used by a\n:class:`ColumnTransformer`.\n\nParameters\n----------\ndf : dataframe of shape (n_features, n_samples)\n    DataFrame to select columns from.",
      "code": "    def __call__(self, df):\n        \"\"\"Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n        Parameters\n        ----------\n        df : dataframe of shape (n_features, n_samples)\n            DataFrame to select columns from.\n        \"\"\"\n        if not hasattr(df, 'iloc'):\n            raise ValueError(\"make_column_selector can only be applied to \"\n                             \"pandas dataframes\")\n        df_row = df.iloc[:1]\n        if self.dtype_include is not None or self.dtype_exclude is not None:\n            df_row = df_row.select_dtypes(include=self.dtype_include,\n                                          exclude=self.dtype_exclude)\n        cols = df_row.columns\n        if self.pattern is not None:\n            cols = cols[cols.str.contains(self.pattern, regex=True)]\n        return cols.tolist()"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__",
      "name": "__init__",
      "qname": "sklearn.compose._column_transformer.make_column_selector.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/pattern",
          "name": "pattern",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.pattern",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Name of columns containing this regex pattern will be included. If\nNone, column selection will not be selected based on pattern."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/dtype_include",
          "name": "dtype_include",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.dtype_include",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "column dtype or list of column dtypes",
            "default_value": "None",
            "description": "A selection of dtypes to include. For more details, see\n:meth:`pandas.DataFrame.select_dtypes`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "column dtype"
              },
              {
                "kind": "NamedType",
                "name": "list of column dtypes"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/dtype_exclude",
          "name": "dtype_exclude",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.dtype_exclude",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "column dtype or list of column dtypes",
            "default_value": "None",
            "description": "A selection of dtypes to exclude. For more details, see\n:meth:`pandas.DataFrame.select_dtypes`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "column dtype"
              },
              {
                "kind": "NamedType",
                "name": "list of column dtypes"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, pattern=None, *, dtype_include=None,\n                 dtype_exclude=None):\n        self.pattern = pattern\n        self.dtype_include = dtype_include\n        self.dtype_exclude = dtype_exclude"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer",
      "name": "make_column_transformer",
      "qname": "sklearn.compose._column_transformer.make_column_transformer",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/transformers",
          "name": "transformers",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.transformers",
          "default_value": null,
          "assigned_by": "POSITIONAL_VARARG",
          "is_public": true,
          "docstring": {
            "type": "tuples",
            "default_value": "",
            "description": "Tuples of the form (transformer, columns) specifying the\ntransformer objects to be applied to subsets of the data.\n\ntransformer : {'drop', 'passthrough'} or estimator\n    Estimator must support :term:`fit` and :term:`transform`.\n    Special-cased strings 'drop' and 'passthrough' are accepted as\n    well, to indicate to drop the columns or to pass them through\n    untransformed, respectively.\ncolumns : str,  array-like of str, int, array-like of int, slice,                 array-like of bool or callable\n    Indexes the data on its second axis. Integers are interpreted as\n    positional columns, while strings can reference DataFrame columns\n    by name. A scalar string or int should be used where\n    ``transformer`` expects X to be a 1d array-like (vector),\n    otherwise a 2d array will be passed to the transformer.\n    A callable is passed the input data `X` and can return any of the\n    above. To select multiple columns by name or dtype, you can use\n    :obj:`make_column_selector`."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuples"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/remainder",
          "name": "remainder",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.remainder",
          "default_value": "'drop'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'drop', 'passthrough'} or estimator",
            "default_value": "'drop'",
            "description": "By default, only the specified columns in `transformers` are\ntransformed and combined in the output, and the non-specified\ncolumns are dropped. (default of ``'drop'``).\nBy specifying ``remainder='passthrough'``, all remaining columns that\nwere not specified in `transformers` will be automatically passed\nthrough. This subset of columns is concatenated with the output of\nthe transformers.\nBy setting ``remainder`` to be an estimator, the remaining\nnon-specified columns will use the ``remainder`` estimator. The\nestimator must support :term:`fit` and :term:`transform`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "drop",
                  "passthrough"
                ]
              },
              {
                "kind": "NamedType",
                "name": "estimator"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/sparse_threshold",
          "name": "sparse_threshold",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.sparse_threshold",
          "default_value": "0.3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.3",
            "description": "If the transformed output consists of a mix of sparse and dense data,\nit will be stacked as a sparse matrix if the density is lower than this\nvalue. Use ``sparse_threshold=0`` to always return dense.\nWhen the transformed output consists of all sparse or all dense data,\nthe stacked result will be sparse or dense, respectively, and this\nkeyword will be ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/verbose",
          "name": "verbose",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the time elapsed while fitting each transformer will be\nprinted as it is completed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Construct a ColumnTransformer from the given transformers.\n\nThis is a shorthand for the ColumnTransformer constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will\nbe given names automatically based on their types. It also does not allow\nweighting with ``transformer_weights``.\n\nRead more in the :ref:`User Guide <make_column_transformer>`.",
      "docstring": "Construct a ColumnTransformer from the given transformers.\n\nThis is a shorthand for the ColumnTransformer constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will\nbe given names automatically based on their types. It also does not allow\nweighting with ``transformer_weights``.\n\nRead more in the :ref:`User Guide <make_column_transformer>`.\n\nParameters\n----------\n*transformers : tuples\n    Tuples of the form (transformer, columns) specifying the\n    transformer objects to be applied to subsets of the data.\n\n    transformer : {'drop', 'passthrough'} or estimator\n        Estimator must support :term:`fit` and :term:`transform`.\n        Special-cased strings 'drop' and 'passthrough' are accepted as\n        well, to indicate to drop the columns or to pass them through\n        untransformed, respectively.\n    columns : str,  array-like of str, int, array-like of int, slice,                 array-like of bool or callable\n        Indexes the data on its second axis. Integers are interpreted as\n        positional columns, while strings can reference DataFrame columns\n        by name. A scalar string or int should be used where\n        ``transformer`` expects X to be a 1d array-like (vector),\n        otherwise a 2d array will be passed to the transformer.\n        A callable is passed the input data `X` and can return any of the\n        above. To select multiple columns by name or dtype, you can use\n        :obj:`make_column_selector`.\n\nremainder : {'drop', 'passthrough'} or estimator, default='drop'\n    By default, only the specified columns in `transformers` are\n    transformed and combined in the output, and the non-specified\n    columns are dropped. (default of ``'drop'``).\n    By specifying ``remainder='passthrough'``, all remaining columns that\n    were not specified in `transformers` will be automatically passed\n    through. This subset of columns is concatenated with the output of\n    the transformers.\n    By setting ``remainder`` to be an estimator, the remaining\n    non-specified columns will use the ``remainder`` estimator. The\n    estimator must support :term:`fit` and :term:`transform`.\n\nsparse_threshold : float, default=0.3\n    If the transformed output consists of a mix of sparse and dense data,\n    it will be stacked as a sparse matrix if the density is lower than this\n    value. Use ``sparse_threshold=0`` to always return dense.\n    When the transformed output consists of all sparse or all dense data,\n    the stacked result will be sparse or dense, respectively, and this\n    keyword will be ignored.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nReturns\n-------\nct : ColumnTransformer\n\nSee Also\n--------\nColumnTransformer : Class that allows combining the\n    outputs of multiple transformer objects used on column subsets\n    of the data into a single feature space.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> make_column_transformer(\n...     (StandardScaler(), ['numerical_column']),\n...     (OneHotEncoder(), ['categorical_column']))\nColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                 ['numerical_column']),\n                                ('onehotencoder', OneHotEncoder(...),\n                                 ['categorical_column'])])",
      "code": "def make_column_transformer(*transformers,\n                            remainder='drop',\n                            sparse_threshold=0.3,\n                            n_jobs=None,\n                            verbose=False):\n    \"\"\"Construct a ColumnTransformer from the given transformers.\n\n    This is a shorthand for the ColumnTransformer constructor; it does not\n    require, and does not permit, naming the transformers. Instead, they will\n    be given names automatically based on their types. It also does not allow\n    weighting with ``transformer_weights``.\n\n    Read more in the :ref:`User Guide <make_column_transformer>`.\n\n    Parameters\n    ----------\n    *transformers : tuples\n        Tuples of the form (transformer, columns) specifying the\n        transformer objects to be applied to subsets of the data.\n\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns : str,  array-like of str, int, array-like of int, slice, \\\n                array-like of bool or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name. A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n\n    sparse_threshold : float, default=0.3\n        If the transformed output consists of a mix of sparse and dense data,\n        it will be stacked as a sparse matrix if the density is lower than this\n        value. Use ``sparse_threshold=0`` to always return dense.\n        When the transformed output consists of all sparse or all dense data,\n        the stacked result will be sparse or dense, respectively, and this\n        keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    Returns\n    -------\n    ct : ColumnTransformer\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> make_column_transformer(\n    ...     (StandardScaler(), ['numerical_column']),\n    ...     (OneHotEncoder(), ['categorical_column']))\n    ColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                     ['numerical_column']),\n                                    ('onehotencoder', OneHotEncoder(...),\n                                     ['categorical_column'])])\n\n    \"\"\"\n    # transformer_weights keyword is not passed through because the user\n    # would need to know the automatically generated names of the transformers\n    transformer_list = _get_transformer_list(transformers)\n    return ColumnTransformer(transformer_list, n_jobs=n_jobs,\n                             remainder=remainder,\n                             sparse_threshold=sparse_threshold,\n                             verbose=verbose)"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/regressor",
          "name": "regressor",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.regressor",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "Regressor object such as derived from ``RegressorMixin``. This\nregressor will automatically be cloned each time prior to fitting.\nIf regressor is ``None``, ``LinearRegression()`` is created and used."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/transformer",
          "name": "transformer",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.transformer",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "Estimator object such as derived from ``TransformerMixin``. Cannot be\nset at the same time as ``func`` and ``inverse_func``. If\n``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\nthe transformer will be an identity transformer. Note that the\ntransformer will be cloned during fitting. Also, the transformer is\nrestricting ``y`` to be a numpy array."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/func",
          "name": "func",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.func",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "function",
            "default_value": "None",
            "description": "Function to apply to ``y`` before passing to ``fit``. Cannot be set at\nthe same time as ``transformer``. The function needs to return a\n2-dimensional array. If ``func`` is ``None``, the function used will be\nthe identity function."
          },
          "type": {
            "kind": "NamedType",
            "name": "function"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/inverse_func",
          "name": "inverse_func",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.inverse_func",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "function",
            "default_value": "None",
            "description": "Function to apply to the prediction of the regressor. Cannot be set at\nthe same time as ``transformer`` as well. The function needs to return\na 2-dimensional array. The inverse function is used to return\npredictions to the same space of the original training labels."
          },
          "type": {
            "kind": "NamedType",
            "name": "function"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/check_inverse",
          "name": "check_inverse",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.check_inverse",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to check that ``transform`` followed by ``inverse_transform``\nor ``func`` followed by ``inverse_func`` leads to the original targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, regressor=None, *, transformer=None,\n                 func=None, inverse_func=None, check_inverse=True):\n        self.regressor = regressor\n        self.transformer = transformer\n        self.func = func\n        self.inverse_func = inverse_func\n        self.check_inverse = check_inverse"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit",
      "name": "fit",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/X",
          "name": "X",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/y",
          "name": "y",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/fit_params",
          "name": "fit_params",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.fit_params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "",
            "description": "Parameters passed to the ``fit`` method of the underlying\nregressor."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model according to the given training data.",
      "docstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\n**fit_params : dict\n    Parameters passed to the ``fit`` method of the underlying\n    regressor.\n\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **fit_params : dict\n            Parameters passed to the ``fit`` method of the underlying\n            regressor.\n\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        y = check_array(y, accept_sparse=False, force_all_finite=True,\n                        ensure_2d=False, dtype='numeric')\n\n        # store the number of dimension of the target to predict an array of\n        # similar shape at predict\n        self._training_dim = y.ndim\n\n        # transformers are designed to modify X which is 2d dimensional, we\n        # need to modify y accordingly.\n        if y.ndim == 1:\n            y_2d = y.reshape(-1, 1)\n        else:\n            y_2d = y\n        self._fit_transformer(y_2d)\n\n        # transform y and convert back to 1d array if needed\n        y_trans = self.transformer_.transform(y_2d)\n        # FIXME: a FunctionTransformer can return a 1D array even when validate\n        # is set to True. Therefore, we need to check the number of dimension\n        # first.\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\n            y_trans = y_trans.squeeze(axis=1)\n\n        if self.regressor is None:\n            from ..linear_model import LinearRegression\n            self.regressor_ = LinearRegression()\n        else:\n            self.regressor_ = clone(self.regressor)\n\n        self.regressor_.fit(X, y_trans, **fit_params)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/n_features_in_@getter",
      "name": "n_features_in_",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.n_features_in_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/n_features_in_@getter/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.n_features_in_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def n_features_in_(self):\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() returns False the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n\n        return self.regressor_.n_features_in_"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict",
      "name": "predict",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the ``inverse_func`` or\n``inverse_transform`` is applied before returning the prediction.",
      "docstring": "Predict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the ``inverse_func`` or\n``inverse_transform`` is applied before returning the prediction.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\ny_hat : ndarray of shape (n_samples,)\n    Predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict using the base regressor, applying inverse.\n\n        The regressor is used to predict and the ``inverse_func`` or\n        ``inverse_transform`` is applied before returning the prediction.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y_hat : ndarray of shape (n_samples,)\n            Predicted values.\n\n        \"\"\"\n        check_is_fitted(self)\n        pred = self.regressor_.predict(X)\n        if pred.ndim == 1:\n            pred_trans = self.transformer_.inverse_transform(\n                pred.reshape(-1, 1))\n        else:\n            pred_trans = self.transformer_.inverse_transform(pred)\n        if (self._training_dim == 1 and\n                pred_trans.ndim == 2 and pred_trans.shape[1] == 1):\n            pred_trans = pred_trans.squeeze(axis=1)\n\n        return pred_trans"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the support of robust location and covariance estimates\nis computed, and a covariance estimate is recomputed from it,\nwithout centering the data.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, the robust location and covariance are directly computed\nwith the FastMCD algorithm without additional treatment."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/support_fraction",
          "name": "support_fraction",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.support_fraction",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The proportion of points to be included in the support of the raw\nMCD estimate. If None, the minimum value of support_fraction will\nbe used within the algorithm: `[n_sample + n_features + 1] / 2`.\nRange is (0, 1)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/contamination",
          "name": "contamination",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.contamination",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The amount of contamination of the data set, i.e. the proportion\nof outliers in the data set. Range is (0, 0.5)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines the pseudo random number generator for shuffling\nthe data. Pass an int for reproducible results across multiple function\ncalls. See :term: `Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, contamination=0.1,\n                 random_state=None):\n        super().__init__(\n            store_precision=store_precision,\n            assume_centered=assume_centered,\n            support_fraction=support_fraction,\n            random_state=random_state)\n        self.contamination = contamination"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function",
      "name": "decision_function",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the decision function of the given observations.",
      "docstring": "Compute the decision function of the given observations.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\ndecision : ndarray of shape (n_samples,)\n    Decision function of the samples.\n    It is equal to the shifted Mahalanobis distances.\n    The threshold for being an outlier is 0, which ensures a\n    compatibility with other outlier detection algorithms.",
      "code": "    def decision_function(self, X):\n        \"\"\"Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.\n        \"\"\"\n        check_is_fitted(self)\n        negative_mahal_dist = self.score_samples(X)\n        return negative_mahal_dist - self.offset_"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit",
      "name": "fit",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the EllipticEnvelope model.",
      "docstring": "Fit the EllipticEnvelope model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n    Not used, present for API consistency by convention.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n        \"\"\"\n        super().fit(X)\n        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict",
      "name": "predict",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the labels (1 inlier, -1 outlier) of X according to the\nfitted model.",
      "docstring": "Predict the labels (1 inlier, -1 outlier) of X according to the\nfitted model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and +1 for inliers.",
      "code": "    def predict(self, X):\n        \"\"\"\n        Predict the labels (1 inlier, -1 outlier) of X according to the\n        fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            Returns -1 for anomalies/outliers and +1 for inliers.\n        \"\"\"\n        X = check_array(X)\n        is_inlier = np.full(X.shape[0], -1, dtype=int)\n        values = self.decision_function(X)\n        is_inlier[values >= 0] = 1\n\n        return is_inlier"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score",
      "name": "score",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Test samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/y",
          "name": "y",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
            "default_value": "",
            "description": "True labels for X."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_outputs)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
      "docstring": "Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    Mean accuracy of self.predict(X) w.r.t. y.",
      "code": "    def score(self, X, y, sample_weight=None):\n        \"\"\"Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples",
      "name": "score_samples",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the negative Mahalanobis distances.",
      "docstring": "Compute the negative Mahalanobis distances.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nnegative_mahal_distances : array-like of shape (n_samples,)\n    Opposite of the Mahalanobis distances.",
      "code": "    def score_samples(self, X):\n        \"\"\"Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.\n        \"\"\"\n        check_is_fitted(self)\n        return -self.mahalanobis(X)"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specifies if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data are not centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False (default), data are centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm",
      "name": "error_norm",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/comp_cov",
          "name": "comp_cov",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.comp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_features, n_features)",
            "default_value": "",
            "description": "The covariance to compare with."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/norm",
          "name": "norm",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.norm",
          "default_value": "'frobenius'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{\"frobenius\", \"spectral\"}",
            "default_value": "\"frobenius\"",
            "description": "The type of norm used to compute the error. Available error types:\n- 'frobenius' (default): sqrt(tr(A^t.A))\n- 'spectral': sqrt(max(eigenvalues(A^t.A))\nwhere A is the error ``(comp_cov - self.covariance_)``."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "frobenius",
              "spectral"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/scaling",
          "name": "scaling",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.scaling",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True (default), the squared error norm is divided by n_features.\nIf False, the squared error norm is not rescaled."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/squared",
          "name": "squared",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.squared",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to compute the squared error norm or the error norm.\nIf True (default), the squared error norm is returned.\nIf False, the error norm is returned."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).",
      "docstring": "Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n\nParameters\n----------\ncomp_cov : array-like of shape (n_features, n_features)\n    The covariance to compare with.\n\nnorm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n    The type of norm used to compute the error. Available error types:\n    - 'frobenius' (default): sqrt(tr(A^t.A))\n    - 'spectral': sqrt(max(eigenvalues(A^t.A))\n    where A is the error ``(comp_cov - self.covariance_)``.\n\nscaling : bool, default=True\n    If True (default), the squared error norm is divided by n_features.\n    If False, the squared error norm is not rescaled.\n\nsquared : bool, default=True\n    Whether to compute the squared error norm or the error norm.\n    If True (default), the squared error norm is returned.\n    If False, the error norm is returned.\n\nReturns\n-------\nresult : float\n    The Mean Squared Error (in the sense of the Frobenius norm) between\n    `self` and `comp_cov` covariance estimators.",
      "code": "    def error_norm(self, comp_cov, norm='frobenius', scaling=True,\n                   squared=True):\n        \"\"\"Computes the Mean Squared Error between two covariance estimators.\n        (In the sense of the Frobenius norm).\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.\n\n        Returns\n        -------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.\n        \"\"\"\n        # compute the error\n        error = comp_cov - self.covariance_\n        # compute the error norm\n        if norm == \"frobenius\":\n            squared_norm = np.sum(error ** 2)\n        elif norm == \"spectral\":\n            squared_norm = np.amax(linalg.svdvals(np.dot(error.T, error)))\n        else:\n            raise NotImplementedError(\n                \"Only spectral and frobenius norms are implemented\")\n        # optionally scale the error norm\n        if scaling:\n            squared_norm = squared_norm / error.shape[0]\n        # finally get either the squared norm or the norm\n        if squared:\n            result = squared_norm\n        else:\n            result = np.sqrt(squared_norm)\n\n        return result"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit",
      "name": "fit",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits the Maximum Likelihood Estimator covariance model\naccording to the given training data and parameters.",
      "docstring": "Fits the Maximum Likelihood Estimator covariance model\naccording to the given training data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n  Training data, where n_samples is the number of samples and\n  n_features is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits the Maximum Likelihood Estimator covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n          Training data, where n_samples is the number of samples and\n          n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/get_precision",
      "name": "get_precision",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.get_precision",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/get_precision/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.get_precision.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Getter for the precision matrix.",
      "docstring": "Getter for the precision matrix.\n\nReturns\n-------\nprecision_ : array-like of shape (n_features, n_features)\n    The precision matrix associated to the current covariance object.",
      "code": "    def get_precision(self):\n        \"\"\"Getter for the precision matrix.\n\n        Returns\n        -------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.\n        \"\"\"\n        if self.store_precision:\n            precision = self.precision_\n        else:\n            precision = linalg.pinvh(self.covariance_, check_finite=False)\n        return precision"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis",
      "name": "mahalanobis",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis/X",
          "name": "X",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The observations, the Mahalanobis distances of the which we\ncompute. Observations are assumed to be drawn from the same\ndistribution than the data used in fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Computes the squared Mahalanobis distances of given observations.",
      "docstring": "Computes the squared Mahalanobis distances of given observations.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The observations, the Mahalanobis distances of the which we\n    compute. Observations are assumed to be drawn from the same\n    distribution than the data used in fit.\n\nReturns\n-------\ndist : ndarray of shape (n_samples,)\n    Squared Mahalanobis distances of the observations.",
      "code": "    def mahalanobis(self, X):\n        \"\"\"Computes the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.\n\n        Returns\n        -------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.\n        \"\"\"\n        precision = self.get_precision()\n        # compute mahalanobis distances\n        dist = pairwise_distances(X, self.location_[np.newaxis, :],\n                                  metric='mahalanobis', VI=precision)\n\n        return np.reshape(dist, (len(X),)) ** 2"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score",
      "name": "score",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score/X_test",
          "name": "X_test",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score.X_test",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Test data of which we compute the likelihood, where n_samples is\nthe number of samples and n_features is the number of features.\nX_test is assumed to be drawn from the same distribution than\nthe data used in fit (including centering)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score/y",
          "name": "y",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.",
      "docstring": "Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n\nParameters\n----------\nX_test : array-like of shape (n_samples, n_features)\n    Test data of which we compute the likelihood, where n_samples is\n    the number of samples and n_features is the number of features.\n    X_test is assumed to be drawn from the same distribution than\n    the data used in fit (including centering).\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nres : float\n    The likelihood of the data set with `self.covariance_` as an\n    estimator of its covariance matrix.",
      "code": "    def score(self, X_test, y=None):\n        \"\"\"Computes the log-likelihood of a Gaussian data set with\n        `self.covariance_` as an estimator of its covariance matrix.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where n_samples is\n            the number of samples and n_features is the number of features.\n            X_test is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        res : float\n            The likelihood of the data set with `self.covariance_` as an\n            estimator of its covariance matrix.\n        \"\"\"\n        # compute empirical covariance of the test set\n        test_cov = empirical_covariance(\n            X_test - self.location_, assume_centered=True)\n        # compute log likelihood\n        res = log_likelihood(test_cov, self.get_precision())\n\n        return res"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/empirical_covariance",
      "name": "empirical_covariance",
      "qname": "sklearn.covariance._empirical_covariance.empirical_covariance",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/empirical_covariance/X",
          "name": "X",
          "qname": "sklearn.covariance._empirical_covariance.empirical_covariance.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/empirical_covariance/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._empirical_covariance.empirical_covariance.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Computes the Maximum likelihood covariance estimator",
      "docstring": "Computes the Maximum likelihood covariance estimator\n\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data will be centered before computation.\n\nReturns\n-------\ncovariance : ndarray of shape (n_features, n_features)\n    Empirical covariance (Maximum Likelihood Estimator).\n\nExamples\n--------\n>>> from sklearn.covariance import empirical_covariance\n>>> X = [[1,1,1],[1,1,1],[1,1,1],\n...      [0,0,0],[0,0,0],[0,0,0]]\n>>> empirical_covariance(X)\narray([[0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25]])",
      "code": "@_deprecate_positional_args\ndef empirical_covariance(X, *, assume_centered=False):\n    \"\"\"Computes the Maximum likelihood covariance estimator\n\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data will be centered before computation.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        Empirical covariance (Maximum Likelihood Estimator).\n\n    Examples\n    --------\n    >>> from sklearn.covariance import empirical_covariance\n    >>> X = [[1,1,1],[1,1,1],[1,1,1],\n    ...      [0,0,0],[0,0,0],[0,0,0]]\n    >>> empirical_covariance(X)\n    array([[0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25]])\n    \"\"\"\n    X = np.asarray(X)\n\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n\n    if X.shape[0] == 1:\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n\n    if assume_centered:\n        covariance = np.dot(X.T, X) / X.shape[0]\n    else:\n        covariance = np.cov(X.T, bias=1)\n\n    if covariance.ndim == 0:\n        covariance = np.array([[covariance]])\n    return covariance"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/log_likelihood",
      "name": "log_likelihood",
      "qname": "sklearn.covariance._empirical_covariance.log_likelihood",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/log_likelihood/emp_cov",
          "name": "emp_cov",
          "qname": "sklearn.covariance._empirical_covariance.log_likelihood.emp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features, n_features)",
            "default_value": "",
            "description": "Maximum Likelihood Estimator of covariance."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/log_likelihood/precision",
          "name": "precision",
          "qname": "sklearn.covariance._empirical_covariance.log_likelihood.precision",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features, n_features)",
            "default_value": "",
            "description": "The precision matrix of the covariance model to be tested."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Computes the sample mean of the log_likelihood under a covariance model\n\ncomputes the empirical expected log-likelihood (accounting for the\nnormalization terms and scaling), allowing for universal comparison (beyond\nthis software package)",
      "docstring": "Computes the sample mean of the log_likelihood under a covariance model\n\ncomputes the empirical expected log-likelihood (accounting for the\nnormalization terms and scaling), allowing for universal comparison (beyond\nthis software package)\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    Maximum Likelihood Estimator of covariance.\n\nprecision : ndarray of shape (n_features, n_features)\n    The precision matrix of the covariance model to be tested.\n\nReturns\n-------\nlog_likelihood_ : float\n    Sample mean of the log-likelihood.",
      "code": "def log_likelihood(emp_cov, precision):\n    \"\"\"Computes the sample mean of the log_likelihood under a covariance model\n\n    computes the empirical expected log-likelihood (accounting for the\n    normalization terms and scaling), allowing for universal comparison (beyond\n    this software package)\n\n    Parameters\n    ----------\n    emp_cov : ndarray of shape (n_features, n_features)\n        Maximum Likelihood Estimator of covariance.\n\n    precision : ndarray of shape (n_features, n_features)\n        The precision matrix of the covariance model to be tested.\n\n    Returns\n    -------\n    log_likelihood_ : float\n        Sample mean of the log-likelihood.\n    \"\"\"\n    p = precision.shape[0]\n    log_likelihood_ = - np.sum(emp_cov * precision) + fast_logdet(precision)\n    log_likelihood_ -= p * np.log(2 * np.pi)\n    log_likelihood_ /= 2.\n    return log_likelihood_"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.alpha",
          "default_value": "0.01",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "The regularization parameter: the higher alpha, the more\nregularization, the sparser the inverse covariance.\nRange is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/mode",
          "name": "mode",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.mode",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'lars'}",
            "default_value": "'cd'",
            "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\nwhich is more numerically stable."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/tol",
          "name": "tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/enet_tol",
          "name": "enet_tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.enet_tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If verbose is True, the objective function and dual gap are\nplotted at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data are not centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data are centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, alpha=.01, *, mode='cd', tol=1e-4, enet_tol=1e-4,\n                 max_iter=100, verbose=False, assume_centered=False):\n        super().__init__(assume_centered=assume_centered)\n        self.alpha = alpha\n        self.mode = mode\n        self.tol = tol\n        self.enet_tol = enet_tol\n        self.max_iter = max_iter\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit",
      "name": "fit",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits the GraphicalLasso model to X.",
      "docstring": "Fits the GraphicalLasso model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, ensure_min_samples=2,\n                                estimator=self)\n\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=self.verbose, return_n_iter=True)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/alphas",
          "name": "alphas",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.alphas",
          "default_value": "4",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or array-like of shape (n_alphas,), dtype=float",
            "default_value": "4",
            "description": "If an integer is given, it fixes the number of points on the\ngrids of alpha to be used. If a list is given, it gives the\ngrid to be used. See the notes in the class docstring for\nmore details. Range is (0, inf] when floats given."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_alphas,)"
              },
              {
                "kind": "NamedType",
                "name": "dtype=float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/n_refinements",
          "name": "n_refinements",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.n_refinements",
          "default_value": "4",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "4",
            "description": "The number of times the grid is refined. Not used if explicit\nvalues of alphas are passed. Range is [1, inf)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/cv",
          "name": "cv",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.cv",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, cross-validation generator or iterable",
            "default_value": "None",
            "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\n- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs :class:`KFold` is used.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here.\n\n.. versionchanged:: 0.20\n    ``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "cross-validation generator"
              },
              {
                "kind": "NamedType",
                "name": "iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/tol",
          "name": "tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/enet_tol",
          "name": "enet_tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.enet_tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/mode",
          "name": "mode",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.mode",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'lars'}",
            "default_value": "'cd'",
            "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where number of features is greater\nthan number of samples. Elsewhere prefer cd which is more numerically\nstable."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "number of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionchanged:: v0.20\n   `n_jobs` default changed from 1 to None"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If verbose is True, the objective function and duality gap are\nprinted at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data are not centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data are centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, alphas=4, n_refinements=4, cv=None, tol=1e-4,\n                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=None,\n                 verbose=False, assume_centered=False):\n        super().__init__(\n            mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,\n            max_iter=max_iter, assume_centered=assume_centered)\n        self.alphas = alphas\n        self.n_refinements = n_refinements\n        self.cv = cv\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/cv_alphas_@getter",
      "name": "cv_alphas_",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.cv_alphas_",
      "decorators": [
        "deprecated(\"The cv_alphas_ attribute is deprecated in version 0.24 in favor of cv_results_['alpha'] and will be removed in version 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/cv_alphas_@getter/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.cv_alphas_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"The cv_alphas_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_['alpha'] and will be removed in version 1.1 \"\n        \"(renaming of 0.26).\"\n    )\n    @property\n    def cv_alphas_(self):\n        return self.cv_results_['alphas'].tolist()"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit",
      "name": "fit",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits the GraphicalLasso covariance model to X.",
      "docstring": "Fits the GraphicalLasso covariance model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, estimator=self)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n\n        cv = check_cv(self.cv, y, classifier=False)\n\n        # List of (alpha, scores, covs)\n        path = list()\n        n_alphas = self.alphas\n        inner_verbose = max(0, self.verbose - 1)\n\n        if isinstance(n_alphas, Sequence):\n            alphas = self.alphas\n            n_refinements = 1\n        else:\n            n_refinements = self.n_refinements\n            alpha_1 = alpha_max(emp_cov)\n            alpha_0 = 1e-2 * alpha_1\n            alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1),\n                                 n_alphas)[::-1]\n\n        t0 = time.time()\n        for i in range(n_refinements):\n            with warnings.catch_warnings():\n                # No need to see the convergence warnings on this grid:\n                # they will always be points that will not converge\n                # during the cross-validation\n                warnings.simplefilter('ignore', ConvergenceWarning)\n                # Compute the cross-validated loss on the current grid\n\n                # NOTE: Warm-restarting graphical_lasso_path has been tried,\n                # and this did not allow to gain anything\n                # (same execution time with or without).\n                this_path = Parallel(\n                    n_jobs=self.n_jobs,\n                    verbose=self.verbose\n                )(delayed(graphical_lasso_path)(X[train], alphas=alphas,\n                                                X_test=X[test], mode=self.mode,\n                                                tol=self.tol,\n                                                enet_tol=self.enet_tol,\n                                                max_iter=int(.1 *\n                                                             self.max_iter),\n                                                verbose=inner_verbose)\n                  for train, test in cv.split(X, y))\n\n            # Little danse to transform the list in what we need\n            covs, _, scores = zip(*this_path)\n            covs = zip(*covs)\n            scores = zip(*scores)\n            path.extend(zip(alphas, scores, covs))\n            path = sorted(path, key=operator.itemgetter(0), reverse=True)\n\n            # Find the maximum (avoid using built in 'max' function to\n            # have a fully-reproducible selection of the smallest alpha\n            # in case of equality)\n            best_score = -np.inf\n            last_finite_idx = 0\n            for index, (alpha, scores, _) in enumerate(path):\n                this_score = np.mean(scores)\n                if this_score >= .1 / np.finfo(np.float64).eps:\n                    this_score = np.nan\n                if np.isfinite(this_score):\n                    last_finite_idx = index\n                if this_score >= best_score:\n                    best_score = this_score\n                    best_index = index\n\n            # Refine the grid\n            if best_index == 0:\n                # We do not need to go back: we have chosen\n                # the highest value of alpha for which there are\n                # non-zero coefficients\n                alpha_1 = path[0][0]\n                alpha_0 = path[1][0]\n            elif (best_index == last_finite_idx\n                    and not best_index == len(path) - 1):\n                # We have non-converged models on the upper bound of the\n                # grid, we need to refine the grid there\n                alpha_1 = path[best_index][0]\n                alpha_0 = path[best_index + 1][0]\n            elif best_index == len(path) - 1:\n                alpha_1 = path[best_index][0]\n                alpha_0 = 0.01 * path[best_index][0]\n            else:\n                alpha_1 = path[best_index - 1][0]\n                alpha_0 = path[best_index + 1][0]\n\n            if not isinstance(n_alphas, Sequence):\n                alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0),\n                                     n_alphas + 2)\n                alphas = alphas[1:-1]\n\n            if self.verbose and n_refinements > 1:\n                print('[GraphicalLassoCV] Done refinement % 2i out of'\n                      ' %i: % 3is' % (i + 1, n_refinements, time.time() - t0))\n\n        path = list(zip(*path))\n        grid_scores = list(path[1])\n        alphas = list(path[0])\n        # Finally, compute the score with alpha = 0\n        alphas.append(0)\n        grid_scores.append(cross_val_score(EmpiricalCovariance(), X,\n                                           cv=cv, n_jobs=self.n_jobs,\n                                           verbose=inner_verbose))\n        grid_scores = np.array(grid_scores)\n        self.cv_results_ = {'alphas': np.array(alphas)}\n        for i in range(grid_scores.shape[1]):\n            key = \"split{}_score\".format(i)\n            self.cv_results_[key] = grid_scores[:, i]\n\n        self.cv_results_[\"mean_score\"] = np.mean(grid_scores, axis=1)\n        self.cv_results_[\"std_score\"] = np.std(grid_scores, axis=1)\n\n        best_alpha = alphas[best_index]\n        self.alpha_ = best_alpha\n\n        # Finally fit the model with the selected alpha\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=inner_verbose, return_n_iter=True)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/grid_scores_@getter",
      "name": "grid_scores_",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.grid_scores_",
      "decorators": [
        "deprecated('The grid_scores_ attribute is deprecated in version 0.24 in favor of cv_results_ and will be removed in version 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/grid_scores_@getter/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.grid_scores_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"The grid_scores_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_ and will be removed in version 1.1 (renaming of 0.26).\"\n    )\n    @property\n    def grid_scores_(self):\n        # remove 3 for mean_score, std_score, and alphas\n        n_alphas = len(self.cv_results_) - 3\n        return np.asarray(\n            [self.cv_results_[\"split{}_score\".format(i)]\n             for i in range(n_alphas)]).T"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso",
      "name": "graphical_lasso",
      "qname": "sklearn.covariance._graph_lasso.graphical_lasso",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/emp_cov",
          "name": "emp_cov",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.emp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features, n_features)",
            "default_value": "",
            "description": "Empirical covariance from which to compute the covariance estimate."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/alpha",
          "name": "alpha",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.alpha",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "",
            "description": "The regularization parameter: the higher alpha, the more\nregularization, the sparser the inverse covariance.\nRange is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/cov_init",
          "name": "cov_init",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.cov_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_features, n_features)",
            "default_value": "None",
            "description": "The initial guess for the covariance. If None, then the empirical\ncovariance is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/mode",
          "name": "mode",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.mode",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'lars'}",
            "default_value": "'cd'",
            "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\nwhich is more numerically stable."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/tol",
          "name": "tol",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/enet_tol",
          "name": "enet_tol",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.enet_tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/max_iter",
          "name": "max_iter",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/verbose",
          "name": "verbose",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If verbose is True, the objective function and dual gap are\nprinted at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/return_costs",
          "name": "return_costs",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.return_costs",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "Flase",
            "description": "If return_costs is True, the objective function and dual gap\nat each iteration are returned."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/eps",
          "name": "eps",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.eps",
          "default_value": "np.finfo(np.float64).eps",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "eps",
            "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Default is `np.finfo(np.float64).eps`."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "l1-penalized covariance estimator\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    graph_lasso has been renamed to graphical_lasso",
      "docstring": "l1-penalized covariance estimator\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    graph_lasso has been renamed to graphical_lasso\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    Empirical covariance from which to compute the covariance estimate.\n\nalpha : float\n    The regularization parameter: the higher alpha, the more\n    regularization, the sparser the inverse covariance.\n    Range is (0, inf].\n\ncov_init : array of shape (n_features, n_features), default=None\n    The initial guess for the covariance. If None, then the empirical\n    covariance is used.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    The maximum number of iterations.\n\nverbose : bool, default=False\n    If verbose is True, the objective function and dual gap are\n    printed at each iteration.\n\nreturn_costs : bool, default=Flase\n    If return_costs is True, the objective function and dual gap\n    at each iteration are returned.\n\neps : float, default=eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Default is `np.finfo(np.float64).eps`.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncovariance : ndarray of shape (n_features, n_features)\n    The estimated covariance matrix.\n\nprecision : ndarray of shape (n_features, n_features)\n    The estimated (sparse) precision matrix.\n\ncosts : list of (objective, dual_gap) pairs\n    The list of values of the objective function and the dual gap at\n    each iteration. Returned only if return_costs is True.\n\nn_iter : int\n    Number of iterations. Returned only if `return_n_iter` is set to True.\n\nSee Also\n--------\nGraphicalLasso, GraphicalLassoCV\n\nNotes\n-----\nThe algorithm employed to solve this problem is the GLasso algorithm,\nfrom the Friedman 2008 Biostatistics paper. It is the same algorithm\nas in the R `glasso` package.\n\nOne possible difference with the `glasso` R package is that the\ndiagonal coefficients are not penalized.",
      "code": "@_deprecate_positional_args\ndef graphical_lasso(emp_cov, alpha, *, cov_init=None, mode='cd', tol=1e-4,\n                    enet_tol=1e-4, max_iter=100, verbose=False,\n                    return_costs=False, eps=np.finfo(np.float64).eps,\n                    return_n_iter=False):\n    \"\"\"l1-penalized covariance estimator\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        graph_lasso has been renamed to graphical_lasso\n\n    Parameters\n    ----------\n    emp_cov : ndarray of shape (n_features, n_features)\n        Empirical covariance from which to compute the covariance estimate.\n\n    alpha : float\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n        Range is (0, inf].\n\n    cov_init : array of shape (n_features, n_features), default=None\n        The initial guess for the covariance. If None, then the empirical\n        covariance is used.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        The maximum number of iterations.\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and dual gap are\n        printed at each iteration.\n\n    return_costs : bool, default=Flase\n        If return_costs is True, the objective function and dual gap\n        at each iteration are returned.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        The estimated covariance matrix.\n\n    precision : ndarray of shape (n_features, n_features)\n        The estimated (sparse) precision matrix.\n\n    costs : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n    n_iter : int\n        Number of iterations. Returned only if `return_n_iter` is set to True.\n\n    See Also\n    --------\n    GraphicalLasso, GraphicalLassoCV\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n    \"\"\"\n    _, n_features = emp_cov.shape\n    if alpha == 0:\n        if return_costs:\n            precision_ = linalg.inv(emp_cov)\n            cost = - 2. * log_likelihood(emp_cov, precision_)\n            cost += n_features * np.log(2 * np.pi)\n            d_gap = np.sum(emp_cov * precision_) - n_features\n            if return_n_iter:\n                return emp_cov, precision_, (cost, d_gap), 0\n            else:\n                return emp_cov, precision_, (cost, d_gap)\n        else:\n            if return_n_iter:\n                return emp_cov, linalg.inv(emp_cov), 0\n            else:\n                return emp_cov, linalg.inv(emp_cov)\n    if cov_init is None:\n        covariance_ = emp_cov.copy()\n    else:\n        covariance_ = cov_init.copy()\n    # As a trivial regularization (Tikhonov like), we scale down the\n    # off-diagonal coefficients of our starting point: This is needed, as\n    # in the cross-validation the cov_init can easily be\n    # ill-conditioned, and the CV loop blows. Beside, this takes\n    # conservative stand-point on the initial conditions, and it tends to\n    # make the convergence go faster.\n    covariance_ *= 0.95\n    diagonal = emp_cov.flat[::n_features + 1]\n    covariance_.flat[::n_features + 1] = diagonal\n    precision_ = linalg.pinvh(covariance_)\n\n    indices = np.arange(n_features)\n    costs = list()\n    # The different l1 regression solver have different numerical errors\n    if mode == 'cd':\n        errors = dict(over='raise', invalid='ignore')\n    else:\n        errors = dict(invalid='raise')\n    try:\n        # be robust to the max_iter=0 edge case, see:\n        # https://github.com/scikit-learn/scikit-learn/issues/4134\n        d_gap = np.inf\n        # set a sub_covariance buffer\n        sub_covariance = np.copy(covariance_[1:, 1:], order='C')\n        for i in range(max_iter):\n            for idx in range(n_features):\n                # To keep the contiguous matrix `sub_covariance` equal to\n                # covariance_[indices != idx].T[indices != idx]\n                # we only need to update 1 column and 1 line when idx changes\n                if idx > 0:\n                    di = idx - 1\n                    sub_covariance[di] = covariance_[di][indices != idx]\n                    sub_covariance[:, di] = covariance_[:, di][indices != idx]\n                else:\n                    sub_covariance[:] = covariance_[1:, 1:]\n                row = emp_cov[idx, indices != idx]\n                with np.errstate(**errors):\n                    if mode == 'cd':\n                        # Use coordinate descent\n                        coefs = -(precision_[indices != idx, idx]\n                                  / (precision_[idx, idx] + 1000 * eps))\n                        coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n                            coefs, alpha, 0, sub_covariance,\n                            row, row, max_iter, enet_tol,\n                            check_random_state(None), False)\n                    else:\n                        # Use LARS\n                        _, _, coefs = lars_path_gram(\n                            Xy=row, Gram=sub_covariance, n_samples=row.size,\n                            alpha_min=alpha / (n_features - 1), copy_Gram=True,\n                            eps=eps, method='lars', return_path=False)\n                # Update the precision matrix\n                precision_[idx, idx] = (\n                    1. / (covariance_[idx, idx]\n                          - np.dot(covariance_[indices != idx, idx], coefs)))\n                precision_[indices != idx, idx] = (- precision_[idx, idx]\n                                                   * coefs)\n                precision_[idx, indices != idx] = (- precision_[idx, idx]\n                                                   * coefs)\n                coefs = np.dot(sub_covariance, coefs)\n                covariance_[idx, indices != idx] = coefs\n                covariance_[indices != idx, idx] = coefs\n            if not np.isfinite(precision_.sum()):\n                raise FloatingPointError('The system is too ill-conditioned '\n                                         'for this solver')\n            d_gap = _dual_gap(emp_cov, precision_, alpha)\n            cost = _objective(emp_cov, precision_, alpha)\n            if verbose:\n                print('[graphical_lasso] Iteration '\n                      '% 3i, cost % 3.2e, dual gap %.3e'\n                      % (i, cost, d_gap))\n            if return_costs:\n                costs.append((cost, d_gap))\n            if np.abs(d_gap) < tol:\n                break\n            if not np.isfinite(cost) and i > 0:\n                raise FloatingPointError('Non SPD result: the system is '\n                                         'too ill-conditioned for this solver')\n        else:\n            warnings.warn('graphical_lasso: did not converge after '\n                          '%i iteration: dual gap: %.3e'\n                          % (max_iter, d_gap), ConvergenceWarning)\n    except FloatingPointError as e:\n        e.args = (e.args[0]\n                  + '. The system is too ill-conditioned for this solver',)\n        raise e\n\n    if return_costs:\n        if return_n_iter:\n            return covariance_, precision_, costs, i + 1\n        else:\n            return covariance_, precision_, costs\n    else:\n        if return_n_iter:\n            return covariance_, precision_, i + 1\n        else:\n            return covariance_, precision_"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the support of the robust location and the covariance\nestimates is computed, and a covariance estimate is recomputed from\nit, without centering the data.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, the robust location and covariance are directly computed\nwith the FastMCD algorithm without additional treatment."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/support_fraction",
          "name": "support_fraction",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.support_fraction",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The proportion of points to be included in the support of the raw\nMCD estimate. Default is None, which implies that the minimum\nvalue of support_fraction will be used within the algorithm:\n`(n_sample + n_features + 1) / 2`. The parameter must be in the range\n(0, 1)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, random_state=None):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n        self.support_fraction = support_fraction\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance",
      "name": "correct_covariance",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.correct_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.correct_covariance.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance/data",
          "name": "data",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.correct_covariance.data",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix, with p features and n samples.\nThe data set must be the one which was used to compute\nthe raw estimates."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [RVD]_.",
      "docstring": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [RVD]_.\n\nParameters\n----------\ndata : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n    The data set must be the one which was used to compute\n    the raw estimates.\n\nReturns\n-------\ncovariance_corrected : ndarray of shape (n_features, n_features)\n    Corrected robust covariance estimate.\n\nReferences\n----------\n\n.. [RVD] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS",
      "code": "    def correct_covariance(self, data):\n        \"\"\"Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n\n        # Check that the covariance of the support data is not equal to 0.\n        # Otherwise self.dist_ = 0 and thus correction = 0.\n        n_samples = len(self.dist_)\n        n_support = np.sum(self.support_)\n        if n_support < n_samples and np.allclose(self.raw_covariance_, 0):\n            raise ValueError('The covariance matrix of the support data '\n                             'is equal to 0, try to increase support_fraction')\n        correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5)\n        covariance_corrected = self.raw_covariance_ * correction\n        self.dist_ /= correction\n        return covariance_corrected"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit",
      "name": "fit",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits a Minimum Covariance Determinant with the FastMCD algorithm.",
      "docstring": "Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator='MinCovDet')\n        random_state = check_random_state(self.random_state)\n        n_samples, n_features = X.shape\n        # check that the empirical covariance is full rank\n        if (linalg.svdvals(np.dot(X.T, X)) > 1e-8).sum() != n_features:\n            warnings.warn(\"The covariance matrix associated to your dataset \"\n                          \"is not full rank\")\n        # compute and store raw estimates\n        raw_location, raw_covariance, raw_support, raw_dist = fast_mcd(\n            X, support_fraction=self.support_fraction,\n            cov_computation_method=self._nonrobust_covariance,\n            random_state=random_state)\n        if self.assume_centered:\n            raw_location = np.zeros(n_features)\n            raw_covariance = self._nonrobust_covariance(X[raw_support],\n                                                        assume_centered=True)\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(raw_covariance)\n            raw_dist = np.sum(np.dot(X, precision) * X, 1)\n        self.raw_location_ = raw_location\n        self.raw_covariance_ = raw_covariance\n        self.raw_support_ = raw_support\n        self.location_ = raw_location\n        self.support_ = raw_support\n        self.dist_ = raw_dist\n        # obtain consistency at normal models\n        self.correct_covariance(X)\n        # re-weight estimator\n        self.reweight_covariance(X)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance",
      "name": "reweight_covariance",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance/data",
          "name": "data",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance.data",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix, with p features and n samples.\nThe data set must be the one which was used to compute\nthe raw estimates."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates) described\nin [RVDriessen]_.",
      "docstring": "Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates) described\nin [RVDriessen]_.\n\nParameters\n----------\ndata : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n    The data set must be the one which was used to compute\n    the raw estimates.\n\nReturns\n-------\nlocation_reweighted : ndarray of shape (n_features,)\n    Re-weighted robust location estimate.\n\ncovariance_reweighted : ndarray of shape (n_features, n_features)\n    Re-weighted robust covariance estimate.\n\nsupport_reweighted : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the re-weighted robust location and covariance estimates.\n\nReferences\n----------\n\n.. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS",
      "code": "    def reweight_covariance(self, data):\n        \"\"\"Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n        n_samples, n_features = data.shape\n        mask = self.dist_ < chi2(n_features).isf(0.025)\n        if self.assume_centered:\n            location_reweighted = np.zeros(n_features)\n        else:\n            location_reweighted = data[mask].mean(0)\n        covariance_reweighted = self._nonrobust_covariance(\n            data[mask], assume_centered=self.assume_centered)\n        support_reweighted = np.zeros(n_samples, dtype=bool)\n        support_reweighted[mask] = True\n        self._set_covariance(covariance_reweighted)\n        self.location_ = location_reweighted\n        self.support_ = support_reweighted\n        X_centered = data - self.location_\n        self.dist_ = np.sum(\n            np.dot(X_centered, self.get_precision()) * X_centered, 1)\n        return location_reweighted, covariance_reweighted, support_reweighted"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd",
      "name": "fast_mcd",
      "qname": "sklearn.covariance._robust_covariance.fast_mcd",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/X",
          "name": "X",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix, with p features and n samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/support_fraction",
          "name": "support_fraction",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.support_fraction",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The proportion of points to be included in the support of the raw\nMCD estimate. Default is `None`, which implies that the minimum\nvalue of `support_fraction` will be used within the algorithm:\n`(n_sample + n_features + 1) / 2`. This parameter must be in the\nrange (0, 1)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/cov_computation_method",
          "name": "cov_computation_method",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.cov_computation_method",
          "default_value": "empirical_covariance",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": ":func:`sklearn.covariance.empirical_covariance`",
            "description": "The function which will be used to compute the covariance.\nMust return an array of shape (n_features, n_features)."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/random_state",
          "name": "random_state",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.random_state",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.",
      "docstring": "Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is `None`, which implies that the minimum\n    value of `support_fraction` will be used within the algorithm:\n    `(n_sample + n_features + 1) / 2`. This parameter must be in the\n    range (0, 1).\n\ncov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n    The function which will be used to compute the covariance.\n    Must return an array of shape (n_features, n_features).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nReturns\n-------\nlocation : ndarray of shape (n_features,)\n    Robust location of the data.\n\ncovariance : ndarray of shape (n_features, n_features)\n    Robust covariance of the features.\n\nsupport : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the robust location and covariance estimates of the data set.\n\nNotes\n-----\nThe FastMCD algorithm has been introduced by Rousseuw and Van Driessen\nin \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n1999, American Statistical Association and the American Society\nfor Quality, TECHNOMETRICS\".\nThe principle is to compute robust estimates and random subsets before\npooling them into a larger subsets, and finally into the full data set.\nDepending on the size of the initial sample, we have one, two or three\nsuch computation levels.\n\nNote that only raw estimates are returned. If one is interested in\nthe correction and reweighting steps described in [RouseeuwVan]_,\nsee the MinCovDet object.\n\nReferences\n----------\n\n.. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS\n\n.. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
      "code": "def fast_mcd(X, support_fraction=None,\n             cov_computation_method=empirical_covariance,\n             random_state=None):\n    \"\"\"Estimates the Minimum Covariance Determinant matrix.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data matrix, with p features and n samples.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. Default is `None`, which implies that the minimum\n        value of `support_fraction` will be used within the algorithm:\n        `(n_sample + n_features + 1) / 2`. This parameter must be in the\n        range (0, 1).\n\n    cov_computation_method : callable, \\\n            default=:func:`sklearn.covariance.empirical_covariance`\n        The function which will be used to compute the covariance.\n        Must return an array of shape (n_features, n_features).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term: `Glossary <random_state>`.\n\n    Returns\n    -------\n    location : ndarray of shape (n_features,)\n        Robust location of the data.\n\n    covariance : ndarray of shape (n_features, n_features)\n        Robust covariance of the features.\n\n    support : ndarray of shape (n_samples,), dtype=bool\n        A mask of the observations that have been used to compute\n        the robust location and covariance estimates of the data set.\n\n    Notes\n    -----\n    The FastMCD algorithm has been introduced by Rousseuw and Van Driessen\n    in \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n    1999, American Statistical Association and the American Society\n    for Quality, TECHNOMETRICS\".\n    The principle is to compute robust estimates and random subsets before\n    pooling them into a larger subsets, and finally into the full data set.\n    Depending on the size of the initial sample, we have one, two or three\n    such computation levels.\n\n    Note that only raw estimates are returned. If one is interested in\n    the correction and reweighting steps described in [RouseeuwVan]_,\n    see the MinCovDet object.\n\n    References\n    ----------\n\n    .. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n        Determinant Estimator, 1999, American Statistical Association\n        and the American Society for Quality, TECHNOMETRICS\n\n    .. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400\n    \"\"\"\n    random_state = check_random_state(random_state)\n\n    X = check_array(X, ensure_min_samples=2, estimator='fast_mcd')\n    n_samples, n_features = X.shape\n\n    # minimum breakdown value\n    if support_fraction is None:\n        n_support = int(np.ceil(0.5 * (n_samples + n_features + 1)))\n    else:\n        n_support = int(support_fraction * n_samples)\n\n    # 1-dimensional case quick computation\n    # (Rousseeuw, P. J. and Leroy, A. M. (2005) References, in Robust\n    #  Regression and Outlier Detection, John Wiley & Sons, chapter 4)\n    if n_features == 1:\n        if n_support < n_samples:\n            # find the sample shortest halves\n            X_sorted = np.sort(np.ravel(X))\n            diff = X_sorted[n_support:] - X_sorted[:(n_samples - n_support)]\n            halves_start = np.where(diff == np.min(diff))[0]\n            # take the middle points' mean to get the robust location estimate\n            location = 0.5 * (X_sorted[n_support + halves_start] +\n                              X_sorted[halves_start]).mean()\n            support = np.zeros(n_samples, dtype=bool)\n            X_centered = X - location\n            support[np.argsort(np.abs(X_centered), 0)[:n_support]] = True\n            covariance = np.asarray([[np.var(X[support])]])\n            location = np.array([location])\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(covariance)\n            dist = (np.dot(X_centered, precision) * (X_centered)).sum(axis=1)\n        else:\n            support = np.ones(n_samples, dtype=bool)\n            covariance = np.asarray([[np.var(X)]])\n            location = np.asarray([np.mean(X)])\n            X_centered = X - location\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(covariance)\n            dist = (np.dot(X_centered, precision) * (X_centered)).sum(axis=1)\n    # Starting FastMCD algorithm for p-dimensional case\n    if (n_samples > 500) and (n_features > 1):\n        # 1. Find candidate supports on subsets\n        # a. split the set in subsets of size ~ 300\n        n_subsets = n_samples // 300\n        n_samples_subsets = n_samples // n_subsets\n        samples_shuffle = random_state.permutation(n_samples)\n        h_subset = int(np.ceil(n_samples_subsets *\n                       (n_support / float(n_samples))))\n        # b. perform a total of 500 trials\n        n_trials_tot = 500\n        # c. select 10 best (location, covariance) for each subset\n        n_best_sub = 10\n        n_trials = max(10, n_trials_tot // n_subsets)\n        n_best_tot = n_subsets * n_best_sub\n        all_best_locations = np.zeros((n_best_tot, n_features))\n        try:\n            all_best_covariances = np.zeros((n_best_tot, n_features,\n                                             n_features))\n        except MemoryError:\n            # The above is too big. Let's try with something much small\n            # (and less optimal)\n            n_best_tot = 10\n            all_best_covariances = np.zeros((n_best_tot, n_features,\n                                             n_features))\n            n_best_sub = 2\n        for i in range(n_subsets):\n            low_bound = i * n_samples_subsets\n            high_bound = low_bound + n_samples_subsets\n            current_subset = X[samples_shuffle[low_bound:high_bound]]\n            best_locations_sub, best_covariances_sub, _, _ = select_candidates(\n                current_subset, h_subset, n_trials,\n                select=n_best_sub, n_iter=2,\n                cov_computation_method=cov_computation_method,\n                random_state=random_state)\n            subset_slice = np.arange(i * n_best_sub, (i + 1) * n_best_sub)\n            all_best_locations[subset_slice] = best_locations_sub\n            all_best_covariances[subset_slice] = best_covariances_sub\n        # 2. Pool the candidate supports into a merged set\n        # (possibly the full dataset)\n        n_samples_merged = min(1500, n_samples)\n        h_merged = int(np.ceil(n_samples_merged *\n                       (n_support / float(n_samples))))\n        if n_samples > 1500:\n            n_best_merged = 10\n        else:\n            n_best_merged = 1\n        # find the best couples (location, covariance) on the merged set\n        selection = random_state.permutation(n_samples)[:n_samples_merged]\n        locations_merged, covariances_merged, supports_merged, d = \\\n            select_candidates(\n                X[selection], h_merged,\n                n_trials=(all_best_locations, all_best_covariances),\n                select=n_best_merged,\n                cov_computation_method=cov_computation_method,\n                random_state=random_state)\n        # 3. Finally get the overall best (locations, covariance) couple\n        if n_samples < 1500:\n            # directly get the best couple (location, covariance)\n            location = locations_merged[0]\n            covariance = covariances_merged[0]\n            support = np.zeros(n_samples, dtype=bool)\n            dist = np.zeros(n_samples)\n            support[selection] = supports_merged[0]\n            dist[selection] = d[0]\n        else:\n            # select the best couple on the full dataset\n            locations_full, covariances_full, supports_full, d = \\\n                select_candidates(\n                    X, n_support,\n                    n_trials=(locations_merged, covariances_merged),\n                    select=1,\n                    cov_computation_method=cov_computation_method,\n                    random_state=random_state)\n            location = locations_full[0]\n            covariance = covariances_full[0]\n            support = supports_full[0]\n            dist = d[0]\n    elif n_features > 1:\n        # 1. Find the 10 best couples (location, covariance)\n        # considering two iterations\n        n_trials = 30\n        n_best = 10\n        locations_best, covariances_best, _, _ = select_candidates(\n            X, n_support, n_trials=n_trials, select=n_best, n_iter=2,\n            cov_computation_method=cov_computation_method,\n            random_state=random_state)\n        # 2. Select the best couple on the full dataset amongst the 10\n        locations_full, covariances_full, supports_full, d = select_candidates(\n            X, n_support, n_trials=(locations_best, covariances_best),\n            select=1, cov_computation_method=cov_computation_method,\n            random_state=random_state)\n        location = locations_full[0]\n        covariance = covariances_full[0]\n        support = supports_full[0]\n        dist = d[0]\n\n    return location, covariance, support, dist"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False (default), data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/block_size",
          "name": "block_size",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.block_size",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Size of blocks into which the covariance matrix will be split\nduring its Ledoit-Wolf estimation. This is purely a memory\noptimization and does not affect results."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 block_size=1000):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.block_size = block_size"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit",
      "name": "fit",
      "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the Ledoit-Wolf shrunk covariance model according to the given\ntraining data and parameters.",
      "docstring": "Fit the Ledoit-Wolf shrunk covariance model according to the given\ntraining data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the Ledoit-Wolf shrunk covariance model according to the given\n        training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance, shrinkage = ledoit_wolf(X - self.location_,\n                                            assume_centered=True,\n                                            block_size=self.block_size)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit",
      "name": "fit",
      "qname": "sklearn.covariance._shrunk_covariance.OAS.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.OAS.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.OAS.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._shrunk_covariance.OAS.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the Oracle Approximating Shrinkage covariance model\naccording to the given training data and parameters.",
      "docstring": "Fit the Oracle Approximating Shrinkage covariance model\naccording to the given training data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the Oracle Approximating Shrinkage covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n\n        covariance, shrinkage = oas(X - self.location_, assume_centered=True)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/shrinkage",
          "name": "shrinkage",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.shrinkage",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate. Range is [0, 1]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 shrinkage=0.1):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.shrinkage = shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit",
      "name": "fit",
      "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the shrunk covariance model according to the given training data\nand parameters.",
      "docstring": "Fit the shrunk covariance model according to the given training data\nand parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the shrunk covariance model according to the given training data\n        and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid a potential\n        # matrix inversion when setting the precision\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        covariance = shrunk_covariance(covariance, self.shrinkage)\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf",
      "name": "ledoit_wolf",
      "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf/block_size",
          "name": "block_size",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf.block_size",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Size of blocks into which the covariance matrix will be split.\nThis is purely a memory optimization and does not affect results."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split.\n    This is purely a memory optimization and does not affect results.\n\nReturns\n-------\nshrunk_cov : ndarray of shape (n_features, n_features)\n    Shrunk covariance.\n\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "@_deprecate_positional_args\ndef ledoit_wolf(X, *, assume_centered=False, block_size=1000):\n    \"\"\"Estimates the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n        This is purely a memory optimization and does not affect results.\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    X = np.asarray(X)\n    # for only one feature, the result is the same whatever the shrinkage\n    if len(X.shape) == 2 and X.shape[1] == 1:\n        if not assume_centered:\n            X = X - X.mean()\n        return np.atleast_2d((X ** 2).mean()), 0.\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n        n_features = X.size\n    else:\n        _, n_features = X.shape\n\n    # get Ledoit-Wolf shrinkage\n    shrinkage = ledoit_wolf_shrinkage(\n        X, assume_centered=assume_centered, block_size=block_size)\n    emp_cov = empirical_covariance(X, assume_centered=assume_centered)\n    mu = np.sum(np.trace(emp_cov)) / n_features\n    shrunk_cov = (1. - shrinkage) * emp_cov\n    shrunk_cov.flat[::n_features + 1] += shrinkage * mu\n\n    return shrunk_cov, shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage",
      "name": "ledoit_wolf_shrinkage",
      "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage.assume_centered",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage/block_size",
          "name": "block_size",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage.block_size",
          "default_value": "1000",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Size of blocks into which the covariance matrix will be split."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split.\n\nReturns\n-------\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "def ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000):\n    \"\"\"Estimates the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n\n    Returns\n    -------\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    X = np.asarray(X)\n    # for only one feature, the result is the same whatever the shrinkage\n    if len(X.shape) == 2 and X.shape[1] == 1:\n        return 0.\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n\n    if X.shape[0] == 1:\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n    n_samples, n_features = X.shape\n\n    # optionally center data\n    if not assume_centered:\n        X = X - X.mean(0)\n\n    # A non-blocked version of the computation is present in the tests\n    # in tests/test_covariance.py\n\n    # number of blocks to split the covariance matrix into\n    n_splits = int(n_features / block_size)\n    X2 = X ** 2\n    emp_cov_trace = np.sum(X2, axis=0) / n_samples\n    mu = np.sum(emp_cov_trace) / n_features\n    beta_ = 0.  # sum of the coefficients of <X2.T, X2>\n    delta_ = 0.  # sum of the *squared* coefficients of <X.T, X>\n    # starting block computation\n    for i in range(n_splits):\n        for j in range(n_splits):\n            rows = slice(block_size * i, block_size * (i + 1))\n            cols = slice(block_size * j, block_size * (j + 1))\n            beta_ += np.sum(np.dot(X2.T[rows], X2[:, cols]))\n            delta_ += np.sum(np.dot(X.T[rows], X[:, cols]) ** 2)\n        rows = slice(block_size * i, block_size * (i + 1))\n        beta_ += np.sum(np.dot(X2.T[rows], X2[:, block_size * n_splits:]))\n        delta_ += np.sum(\n            np.dot(X.T[rows], X[:, block_size * n_splits:]) ** 2)\n    for j in range(n_splits):\n        cols = slice(block_size * j, block_size * (j + 1))\n        beta_ += np.sum(np.dot(X2.T[block_size * n_splits:], X2[:, cols]))\n        delta_ += np.sum(\n            np.dot(X.T[block_size * n_splits:], X[:, cols]) ** 2)\n    delta_ += np.sum(np.dot(X.T[block_size * n_splits:],\n                            X[:, block_size * n_splits:]) ** 2)\n    delta_ /= n_samples ** 2\n    beta_ += np.sum(np.dot(X2.T[block_size * n_splits:],\n                           X2[:, block_size * n_splits:]))\n    # use delta_ to compute beta\n    beta = 1. / (n_features * n_samples) * (beta_ / n_samples - delta_)\n    # delta is the sum of the squared coefficients of (<X.T,X> - mu*Id) / p\n    delta = delta_ - 2. * mu * emp_cov_trace.sum() + n_features * mu ** 2\n    delta /= n_features\n    # get final beta as the min between beta and delta\n    # We do this to prevent shrinking more than \"1\", which whould invert\n    # the value of covariances\n    beta = min(beta, delta)\n    # finally get shrinkage\n    shrinkage = 0 if beta == 0 else beta / delta\n    return shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/oas",
      "name": "oas",
      "qname": "sklearn.covariance._shrunk_covariance.oas",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/oas/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.oas.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/oas/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.oas.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.",
      "docstring": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate.\n\nassume_centered : bool, default=False\n  If True, data will not be centered before computation.\n  Useful to work with data whose mean is significantly equal to\n  zero but is not exactly zero.\n  If False, data will be centered before computation.\n\nReturns\n-------\nshrunk_cov : array-like of shape (n_features, n_features)\n    Shrunk covariance.\n\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularised (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\n\nThe formula we used to implement the OAS is slightly modified compared\nto the one given in the article. See :class:`OAS` for more details.",
      "code": "@_deprecate_positional_args\ndef oas(X, *, assume_centered=False):\n    \"\"\"Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n      If True, data will not be centered before computation.\n      Useful to work with data whose mean is significantly equal to\n      zero but is not exactly zero.\n      If False, data will be centered before computation.\n\n    Returns\n    -------\n    shrunk_cov : array-like of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularised (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    The formula we used to implement the OAS is slightly modified compared\n    to the one given in the article. See :class:`OAS` for more details.\n    \"\"\"\n    X = np.asarray(X)\n    # for only one feature, the result is the same whatever the shrinkage\n    if len(X.shape) == 2 and X.shape[1] == 1:\n        if not assume_centered:\n            X = X - X.mean()\n        return np.atleast_2d((X ** 2).mean()), 0.\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n        n_samples = 1\n        n_features = X.size\n    else:\n        n_samples, n_features = X.shape\n\n    emp_cov = empirical_covariance(X, assume_centered=assume_centered)\n    mu = np.trace(emp_cov) / n_features\n\n    # formula from Chen et al.'s **implementation**\n    alpha = np.mean(emp_cov ** 2)\n    num = alpha + mu ** 2\n    den = (n_samples + 1.) * (alpha - (mu ** 2) / n_features)\n\n    shrinkage = 1. if den == 0 else min(num / den, 1.)\n    shrunk_cov = (1. - shrinkage) * emp_cov\n    shrunk_cov.flat[::n_features + 1] += shrinkage * mu\n\n    return shrunk_cov, shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/shrunk_covariance",
      "name": "shrunk_covariance",
      "qname": "sklearn.covariance._shrunk_covariance.shrunk_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/shrunk_covariance/emp_cov",
          "name": "emp_cov",
          "qname": "sklearn.covariance._shrunk_covariance.shrunk_covariance.emp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_features, n_features)",
            "default_value": "",
            "description": "Covariance matrix to be shrunk"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/shrunk_covariance/shrinkage",
          "name": "shrinkage",
          "qname": "sklearn.covariance._shrunk_covariance.shrunk_covariance.shrinkage",
          "default_value": "0.1",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate. Range is [0, 1]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Calculates a covariance matrix shrunk on the diagonal\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Calculates a covariance matrix shrunk on the diagonal\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nemp_cov : array-like of shape (n_features, n_features)\n    Covariance matrix to be shrunk\n\nshrinkage : float, default=0.1\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nReturns\n-------\nshrunk_cov : ndarray of shape (n_features, n_features)\n    Shrunk covariance.\n\nNotes\n-----\nThe regularized (shrunk) covariance is given by:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "def shrunk_covariance(emp_cov, shrinkage=0.1):\n    \"\"\"Calculates a covariance matrix shrunk on the diagonal\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    emp_cov : array-like of shape (n_features, n_features)\n        Covariance matrix to be shrunk\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    emp_cov = check_array(emp_cov)\n    n_features = emp_cov.shape[0]\n\n    mu = np.trace(emp_cov) / n_features\n    shrunk_cov = (1. - shrinkage) * emp_cov\n    shrunk_cov.flat[::n_features + 1] += shrinkage * mu\n\n    return shrunk_cov"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.CCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of components to keep. Should be in `[1, min(n_samples,\nn_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.max_iter",
          "default_value": "500",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "500",
            "description": "the maximum number of iterations of the power method."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.tol",
          "default_value": "1e-06",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-06",
            "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\nthan `tol`, where `u` corresponds to the left singular vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(n_components=n_components, scale=scale,\n                         deflation_mode=\"canonical\", mode=\"B\",\n                         algorithm=\"nipals\", max_iter=max_iter, tol=tol,\n                         copy=copy)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of components to keep. Should be in `[1, min(n_samples,\nn_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.algorithm",
          "default_value": "'nipals'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'nipals', 'svd'}",
            "default_value": "'nipals'",
            "description": "The algorithm used to estimate the first singular vectors of the\ncross-covariance matrix. 'nipals' uses the power method while 'svd'\nwill compute the whole SVD."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "nipals",
              "svd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.max_iter",
          "default_value": "500",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "500",
            "description": "the maximum number of iterations of the power method when\n`algorithm='nipals'`. Ignored otherwise."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.tol",
          "default_value": "1e-06",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-06",
            "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\nthan `tol`, where `u` corresponds to the left singular vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, algorithm=\"nipals\",\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"canonical\", mode=\"A\",\n            algorithm=algorithm,\n            max_iter=max_iter, tol=tol, copy=copy)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of components to keep. Should be in `[1, min(n_samples,\nn_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.max_iter",
          "default_value": "500",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "500",
            "description": "The maximum number of iterations of the power method when\n`algorithm='nipals'`. Ignored otherwise."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.tol",
          "default_value": "1e-06",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-06",
            "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\nthan `tol`, where `u` corresponds to the left singular vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"regression\", mode=\"A\",\n            algorithm='nipals', max_iter=max_iter,\n            tol=tol, copy=copy)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of components to keep. Should be in `[1,\nmin(n_samples, n_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, copy=True):\n        self.n_components = n_components\n        self.scale = scale\n        self.copy = copy"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit",
      "name": "fit",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit/X",
          "name": "X",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit/Y",
          "name": "Y",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit.Y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
            "default_value": "",
            "description": "Targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_targets)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit model to data.",
      "docstring": "Fit model to data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training samples.\n\nY : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Targets.",
      "code": "    def fit(self, X, Y):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Targets.\n        \"\"\"\n        check_consistent_length(X, Y)\n        X = self._validate_data(X, dtype=np.float64, copy=self.copy,\n                                ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)\n        # This matrix rank is at most min(n_samples, n_features, n_targets) so\n        # n_components cannot be bigger than that.\n        n_components = self.n_components\n        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])\n        if not 1 <= n_components <= rank_upper_bound:\n            # TODO: raise an error in 1.1\n            warnings.warn(\n                f\"As of version 0.24, n_components({n_components}) should be \"\n                f\"in [1, min(n_features, n_samples, n_targets)] = \"\n                f\"[1, {rank_upper_bound}]. \"\n                f\"n_components={rank_upper_bound} will be used instead. \"\n                f\"In version 1.1 (renaming of 0.26), an error will be raised.\",\n                FutureWarning\n            )\n            n_components = rank_upper_bound\n\n        X, Y, self._x_mean, self._y_mean, self._x_std, self._y_std = (\n            _center_scale_xy(X, Y, self.scale))\n\n        # Compute SVD of cross-covariance matrix\n        C = np.dot(X.T, Y)\n        U, s, Vt = svd(C, full_matrices=False)\n        U = U[:, :n_components]\n        Vt = Vt[:n_components]\n        U, Vt = svd_flip(U, Vt)\n        V = Vt.T\n\n        self._x_scores = np.dot(X, U)  # TODO: remove in 1.1\n        self._y_scores = np.dot(Y, V)  # TODO: remove in 1.1\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform/X",
          "name": "X",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform/y",
          "name": "y",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
            "default_value": "None",
            "description": "Targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_targets)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn and apply the dimensionality reduction.",
      "docstring": "Learn and apply the dimensionality reduction.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Targets.\n\nReturns\n-------\nout : array-like or tuple of array-like\n    The transformed data `X_tranformed` if `Y` is not None,\n    `(X_transformed, Y_transformed)` otherwise.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform",
      "name": "transform",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform/X",
          "name": "X",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Samples to be transformed."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform/Y",
          "name": "Y",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform.Y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
            "default_value": "None",
            "description": "Targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_targets)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply the dimensionality reduction.",
      "docstring": "Apply the dimensionality reduction.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Samples to be transformed.\n\nY : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Targets.\n\nReturns\n-------\nout : array-like or tuple of array-like\n    The transformed data `X_tranformed` if `Y` is not None,\n    `(X_transformed, Y_transformed)` otherwise.",
      "code": "    def transform(self, X, Y=None):\n        \"\"\"\n        Apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to be transformed.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, dtype=np.float64)\n        Xr = (X - self._x_mean) / self._x_std\n        x_scores = np.dot(Xr, self.x_weights_)\n        if Y is not None:\n            Y = check_array(Y, ensure_2d=False, dtype=np.float64)\n            if Y.ndim == 1:\n                Y = Y.reshape(-1, 1)\n            Yr = (Y - self._y_mean) / self._y_std\n            y_scores = np.dot(Yr, self.y_weights_)\n            return x_scores, y_scores\n        return x_scores"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_mean_@getter",
      "name": "x_mean_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_mean_",
      "decorators": [
        "deprecated('Attribute x_mean_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_mean_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_mean_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute x_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_mean_(self):\n        return self._x_mean"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_scores_@getter",
      "name": "x_scores_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_scores_",
      "decorators": [
        "deprecated('Attribute x_scores_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). Use est.transform(X) on the training data instead.')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_scores_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_scores_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute x_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X) on \"\n        \"the training data instead.\"\n    )\n    @property\n    def x_scores_(self):\n        return self._x_scores"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_std_@getter",
      "name": "x_std_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_std_",
      "decorators": [
        "deprecated('Attribute x_std_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_std_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_std_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute x_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_std_(self):\n        return self._x_std"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_mean_@getter",
      "name": "y_mean_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_mean_",
      "decorators": [
        "deprecated('Attribute y_mean_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_mean_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_mean_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute y_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_mean_(self):\n        return self._y_mean"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_scores_@getter",
      "name": "y_scores_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_scores_",
      "decorators": [
        "deprecated('Attribute y_scores_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). Use est.transform(X, Y) on the training data instead.')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_scores_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_scores_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute y_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X, Y) \"\n        \"on the training data instead.\"\n    )\n    @property\n    def y_scores_(self):\n        return self._y_scores"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_std_@getter",
      "name": "y_std_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_std_",
      "decorators": [
        "deprecated('Attribute y_std_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_std_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_std_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute y_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_std_(self):\n        return self._y_std"
    }
  ]
}