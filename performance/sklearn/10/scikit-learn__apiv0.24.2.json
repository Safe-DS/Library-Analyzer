{
  "schemaVersion": 1,
  "distribution": "",
  "package": "scikit-learn",
  "version": "",
  "modules": [],
  "classes": [
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation",
      "name": "AffinityPropagation",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/_pairwise@getter",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/_more_tags",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict",
        "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.",
      "docstring": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\ndamping : float, default=0.5\n    Damping factor (between 0.5 and 1) is the extent to\n    which the current value is maintained relative to\n    incoming values (weighted 1 - damping). This in order\n    to avoid numerical oscillations when updating these\n    values (messages).\n\nmax_iter : int, default=200\n    Maximum number of iterations.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\ncopy : bool, default=True\n    Make a copy of input data.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number\n    of exemplars, ie of clusters, is influenced by the input\n    preferences value. If the preferences are not passed as arguments,\n    they will be set to the median of the input similarities.\n\naffinity : {'euclidean', 'precomputed'}, default='euclidean'\n    Which affinity to use. At the moment 'precomputed' and\n    ``euclidean`` are supported. 'euclidean' uses the\n    negative squared euclidean distance between points.\n\nverbose : bool, default=False\n    Whether to be verbose.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nAttributes\n----------\ncluster_centers_indices_ : ndarray of shape (n_clusters,)\n    Indices of cluster centers.\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Cluster centers (if affinity != ``precomputed``).\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\naffinity_matrix_ : ndarray of shape (n_samples, n_samples)\n    Stores the affinity matrix used in ``fit``.\n\nn_iter_ : int\n    Number of iterations taken to converge.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nThe algorithmic complexity of affinity propagation is quadratic\nin the number of points.\n\nWhen ``fit`` does not converge, ``cluster_centers_`` becomes an empty\narray and all training samples will be labelled as ``-1``. In addition,\n``predict`` will then label every sample as ``-1``.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, ``fit`` will result in\na single cluster center and label ``0`` for every sample. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\n\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007\n\nExamples\n--------\n>>> from sklearn.cluster import AffinityPropagation\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AffinityPropagation(random_state=5).fit(X)\n>>> clustering\nAffinityPropagation(random_state=5)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])\n>>> clustering.predict([[0, 0], [4, 4]])\narray([0, 1])\n>>> clustering.cluster_centers_\narray([[1, 2],\n       [4, 2]])",
      "code": "class AffinityPropagation(ClusterMixin, BaseEstimator):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, default=0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, default=200\n        Maximum number of iterations.\n\n    convergence_iter : int, default=15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : bool, default=True\n        Make a copy of input data.\n\n    preference : array-like of shape (n_samples,) or float, default=None\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : {'euclidean', 'precomputed'}, default='euclidean'\n        Which affinity to use. At the moment 'precomputed' and\n        ``euclidean`` are supported. 'euclidean' uses the\n        negative squared euclidean distance between points.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    random_state : int, RandomState instance or None, default=0\n        Pseudo-random number generator to control the starting state.\n        Use an int for reproducible results across function calls.\n        See the :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.23\n            this parameter was previously hardcoded as 0.\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : ndarray of shape (n_clusters,)\n        Indices of cluster centers.\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    affinity_matrix_ : ndarray of shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n    array and all training samples will be labelled as ``-1``. In addition,\n    ``predict`` will then label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation(random_state=5).fit(X)\n    >>> clustering\n    AffinityPropagation(random_state=5)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, damping=.5, max_iter=200, convergence_iter=15,\n                 copy=True, preference=None, affinity='euclidean',\n                 verbose=False, random_state='warn'):\n\n        self.damping = damping\n        self.max_iter = max_iter\n        self.convergence_iter = convergence_iter\n        self.copy = copy\n        self.verbose = verbose\n        self.preference = preference\n        self.affinity = affinity\n        self.random_state = random_state\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute _pairwise was deprecated in \"  # type: ignore\n                \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def _pairwise(self):\n        return self.affinity == \"precomputed\"\n\n    def _more_tags(self):\n        return {'pairwise': self.affinity == 'precomputed'}\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        if self.affinity == \"precomputed\":\n            accept_sparse = False\n        else:\n            accept_sparse = 'csr'\n        X = self._validate_data(X, accept_sparse=accept_sparse)\n        if self.affinity == \"precomputed\":\n            self.affinity_matrix_ = X\n        elif self.affinity == \"euclidean\":\n            self.affinity_matrix_ = -euclidean_distances(X, squared=True)\n        else:\n            raise ValueError(\"Affinity must be 'precomputed' or \"\n                             \"'euclidean'. Got %s instead\"\n                             % str(self.affinity))\n\n        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \\\n            affinity_propagation(\n                self.affinity_matrix_, preference=self.preference,\n                max_iter=self.max_iter,\n                convergence_iter=self.convergence_iter, damping=self.damping,\n                copy=self.copy, verbose=self.verbose, return_n_iter=True,\n                random_state=self.random_state)\n\n        if self.affinity != \"precomputed\":\n            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        if not hasattr(self, \"cluster_centers_\"):\n            raise ValueError(\"Predict method is not supported when \"\n                             \"affinity='precomputed'.\")\n\n        if self.cluster_centers_.shape[0] > 0:\n            with config_context(assume_finite=True):\n                return pairwise_distances_argmin(X, self.cluster_centers_)\n        else:\n            warnings.warn(\"This model does not have any cluster centers \"\n                          \"because affinity propagation did not converge. \"\n                          \"Labeling every sample as '-1'.\", ConvergenceWarning)\n            return np.array([-1] * X.shape[0])\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Fit the clustering from features or affinity matrix, and return\n        cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)",
      "instance_attributes": [
        {
          "name": "damping",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "convergence_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "preference",
          "types": null
        },
        {
          "name": "affinity",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "random_state",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "affinity_matrix_",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "cluster_centers_indices_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cluster_centers_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering",
      "name": "AgglomerativeClustering",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__",
        "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit",
        "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int or None, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\naffinity : str or callable, default='euclidean'\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or \"precomputed\".\n    If linkage is \"ward\", only \"euclidean\" is accepted.\n    If \"precomputed\", a distance matrix (instead of a similarity matrix)\n    is needed as input for the fit method.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each sample the neighboring\n    samples following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    kneighbors_graph. Default is ``None``, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at ``n_clusters``. This is\n    useful to decrease computation time if the number of clusters is not\n    small compared to the number of samples. This option is useful only\n    when specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of observation. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - 'ward' minimizes the variance of the clusters being merged.\n    - 'average' uses the average of the distances of each observation of\n      the two sets.\n    - 'complete' or 'maximum' linkage uses the maximum distances between\n      all observations of the two sets.\n    - 'single' uses the minimum of the distances between all observations\n      of the two sets.\n\n    .. versionadded:: 0.20\n        Added the 'single' option\n\ndistance_threshold : float, default=None\n    The linkage distance threshold above which, clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : ndarray of shape (n_samples)\n    cluster labels for each point\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nchildren_ : array-like of shape (n_samples-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nExamples\n--------\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 4], [4, 0]])\n>>> clustering = AgglomerativeClustering().fit(X)\n>>> clustering\nAgglomerativeClustering()\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])",
      "code": "class AgglomerativeClustering(ClusterMixin, BaseEstimator):\n    \"\"\"\n    Agglomerative Clustering\n\n    Recursively merges the pair of clusters that minimally increases\n    a given linkage distance.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    affinity : str or callable, default='euclidean'\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\".\n        If linkage is \"ward\", only \"euclidean\" is accepted.\n        If \"precomputed\", a distance matrix (instead of a similarity matrix)\n        is needed as input for the fit method.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like or callable, default=None\n        Connectivity matrix. Defines for each sample the neighboring\n        samples following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        kneighbors_graph. Default is ``None``, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at ``n_clusters``. This is\n        useful to decrease computation time if the number of clusters is not\n        small compared to the number of samples. This option is useful only\n        when specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of observation. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - 'ward' minimizes the variance of the clusters being merged.\n        - 'average' uses the average of the distances of each observation of\n          the two sets.\n        - 'complete' or 'maximum' linkage uses the maximum distances between\n          all observations of the two sets.\n        - 'single' uses the minimum of the distances between all observations\n          of the two sets.\n\n        .. versionadded:: 0.20\n            Added the 'single' option\n\n    distance_threshold : float, default=None\n        The linkage distance threshold above which, clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : ndarray of shape (n_samples)\n        cluster labels for each point\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    children_ : array-like of shape (n_samples-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AgglomerativeClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AgglomerativeClustering().fit(X)\n    >>> clustering\n    AgglomerativeClustering()\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', distance_threshold=None,\n                 compute_distances=False):\n        self.n_clusters = n_clusters\n        self.distance_threshold = distance_threshold\n        self.memory = memory\n        self.connectivity = connectivity\n        self.compute_full_tree = compute_full_tree\n        self.linkage = linkage\n        self.affinity = affinity\n        self.compute_distances = compute_distances\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator=self)\n        memory = check_memory(self.memory)\n\n        if self.n_clusters is not None and self.n_clusters <= 0:\n            raise ValueError(\"n_clusters should be an integer greater than 0.\"\n                             \" %s was provided.\" % str(self.n_clusters))\n\n        if not ((self.n_clusters is None) ^ (self.distance_threshold is None)):\n            raise ValueError(\"Exactly one of n_clusters and \"\n                             \"distance_threshold has to be set, and the other \"\n                             \"needs to be None.\")\n\n        if (self.distance_threshold is not None\n                and not self.compute_full_tree):\n            raise ValueError(\"compute_full_tree must be True if \"\n                             \"distance_threshold is set.\")\n\n        if self.linkage == \"ward\" and self.affinity != \"euclidean\":\n            raise ValueError(\"%s was provided as affinity. Ward can only \"\n                             \"work with euclidean distances.\" %\n                             (self.affinity, ))\n\n        if self.linkage not in _TREE_BUILDERS:\n            raise ValueError(\"Unknown linkage type %s. \"\n                             \"Valid options are %s\" % (self.linkage,\n                                                       _TREE_BUILDERS.keys()))\n        tree_builder = _TREE_BUILDERS[self.linkage]\n\n        connectivity = self.connectivity\n        if self.connectivity is not None:\n            if callable(self.connectivity):\n                connectivity = self.connectivity(X)\n            connectivity = check_array(\n                connectivity, accept_sparse=['csr', 'coo', 'lil'])\n\n        n_samples = len(X)\n        compute_full_tree = self.compute_full_tree\n        if self.connectivity is None:\n            compute_full_tree = True\n        if compute_full_tree == 'auto':\n            if self.distance_threshold is not None:\n                compute_full_tree = True\n            else:\n                # Early stopping is likely to give a speed up only for\n                # a large number of clusters. The actual threshold\n                # implemented here is heuristic\n                compute_full_tree = self.n_clusters < max(100, .02 * n_samples)\n        n_clusters = self.n_clusters\n        if compute_full_tree:\n            n_clusters = None\n\n        # Construct the tree\n        kwargs = {}\n        if self.linkage != 'ward':\n            kwargs['linkage'] = self.linkage\n            kwargs['affinity'] = self.affinity\n\n        distance_threshold = self.distance_threshold\n\n        return_distance = (\n            (distance_threshold is not None) or self.compute_distances\n        )\n\n        out = memory.cache(tree_builder)(X, connectivity=connectivity,\n                                         n_clusters=n_clusters,\n                                         return_distance=return_distance,\n                                         **kwargs)\n        (self.children_,\n         self.n_connected_components_,\n         self.n_leaves_,\n         parents) = out[:4]\n\n        if return_distance:\n            self.distances_ = out[-1]\n\n        if self.distance_threshold is not None:  # distance_threshold is used\n            self.n_clusters_ = np.count_nonzero(\n                self.distances_ >= distance_threshold) + 1\n        else:  # n_clusters is used\n            self.n_clusters_ = self.n_clusters\n\n        # Cut the tree\n        if compute_full_tree:\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_,\n                                   self.n_leaves_)\n        else:\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\n            # copy to avoid holding a reference on the original array\n            labels = np.copy(labels[:n_samples])\n            # Reassign cluster numbers\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)",
      "instance_attributes": [
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "distance_threshold",
          "types": null
        },
        {
          "name": "memory",
          "types": null
        },
        {
          "name": "connectivity",
          "types": null
        },
        {
          "name": "compute_full_tree",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "linkage",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "affinity",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "compute_distances",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "children_",
          "types": null
        },
        {
          "name": "n_connected_components_",
          "types": null
        },
        {
          "name": "n_leaves_",
          "types": null
        },
        {
          "name": "distances_",
          "types": null
        },
        {
          "name": "n_clusters_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration",
      "name": "FeatureAgglomeration",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration",
      "decorators": [],
      "superclasses": [
        "AgglomerativeClustering",
        "AgglomerationTransform"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__",
        "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit",
        "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit_predict@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=2\n    The number of clusters to find. It must be ``None`` if\n    ``distance_threshold`` is not ``None``.\n\naffinity : str or callable, default='euclidean'\n    Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n    \"manhattan\", \"cosine\", or 'precomputed'.\n    If linkage is \"ward\", only \"euclidean\" is accepted.\n\nmemory : str or object with the joblib.Memory interface, default=None\n    Used to cache the output of the computation of the tree.\n    By default, no caching is done. If a string is given, it is the\n    path to the caching directory.\n\nconnectivity : array-like or callable, default=None\n    Connectivity matrix. Defines for each feature the neighboring\n    features following a given structure of the data.\n    This can be a connectivity matrix itself or a callable that transforms\n    the data into a connectivity matrix, such as derived from\n    kneighbors_graph. Default is None, i.e, the\n    hierarchical clustering algorithm is unstructured.\n\ncompute_full_tree : 'auto' or bool, default='auto'\n    Stop early the construction of the tree at n_clusters. This is useful\n    to decrease computation time if the number of clusters is not small\n    compared to the number of features. This option is useful only when\n    specifying a connectivity matrix. Note also that when varying the\n    number of clusters and using caching, it may be advantageous to compute\n    the full tree. It must be ``True`` if ``distance_threshold`` is not\n    ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n    to `True` when `distance_threshold` is not `None` or that `n_clusters`\n    is inferior to the maximum between 100 or `0.02 * n_samples`.\n    Otherwise, \"auto\" is equivalent to `False`.\n\nlinkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n    Which linkage criterion to use. The linkage criterion determines which\n    distance to use between sets of features. The algorithm will merge\n    the pairs of cluster that minimize this criterion.\n\n    - ward minimizes the variance of the clusters being merged.\n    - average uses the average of the distances of each feature of\n      the two sets.\n    - complete or maximum linkage uses the maximum distances between\n      all features of the two sets.\n    - single uses the minimum of the distances between all features\n      of the two sets.\n\npooling_func : callable, default=np.mean\n    This combines the values of agglomerated features into a single\n    value, and should accept an array of shape [M, N] and the keyword\n    argument `axis=1`, and reduce it to an array of size [M].\n\ndistance_threshold : float, default=None\n    The linkage distance threshold above which, clusters will not be\n    merged. If not ``None``, ``n_clusters`` must be ``None`` and\n    ``compute_full_tree`` must be ``True``.\n\n    .. versionadded:: 0.21\n\ncompute_distances : bool, default=False\n    Computes distances between clusters even if `distance_threshold` is not\n    used. This can be used to make dendrogram visualization, but introduces\n    a computational and memory overhead.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nn_clusters_ : int\n    The number of clusters found by the algorithm. If\n    ``distance_threshold=None``, it will be equal to the given\n    ``n_clusters``.\n\nlabels_ : array-like of (n_features,)\n    cluster labels for each feature.\n\nn_leaves_ : int\n    Number of leaves in the hierarchical tree.\n\nn_connected_components_ : int\n    The estimated number of connected components in the graph.\n\n    .. versionadded:: 0.21\n        ``n_connected_components_`` was added to replace ``n_components_``.\n\nchildren_ : array-like of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_features`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_features` is a non-leaf\n    node and has children `children_[i - n_features]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_features + i`\n\ndistances_ : array-like of shape (n_nodes-1,)\n    Distances between nodes in the corresponding place in `children_`.\n    Only computed if `distance_threshold` is used or `compute_distances`\n    is set to `True`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn import datasets, cluster\n>>> digits = datasets.load_digits()\n>>> images = digits.images\n>>> X = np.reshape(images, (len(images), -1))\n>>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n>>> agglo.fit(X)\nFeatureAgglomeration(n_clusters=32)\n>>> X_reduced = agglo.transform(X)\n>>> X_reduced.shape\n(1797, 32)",
      "code": "class FeatureAgglomeration(AgglomerativeClustering, AgglomerationTransform):\n    \"\"\"Agglomerate features.\n\n    Similar to AgglomerativeClustering, but recursively merges features\n    instead of samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=2\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    affinity : str or callable, default='euclidean'\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or 'precomputed'.\n        If linkage is \"ward\", only \"euclidean\" is accepted.\n\n    memory : str or object with the joblib.Memory interface, default=None\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like or callable, default=None\n        Connectivity matrix. Defines for each feature the neighboring\n        features following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        kneighbors_graph. Default is None, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : 'auto' or bool, default='auto'\n        Stop early the construction of the tree at n_clusters. This is useful\n        to decrease computation time if the number of clusters is not small\n        compared to the number of features. This option is useful only when\n        specifying a connectivity matrix. Note also that when varying the\n        number of clusters and using caching, it may be advantageous to compute\n        the full tree. It must be ``True`` if ``distance_threshold`` is not\n        ``None``. By default `compute_full_tree` is \"auto\", which is equivalent\n        to `True` when `distance_threshold` is not `None` or that `n_clusters`\n        is inferior to the maximum between 100 or `0.02 * n_samples`.\n        Otherwise, \"auto\" is equivalent to `False`.\n\n    linkage : {'ward', 'complete', 'average', 'single'}, default='ward'\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of features. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - ward minimizes the variance of the clusters being merged.\n        - average uses the average of the distances of each feature of\n          the two sets.\n        - complete or maximum linkage uses the maximum distances between\n          all features of the two sets.\n        - single uses the minimum of the distances between all features\n          of the two sets.\n\n    pooling_func : callable, default=np.mean\n        This combines the values of agglomerated features into a single\n        value, and should accept an array of shape [M, N] and the keyword\n        argument `axis=1`, and reduce it to an array of size [M].\n\n    distance_threshold : float, default=None\n        The linkage distance threshold above which, clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    compute_distances : bool, default=False\n        Computes distances between clusters even if `distance_threshold` is not\n        used. This can be used to make dendrogram visualization, but introduces\n        a computational and memory overhead.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array-like of (n_features,)\n        cluster labels for each feature.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n        .. versionadded:: 0.21\n            ``n_connected_components_`` was added to replace ``n_components_``.\n\n    children_ : array-like of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_features`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_features` is a non-leaf\n        node and has children `children_[i - n_features]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_features + i`\n\n    distances_ : array-like of shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if `distance_threshold` is used or `compute_distances`\n        is set to `True`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', pooling_func=np.mean,\n                 distance_threshold=None, compute_distances=False):\n        super().__init__(\n            n_clusters=n_clusters, memory=memory, connectivity=connectivity,\n            compute_full_tree=compute_full_tree, linkage=linkage,\n            affinity=affinity, distance_threshold=distance_threshold,\n            compute_distances=compute_distances)\n        self.pooling_func = pooling_func\n\n    def fit(self, X, y=None, **params):\n        \"\"\"Fit the hierarchical clustering on the data\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                ensure_min_features=2, estimator=self)\n        # save n_features_in_ attribute here to reset it after, because it will\n        # be overridden in AgglomerativeClustering since we passed it X.T.\n        n_features_in_ = self.n_features_in_\n        AgglomerativeClustering.fit(self, X.T, **params)\n        self.n_features_in_ = n_features_in_\n        return self\n\n    @property\n    def fit_predict(self):\n        raise AttributeError",
      "instance_attributes": [
        {
          "name": "pooling_func",
          "types": {
            "kind": "NamedType",
            "name": "Callable"
          }
        },
        {
          "name": "n_features_in_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering",
      "name": "SpectralBiclustering",
      "qname": "sklearn.cluster._bicluster.SpectralBiclustering",
      "decorators": [],
      "superclasses": [
        "BaseSpectral"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_check_parameters",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_fit",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_fit_best_piecewise",
        "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/_project_and_cluster"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.",
      "docstring": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.\n\nParameters\n----------\nn_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n    The number of row and column clusters in the checkerboard\n    structure.\n\nmethod : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n    Method of normalizing and converting singular vectors into\n    biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n    The authors recommend using 'log'. If the data is sparse,\n    however, log normalization will not work, which is why the\n    default is 'bistochastic'.\n\n    .. warning::\n       if `method='log'`, the data must be sparse.\n\nn_components : int, default=6\n    Number of singular vectors to check.\n\nn_best : int, default=3\n    Number of best singular vectors to which to project the data\n    for clustering.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', uses\n    :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', uses\n    `scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random'} or ndarray of (n_clusters, n_features),             default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    Row partition labels.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    Column partition labels.\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralBiclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_\narray([0, 1], dtype=int32)\n>>> clustering\nSpectralBiclustering(n_clusters=2, random_state=0)\n\nReferences\n----------\n\n* Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n  data: coclustering genes and conditions\n  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.",
      "code": "class SpectralBiclustering(BaseSpectral):\n    \"\"\"Spectral biclustering (Kluger, 2003).\n\n    Partitions rows and columns under the assumption that the data has\n    an underlying checkerboard structure. For instance, if there are\n    two row partitions and three column partitions, each row will\n    belong to three biclusters, and each column will belong to two\n    biclusters. The outer product of the corresponding row and column\n    label vectors gives this checkerboard structure.\n\n    Read more in the :ref:`User Guide <spectral_biclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3\n        The number of row and column clusters in the checkerboard\n        structure.\n\n    method : {'bistochastic', 'scale', 'log'}, default='bistochastic'\n        Method of normalizing and converting singular vectors into\n        biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n        The authors recommend using 'log'. If the data is sparse,\n        however, log normalization will not work, which is why the\n        default is 'bistochastic'.\n\n        .. warning::\n           if `method='log'`, the data must be sparse.\n\n    n_components : int, default=6\n        Number of singular vectors to check.\n\n    n_best : int, default=3\n        Number of best singular vectors to which to project the data\n        for clustering.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', uses\n        :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', uses\n        `scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random'} or ndarray of (n_clusters, n_features), \\\n            default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        Row partition labels.\n\n    column_labels_ : array-like of shape (n_cols,)\n        Column partition labels.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralBiclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([0, 1], dtype=int32)\n    >>> clustering\n    SpectralBiclustering(n_clusters=2, random_state=0)\n\n    References\n    ----------\n\n    * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n      data: coclustering genes and conditions\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, method='bistochastic',\n                 n_components=6, n_best=3, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)\n        self.method = method\n        self.n_components = n_components\n        self.n_best = n_best\n\n    def _check_parameters(self):\n        super()._check_parameters()\n        legal_methods = ('bistochastic', 'scale', 'log')\n        if self.method not in legal_methods:\n            raise ValueError(\"Unknown method: '{0}'. method must be\"\n                             \" one of {1}.\".format(self.method, legal_methods))\n        try:\n            int(self.n_clusters)\n        except TypeError:\n            try:\n                r, c = self.n_clusters\n                int(r)\n                int(c)\n            except (ValueError, TypeError) as e:\n                raise ValueError(\"Incorrect parameter n_clusters has value:\"\n                                 \" {}. It should either be a single integer\"\n                                 \" or an iterable with two integers:\"\n                                 \" (n_row_clusters, n_column_clusters)\") from e\n        if self.n_components < 1:\n            raise ValueError(\"Parameter n_components must be greater than 0,\"\n                             \" but its value is {}\".format(self.n_components))\n        if self.n_best < 1:\n            raise ValueError(\"Parameter n_best must be greater than 0,\"\n                             \" but its value is {}\".format(self.n_best))\n        if self.n_best > self.n_components:\n            raise ValueError(\"n_best cannot be larger than\"\n                             \" n_components, but {} >  {}\"\n                             \"\".format(self.n_best, self.n_components))\n\n    def _fit(self, X):\n        n_sv = self.n_components\n        if self.method == 'bistochastic':\n            normalized_data = _bistochastic_normalize(X)\n            n_sv += 1\n        elif self.method == 'scale':\n            normalized_data, _, _ = _scale_normalize(X)\n            n_sv += 1\n        elif self.method == 'log':\n            normalized_data = _log_normalize(X)\n        n_discard = 0 if self.method == 'log' else 1\n        u, v = self._svd(normalized_data, n_sv, n_discard)\n        ut = u.T\n        vt = v.T\n\n        try:\n            n_row_clusters, n_col_clusters = self.n_clusters\n        except TypeError:\n            n_row_clusters = n_col_clusters = self.n_clusters\n\n        best_ut = self._fit_best_piecewise(ut, self.n_best,\n                                           n_row_clusters)\n\n        best_vt = self._fit_best_piecewise(vt, self.n_best,\n                                           n_col_clusters)\n\n        self.row_labels_ = self._project_and_cluster(X, best_vt.T,\n                                                     n_row_clusters)\n\n        self.column_labels_ = self._project_and_cluster(X.T, best_ut.T,\n                                                        n_col_clusters)\n\n        self.rows_ = np.vstack([self.row_labels_ == label\n                                for label in range(n_row_clusters)\n                                for _ in range(n_col_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == label\n                                   for _ in range(n_row_clusters)\n                                   for label in range(n_col_clusters)])\n\n    def _fit_best_piecewise(self, vectors, n_best, n_clusters):\n        \"\"\"Find the ``n_best`` vectors that are best approximated by piecewise\n        constant vectors.\n\n        The piecewise vectors are found by k-means; the best is chosen\n        according to Euclidean distance.\n\n        \"\"\"\n        def make_piecewise(v):\n            centroid, labels = self._k_means(v.reshape(-1, 1), n_clusters)\n            return centroid[labels].ravel()\n        piecewise_vectors = np.apply_along_axis(make_piecewise,\n                                                axis=1, arr=vectors)\n        dists = np.apply_along_axis(norm, axis=1,\n                                    arr=(vectors - piecewise_vectors))\n        result = vectors[np.argsort(dists)[:n_best]]\n        return result\n\n    def _project_and_cluster(self, data, vectors, n_clusters):\n        \"\"\"Project ``data`` to ``vectors`` and cluster the result.\"\"\"\n        projected = safe_sparse_dot(data, vectors)\n        _, labels = self._k_means(projected, n_clusters)\n        return labels",
      "instance_attributes": [
        {
          "name": "method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_components",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_best",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "row_labels_",
          "types": null
        },
        {
          "name": "column_labels_",
          "types": null
        },
        {
          "name": "rows_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "columns_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering",
      "name": "SpectralCoclustering",
      "qname": "sklearn.cluster._bicluster.SpectralCoclustering",
      "decorators": [],
      "superclasses": [
        "BaseSpectral"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__",
        "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/_fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.",
      "docstring": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.\n\nParameters\n----------\nn_clusters : int, default=3\n    The number of biclusters to find.\n\nsvd_method : {'randomized', 'arpack'}, default='randomized'\n    Selects the algorithm for finding singular vectors. May be\n    'randomized' or 'arpack'. If 'randomized', use\n    :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n    for large matrices. If 'arpack', use\n    :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n    possibly slower in some cases.\n\nn_svd_vecs : int, default=None\n    Number of vectors to use in calculating the SVD. Corresponds\n    to `ncv` when `svd_method=arpack` and `n_oversamples` when\n    `svd_method` is 'randomized`.\n\nmini_batch : bool, default=False\n    Whether to use mini-batch k-means, which is faster but may get\n    different results.\n\ninit : {'k-means++', 'random', or ndarray of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization of k-means algorithm; defaults to\n    'k-means++'.\n\nn_init : int, default=10\n    Number of random initializations that are tried with the\n    k-means algorithm.\n\n    If mini-batch k-means is used, the best initialization is\n    chosen and the algorithm runs once. Otherwise, the algorithm\n    is run for each initialization and the best solution chosen.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by breaking\n    down the pairwise matrix into n_jobs even slices and computing them in\n    parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nrandom_state : int, RandomState instance, default=None\n    Used for randomizing the singular value decomposition and the k-means\n    initialization. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nrows_ : array-like of shape (n_row_clusters, n_rows)\n    Results of the clustering. `rows[i, r]` is True if\n    cluster `i` contains row `r`. Available only after calling ``fit``.\n\ncolumns_ : array-like of shape (n_column_clusters, n_columns)\n    Results of the clustering, like `rows`.\n\nrow_labels_ : array-like of shape (n_rows,)\n    The bicluster label of each row.\n\ncolumn_labels_ : array-like of shape (n_cols,)\n    The bicluster label of each column.\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralCoclustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n>>> clustering.row_labels_ #doctest: +SKIP\narray([0, 1, 1, 0, 0, 0], dtype=int32)\n>>> clustering.column_labels_ #doctest: +SKIP\narray([0, 0], dtype=int32)\n>>> clustering\nSpectralCoclustering(n_clusters=2, random_state=0)\n\nReferences\n----------\n\n* Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n  bipartite spectral graph partitioning\n  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.",
      "code": "class SpectralCoclustering(BaseSpectral):\n    \"\"\"Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=3\n        The number of biclusters to find.\n\n    svd_method : {'randomized', 'arpack'}, default='randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, default=None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, default=False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random', or ndarray of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization of k-means algorithm; defaults to\n        'k-means++'.\n\n    n_init : int, default=10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    random_state : int, RandomState instance, default=None\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like of shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like of shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like of shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like of shape (n_cols,)\n        The bicluster label of each column.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_ #doctest: +SKIP\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_ #doctest: +SKIP\n    array([0, 0], dtype=int32)\n    >>> clustering\n    SpectralCoclustering(n_clusters=2, random_state=0)\n\n    References\n    ----------\n\n    * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n      bipartite spectral graph partitioning\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)\n\n    def _fit(self, X):\n        normalized_data, row_diag, col_diag = _scale_normalize(X)\n        n_sv = 1 + int(np.ceil(np.log2(self.n_clusters)))\n        u, v = self._svd(normalized_data, n_sv, n_discard=1)\n        z = np.vstack((row_diag[:, np.newaxis] * u,\n                       col_diag[:, np.newaxis] * v))\n\n        _, labels = self._k_means(z, self.n_clusters)\n\n        n_rows = X.shape[0]\n        self.row_labels_ = labels[:n_rows]\n        self.column_labels_ = labels[n_rows:]\n\n        self.rows_ = np.vstack([self.row_labels_ == c\n                                for c in range(self.n_clusters)])\n        self.columns_ = np.vstack([self.column_labels_ == c\n                                   for c in range(self.n_clusters)])",
      "instance_attributes": [
        {
          "name": "row_labels_",
          "types": null
        },
        {
          "name": "column_labels_",
          "types": null
        },
        {
          "name": "rows_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "columns_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch",
      "name": "Birch",
      "qname": "sklearn.cluster._birch.Birch",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._birch/Birch/__init__",
        "scikit-learn/sklearn.cluster._birch/Birch/fit",
        "scikit-learn/sklearn.cluster._birch/Birch/_fit",
        "scikit-learn/sklearn.cluster._birch/Birch/_get_leaves",
        "scikit-learn/sklearn.cluster._birch/Birch/partial_fit",
        "scikit-learn/sklearn.cluster._birch/Birch/_check_fit",
        "scikit-learn/sklearn.cluster._birch/Birch/predict",
        "scikit-learn/sklearn.cluster._birch/Birch/transform",
        "scikit-learn/sklearn.cluster._birch/Birch/_global_clustering"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16",
      "docstring": "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nthreshold : float, default=0.5\n    The radius of the subcluster obtained by merging a new sample and the\n    closest subcluster should be lesser than the threshold. Otherwise a new\n    subcluster is started. Setting this value to be very low promotes\n    splitting and vice-versa.\n\nbranching_factor : int, default=50\n    Maximum number of CF subclusters in each node. If a new samples enters\n    such that the number of subclusters exceed the branching_factor then\n    that node is split into two nodes with the subclusters redistributed\n    in each. The parent subcluster of that node is removed and two new\n    subclusters are added as parents of the 2 split nodes.\n\nn_clusters : int, instance of sklearn.cluster model, default=3\n    Number of clusters after the final clustering step, which treats the\n    subclusters from the leaves as new samples.\n\n    - `None` : the final clustering step is not performed and the\n      subclusters are returned as they are.\n\n    - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n      is fit treating the subclusters as new samples and the initial data\n      is mapped to the label of the closest subcluster.\n\n    - `int` : the model fit is :class:`AgglomerativeClustering` with\n      `n_clusters` set to be equal to the int.\n\ncompute_labels : bool, default=True\n    Whether or not to compute labels for each fit.\n\ncopy : bool, default=True\n    Whether or not to make a copy of the given data. If set to False,\n    the initial data will be overwritten.\n\nAttributes\n----------\nroot_ : _CFNode\n    Root of the CFTree.\n\ndummy_leaf_ : _CFNode\n    Start pointer to all the leaves.\n\nsubcluster_centers_ : ndarray\n    Centroids of all subclusters read directly from the leaves.\n\nsubcluster_labels_ : ndarray\n    Labels assigned to the centroids of the subclusters after\n    they are clustered globally.\n\nlabels_ : ndarray of shape (n_samples,)\n    Array of labels assigned to the input data.\n    if partial_fit is used instead of fit, they are assigned to the\n    last batch of data.\n\nSee Also\n--------\nMiniBatchKMeans : Alternative implementation that does incremental updates\n    of the centers' positions using mini-batches.\n\nNotes\n-----\nThe tree data structure consists of nodes with each node consisting of\na number of subclusters. The maximum number of subclusters in a node\nis determined by the branching factor. Each subcluster maintains a\nlinear sum, squared sum and the number of samples in that subcluster.\nIn addition, each subcluster can also have a node as its child, if the\nsubcluster is not a member of a leaf node.\n\nFor a new point entering the root, it is merged with the subcluster closest\nto it and the linear sum, squared sum and the number of samples of that\nsubcluster are updated. This is done recursively till the properties of\nthe leaf node are updated.\n\nReferences\n----------\n* Tian Zhang, Raghu Ramakrishnan, Maron Livny\n  BIRCH: An efficient data clustering method for large databases.\n  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n* Roberto Perdisci\n  JBirch - Java implementation of BIRCH clustering algorithm\n  https://code.google.com/archive/p/jbirch\n\nExamples\n--------\n>>> from sklearn.cluster import Birch\n>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n>>> brc = Birch(n_clusters=None)\n>>> brc.fit(X)\nBirch(n_clusters=None)\n>>> brc.predict(X)\narray([0, 0, 0, 1, 1, 1])",
      "code": "class Birch(ClusterMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Implements the BIRCH clustering algorithm.\n\n    It is a memory-efficient, online-learning algorithm provided as an\n    alternative to :class:`MiniBatchKMeans`. It constructs a tree\n    data structure with the cluster centroids being read off the leaf.\n    These can be either the final cluster centroids or can be provided as input\n    to another clustering algorithm such as :class:`AgglomerativeClustering`.\n\n    Read more in the :ref:`User Guide <birch>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    threshold : float, default=0.5\n        The radius of the subcluster obtained by merging a new sample and the\n        closest subcluster should be lesser than the threshold. Otherwise a new\n        subcluster is started. Setting this value to be very low promotes\n        splitting and vice-versa.\n\n    branching_factor : int, default=50\n        Maximum number of CF subclusters in each node. If a new samples enters\n        such that the number of subclusters exceed the branching_factor then\n        that node is split into two nodes with the subclusters redistributed\n        in each. The parent subcluster of that node is removed and two new\n        subclusters are added as parents of the 2 split nodes.\n\n    n_clusters : int, instance of sklearn.cluster model, default=3\n        Number of clusters after the final clustering step, which treats the\n        subclusters from the leaves as new samples.\n\n        - `None` : the final clustering step is not performed and the\n          subclusters are returned as they are.\n\n        - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n          is fit treating the subclusters as new samples and the initial data\n          is mapped to the label of the closest subcluster.\n\n        - `int` : the model fit is :class:`AgglomerativeClustering` with\n          `n_clusters` set to be equal to the int.\n\n    compute_labels : bool, default=True\n        Whether or not to compute labels for each fit.\n\n    copy : bool, default=True\n        Whether or not to make a copy of the given data. If set to False,\n        the initial data will be overwritten.\n\n    Attributes\n    ----------\n    root_ : _CFNode\n        Root of the CFTree.\n\n    dummy_leaf_ : _CFNode\n        Start pointer to all the leaves.\n\n    subcluster_centers_ : ndarray\n        Centroids of all subclusters read directly from the leaves.\n\n    subcluster_labels_ : ndarray\n        Labels assigned to the centroids of the subclusters after\n        they are clustered globally.\n\n    labels_ : ndarray of shape (n_samples,)\n        Array of labels assigned to the input data.\n        if partial_fit is used instead of fit, they are assigned to the\n        last batch of data.\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative implementation that does incremental updates\n        of the centers' positions using mini-batches.\n\n    Notes\n    -----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch\n\n    Examples\n    --------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(n_clusters=None)\n    >>> brc.fit(X)\n    Birch(n_clusters=None)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, threshold=0.5, branching_factor=50, n_clusters=3,\n                 compute_labels=True, copy=True):\n        self.threshold = threshold\n        self.branching_factor = branching_factor\n        self.n_clusters = n_clusters\n        self.compute_labels = compute_labels\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Build a CF Tree for the input data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.fit_, self.partial_fit_ = True, False\n        return self._fit(X)\n\n    def _fit(self, X):\n        has_root = getattr(self, 'root_', None)\n        first_call = self.fit_ or (self.partial_fit_ and not has_root)\n\n        X = self._validate_data(X, accept_sparse='csr', copy=self.copy,\n                                reset=first_call)\n        threshold = self.threshold\n        branching_factor = self.branching_factor\n\n        if branching_factor <= 1:\n            raise ValueError(\"Branching_factor should be greater than one.\")\n        n_samples, n_features = X.shape\n\n        # If partial_fit is called for the first time or fit is called, we\n        # start a new tree.\n        if first_call:\n            # The first root is the leaf. Manipulate this object throughout.\n            self.root_ = _CFNode(threshold=threshold,\n                                 branching_factor=branching_factor,\n                                 is_leaf=True,\n                                 n_features=n_features)\n\n            # To enable getting back subclusters.\n            self.dummy_leaf_ = _CFNode(threshold=threshold,\n                                       branching_factor=branching_factor,\n                                       is_leaf=True, n_features=n_features)\n            self.dummy_leaf_.next_leaf_ = self.root_\n            self.root_.prev_leaf_ = self.dummy_leaf_\n\n        # Cannot vectorize. Enough to convince to use cython.\n        if not sparse.issparse(X):\n            iter_func = iter\n        else:\n            iter_func = _iterate_sparse_X\n\n        for sample in iter_func(X):\n            subcluster = _CFSubcluster(linear_sum=sample)\n            split = self.root_.insert_cf_subcluster(subcluster)\n\n            if split:\n                new_subcluster1, new_subcluster2 = _split_node(\n                    self.root_, threshold, branching_factor)\n                del self.root_\n                self.root_ = _CFNode(threshold=threshold,\n                                     branching_factor=branching_factor,\n                                     is_leaf=False,\n                                     n_features=n_features)\n                self.root_.append_subcluster(new_subcluster1)\n                self.root_.append_subcluster(new_subcluster2)\n\n        centroids = np.concatenate([\n            leaf.centroids_ for leaf in self._get_leaves()])\n        self.subcluster_centers_ = centroids\n\n        self._global_clustering(X)\n        return self\n\n    def _get_leaves(self):\n        \"\"\"\n        Retrieve the leaves of the CF Node.\n\n        Returns\n        -------\n        leaves : list of shape (n_leaves,)\n            List of the leaf nodes.\n        \"\"\"\n        leaf_ptr = self.dummy_leaf_.next_leaf_\n        leaves = []\n        while leaf_ptr is not None:\n            leaves.append(leaf_ptr)\n            leaf_ptr = leaf_ptr.next_leaf_\n        return leaves\n\n    def partial_fit(self, X=None, y=None):\n        \"\"\"\n        Online learning. Prevents rebuilding of CFTree from scratch.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), \\\n            default=None\n            Input data. If X is not provided, only the global clustering\n            step is done.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.partial_fit_, self.fit_ = True, False\n        if X is None:\n            # Perform just the final global clustering step.\n            self._global_clustering()\n            return self\n        else:\n            return self._fit(X)\n\n    def _check_fit(self, X):\n        check_is_fitted(self)\n\n        if (hasattr(self, 'subcluster_centers_') and\n                X.shape[1] != self.subcluster_centers_.shape[1]):\n            raise ValueError(\n                \"Training data and predicted data do \"\n                \"not have same number of features.\")\n\n    def predict(self, X):\n        \"\"\"\n        Predict data using the ``centroids_`` of subclusters.\n\n        Avoid computation of the row norms of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        labels : ndarray of shape(n_samples,)\n            Labelled data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n        kwargs = {'Y_norm_squared': self._subcluster_norms}\n\n        with config_context(assume_finite=True):\n            argmin = pairwise_distances_argmin(X, self.subcluster_centers_,\n                                               metric_kwargs=kwargs)\n        return self.subcluster_labels_[argmin]\n\n    def transform(self, X):\n        \"\"\"\n        Transform X into subcluster centroids dimension.\n\n        Each dimension represents the distance from the sample point to each\n        cluster centroid.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        self._validate_data(X, accept_sparse='csr', reset=False)\n        with config_context(assume_finite=True):\n            return euclidean_distances(X, self.subcluster_centers_)\n\n    def _global_clustering(self, X=None):\n        \"\"\"\n        Global clustering for the subclusters obtained after fitting\n        \"\"\"\n        clusterer = self.n_clusters\n        centroids = self.subcluster_centers_\n        compute_labels = (X is not None) and self.compute_labels\n\n        # Preprocessing for the global clustering.\n        not_enough_centroids = False\n        if isinstance(clusterer, numbers.Integral):\n            clusterer = AgglomerativeClustering(\n                n_clusters=self.n_clusters)\n            # There is no need to perform the global clustering step.\n            if len(centroids) < self.n_clusters:\n                not_enough_centroids = True\n        elif (clusterer is not None and not\n              hasattr(clusterer, 'fit_predict')):\n            raise ValueError(\"n_clusters should be an instance of \"\n                             \"ClusterMixin or an int\")\n\n        # To use in predict to avoid recalculation.\n        self._subcluster_norms = row_norms(\n            self.subcluster_centers_, squared=True)\n\n        if clusterer is None or not_enough_centroids:\n            self.subcluster_labels_ = np.arange(len(centroids))\n            if not_enough_centroids:\n                warnings.warn(\n                    \"Number of subclusters found (%d) by BIRCH is less \"\n                    \"than (%d). Decrease the threshold.\"\n                    % (len(centroids), self.n_clusters), ConvergenceWarning)\n        else:\n            # The global clustering step that clusters the subclusters of\n            # the leaves. It assumes the centroids of the subclusters as\n            # samples and finds the final centroids.\n            self.subcluster_labels_ = clusterer.fit_predict(\n                self.subcluster_centers_)\n\n        if compute_labels:\n            self.labels_ = self.predict(X)",
      "instance_attributes": [
        {
          "name": "threshold",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "branching_factor",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "compute_labels",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "fit_",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "partial_fit_",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "root_",
          "types": {
            "kind": "NamedType",
            "name": "_CFNode"
          }
        },
        {
          "name": "dummy_leaf_",
          "types": {
            "kind": "NamedType",
            "name": "_CFNode"
          }
        },
        {
          "name": "subcluster_centers_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_subcluster_norms",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "subcluster_labels_",
          "types": null
        },
        {
          "name": "labels_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN",
      "name": "DBSCAN",
      "qname": "sklearn.cluster._dbscan.DBSCAN",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__",
        "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit",
        "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.",
      "docstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : string, or callable, default='euclidean'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square. X may be a :term:`Glossary <sparse graph>`, in which\n    case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n    .. versionadded:: 0.17\n       metric *precomputed* to accept precomputed sparse matrix.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=None\n    The power of the Minkowski metric to be used to calculate distance\n    between points. If None, then ``p=2`` (equivalent to the Euclidean\n    distance).\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\ncore_sample_indices_ : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\ncomponents_ : ndarray of shape (n_core_samples, n_features)\n    Copy of each core sample found by training.\n\nlabels_ : ndarray of shape (n_samples)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples are given the label -1.\n\nExamples\n--------\n>>> from sklearn.cluster import DBSCAN\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 2], [2, 3],\n...               [8, 7], [8, 8], [25, 80]])\n>>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n>>> clustering.labels_\narray([ 0,  0,  0,  1,  1, -1])\n>>> clustering\nDBSCAN(eps=3, min_samples=2)\n\nSee Also\n--------\nOPTICS : A similar clustering at multiple values of eps. Our implementation\n    is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:class:`cluster.OPTICS` provides a similar clustering with lower memory\nusage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\".\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.",
      "code": "class DBSCAN(ClusterMixin, BaseEstimator):\n    \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n\n    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n    Finds core samples of high density and expands clusters from them.\n    Good for data which contains clusters of similar density.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : string, or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`Glossary <sparse graph>`, in which\n        case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n        .. versionadded:: 0.17\n           metric *precomputed* to accept precomputed sparse matrix.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=None\n        The power of the Minkowski metric to be used to calculate distance\n        between points. If None, then ``p=2`` (equivalent to the Euclidean\n        distance).\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    core_sample_indices_ : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    components_ : ndarray of shape (n_core_samples, n_features)\n        Copy of each core sample found by training.\n\n    labels_ : ndarray of shape (n_samples)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)\n\n    See Also\n    --------\n    OPTICS : A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, eps=0.5, *, min_samples=5, metric='euclidean',\n                 metric_params=None, algorithm='auto', leaf_size=30, p=None,\n                 n_jobs=None):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.metric = metric\n        self.metric_params = metric_params\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.p = p\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr')\n\n        if not self.eps > 0.0:\n            raise ValueError(\"eps must be positive.\")\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        # Calculate neighborhood for all samples. This leaves the original\n        # point in, which needs to be considered later (i.e. point i is in the\n        # neighborhood of point i. While True, its useless information)\n        if self.metric == 'precomputed' and sparse.issparse(X):\n            # set the diagonal to explicit values, as a point is its own\n            # neighbor\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n                X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n\n        neighbors_model = NearestNeighbors(\n            radius=self.eps, algorithm=self.algorithm,\n            leaf_size=self.leaf_size, metric=self.metric,\n            metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n        neighbors_model.fit(X)\n        # This has worst case O(n^2) memory complexity\n        neighborhoods = neighbors_model.radius_neighbors(X,\n                                                         return_distance=False)\n\n        if sample_weight is None:\n            n_neighbors = np.array([len(neighbors)\n                                    for neighbors in neighborhoods])\n        else:\n            n_neighbors = np.array([np.sum(sample_weight[neighbors])\n                                    for neighbors in neighborhoods])\n\n        # Initially, all samples are noise.\n        labels = np.full(X.shape[0], -1, dtype=np.intp)\n\n        # A list of all core samples found.\n        core_samples = np.asarray(n_neighbors >= self.min_samples,\n                                  dtype=np.uint8)\n        dbscan_inner(core_samples, neighborhoods, labels)\n\n        self.core_sample_indices_ = np.where(core_samples)[0]\n        self.labels_ = labels\n\n        if len(self.core_sample_indices_):\n            # fix for scipy sparse indexing issue\n            self.components_ = X[self.core_sample_indices_].copy()\n        else:\n            # no core samples\n            self.components_ = np.empty((0, X.shape[1]))\n        return self\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.\n        \"\"\"\n        self.fit(X, sample_weight=sample_weight)\n        return self.labels_",
      "instance_attributes": [
        {
          "name": "eps",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_samples",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "metric",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "metric_params",
          "types": null
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "leaf_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "p",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "core_sample_indices_",
          "types": null
        },
        {
          "name": "labels_",
          "types": null
        },
        {
          "name": "components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans",
      "name": "KMeans",
      "qname": "sklearn.cluster._kmeans.KMeans",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_check_params",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_validate_center_shape",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_check_test_data",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_check_mkl_vcomp",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_init_centroids",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/fit",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/transform",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_transform",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/predict",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/score",
        "scikit-learn/sklearn.cluster._kmeans/KMeans/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.",
      "docstring": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nprecompute_distances : {'auto', True, False}, default='auto'\n    Precompute distances (faster but takes more memory).\n\n    'auto' : do not precompute distances if n_samples * n_clusters > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n\n    True : always precompute distances.\n\n    False : never precompute distances.\n\n    .. deprecated:: 0.23\n        'precompute_distances' was deprecated in version 0.22 and will be\n        removed in 1.0 (renaming of 0.25). It has no effect.\n\nverbose : int, default=0\n    Verbosity mode.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nn_jobs : int, default=None\n    The number of OpenMP threads to use for the computation. Parallelism is\n    sample-wise on the main cython loop which assigns each sample to its\n    closest center.\n\n    ``None`` or ``-1`` means using all processors.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nalgorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n    The \"elkan\" variation is more efficient on data with well-defined\n    clusters, by using the triangle inequality. However it's more memory\n    intensive due to the allocation of an extra array of shape\n    (n_samples, n_clusters).\n\n    For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n    might change in the future for a better heuristic.\n\n    .. versionchanged:: 0.18\n        Added Elkan algorithm\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers. If the algorithm stops before fully\n    converging (see ``tol`` and ``max_iter``), these will not be\n    consistent with ``labels_``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\ninertia_ : float\n    Sum of squared distances of samples to their closest cluster center.\n\nn_iter_ : int\n    Number of iterations run.\n\nSee Also\n--------\nMiniBatchKMeans : Alternative online implementation that does incremental\n    updates of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster than the default batch implementation.\n\nNotes\n-----\nThe k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of\nsamples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n'How slow is the k-means method?' SoCG2006)\n\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That's why\nit can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of ``tol`` or\n``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\ni.e. the ``cluster_centers_`` will not be the means of the points in each\ncluster. Also, the estimator will reassign ``labels_`` after the last\niteration to make ``labels_`` consistent with ``predict`` on the training\nset.\n\nExamples\n--------\n\n>>> from sklearn.cluster import KMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n>>> kmeans.labels_\narray([1, 1, 1, 0, 0, 0], dtype=int32)\n>>> kmeans.predict([[0, 0], [12, 3]])\narray([1, 0], dtype=int32)\n>>> kmeans.cluster_centers_\narray([[10.,  2.],\n       [ 1.,  2.]])",
      "code": "class KMeans(TransformerMixin, ClusterMixin, BaseEstimator):\n    \"\"\"K-Means clustering.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm for a\n        single run.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    precompute_distances : {'auto', True, False}, default='auto'\n        Precompute distances (faster but takes more memory).\n\n        'auto' : do not precompute distances if n_samples * n_clusters > 12\n        million. This corresponds to about 100MB overhead per job using\n        double precision.\n\n        True : always precompute distances.\n\n        False : never precompute distances.\n\n        .. deprecated:: 0.23\n            'precompute_distances' was deprecated in version 0.22 and will be\n            removed in 1.0 (renaming of 0.25). It has no effect.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    n_jobs : int, default=None\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n        ``None`` or ``-1`` means using all processors.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n        The \"elkan\" variation is more efficient on data with well-defined\n        clusters, by using the triangle inequality. However it's more memory\n        intensive due to the allocation of an extra array of shape\n        (n_samples, n_clusters).\n\n        For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n        might change in the future for a better heuristic.\n\n        .. versionchanged:: 0.18\n            Added Elkan algorithm\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    See Also\n    --------\n    MiniBatchKMeans : Alternative online implementation that does incremental\n        updates of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), where n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n    'How slow is the k-means method?' SoCG2006)\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', n_init=10,\n                 max_iter=300, tol=1e-4, precompute_distances='deprecated',\n                 verbose=0, random_state=None, copy_x=True,\n                 n_jobs='deprecated', algorithm='auto'):\n\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.precompute_distances = precompute_distances\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n        self.copy_x = copy_x\n        self.n_jobs = n_jobs\n        self.algorithm = algorithm\n\n    def _check_params(self, X):\n        # precompute_distances\n        if self.precompute_distances != 'deprecated':\n            warnings.warn(\"'precompute_distances' was deprecated in version \"\n                          \"0.23 and will be removed in 1.0 (renaming of 0.25)\"\n                          \". It has no effect\", FutureWarning)\n\n        # n_jobs\n        if self.n_jobs != 'deprecated':\n            warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n                          \" removed in 1.0 (renaming of 0.25).\", FutureWarning)\n            self._n_threads = self.n_jobs\n        else:\n            self._n_threads = None\n        self._n_threads = _openmp_effective_n_threads(self._n_threads)\n\n        # n_init\n        if self.n_init <= 0:\n            raise ValueError(\n                f\"n_init should be > 0, got {self.n_init} instead.\")\n        self._n_init = self.n_init\n\n        # max_iter\n        if self.max_iter <= 0:\n            raise ValueError(\n                f\"max_iter should be > 0, got {self.max_iter} instead.\")\n\n        # n_clusters\n        if X.shape[0] < self.n_clusters:\n            raise ValueError(f\"n_samples={X.shape[0]} should be >= \"\n                             f\"n_clusters={self.n_clusters}.\")\n\n        # tol\n        self._tol = _tolerance(X, self.tol)\n\n        # algorithm\n        if self.algorithm not in (\"auto\", \"full\", \"elkan\"):\n            raise ValueError(f\"Algorithm must be 'auto', 'full' or 'elkan', \"\n                             f\"got {self.algorithm} instead.\")\n\n        self._algorithm = self.algorithm\n        if self._algorithm == \"auto\":\n            self._algorithm = \"full\" if self.n_clusters == 1 else \"elkan\"\n        if self._algorithm == \"elkan\" and self.n_clusters == 1:\n            warnings.warn(\"algorithm='elkan' doesn't make sense for a single \"\n                          \"cluster. Using 'full' instead.\", RuntimeWarning)\n            self._algorithm = \"full\"\n\n        # init\n        if not (hasattr(self.init, '__array__') or callable(self.init)\n                or (isinstance(self.init, str)\n                    and self.init in [\"k-means++\", \"random\"])):\n            raise ValueError(\n                f\"init should be either 'k-means++', 'random', a ndarray or a \"\n                f\"callable, got '{self.init}' instead.\")\n\n        if hasattr(self.init, '__array__') and self._n_init != 1:\n            warnings.warn(\n                f\"Explicit initial center position passed: performing only\"\n                f\" one init in {self.__class__.__name__} instead of \"\n                f\"n_init={self._n_init}.\", RuntimeWarning, stacklevel=2)\n            self._n_init = 1\n\n    def _validate_center_shape(self, X, centers):\n        \"\"\"Check if centers is compatible with X and n_clusters.\"\"\"\n        if centers.shape[0] != self.n_clusters:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of clusters {self.n_clusters}.\")\n        if centers.shape[1] != X.shape[1]:\n            raise ValueError(\n                f\"The shape of the initial centers {centers.shape} does not \"\n                f\"match the number of features of the data {X.shape[1]}.\")\n\n    def _check_test_data(self, X):\n        X = self._validate_data(X, accept_sparse='csr', reset=False,\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n        return X\n\n    def _check_mkl_vcomp(self, X, n_samples):\n        \"\"\"Warns when vcomp and mkl are both present\"\"\"\n        # The BLAS call inside a prange in lloyd_iter_chunked_dense is known to\n        # cause a small memory leak when there are less chunks than the number\n        # of available threads. It only happens when the OpenMP library is\n        # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653\n        if sp.issparse(X):\n            return\n\n        active_threads = int(np.ceil(n_samples / CHUNK_SIZE))\n        if active_threads < self._n_threads:\n            modules = threadpool_info()\n            has_vcomp = \"vcomp\" in [module[\"prefix\"] for module in modules]\n            has_mkl = (\"mkl\", \"intel\") in [\n                (module[\"internal_api\"], module.get(\"threading_layer\", None))\n                for module in modules]\n            if has_vcomp and has_mkl:\n                if not hasattr(self, \"batch_size\"):  # KMeans\n                    warnings.warn(\n                        f\"KMeans is known to have a memory leak on Windows \"\n                        f\"with MKL, when there are less chunks than available \"\n                        f\"threads. You can avoid it by setting the environment\"\n                        f\" variable OMP_NUM_THREADS={active_threads}.\")\n                else:  # MiniBatchKMeans\n                    warnings.warn(\n                        f\"MiniBatchKMeans is known to have a memory leak on \"\n                        f\"Windows with MKL, when there are less chunks than \"\n                        f\"available threads. You can prevent it by setting \"\n                        f\"batch_size >= {self._n_threads * CHUNK_SIZE} or by \"\n                        f\"setting the environment variable \"\n                        f\"OMP_NUM_THREADS={active_threads}\")\n\n    def _init_centroids(self, X, x_squared_norms, init, random_state,\n                        init_size=None):\n        \"\"\"Compute the initial centroids.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        x_squared_norms : ndarray of shape (n_samples,)\n            Squared euclidean norm of each data point. Pass it if you have it\n            at hands already to avoid it being recomputed here.\n\n        init : {'k-means++', 'random'}, callable or ndarray of shape \\\n                (n_clusters, n_features)\n            Method for initialization.\n\n        random_state : RandomState instance\n            Determines random number generation for centroid initialization.\n            See :term:`Glossary <random_state>`.\n\n        init_size : int, default=None\n            Number of samples to randomly sample for speeding up the\n            initialization (sometimes at the expense of accuracy).\n\n        Returns\n        -------\n        centers : ndarray of shape (n_clusters, n_features)\n        \"\"\"\n        n_samples = X.shape[0]\n        n_clusters = self.n_clusters\n\n        if init_size is not None and init_size < n_samples:\n            init_indices = random_state.randint(0, n_samples, init_size)\n            X = X[init_indices]\n            x_squared_norms = x_squared_norms[init_indices]\n            n_samples = X.shape[0]\n\n        if isinstance(init, str) and init == 'k-means++':\n            centers, _ = _kmeans_plusplus(X, n_clusters,\n                                          random_state=random_state,\n                                          x_squared_norms=x_squared_norms)\n        elif isinstance(init, str) and init == 'random':\n            seeds = random_state.permutation(n_samples)[:n_clusters]\n            centers = X[seeds]\n        elif hasattr(init, '__array__'):\n            centers = init\n        elif callable(init):\n            centers = init(X, n_clusters, random_state=random_state)\n            centers = check_array(\n                centers, dtype=X.dtype, copy=False, order='C')\n            self._validate_center_shape(X, centers)\n\n        if sp.issparse(centers):\n            centers = centers.toarray()\n\n        return centers\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', copy=self.copy_x,\n                                accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if hasattr(init, '__array__'):\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"full\":\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n        else:\n            kmeans_single = _kmeans_single_elkan\n\n        best_inertia = None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X, x_squared_norms=x_squared_norms, init=init,\n                random_state=random_state)\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X, sample_weight, centers_init, max_iter=self.max_iter,\n                verbose=self.verbose, tol=self._tol,\n                x_squared_norms=x_squared_norms, n_threads=self._n_threads)\n\n            # determine if these results are the best so far\n            if best_inertia is None or inertia < best_inertia:\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning, stacklevel=2)\n\n        self.cluster_centers_ = best_centers\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_\n\n    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        # Currently, this just skips a copy of the data if it is not in\n        # np.array or CSR format already.\n        # XXX This skips _check_test_data, which may change the dtype;\n        # we should refactor the input validation.\n        return self.fit(X, sample_weight=sample_weight)._transform(X)\n\n    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)\n\n    def _transform(self, X):\n        \"\"\"Guts of transform method; no input validation.\"\"\"\n        return euclidean_distances(X, self.cluster_centers_)\n\n    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return _labels_inertia(X, sample_weight, x_squared_norms,\n                               self.cluster_centers_, self._n_threads)[0]\n\n    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return -_labels_inertia(X, sample_weight, x_squared_norms,\n                                self.cluster_centers_)[1]\n\n    def _more_tags(self):\n        return {\n            '_xfail_checks': {\n                'check_sample_weights_invariance':\n                'zero sample_weight is not equivalent to removing samples',\n            },\n        }",
      "instance_attributes": [
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "init",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "precompute_distances",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_init",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "copy_x",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "n_jobs",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "_n_threads",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "_n_init",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "_tol",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "_algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "cluster_centers_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "labels_",
          "types": null
        },
        {
          "name": "inertia_",
          "types": null
        },
        {
          "name": "n_iter_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans",
      "name": "MiniBatchKMeans",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans",
      "decorators": [],
      "superclasses": [
        "KMeans"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/counts_@getter",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/init_size_@getter",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/random_state_@getter",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/_check_params",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/_labels_inertia_minibatch",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict",
        "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.",
      "docstring": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.\n\nParameters\n----------\n\nn_clusters : int, default=8\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nmax_iter : int, default=100\n    Maximum number of iterations over the complete dataset before\n    stopping independently of any early stopping criterion heuristics.\n\nbatch_size : int, default=100\n    Size of the mini batches.\n\nverbose : int, default=0\n    Verbosity mode.\n\ncompute_labels : bool, default=True\n    Compute label assignment and inertia for the complete dataset\n    once the minibatch optimization has converged in fit.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization and\n    random reassignment. Use an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.0\n    Control early stopping based on the relative center changes as\n    measured by a smoothed, variance-normalized of the mean center\n    squared position changes. This early stopping heuristics is\n    closer to the one used for the batch variant of the algorithms\n    but induces a slight computational and memory overhead over the\n    inertia heuristic.\n\n    To disable convergence detection based on normalized center\n    change, set tol to 0.0 (default).\n\nmax_no_improvement : int, default=10\n    Control early stopping based on the consecutive number of mini\n    batches that does not yield an improvement on the smoothed inertia.\n\n    To disable convergence detection based on inertia, set\n    max_no_improvement to None.\n\ninit_size : int, default=None\n    Number of samples to randomly sample for speeding up the\n    initialization (sometimes at the expense of accuracy): the\n    only algorithm is initialized by running a batch KMeans on a\n    random subset of the data. This needs to be larger than n_clusters.\n\n    If `None`, `init_size= 3 * batch_size`.\n\nn_init : int, default=3\n    Number of random initializations that are tried.\n    In contrast to KMeans, the algorithm is only run once, using the\n    best of the ``n_init`` initializations as measured by inertia.\n\nreassignment_ratio : float, default=0.01\n    Control the fraction of the maximum number of counts for a\n    center to be reassigned. A higher value means that low count\n    centers are more easily reassigned, which means that the\n    model will take longer to converge, but should converge in a\n    better clustering.\n\nAttributes\n----------\n\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : int\n    Labels of each point (if compute_labels is set to True).\n\ninertia_ : float\n    The value of the inertia criterion associated with the chosen\n    partition (if compute_labels is set to True). The inertia is\n    defined as the sum of square distances of samples to their nearest\n    neighbor.\n\nn_iter_ : int\n    Number of batches processed.\n\ncounts_ : ndarray of shape (n_clusters,)\n    Weigth sum of each cluster.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26).\n\ninit_size_ : int\n    The effective number of samples used for the initialization.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26).\n\nSee Also\n--------\nKMeans : The classic implementation of the clustering method based on the\n    Lloyd's algorithm. It consumes the whole set of input data at each\n    iteration.\n\nNotes\n-----\nSee https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\nExamples\n--------\n>>> from sklearn.cluster import MiniBatchKMeans\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [4, 2], [4, 0], [4, 4],\n...               [4, 5], [0, 1], [2, 2],\n...               [3, 2], [5, 5], [1, -1]])\n>>> # manually fit on batches\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6)\n>>> kmeans = kmeans.partial_fit(X[0:6,:])\n>>> kmeans = kmeans.partial_fit(X[6:12,:])\n>>> kmeans.cluster_centers_\narray([[2. , 1. ],\n       [3.5, 4.5]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([0, 1], dtype=int32)\n>>> # fit on the whole data\n>>> kmeans = MiniBatchKMeans(n_clusters=2,\n...                          random_state=0,\n...                          batch_size=6,\n...                          max_iter=10).fit(X)\n>>> kmeans.cluster_centers_\narray([[3.95918367, 2.40816327],\n       [1.12195122, 1.3902439 ]])\n>>> kmeans.predict([[0, 0], [4, 4]])\narray([1, 0], dtype=int32)",
      "code": "class MiniBatchKMeans(KMeans):\n    \"\"\"\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, default=8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, default=100\n        Size of the mini batches.\n\n    verbose : int, default=0\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, default=None\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n        If `None`, `init_size= 3 * batch_size`.\n\n    n_init : int, default=3\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the\n        best of the ``n_init`` initializations as measured by inertia.\n\n    reassignment_ratio : float, default=0.01\n        Control the fraction of the maximum number of counts for a\n        center to be reassigned. A higher value means that low count\n        centers are more easily reassigned, which means that the\n        model will take longer to converge, but should converge in a\n        better clustering.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : int\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition (if compute_labels is set to True). The inertia is\n        defined as the sum of square distances of samples to their nearest\n        neighbor.\n\n    n_iter_ : int\n        Number of batches processed.\n\n    counts_ : ndarray of shape (n_clusters,)\n        Weigth sum of each cluster.\n\n        .. deprecated:: 0.24\n           This attribute is deprecated in 0.24 and will be removed in\n           1.1 (renaming of 0.26).\n\n    init_size_ : int\n        The effective number of samples used for the initialization.\n\n        .. deprecated:: 0.24\n           This attribute is deprecated in 0.24 and will be removed in\n           1.1 (renaming of 0.26).\n\n    See Also\n    --------\n    KMeans : The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6)\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[2. , 1. ],\n           [3.5, 4.5]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([0, 1], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10).fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.95918367, 2.40816327],\n           [1.12195122, 1.3902439 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', max_iter=100,\n                 batch_size=100, verbose=0, compute_labels=True,\n                 random_state=None, tol=0.0, max_no_improvement=10,\n                 init_size=None, n_init=3, reassignment_ratio=0.01):\n\n        super().__init__(\n            n_clusters=n_clusters, init=init, max_iter=max_iter,\n            verbose=verbose, random_state=random_state, tol=tol, n_init=n_init)\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio\n\n    @deprecated(\"The attribute 'counts_' is deprecated in 0.24\"  # type: ignore\n                \" and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def counts_(self):\n        return self._counts\n\n    @deprecated(\"The attribute 'init_size_' is deprecated in \"  # type: ignore\n                \"0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def init_size_(self):\n        return self._init_size\n\n    @deprecated(\"The attribute 'random_state_' is deprecated \"  # type: ignore\n                \"in 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def random_state_(self):\n        return getattr(self, \"_random_state\", None)\n\n    def _check_params(self, X):\n        super()._check_params(X)\n\n        # max_no_improvement\n        if self.max_no_improvement is not None and self.max_no_improvement < 0:\n            raise ValueError(\n                f\"max_no_improvement should be >= 0, got \"\n                f\"{self.max_no_improvement} instead.\")\n\n        # batch_size\n        if self.batch_size <= 0:\n            raise ValueError(\n                f\"batch_size should be > 0, got {self.batch_size} instead.\")\n\n        # init_size\n        if self.init_size is not None and self.init_size <= 0:\n            raise ValueError(\n                f\"init_size should be > 0, got {self.init_size} instead.\")\n        self._init_size = self.init_size\n        if self._init_size is None:\n            self._init_size = 3 * self.batch_size\n            if self._init_size < self.n_clusters:\n                self._init_size = 3 * self.n_clusters\n        elif self._init_size < self.n_clusters:\n            warnings.warn(\n                f\"init_size={self._init_size} should be larger than \"\n                f\"n_clusters={self.n_clusters}. Setting it to \"\n                f\"min(3*n_clusters, n_samples)\",\n                RuntimeWarning, stacklevel=2)\n            self._init_size = 3 * self.n_clusters\n        self._init_size = min(self._init_size, X.shape[0])\n\n        # reassignment_ratio\n        if self.reassignment_ratio < 0:\n            raise ValueError(\n                f\"reassignment_ratio should be >= 0, got \"\n                f\"{self.reassignment_ratio} instead.\")\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        n_samples, n_features = X.shape\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self.tol > 0.0:\n            tol = _tolerance(X, self.tol)\n\n            # using tol-based early stopping needs the allocation of a\n            # dedicated before which can be expensive for high dim data:\n            # hence we allocate it outside of the main loop\n            old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n        else:\n            tol = 0.0\n            # no need for the center buffer if tol-based early stopping is\n            # disabled\n            old_center_buffer = np.zeros(0, dtype=X.dtype)\n\n        distances = np.zeros(self.batch_size, dtype=X.dtype)\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n        n_iter = int(self.max_iter * n_batches)\n\n        self._check_mkl_vcomp(X, self.batch_size)\n\n        validation_indices = random_state.randint(0, n_samples,\n                                                  self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        x_squared_norms_valid = x_squared_norms[validation_indices]\n\n        # perform several inits with random sub-sets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(\"Init %d/%d with method: %s\"\n                      % (init_idx + 1, self._n_init, init))\n            weight_sums = np.zeros(self.n_clusters, dtype=sample_weight.dtype)\n\n            # TODO: once the `k_means` function works with sparse input we\n            # should refactor the following init to use it instead.\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans\n            cluster_centers = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size)\n\n            # Compute the label assignment on the init dataset\n            _mini_batch_step(\n                X_valid, sample_weight_valid,\n                x_squared_norms[validation_indices], cluster_centers,\n                weight_sums, old_center_buffer, False, distances=None,\n                verbose=self.verbose)\n\n            # Keep only the best cluster centers across independent inits on\n            # the common validation set\n            _, inertia = _labels_inertia(X_valid, sample_weight_valid,\n                                         x_squared_norms_valid,\n                                         cluster_centers)\n            if self.verbose:\n                print(\"Inertia for init %d/%d: %f\"\n                      % (init_idx + 1, self._n_init, inertia))\n            if best_inertia is None or inertia < best_inertia:\n                self.cluster_centers_ = cluster_centers\n                self._counts = weight_sums\n                best_inertia = inertia\n\n        # Empty context to be used inplace by the convergence check routine\n        convergence_context = {}\n\n        # Perform the iterative optimization until the final convergence\n        # criterion\n        for iteration_idx in range(n_iter):\n            # Sample a minibatch from the full dataset\n            minibatch_indices = random_state.randint(\n                0, n_samples, self.batch_size)\n\n            # Perform the actual update step on the minibatch data\n            batch_inertia, centers_squared_diff = _mini_batch_step(\n                X[minibatch_indices], sample_weight[minibatch_indices],\n                x_squared_norms[minibatch_indices],\n                self.cluster_centers_, self._counts,\n                old_center_buffer, tol > 0.0, distances=distances,\n                # Here we randomly choose whether to perform\n                # random reassignment: the choice is done as a function\n                # of the iteration index, and the minimum number of\n                # counts, in order to force this reassignment to happen\n                # every once in a while\n                random_reassign=((iteration_idx + 1)\n                                 % (10 + int(self._counts.min())) == 0),\n                random_state=random_state,\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose)\n\n            # Monitor convergence and do early stopping if necessary\n            if _mini_batch_convergence(\n                    self, iteration_idx, n_iter, tol, n_samples,\n                    centers_squared_diff, batch_inertia, convergence_context,\n                    verbose=self.verbose):\n                break\n\n        self.n_iter_ = iteration_idx + 1\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = \\\n                    self._labels_inertia_minibatch(X, sample_weight)\n\n        return self\n\n    def _labels_inertia_minibatch(self, X, sample_weight):\n        \"\"\"Compute labels and inertia using mini batches.\n\n        This is slightly slower than doing everything at once but prevents\n        memory errors / segfaults.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        sample_weight : array-like of shape (n_samples,)\n            The weights for each observation in X.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels for each point.\n\n        inertia : float\n            Sum of squared distances of points to nearest cluster.\n        \"\"\"\n        if self.verbose:\n            print('Computing label assignment and total inertia')\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        x_squared_norms = row_norms(X, squared=True)\n        slices = gen_batches(X.shape[0], self.batch_size)\n        results = [_labels_inertia(X[s], sample_weight[s], x_squared_norms[s],\n                                   self.cluster_centers_) for s in slices]\n        labels, inertia = zip(*results)\n        return np.hstack(labels), np.sum(inertia)\n\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Coordinates of the data points to cluster. It must be noted that\n            X will be copied if it is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        self\n        \"\"\"\n        is_first_call_to_partial_fit = not hasattr(self, 'cluster_centers_')\n\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False,\n                                reset=is_first_call_to_partial_fit)\n\n        self._random_state = getattr(self, \"_random_state\",\n                                     check_random_state(self.random_state))\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        x_squared_norms = row_norms(X, squared=True)\n\n        if is_first_call_to_partial_fit:\n            # this is the first call to partial_fit on this object\n            self._check_params(X)\n\n            # Validate init array\n            init = self.init\n            if hasattr(init, '__array__'):\n                init = check_array(init, dtype=X.dtype, copy=True, order='C')\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size)\n\n            self._counts = np.zeros(self.n_clusters,\n                                    dtype=sample_weight.dtype)\n            random_reassign = False\n            distances = None\n        else:\n            # The lower the minimum count is, the more we do random\n            # reassignment, however, we don't want to do random\n            # reassignment too often, to allow for building up counts\n            random_reassign = self._random_state.randint(\n                10 * (1 + self._counts.min())) == 0\n            distances = np.zeros(X.shape[0], dtype=X.dtype)\n\n        _mini_batch_step(X, sample_weight, x_squared_norms,\n                         self.cluster_centers_, self._counts,\n                         np.zeros(0, dtype=X.dtype), 0,\n                         random_reassign=random_reassign, distances=distances,\n                         random_state=self._random_state,\n                         reassignment_ratio=self.reassignment_ratio,\n                         verbose=self.verbose)\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia(\n                X, sample_weight, x_squared_norms, self.cluster_centers_)\n\n        return self\n\n    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._labels_inertia_minibatch(X, sample_weight)[0]\n\n    def _more_tags(self):\n        return {\n            '_xfail_checks': {\n                'check_sample_weights_invariance':\n                'zero sample_weight is not equivalent to removing samples',\n            }\n        }",
      "instance_attributes": [
        {
          "name": "max_no_improvement",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "batch_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "compute_labels",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "init_size",
          "types": null
        },
        {
          "name": "reassignment_ratio",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "_init_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cluster_centers_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_counts",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_iter_",
          "types": null
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "inertia_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_random_state",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift",
      "name": "MeanShift",
      "qname": "sklearn.cluster._mean_shift.MeanShift",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__",
        "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit",
        "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.",
      "docstring": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\nbandwidth : float, default=None\n    Bandwidth used in the RBF kernel.\n\n    If not given, the bandwidth is estimated using\n    sklearn.cluster.estimate_bandwidth; see the documentation for that\n    function for hints on scalability (see also the Notes, below).\n\nseeds : array-like of shape (n_samples, n_features), default=None\n    Seeds used to initialize kernels. If not set,\n    the seeds are calculated by clustering.get_bin_seeds\n    with bandwidth as the grid size and default values for\n    other parameters.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    The default value is False.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by computing\n    each of the n_init runs in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncluster_centers_ : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point.\n\nn_iter_ : int\n    Maximum number of iterations performed on each seed.\n\n    .. versionadded:: 0.22\n\nExamples\n--------\n>>> from sklearn.cluster import MeanShift\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = MeanShift(bandwidth=2).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering.predict([[0, 0], [5, 5]])\narray([1, 0])\n>>> clustering\nMeanShift(bandwidth=2)\n\nNotes\n-----\n\nScalability:\n\nBecause this implementation uses a flat kernel and\na Ball Tree to look up members of each kernel, the complexity will tend\ntowards O(T*n*log(n)) in lower dimensions, with n the number of samples\nand T the number of points. In higher dimensions the complexity will\ntend towards O(T*n^2).\n\nScalability can be boosted by using fewer seeds, for example by using\na higher value of min_bin_freq in the get_bin_seeds function.\n\nNote that the estimate_bandwidth function is much less scalable than the\nmean shift algorithm and will be the bottleneck if it is used.\n\nReferences\n----------\n\nDorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\nfeature space analysis\". IEEE Transactions on Pattern Analysis and\nMachine Intelligence. 2002. pp. 603-619.",
      "code": "class MeanShift(ClusterMixin, BaseEstimator):\n    \"\"\"Mean shift clustering using a flat kernel.\n\n    Mean shift clustering aims to discover \"blobs\" in a smooth density of\n    samples. It is a centroid-based algorithm, which works by updating\n    candidates for centroids to be the mean of the points within a given\n    region. These candidates are then filtered in a post-processing stage to\n    eliminate near-duplicates to form the final set of centroids.\n\n    Seeding is performed using a binning technique for scalability.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n    bandwidth : float, default=None\n        Bandwidth used in the RBF kernel.\n\n        If not given, the bandwidth is estimated using\n        sklearn.cluster.estimate_bandwidth; see the documentation for that\n        function for hints on scalability (see also the Notes, below).\n\n    seeds : array-like of shape (n_samples, n_features), default=None\n        Seeds used to initialize kernels. If not set,\n        the seeds are calculated by clustering.get_bin_seeds\n        with bandwidth as the grid size and default values for\n        other parameters.\n\n    bin_seeding : bool, default=False\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        The default value is False.\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, default=1\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds.\n\n    cluster_all : bool, default=True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by computing\n        each of the n_init runs in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point.\n\n    n_iter_ : int\n        Maximum number of iterations performed on each seed.\n\n        .. versionadded:: 0.22\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MeanShift\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = MeanShift(bandwidth=2).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering.predict([[0, 0], [5, 5]])\n    array([1, 0])\n    >>> clustering\n    MeanShift(bandwidth=2)\n\n    Notes\n    -----\n\n    Scalability:\n\n    Because this implementation uses a flat kernel and\n    a Ball Tree to look up members of each kernel, the complexity will tend\n    towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n    and T the number of points. In higher dimensions the complexity will\n    tend towards O(T*n^2).\n\n    Scalability can be boosted by using fewer seeds, for example by using\n    a higher value of min_bin_freq in the get_bin_seeds function.\n\n    Note that the estimate_bandwidth function is much less scalable than the\n    mean shift algorithm and will be the bottleneck if it is used.\n\n    References\n    ----------\n\n    Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n    feature space analysis\". IEEE Transactions on Pattern Analysis and\n    Machine Intelligence. 2002. pp. 603-619.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, bandwidth=None, seeds=None, bin_seeding=False,\n                 min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300):\n        self.bandwidth = bandwidth\n        self.seeds = seeds\n        self.bin_seeding = bin_seeding\n        self.cluster_all = cluster_all\n        self.min_bin_freq = min_bin_freq\n        self.n_jobs = n_jobs\n        self.max_iter = max_iter\n\n    def fit(self, X, y=None):\n        \"\"\"Perform clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to cluster.\n\n        y : Ignored\n\n        \"\"\"\n        X = self._validate_data(X)\n        bandwidth = self.bandwidth\n        if bandwidth is None:\n            bandwidth = estimate_bandwidth(X, n_jobs=self.n_jobs)\n        elif bandwidth <= 0:\n            raise ValueError(\"bandwidth needs to be greater than zero or None,\"\n                             \" got %f\" % bandwidth)\n\n        seeds = self.seeds\n        if seeds is None:\n            if self.bin_seeding:\n                seeds = get_bin_seeds(X, bandwidth, self.min_bin_freq)\n            else:\n                seeds = X\n        n_samples, n_features = X.shape\n        center_intensity_dict = {}\n\n        # We use n_jobs=1 because this will be used in nested calls under\n        # parallel calls to _mean_shift_single_seed so there is no need for\n        # for further parallelism.\n        nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n\n        # execute iterations on all seeds in parallel\n        all_res = Parallel(n_jobs=self.n_jobs)(\n            delayed(_mean_shift_single_seed)\n            (seed, X, nbrs, self.max_iter) for seed in seeds)\n        # copy results in a dictionary\n        for i in range(len(seeds)):\n            if all_res[i][1]:  # i.e. len(points_within) > 0\n                center_intensity_dict[all_res[i][0]] = all_res[i][1]\n\n        self.n_iter_ = max([x[2] for x in all_res])\n\n        if not center_intensity_dict:\n            # nothing near seeds\n            raise ValueError(\"No point was within bandwidth=%f of any seed.\"\n                             \" Try a different seeding strategy \\\n                             or increase the bandwidth.\"\n                             % bandwidth)\n\n        # POST PROCESSING: remove near duplicate points\n        # If the distance between two kernels is less than the bandwidth,\n        # then we have to remove one because it is a duplicate. Remove the\n        # one with fewer points.\n\n        sorted_by_intensity = sorted(center_intensity_dict.items(),\n                                     key=lambda tup: (tup[1], tup[0]),\n                                     reverse=True)\n        sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\n        unique = np.ones(len(sorted_centers), dtype=bool)\n        nbrs = NearestNeighbors(radius=bandwidth,\n                                n_jobs=self.n_jobs).fit(sorted_centers)\n        for i, center in enumerate(sorted_centers):\n            if unique[i]:\n                neighbor_idxs = nbrs.radius_neighbors([center],\n                                                      return_distance=False)[0]\n                unique[neighbor_idxs] = 0\n                unique[i] = 1  # leave the current point as unique\n        cluster_centers = sorted_centers[unique]\n\n        # ASSIGN LABELS: a point belongs to the cluster that it is closest to\n        nbrs = NearestNeighbors(n_neighbors=1,\n                                n_jobs=self.n_jobs).fit(cluster_centers)\n        labels = np.zeros(n_samples, dtype=int)\n        distances, idxs = nbrs.kneighbors(X)\n        if self.cluster_all:\n            labels = idxs.flatten()\n        else:\n            labels.fill(-1)\n            bool_selector = distances.flatten() <= bandwidth\n            labels[bool_selector] = idxs.flatten()[bool_selector]\n\n        self.cluster_centers_, self.labels_ = cluster_centers, labels\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        with config_context(assume_finite=True):\n            return pairwise_distances_argmin(X, self.cluster_centers_)",
      "instance_attributes": [
        {
          "name": "bandwidth",
          "types": null
        },
        {
          "name": "seeds",
          "types": null
        },
        {
          "name": "bin_seeding",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "cluster_all",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "min_bin_freq",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_iter_",
          "types": null
        },
        {
          "name": "cluster_centers_",
          "types": null
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/OPTICS",
      "name": "OPTICS",
      "qname": "sklearn.cluster._optics.OPTICS",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._optics/OPTICS/__init__",
        "scikit-learn/sklearn.cluster._optics/OPTICS/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.",
      "docstring": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nmin_samples : int > 1 or float between 0 and 1, default=5\n    The number of samples in a neighborhood for a point to be considered as\n    a core point. Also, up and down steep regions can't have more than\n    ``min_samples`` consecutive non-steep points. Expressed as an absolute\n    number or a fraction of the number of samples (rounded to be at least\n    2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\ncluster_method : str, default='xi'\n    The extraction method used to extract clusters using the calculated\n    reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\neps : float, default=None\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. By default it assumes the same value\n    as ``max_eps``.\n    Used only when ``cluster_method='dbscan'``.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n    Used only when ``cluster_method='xi'``.\n\npredecessor_correction : bool, default=True\n    Correct clusters according to the predecessors calculated by OPTICS\n    [2]_. This parameter has minimal effect on most datasets.\n    Used only when ``cluster_method='xi'``.\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n    Used only when ``cluster_method='xi'``.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method. (default)\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nlabels_ : ndarray of shape (n_samples,)\n    Cluster labels for each point in the dataset given to fit().\n    Noisy samples and points which are not included in a leaf cluster\n    of ``cluster_hierarchy_`` are labeled as -1.\n\nreachability_ : ndarray of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\nordering_ : ndarray of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : ndarray of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : ndarray of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\ncluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to\n    ``(end, -start)`` (ascending) so that larger clusters encompassing\n    smaller clusters come after those smaller ones. Since ``labels_`` does\n    not reflect the hierarchy, usually\n    ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n    note that these indices are of the ``ordering_``, i.e.\n    ``X[ordering_][start:end + 1]`` form a cluster.\n    Only available when ``cluster_method='xi'``.\n\nSee Also\n--------\nDBSCAN : A similar clustering for a specified neighborhood radius (eps).\n    Our implementation is optimized for runtime.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n.. [2] Schubert, Erich, Michael Gertz.\n   \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n   the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\nExamples\n--------\n>>> from sklearn.cluster import OPTICS\n>>> import numpy as np\n>>> X = np.array([[1, 2], [2, 5], [3, 6],\n...               [8, 7], [8, 8], [7, 3]])\n>>> clustering = OPTICS(min_samples=2).fit(X)\n>>> clustering.labels_\narray([0, 0, 0, 1, 1, 1])",
      "code": "class OPTICS(ClusterMixin, BaseEstimator):\n    \"\"\"Estimate clustering structure from vector array.\n\n    OPTICS (Ordering Points To Identify the Clustering Structure), closely\n    related to DBSCAN, finds core sample of high density and expands clusters\n    from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\n    neighborhood radius. Better suited for usage on large datasets than the\n    current sklearn implementation of DBSCAN.\n\n    Clusters are then extracted using a DBSCAN-like method\n    (cluster_method = 'dbscan') or an automatic\n    technique proposed in [1]_ (cluster_method = 'xi').\n\n    This implementation deviates from the original OPTICS by first performing\n    k-nearest-neighborhood searches on all points to identify core sizes, then\n    computing only the distances to unprocessed points when constructing the\n    cluster order. Note that we do not employ a heap to manage the expansion\n    candidates, so the time complexity will be O(n^2).\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    min_samples : int > 1 or float between 0 and 1, default=5\n        The number of samples in a neighborhood for a point to be considered as\n        a core point. Also, up and down steep regions can't have more than\n        ``min_samples`` consecutive non-steep points. Expressed as an absolute\n        number or a fraction of the number of samples (rounded to be at least\n        2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n    p : int, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    cluster_method : str, default='xi'\n        The extraction method used to extract clusters using the calculated\n        reachability and ordering. Possible values are \"xi\" and \"dbscan\".\n\n    eps : float, default=None\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. By default it assumes the same value\n        as ``max_eps``.\n        Used only when ``cluster_method='dbscan'``.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n        Used only when ``cluster_method='xi'``.\n\n    predecessor_correction : bool, default=True\n        Correct clusters according to the predecessors calculated by OPTICS\n        [2]_. This parameter has minimal effect on most datasets.\n        Used only when ``cluster_method='xi'``.\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n        Used only when ``cluster_method='xi'``.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method. (default)\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n        affect the speed of the construction and query, as well as the memory\n        required to store the tree. The optimal value depends on the\n        nature of the problem.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples and points which are not included in a leaf cluster\n        of ``cluster_hierarchy_`` are labeled as -1.\n\n    reachability_ : ndarray of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    ordering_ : ndarray of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : ndarray of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : ndarray of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    cluster_hierarchy_ : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to\n        ``(end, -start)`` (ascending) so that larger clusters encompassing\n        smaller clusters come after those smaller ones. Since ``labels_`` does\n        not reflect the hierarchy, usually\n        ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also\n        note that these indices are of the ``ordering_``, i.e.\n        ``X[ordering_][start:end + 1]`` form a cluster.\n        Only available when ``cluster_method='xi'``.\n\n    See Also\n    --------\n    DBSCAN : A similar clustering for a specified neighborhood radius (eps).\n        Our implementation is optimized for runtime.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n\n    .. [2] Schubert, Erich, Michael Gertz.\n       \"Improving the Cluster Structure Extracted from OPTICS Plots.\" Proc. of\n       the Conference \"Lernen, Wissen, Daten, Analysen\" (LWDA) (2018): 318-329.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import OPTICS\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 5], [3, 6],\n    ...               [8, 7], [8, 8], [7, 3]])\n    >>> clustering = OPTICS(min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, min_samples=5, max_eps=np.inf, metric='minkowski',\n                 p=2, metric_params=None, cluster_method='xi', eps=None,\n                 xi=0.05, predecessor_correction=True, min_cluster_size=None,\n                 algorithm='auto', leaf_size=30, n_jobs=None):\n        self.max_eps = max_eps\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.algorithm = algorithm\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.leaf_size = leaf_size\n        self.cluster_method = cluster_method\n        self.eps = eps\n        self.xi = xi\n        self.predecessor_correction = predecessor_correction\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features), or \\\n                (n_samples, n_samples) if metric=\u2019precomputed\u2019\n            A feature array, or array of distances between samples if\n            metric='precomputed'.\n\n        y : ignored\n            Ignored.\n\n        Returns\n        -------\n        self : instance of OPTICS\n            The instance.\n        \"\"\"\n        X = self._validate_data(X, dtype=float)\n\n        if self.cluster_method not in ['dbscan', 'xi']:\n            raise ValueError(\"cluster_method should be one of\"\n                             \" 'dbscan' or 'xi' but is %s\" %\n                             self.cluster_method)\n\n        (self.ordering_, self.core_distances_, self.reachability_,\n         self.predecessor_) = compute_optics_graph(\n             X=X, min_samples=self.min_samples, algorithm=self.algorithm,\n             leaf_size=self.leaf_size, metric=self.metric,\n             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs,\n             max_eps=self.max_eps)\n\n        # Extract clusters from the calculated orders and reachability\n        if self.cluster_method == 'xi':\n            labels_, clusters_ = cluster_optics_xi(\n                reachability=self.reachability_,\n                predecessor=self.predecessor_,\n                ordering=self.ordering_,\n                min_samples=self.min_samples,\n                min_cluster_size=self.min_cluster_size,\n                xi=self.xi,\n                predecessor_correction=self.predecessor_correction)\n            self.cluster_hierarchy_ = clusters_\n        elif self.cluster_method == 'dbscan':\n            if self.eps is None:\n                eps = self.max_eps\n            else:\n                eps = self.eps\n\n            if eps > self.max_eps:\n                raise ValueError('Specify an epsilon smaller than %s. Got %s.'\n                                 % (self.max_eps, eps))\n\n            labels_ = cluster_optics_dbscan(\n                reachability=self.reachability_,\n                core_distances=self.core_distances_,\n                ordering=self.ordering_, eps=eps)\n\n        self.labels_ = labels_\n        return self",
      "instance_attributes": [
        {
          "name": "max_eps",
          "types": null
        },
        {
          "name": "min_samples",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_cluster_size",
          "types": null
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "metric",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "metric_params",
          "types": null
        },
        {
          "name": "p",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "leaf_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cluster_method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "eps",
          "types": null
        },
        {
          "name": "xi",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "predecessor_correction",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "ordering_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "core_distances_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "reachability_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "predecessor_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "cluster_hierarchy_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "labels_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering",
      "name": "SpectralClustering",
      "qname": "sklearn.cluster._spectral.SpectralClustering",
      "decorators": [],
      "superclasses": [
        "ClusterMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/_more_tags",
        "scikit-learn/sklearn.cluster._spectral/SpectralClustering/_pairwise@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.",
      "docstring": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\nn_clusters : int, default=8\n    The dimension of the projection subspace.\n\neigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nn_components : int, default=n_clusters\n    Number of eigenvectors to use for the spectral embedding\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization of the\n    lobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and by\n    the K-Means initialization. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of n_init\n    consecutive runs in terms of inertia. Only used if\n    ``assign_labels='kmeans'``.\n\ngamma : float, default=1.0\n    Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n    Ignored for ``affinity='nearest_neighbors'``.\n\naffinity : str or callable, default='rbf'\n    How to construct the affinity matrix.\n     - 'nearest_neighbors': construct the affinity matrix by computing a\n       graph of nearest neighbors.\n     - 'rbf': construct the affinity matrix using a radial basis function\n       (RBF) kernel.\n     - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n       where larger values indicate greater similarity between instances.\n     - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n       of precomputed distances, and construct a binary affinity matrix\n       from the ``n_neighbors`` nearest neighbors of each instance.\n     - one of the kernels supported by\n       :func:`~sklearn.metrics.pairwise_kernels`.\n\n    Only kernels that produce similarity scores (non-negative values that\n    increase with similarity) should be used. This property is not checked\n    by the clustering algorithm.\n\nn_neighbors : int, default=10\n    Number of neighbors to use when constructing the affinity matrix using\n    the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when ``eigen_solver='arpack'``.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans'\n    The strategy for assigning labels in the embedding space. There are two\n    ways to assign labels after the Laplacian embedding. k-means is a\n    popular choice, but it can be sensitive to initialization.\n    Discretization is another approach which is less sensitive to random\n    initialization.\n\ndegree : float, default=3\n    Degree of the polynomial kernel. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Zero coefficient for polynomial and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict of str to any, default=None\n    Parameters (keyword arguments) and values for kernel passed as\n    callable object. Ignored by other kernels.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run when `affinity='nearest_neighbors'`\n    or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n    will be done in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\naffinity_matrix_ : array-like of shape (n_samples, n_samples)\n    Affinity matrix used for clustering. Available only after calling\n    ``fit``.\n\nlabels_ : ndarray of shape (n_samples,)\n    Labels of each point\n\nExamples\n--------\n>>> from sklearn.cluster import SpectralClustering\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [1, 0],\n...               [4, 7], [3, 5], [3, 6]])\n>>> clustering = SpectralClustering(n_clusters=2,\n...         assign_labels='discretize',\n...         random_state=0).fit(X)\n>>> clustering.labels_\narray([1, 1, 1, 0, 0, 0])\n>>> clustering\nSpectralClustering(assign_labels='discretize', n_clusters=2,\n    random_state=0)\n\nNotes\n-----\nA distance matrix for which 0 indicates identical elements and high values\nindicate very dissimilar elements can be transformed into an affinity /\nsimilarity matrix that is well-suited for the algorithm by\napplying the Gaussian (aka RBF, heat) kernel::\n\n    np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\nwhere ``delta`` is a free parameter representing the width of the Gaussian\nkernel.\n\nAn alternative is to take a symmetric version of the k-nearest neighbors\nconnectivity matrix of the points.\n\nIf the pyamg package is installed, it is used: this greatly\nspeeds up computation.\n\nReferences\n----------\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf",
      "code": "class SpectralClustering(ClusterMixin, BaseEstimator):\n    \"\"\"Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex, or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster, such as when clusters are\n    nested circles on the 2D plane.\n\n    If the affinity matrix is the adjacency matrix of a graph, this method\n    can be used to find normalized graph cuts.\n\n    When calling ``fit``, an affinity matrix is constructed using either\n    a kernel function such the Gaussian (aka RBF) kernel with Euclidean\n    distance ``d(X, X)``::\n\n            np.exp(-gamma * d(X,X) ** 2)\n\n    or a k-nearest neighbors connectivity matrix.\n\n    Alternatively, a user-provided affinity matrix can be specified by\n    setting ``affinity='precomputed'``.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int, default=8\n        The dimension of the projection subspace.\n\n    eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities. If None, then ``'arpack'`` is\n        used.\n\n    n_components : int, default=n_clusters\n        Number of eigenvectors to use for the spectral embedding\n\n    random_state : int, RandomState instance, default=None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and by\n        the K-Means initialization. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of n_init\n        consecutive runs in terms of inertia. Only used if\n        ``assign_labels='kmeans'``.\n\n    gamma : float, default=1.0\n        Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n        Ignored for ``affinity='nearest_neighbors'``.\n\n    affinity : str or callable, default='rbf'\n        How to construct the affinity matrix.\n         - 'nearest_neighbors': construct the affinity matrix by computing a\n           graph of nearest neighbors.\n         - 'rbf': construct the affinity matrix using a radial basis function\n           (RBF) kernel.\n         - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n           where larger values indicate greater similarity between instances.\n         - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n           of precomputed distances, and construct a binary affinity matrix\n           from the ``n_neighbors`` nearest neighbors of each instance.\n         - one of the kernels supported by\n           :func:`~sklearn.metrics.pairwise_kernels`.\n\n        Only kernels that produce similarity scores (non-negative values that\n        increase with similarity) should be used. This property is not checked\n        by the clustering algorithm.\n\n    n_neighbors : int, default=10\n        Number of neighbors to use when constructing the affinity matrix using\n        the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\n    eigen_tol : float, default=0.0\n        Stopping criterion for eigendecomposition of the Laplacian matrix\n        when ``eigen_solver='arpack'``.\n\n    assign_labels : {'kmeans', 'discretize'}, default='kmeans'\n        The strategy for assigning labels in the embedding space. There are two\n        ways to assign labels after the Laplacian embedding. k-means is a\n        popular choice, but it can be sensitive to initialization.\n        Discretization is another approach which is less sensitive to random\n        initialization.\n\n    degree : float, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict of str to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run when `affinity='nearest_neighbors'`\n        or `affinity='precomputed_nearest_neighbors'`. The neighbors search\n        will be done in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    affinity_matrix_ : array-like of shape (n_samples, n_samples)\n        Affinity matrix used for clustering. Available only after calling\n        ``fit``.\n\n    labels_ : ndarray of shape (n_samples,)\n        Labels of each point\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralClustering(n_clusters=2,\n    ...         assign_labels='discretize',\n    ...         random_state=0).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering\n    SpectralClustering(assign_labels='discretize', n_clusters=2,\n        random_state=0)\n\n    Notes\n    -----\n    A distance matrix for which 0 indicates identical elements and high values\n    indicate very dissimilar elements can be transformed into an affinity /\n    similarity matrix that is well-suited for the algorithm by\n    applying the Gaussian (aka RBF, heat) kernel::\n\n        np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\n    where ``delta`` is a free parameter representing the width of the Gaussian\n    kernel.\n\n    An alternative is to take a symmetric version of the k-nearest neighbors\n    connectivity matrix of the points.\n\n    If the pyamg package is installed, it is used: this greatly\n    speeds up computation.\n\n    References\n    ----------\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - Multiclass spectral clustering, 2003\n      Stella X. Yu, Jianbo Shi\n      https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, eigen_solver=None, n_components=None,\n                 random_state=None, n_init=10, gamma=1., affinity='rbf',\n                 n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans',\n                 degree=3, coef0=1, kernel_params=None, n_jobs=None,\n                 verbose=False):\n        self.n_clusters = n_clusters\n        self.eigen_solver = eigen_solver\n        self.n_components = n_components\n        self.random_state = random_state\n        self.n_init = n_init\n        self.gamma = gamma\n        self.affinity = affinity\n        self.n_neighbors = n_neighbors\n        self.eigen_tol = eigen_tol\n        self.assign_labels = assign_labels\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n    def fit(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                dtype=np.float64, ensure_min_samples=2)\n        allow_squared = self.affinity in [\"precomputed\",\n                                          \"precomputed_nearest_neighbors\"]\n        if X.shape[0] == X.shape[1] and not allow_squared:\n            warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n                          \"now constructs an affinity matrix from data. To use\"\n                          \" a custom affinity matrix, \"\n                          \"set ``affinity=precomputed``.\")\n\n        if self.affinity == 'nearest_neighbors':\n            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors,\n                                            include_self=True,\n                                            n_jobs=self.n_jobs)\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed_nearest_neighbors':\n            estimator = NearestNeighbors(n_neighbors=self.n_neighbors,\n                                         n_jobs=self.n_jobs,\n                                         metric=\"precomputed\").fit(X)\n            connectivity = estimator.kneighbors_graph(X=X, mode='connectivity')\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed':\n            self.affinity_matrix_ = X\n        else:\n            params = self.kernel_params\n            if params is None:\n                params = {}\n            if not callable(self.affinity):\n                params['gamma'] = self.gamma\n                params['degree'] = self.degree\n                params['coef0'] = self.coef0\n            self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n                                                     filter_params=True,\n                                                     **params)\n\n        random_state = check_random_state(self.random_state)\n        self.labels_ = spectral_clustering(self.affinity_matrix_,\n                                           n_clusters=self.n_clusters,\n                                           n_components=self.n_components,\n                                           eigen_solver=self.eigen_solver,\n                                           random_state=random_state,\n                                           n_init=self.n_init,\n                                           eigen_tol=self.eigen_tol,\n                                           assign_labels=self.assign_labels,\n                                           verbose=self.verbose)\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)\n\n    def _more_tags(self):\n        return {'pairwise': self.affinity in [\"precomputed\",\n                                              \"precomputed_nearest_neighbors\"]}\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute _pairwise was deprecated in \"  # type: ignore\n                \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def _pairwise(self):\n        return self.affinity in [\"precomputed\",\n                                 \"precomputed_nearest_neighbors\"]",
      "instance_attributes": [
        {
          "name": "n_clusters",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "eigen_solver",
          "types": null
        },
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "n_init",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "gamma",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "affinity",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_neighbors",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "eigen_tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "assign_labels",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "degree",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "coef0",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "kernel_params",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "affinity_matrix_",
          "types": null
        },
        {
          "name": "labels_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer",
      "name": "ColumnTransformer",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "_BaseComposition"
      ],
      "methods": [
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_transformers@getter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_transformers@setter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_iter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_transformers",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_column_callables",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_remainder",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/named_transformers_@getter",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_feature_names",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_update_fitted_transformers",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_validate_output",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_log_message",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_fit_transform",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_hstack",
        "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/_sk_visual_block_"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20",
      "docstring": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\ntransformers : list of tuples\n    List of (name, transformer, columns) tuples specifying the\n    transformer objects to be applied to subsets of the data.\n\n    name : str\n        Like in Pipeline and FeatureUnion, this allows the transformer and\n        its parameters to be set using ``set_params`` and searched in grid\n        search.\n    transformer : {'drop', 'passthrough'} or estimator\n        Estimator must support :term:`fit` and :term:`transform`.\n        Special-cased strings 'drop' and 'passthrough' are accepted as\n        well, to indicate to drop the columns or to pass them through\n        untransformed, respectively.\n    columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n        Indexes the data on its second axis. Integers are interpreted as\n        positional columns, while strings can reference DataFrame columns\n        by name.  A scalar string or int should be used where\n        ``transformer`` expects X to be a 1d array-like (vector),\n        otherwise a 2d array will be passed to the transformer.\n        A callable is passed the input data `X` and can return any of the\n        above. To select multiple columns by name or dtype, you can use\n        :obj:`make_column_selector`.\n\nremainder : {'drop', 'passthrough'} or estimator, default='drop'\n    By default, only the specified columns in `transformers` are\n    transformed and combined in the output, and the non-specified\n    columns are dropped. (default of ``'drop'``).\n    By specifying ``remainder='passthrough'``, all remaining columns that\n    were not specified in `transformers` will be automatically passed\n    through. This subset of columns is concatenated with the output of\n    the transformers.\n    By setting ``remainder`` to be an estimator, the remaining\n    non-specified columns will use the ``remainder`` estimator. The\n    estimator must support :term:`fit` and :term:`transform`.\n    Note that using this feature requires that the DataFrame columns\n    input at :term:`fit` and :term:`transform` have identical order.\n\nsparse_threshold : float, default=0.3\n    If the output of the different transformers contains sparse matrices,\n    these will be stacked as a sparse matrix if the overall density is\n    lower than this value. Use ``sparse_threshold=0`` to always return\n    dense.  When the transformed output consists of all dense data, the\n    stacked result will be dense, and this keyword will be ignored.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ntransformer_weights : dict, default=None\n    Multiplicative weights for features per transformer. The output of the\n    transformer is multiplied by these weights. Keys are transformer names,\n    values the weights.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nAttributes\n----------\ntransformers_ : list\n    The collection of fitted transformers as tuples of\n    (name, fitted_transformer, column). `fitted_transformer` can be an\n    estimator, 'drop', or 'passthrough'. In case there were no columns\n    selected, this will be the unfitted transformer.\n    If there are remaining columns, the final element is a tuple of the\n    form:\n    ('remainder', transformer, remaining_columns) corresponding to the\n    ``remainder`` parameter. If there are remaining columns, then\n    ``len(transformers_)==len(transformers)+1``, otherwise\n    ``len(transformers_)==len(transformers)``.\n\nnamed_transformers_ : :class:`~sklearn.utils.Bunch`\n    Read-only attribute to access any transformer by given name.\n    Keys are transformer names and values are the fitted transformer\n    objects.\n\nsparse_output_ : bool\n    Boolean flag indicating whether the output of ``transform`` is a\n    sparse matrix or a dense numpy array, which depends on the output\n    of the individual transformers and the `sparse_threshold` keyword.\n\nNotes\n-----\nThe order of the columns in the transformed feature matrix follows the\norder of how the columns are specified in the `transformers` list.\nColumns of the original feature matrix that are not specified are\ndropped from the resulting transformed feature matrix, unless specified\nin the `passthrough` keyword. Those columns specified with `passthrough`\nare added at the right to the output of the transformers.\n\nSee Also\n--------\nmake_column_transformer : Convenience function for\n    combining the outputs of multiple transformer objects applied to\n    column subsets of the original feature space.\nmake_column_selector : Convenience function for selecting\n    columns based on datatype or the columns name with a regex pattern.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.preprocessing import Normalizer\n>>> ct = ColumnTransformer(\n...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n>>> X = np.array([[0., 1., 2., 2.],\n...               [1., 1., 0., 1.]])\n>>> # Normalizer scales each row of X to unit norm. A separate scaling\n>>> # is applied for the two first and two last elements of each\n>>> # row independently.\n>>> ct.fit_transform(X)\narray([[0. , 1. , 0.5, 0.5],\n       [0.5, 0.5, 0. , 1. ]])",
      "code": "class ColumnTransformer(TransformerMixin, _BaseComposition):\n    \"\"\"Applies transformers to columns of an array or pandas DataFrame.\n\n    This estimator allows different columns or column subsets of the input\n    to be transformed separately and the features generated by each transformer\n    will be concatenated to form a single feature space.\n    This is useful for heterogeneous or columnar data, to combine several\n    feature extraction mechanisms or transformations into a single transformer.\n\n    Read more in the :ref:`User Guide <column_transformer>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    transformers : list of tuples\n        List of (name, transformer, columns) tuples specifying the\n        transformer objects to be applied to subsets of the data.\n\n        name : str\n            Like in Pipeline and FeatureUnion, this allows the transformer and\n            its parameters to be set using ``set_params`` and searched in grid\n            search.\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns :  str, array-like of str, int, array-like of int, \\\n                array-like of bool, slice or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name.  A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n        Note that using this feature requires that the DataFrame columns\n        input at :term:`fit` and :term:`transform` have identical order.\n\n    sparse_threshold : float, default=0.3\n        If the output of the different transformers contains sparse matrices,\n        these will be stacked as a sparse matrix if the overall density is\n        lower than this value. Use ``sparse_threshold=0`` to always return\n        dense.  When the transformed output consists of all dense data, the\n        stacked result will be dense, and this keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    transformer_weights : dict, default=None\n        Multiplicative weights for features per transformer. The output of the\n        transformer is multiplied by these weights. Keys are transformer names,\n        values the weights.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    Attributes\n    ----------\n    transformers_ : list\n        The collection of fitted transformers as tuples of\n        (name, fitted_transformer, column). `fitted_transformer` can be an\n        estimator, 'drop', or 'passthrough'. In case there were no columns\n        selected, this will be the unfitted transformer.\n        If there are remaining columns, the final element is a tuple of the\n        form:\n        ('remainder', transformer, remaining_columns) corresponding to the\n        ``remainder`` parameter. If there are remaining columns, then\n        ``len(transformers_)==len(transformers)+1``, otherwise\n        ``len(transformers_)==len(transformers)``.\n\n    named_transformers_ : :class:`~sklearn.utils.Bunch`\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n    sparse_output_ : bool\n        Boolean flag indicating whether the output of ``transform`` is a\n        sparse matrix or a dense numpy array, which depends on the output\n        of the individual transformers and the `sparse_threshold` keyword.\n\n    Notes\n    -----\n    The order of the columns in the transformed feature matrix follows the\n    order of how the columns are specified in the `transformers` list.\n    Columns of the original feature matrix that are not specified are\n    dropped from the resulting transformed feature matrix, unless specified\n    in the `passthrough` keyword. Those columns specified with `passthrough`\n    are added at the right to the output of the transformers.\n\n    See Also\n    --------\n    make_column_transformer : Convenience function for\n        combining the outputs of multiple transformer objects applied to\n        column subsets of the original feature space.\n    make_column_selector : Convenience function for selecting\n        columns based on datatype or the columns name with a regex pattern.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n    ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    \"\"\"\n    _required_parameters = ['transformers']\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 transformers, *,\n                 remainder='drop',\n                 sparse_threshold=0.3,\n                 n_jobs=None,\n                 transformer_weights=None,\n                 verbose=False):\n        self.transformers = transformers\n        self.remainder = remainder\n        self.sparse_threshold = sparse_threshold\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose\n\n    @property\n    def _transformers(self):\n        \"\"\"\n        Internal list of transformer only containing the name and\n        transformers, dropping the columns. This is for the implementation\n        of get_params via BaseComposition._get_params which expects lists\n        of tuples of len 2.\n        \"\"\"\n        return [(name, trans) for name, trans, _ in self.transformers]\n\n    @_transformers.setter\n    def _transformers(self, value):\n        self.transformers = [\n            (name, trans, col) for ((name, trans), (_, _, col))\n            in zip(value, self.transformers)]\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `transformers` of the\n        `ColumnTransformer`.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        return self._get_params('_transformers', deep=deep)\n\n    def set_params(self, **kwargs):\n        \"\"\"Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``. Note that you\n        can directly set the parameters of the estimators contained in\n        `transformers` of `ColumnTransformer`.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._set_params('_transformers', **kwargs)\n        return self\n\n    def _iter(self, fitted=False, replace_strings=False):\n        \"\"\"\n        Generate (name, trans, column, weight) tuples.\n\n        If fitted=True, use the fitted transformers, else use the\n        user specified transformers updated with converted column names\n        and potentially appended with transformer for remainder.\n\n        \"\"\"\n        if fitted:\n            transformers = self.transformers_\n        else:\n            # interleave the validated column specifiers\n            transformers = [\n                (name, trans, column) for (name, trans, _), column\n                in zip(self.transformers, self._columns)\n            ]\n            # add transformer tuple for remainder\n            if self._remainder[2] is not None:\n                transformers = chain(transformers, [self._remainder])\n        get_weight = (self.transformer_weights or {}).get\n\n        for name, trans, column in transformers:\n            if replace_strings:\n                # replace 'passthrough' with identity transformer and\n                # skip in case of 'drop'\n                if trans == 'passthrough':\n                    trans = FunctionTransformer(\n                        accept_sparse=True, check_inverse=False\n                    )\n                elif trans == 'drop':\n                    continue\n                elif _is_empty_column_selection(column):\n                    continue\n\n            yield (name, trans, column, get_weight(name))\n\n    def _validate_transformers(self):\n        if not self.transformers:\n            return\n\n        names, transformers, _ = zip(*self.transformers)\n\n        # validate names\n        self._validate_names(names)\n\n        # validate estimators\n        for t in transformers:\n            if t in ('drop', 'passthrough'):\n                continue\n            if (not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not\n                    hasattr(t, \"transform\")):\n                raise TypeError(\"All estimators should implement fit and \"\n                                \"transform, or can be 'drop' or 'passthrough' \"\n                                \"specifiers. '%s' (type %s) doesn't.\" %\n                                (t, type(t)))\n\n    def _validate_column_callables(self, X):\n        \"\"\"\n        Converts callable column specifications.\n        \"\"\"\n        columns = []\n        for _, _, column in self.transformers:\n            if callable(column):\n                column = column(X)\n            columns.append(column)\n        self._columns = columns\n\n    def _validate_remainder(self, X):\n        \"\"\"\n        Validates ``remainder`` and defines ``_remainder`` targeting\n        the remaining columns.\n        \"\"\"\n        is_transformer = ((hasattr(self.remainder, \"fit\")\n                           or hasattr(self.remainder, \"fit_transform\"))\n                          and hasattr(self.remainder, \"transform\"))\n        if (self.remainder not in ('drop', 'passthrough')\n                and not is_transformer):\n            raise ValueError(\n                \"The remainder keyword needs to be one of 'drop', \"\n                \"'passthrough', or estimator. '%s' was passed instead\" %\n                self.remainder)\n\n        # Make it possible to check for reordered named columns on transform\n        self._has_str_cols = any(_determine_key_type(cols) == 'str'\n                                 for cols in self._columns)\n        if hasattr(X, 'columns'):\n            self._df_columns = X.columns\n\n        self._n_features = X.shape[1]\n        cols = []\n        for columns in self._columns:\n            cols.extend(_get_column_indices(X, columns))\n\n        remaining_idx = sorted(set(range(self._n_features)) - set(cols))\n        self._remainder = ('remainder', self.remainder, remaining_idx or None)\n\n    @property\n    def named_transformers_(self):\n        \"\"\"Access the fitted transformer by name.\n\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n        \"\"\"\n        # Use Bunch object to improve autocomplete\n        return Bunch(**{name: trans for name, trans, _\n                        in self.transformers_})\n\n    def get_feature_names(self):\n        \"\"\"Get feature names from all transformers.\n\n        Returns\n        -------\n        feature_names : list of strings\n            Names of the features produced by transform.\n        \"\"\"\n        check_is_fitted(self)\n        feature_names = []\n        for name, trans, column, _ in self._iter(fitted=True):\n            if trans == 'drop' or _is_empty_column_selection(column):\n                continue\n            if trans == 'passthrough':\n                if hasattr(self, '_df_columns'):\n                    if ((not isinstance(column, slice))\n                            and all(isinstance(col, str) for col in column)):\n                        feature_names.extend(column)\n                    else:\n                        feature_names.extend(self._df_columns[column])\n                else:\n                    indices = np.arange(self._n_features)\n                    feature_names.extend(['x%d' % i for i in indices[column]])\n                continue\n            if not hasattr(trans, 'get_feature_names'):\n                raise AttributeError(\"Transformer %s (type %s) does not \"\n                                     \"provide get_feature_names.\"\n                                     % (str(name), type(trans).__name__))\n            feature_names.extend([name + \"__\" + f for f in\n                                  trans.get_feature_names()])\n        return feature_names\n\n    def _update_fitted_transformers(self, transformers):\n        # transformers are fitted; excludes 'drop' cases\n        fitted_transformers = iter(transformers)\n        transformers_ = []\n\n        for name, old, column, _ in self._iter():\n            if old == 'drop':\n                trans = 'drop'\n            elif old == 'passthrough':\n                # FunctionTransformer is present in list of transformers,\n                # so get next transformer, but save original string\n                next(fitted_transformers)\n                trans = 'passthrough'\n            elif _is_empty_column_selection(column):\n                trans = old\n            else:\n                trans = next(fitted_transformers)\n            transformers_.append((name, trans, column))\n\n        # sanity check that transformers is exhausted\n        assert not list(fitted_transformers)\n        self.transformers_ = transformers_\n\n    def _validate_output(self, result):\n        \"\"\"\n        Ensure that the output of each transformer is 2D. Otherwise\n        hstack can raise an error or produce incorrect results.\n        \"\"\"\n        names = [name for name, _, _, _ in self._iter(fitted=True,\n                                                      replace_strings=True)]\n        for Xs, name in zip(result, names):\n            if not getattr(Xs, 'ndim', 0) == 2:\n                raise ValueError(\n                    \"The output of the '{0}' transformer should be 2D (scipy \"\n                    \"matrix, array, or pandas DataFrame).\".format(name))\n\n    def _log_message(self, name, idx, total):\n        if not self.verbose:\n            return None\n        return '(%d of %d) Processing %s' % (idx, total, name)\n\n    def _fit_transform(self, X, y, func, fitted=False):\n        \"\"\"\n        Private function to fit and/or transform on demand.\n\n        Return value (transformers and/or transformed X data) depends\n        on the passed function.\n        ``fitted=True`` ensures the fitted transformers are used.\n        \"\"\"\n        transformers = list(\n            self._iter(fitted=fitted, replace_strings=True))\n        try:\n            return Parallel(n_jobs=self.n_jobs)(\n                delayed(func)(\n                    transformer=clone(trans) if not fitted else trans,\n                    X=_safe_indexing(X, column, axis=1),\n                    y=y,\n                    weight=weight,\n                    message_clsname='ColumnTransformer',\n                    message=self._log_message(name, idx, len(transformers)))\n                for idx, (name, trans, column, weight) in enumerate(\n                        self._iter(fitted=fitted, replace_strings=True), 1))\n        except ValueError as e:\n            if \"Expected 2D array, got 1D array instead\" in str(e):\n                raise ValueError(_ERR_MSG_1DCOLUMN) from e\n            else:\n                raise\n\n    def fit(self, X, y=None):\n        \"\"\"Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,...), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        self : ColumnTransformer\n            This estimator\n\n        \"\"\"\n        # we use fit_transform to make sure to set sparse_output_ (for which we\n        # need the transformed data) to have consistent output type in predict\n        self.fit_transform(X, y=y)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        # TODO: this should be `feature_names_in_` when we start having it\n        if hasattr(X, \"columns\"):\n            self._feature_names_in = np.asarray(X.columns)\n        else:\n            self._feature_names_in = None\n        X = _check_X(X)\n        # set n_features_in_ attribute\n        self._check_n_features(X, reset=True)\n        self._validate_transformers()\n        self._validate_column_callables(X)\n        self._validate_remainder(X)\n\n        result = self._fit_transform(X, y, _fit_transform_one)\n\n        if not result:\n            self._update_fitted_transformers([])\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*result)\n\n        # determine if concatenated output will be sparse or not\n        if any(sparse.issparse(X) for X in Xs):\n            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)\n            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)\n                        else X.size for X in Xs)\n            density = nnz / total\n            self.sparse_output_ = density < self.sparse_threshold\n        else:\n            self.sparse_output_ = False\n\n        self._update_fitted_transformers(transformers)\n        self._validate_output(Xs)\n\n        return self._hstack(list(Xs))\n\n    def transform(self, X):\n        \"\"\"Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            The data to be transformed by subset.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        check_is_fitted(self)\n        X = _check_X(X)\n        if hasattr(X, \"columns\"):\n            X_feature_names = np.asarray(X.columns)\n        else:\n            X_feature_names = None\n\n        self._check_n_features(X, reset=False)\n        if (self._feature_names_in is not None and\n            X_feature_names is not None and\n                np.any(self._feature_names_in != X_feature_names)):\n            raise RuntimeError(\n                \"Given feature/column names do not match the ones for the \"\n                \"data given during fit.\"\n            )\n        Xs = self._fit_transform(X, None, _transform_one, fitted=True)\n        self._validate_output(Xs)\n\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        return self._hstack(list(Xs))\n\n    def _hstack(self, Xs):\n        \"\"\"Stacks Xs horizontally.\n\n        This allows subclasses to control the stacking behavior, while reusing\n        everything else from ColumnTransformer.\n\n        Parameters\n        ----------\n        Xs : list of {array-like, sparse matrix, dataframe}\n        \"\"\"\n        if self.sparse_output_:\n            try:\n                # since all columns should be numeric before stacking them\n                # in a sparse matrix, `check_array` is used for the\n                # dtype conversion if necessary.\n                converted_Xs = [check_array(X,\n                                            accept_sparse=True,\n                                            force_all_finite=False)\n                                for X in Xs]\n            except ValueError as e:\n                raise ValueError(\n                    \"For a sparse output, all columns should \"\n                    \"be a numeric or convertible to a numeric.\"\n                ) from e\n\n            return sparse.hstack(converted_Xs).tocsr()\n        else:\n            Xs = [f.toarray() if sparse.issparse(f) else f for f in Xs]\n            return np.hstack(Xs)\n\n    def _sk_visual_block_(self):\n        if isinstance(self.remainder, str) and self.remainder == 'drop':\n            transformers = self.transformers\n        elif hasattr(self, \"_remainder\"):\n            remainder_columns = self._remainder[2]\n            if hasattr(self, '_df_columns'):\n                remainder_columns = (\n                    self._df_columns[remainder_columns].tolist()\n                )\n            transformers = chain(self.transformers,\n                                 [('remainder', self.remainder,\n                                   remainder_columns)])\n        else:\n            transformers = chain(self.transformers,\n                                 [('remainder', self.remainder, '')])\n\n        names, transformers, name_details = zip(*transformers)\n        return _VisualBlock('parallel', transformers,\n                            names=names, name_details=name_details)",
      "instance_attributes": [
        {
          "name": "transformers",
          "types": null
        },
        {
          "name": "remainder",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "sparse_threshold",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "transformer_weights",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "_columns",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "_has_str_cols",
          "types": null
        },
        {
          "name": "_df_columns",
          "types": null
        },
        {
          "name": "_n_features",
          "types": null
        },
        {
          "name": "_remainder",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "transformers_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "_feature_names_in",
          "types": null
        },
        {
          "name": "sparse_output_",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector",
      "name": "make_column_selector",
      "qname": "sklearn.compose._column_transformer.make_column_selector",
      "decorators": [],
      "superclasses": [],
      "methods": [
        "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__",
        "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.",
      "docstring": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.\n\nParameters\n----------\npattern : str, default=None\n    Name of columns containing this regex pattern will be included. If\n    None, column selection will not be selected based on pattern.\n\ndtype_include : column dtype or list of column dtypes, default=None\n    A selection of dtypes to include. For more details, see\n    :meth:`pandas.DataFrame.select_dtypes`.\n\ndtype_exclude : column dtype or list of column dtypes, default=None\n    A selection of dtypes to exclude. For more details, see\n    :meth:`pandas.DataFrame.select_dtypes`.\n\nReturns\n-------\nselector : callable\n    Callable for column selection to be used by a\n    :class:`ColumnTransformer`.\n\nSee Also\n--------\nColumnTransformer : Class that allows combining the\n    outputs of multiple transformer objects used on column subsets\n    of the data into a single feature space.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> from sklearn.compose import make_column_selector\n>>> import pandas as pd  # doctest: +SKIP\n>>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n>>> ct = make_column_transformer(\n...       (StandardScaler(),\n...        make_column_selector(dtype_include=np.number)),  # rating\n...       (OneHotEncoder(),\n...        make_column_selector(dtype_include=object)))  # city\n>>> ct.fit_transform(X)  # doctest: +SKIP\narray([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n       [-1.50755672,  1.        ,  0.        ,  0.        ],\n       [-0.30151134,  0.        ,  1.        ,  0.        ],\n       [ 0.90453403,  0.        ,  0.        ,  1.        ]])",
      "code": "class make_column_selector:\n    \"\"\"Create a callable to select columns to be used with\n    :class:`ColumnTransformer`.\n\n    :func:`make_column_selector` can select columns based on datatype or the\n    columns name with a regex. When using multiple selection criteria, **all**\n    criteria must match for a column to be selected.\n\n    Parameters\n    ----------\n    pattern : str, default=None\n        Name of columns containing this regex pattern will be included. If\n        None, column selection will not be selected based on pattern.\n\n    dtype_include : column dtype or list of column dtypes, default=None\n        A selection of dtypes to include. For more details, see\n        :meth:`pandas.DataFrame.select_dtypes`.\n\n    dtype_exclude : column dtype or list of column dtypes, default=None\n        A selection of dtypes to exclude. For more details, see\n        :meth:`pandas.DataFrame.select_dtypes`.\n\n    Returns\n    -------\n    selector : callable\n        Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> from sklearn.compose import make_column_selector\n    >>> import pandas as pd  # doctest: +SKIP\n    >>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n    ...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP\n    >>> ct = make_column_transformer(\n    ...       (StandardScaler(),\n    ...        make_column_selector(dtype_include=np.number)),  # rating\n    ...       (OneHotEncoder(),\n    ...        make_column_selector(dtype_include=object)))  # city\n    >>> ct.fit_transform(X)  # doctest: +SKIP\n    array([[ 0.90453403,  1.        ,  0.        ,  0.        ],\n           [-1.50755672,  1.        ,  0.        ,  0.        ],\n           [-0.30151134,  0.        ,  1.        ,  0.        ],\n           [ 0.90453403,  0.        ,  0.        ,  1.        ]])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, pattern=None, *, dtype_include=None,\n                 dtype_exclude=None):\n        self.pattern = pattern\n        self.dtype_include = dtype_include\n        self.dtype_exclude = dtype_exclude\n\n    def __call__(self, df):\n        \"\"\"Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n        Parameters\n        ----------\n        df : dataframe of shape (n_features, n_samples)\n            DataFrame to select columns from.\n        \"\"\"\n        if not hasattr(df, 'iloc'):\n            raise ValueError(\"make_column_selector can only be applied to \"\n                             \"pandas dataframes\")\n        df_row = df.iloc[:1]\n        if self.dtype_include is not None or self.dtype_exclude is not None:\n            df_row = df_row.select_dtypes(include=self.dtype_include,\n                                          exclude=self.dtype_exclude)\n        cols = df_row.columns\n        if self.pattern is not None:\n            cols = cols[cols.str.contains(self.pattern, regex=True)]\n        return cols.tolist()",
      "instance_attributes": [
        {
          "name": "pattern",
          "types": null
        },
        {
          "name": "dtype_include",
          "types": null
        },
        {
          "name": "dtype_exclude",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor",
      "name": "TransformedTargetRegressor",
      "qname": "sklearn.compose._target.TransformedTargetRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/_fit_transformer",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/_more_tags",
        "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/n_features_in_@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20",
      "docstring": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20\n\nParameters\n----------\nregressor : object, default=None\n    Regressor object such as derived from ``RegressorMixin``. This\n    regressor will automatically be cloned each time prior to fitting.\n    If regressor is ``None``, ``LinearRegression()`` is created and used.\n\ntransformer : object, default=None\n    Estimator object such as derived from ``TransformerMixin``. Cannot be\n    set at the same time as ``func`` and ``inverse_func``. If\n    ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\n    the transformer will be an identity transformer. Note that the\n    transformer will be cloned during fitting. Also, the transformer is\n    restricting ``y`` to be a numpy array.\n\nfunc : function, default=None\n    Function to apply to ``y`` before passing to ``fit``. Cannot be set at\n    the same time as ``transformer``. The function needs to return a\n    2-dimensional array. If ``func`` is ``None``, the function used will be\n    the identity function.\n\ninverse_func : function, default=None\n    Function to apply to the prediction of the regressor. Cannot be set at\n    the same time as ``transformer`` as well. The function needs to return\n    a 2-dimensional array. The inverse function is used to return\n    predictions to the same space of the original training labels.\n\ncheck_inverse : bool, default=True\n    Whether to check that ``transform`` followed by ``inverse_transform``\n    or ``func`` followed by ``inverse_func`` leads to the original targets.\n\nAttributes\n----------\nregressor_ : object\n    Fitted regressor.\n\ntransformer_ : object\n    Transformer used in ``fit`` and ``predict``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.compose import TransformedTargetRegressor\n>>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n...                                 func=np.log, inverse_func=np.exp)\n>>> X = np.arange(4).reshape(-1, 1)\n>>> y = np.exp(2 * X).ravel()\n>>> tt.fit(X, y)\nTransformedTargetRegressor(...)\n>>> tt.score(X, y)\n1.0\n>>> tt.regressor_.coef_\narray([2.])\n\nNotes\n-----\nInternally, the target ``y`` is always converted into a 2-dimensional array\nto be used by scikit-learn transformers. At the time of prediction, the\noutput will be reshaped to a have the same number of dimensions as ``y``.\n\nSee :ref:`examples/compose/plot_transformed_target.py\n<sphx_glr_auto_examples_compose_plot_transformed_target.py>`.",
      "code": "class TransformedTargetRegressor(RegressorMixin, BaseEstimator):\n    \"\"\"Meta-estimator to regress on a transformed target.\n\n    Useful for applying a non-linear transformation to the target ``y`` in\n    regression problems. This transformation can be given as a Transformer\n    such as the QuantileTransformer or as a function and its inverse such as\n    ``log`` and ``exp``.\n\n    The computation during ``fit`` is::\n\n        regressor.fit(X, func(y))\n\n    or::\n\n        regressor.fit(X, transformer.transform(y))\n\n    The computation during ``predict`` is::\n\n        inverse_func(regressor.predict(X))\n\n    or::\n\n        transformer.inverse_transform(regressor.predict(X))\n\n    Read more in the :ref:`User Guide <transformed_target_regressor>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    regressor : object, default=None\n        Regressor object such as derived from ``RegressorMixin``. This\n        regressor will automatically be cloned each time prior to fitting.\n        If regressor is ``None``, ``LinearRegression()`` is created and used.\n\n    transformer : object, default=None\n        Estimator object such as derived from ``TransformerMixin``. Cannot be\n        set at the same time as ``func`` and ``inverse_func``. If\n        ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\n        the transformer will be an identity transformer. Note that the\n        transformer will be cloned during fitting. Also, the transformer is\n        restricting ``y`` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to ``y`` before passing to ``fit``. Cannot be set at\n        the same time as ``transformer``. The function needs to return a\n        2-dimensional array. If ``func`` is ``None``, the function used will be\n        the identity function.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as ``transformer`` as well. The function needs to return\n        a 2-dimensional array. The inverse function is used to return\n        predictions to the same space of the original training labels.\n\n    check_inverse : bool, default=True\n        Whether to check that ``transform`` followed by ``inverse_transform``\n        or ``func`` followed by ``inverse_func`` leads to the original targets.\n\n    Attributes\n    ----------\n    regressor_ : object\n        Fitted regressor.\n\n    transformer_ : object\n        Transformer used in ``fit`` and ``predict``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.compose import TransformedTargetRegressor\n    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n    ...                                 func=np.log, inverse_func=np.exp)\n    >>> X = np.arange(4).reshape(-1, 1)\n    >>> y = np.exp(2 * X).ravel()\n    >>> tt.fit(X, y)\n    TransformedTargetRegressor(...)\n    >>> tt.score(X, y)\n    1.0\n    >>> tt.regressor_.coef_\n    array([2.])\n\n    Notes\n    -----\n    Internally, the target ``y`` is always converted into a 2-dimensional array\n    to be used by scikit-learn transformers. At the time of prediction, the\n    output will be reshaped to a have the same number of dimensions as ``y``.\n\n    See :ref:`examples/compose/plot_transformed_target.py\n    <sphx_glr_auto_examples_compose_plot_transformed_target.py>`.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, regressor=None, *, transformer=None,\n                 func=None, inverse_func=None, check_inverse=True):\n        self.regressor = regressor\n        self.transformer = transformer\n        self.func = func\n        self.inverse_func = inverse_func\n        self.check_inverse = check_inverse\n\n    def _fit_transformer(self, y):\n        \"\"\"Check transformer and fit transformer.\n\n        Create the default transformer, fit it and make additional inverse\n        check on a subset (optional).\n\n        \"\"\"\n        if (self.transformer is not None and\n                (self.func is not None or self.inverse_func is not None)):\n            raise ValueError(\"'transformer' and functions 'func'/\"\n                             \"'inverse_func' cannot both be set.\")\n        elif self.transformer is not None:\n            self.transformer_ = clone(self.transformer)\n        else:\n            if self.func is not None and self.inverse_func is None:\n                raise ValueError(\"When 'func' is provided, 'inverse_func' must\"\n                                 \" also be provided\")\n            self.transformer_ = FunctionTransformer(\n                func=self.func, inverse_func=self.inverse_func, validate=True,\n                check_inverse=self.check_inverse)\n        # XXX: sample_weight is not currently passed to the\n        # transformer. However, if transformer starts using sample_weight, the\n        # code should be modified accordingly. At the time to consider the\n        # sample_prop feature, it is also a good use case to be considered.\n        self.transformer_.fit(y)\n        if self.check_inverse:\n            idx_selected = slice(None, None, max(1, y.shape[0] // 10))\n            y_sel = _safe_indexing(y, idx_selected)\n            y_sel_t = self.transformer_.transform(y_sel)\n            if not np.allclose(y_sel,\n                               self.transformer_.inverse_transform(y_sel_t)):\n                warnings.warn(\"The provided functions or transformer are\"\n                              \" not strictly inverse of each other. If\"\n                              \" you are sure you want to proceed regardless\"\n                              \", set 'check_inverse=False'\", UserWarning)\n\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **fit_params : dict\n            Parameters passed to the ``fit`` method of the underlying\n            regressor.\n\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        y = check_array(y, accept_sparse=False, force_all_finite=True,\n                        ensure_2d=False, dtype='numeric')\n\n        # store the number of dimension of the target to predict an array of\n        # similar shape at predict\n        self._training_dim = y.ndim\n\n        # transformers are designed to modify X which is 2d dimensional, we\n        # need to modify y accordingly.\n        if y.ndim == 1:\n            y_2d = y.reshape(-1, 1)\n        else:\n            y_2d = y\n        self._fit_transformer(y_2d)\n\n        # transform y and convert back to 1d array if needed\n        y_trans = self.transformer_.transform(y_2d)\n        # FIXME: a FunctionTransformer can return a 1D array even when validate\n        # is set to True. Therefore, we need to check the number of dimension\n        # first.\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\n            y_trans = y_trans.squeeze(axis=1)\n\n        if self.regressor is None:\n            from ..linear_model import LinearRegression\n            self.regressor_ = LinearRegression()\n        else:\n            self.regressor_ = clone(self.regressor)\n\n        self.regressor_.fit(X, y_trans, **fit_params)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict using the base regressor, applying inverse.\n\n        The regressor is used to predict and the ``inverse_func`` or\n        ``inverse_transform`` is applied before returning the prediction.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y_hat : ndarray of shape (n_samples,)\n            Predicted values.\n\n        \"\"\"\n        check_is_fitted(self)\n        pred = self.regressor_.predict(X)\n        if pred.ndim == 1:\n            pred_trans = self.transformer_.inverse_transform(\n                pred.reshape(-1, 1))\n        else:\n            pred_trans = self.transformer_.inverse_transform(pred)\n        if (self._training_dim == 1 and\n                pred_trans.ndim == 2 and pred_trans.shape[1] == 1):\n            pred_trans = pred_trans.squeeze(axis=1)\n\n        return pred_trans\n\n    def _more_tags(self):\n        return {'poor_score': True, 'no_validation': True}\n\n    @property\n    def n_features_in_(self):\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() returns False the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n\n        return self.regressor_.n_features_in_",
      "instance_attributes": [
        {
          "name": "regressor",
          "types": null
        },
        {
          "name": "transformer",
          "types": null
        },
        {
          "name": "func",
          "types": null
        },
        {
          "name": "inverse_func",
          "types": null
        },
        {
          "name": "check_inverse",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "transformer_",
          "types": {
            "kind": "NamedType",
            "name": "Kernel"
          }
        },
        {
          "name": "_training_dim",
          "types": null
        },
        {
          "name": "regressor_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "LinearRegression"
              },
              {
                "kind": "NamedType",
                "name": "Kernel"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope",
      "name": "EllipticEnvelope",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope",
      "decorators": [],
      "superclasses": [
        "OutlierMixin",
        "MinCovDet"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict",
        "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.",
      "docstring": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, the support of robust location and covariance estimates\n    is computed, and a covariance estimate is recomputed from it,\n    without centering the data.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, the robust location and covariance are directly computed\n    with the FastMCD algorithm without additional treatment.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. If None, the minimum value of support_fraction will\n    be used within the algorithm: `[n_sample + n_features + 1] / 2`.\n    Range is (0, 1).\n\ncontamination : float, default=0.1\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. Range is (0, 0.5).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling\n    the data. Pass an int for reproducible results across multiple function\n    calls. See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated robust location.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated robust covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nsupport_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute the\n    robust estimates of location and shape.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores.\n    We have the relation: ``decision_function = score_samples - offset_``.\n    The offset depends on the contamination parameter and is defined in\n    such a way we obtain the expected number of outliers (samples with\n    decision function < 0) in training.\n\n    .. versionadded:: 0.20\n\nraw_location_ : ndarray of shape (n_features,)\n    The raw robust estimated location before correction and re-weighting.\n\nraw_covariance_ : ndarray of shape (n_features, n_features)\n    The raw robust estimated covariance before correction and re-weighting.\n\nraw_support_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the raw robust estimates of location and shape, before correction\n    and re-weighting.\n\ndist_ : ndarray of shape (n_samples,)\n    Mahalanobis distances of the training set (on which :meth:`fit` is\n    called) observations.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EllipticEnvelope\n>>> true_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n...                                                  cov=true_cov,\n...                                                  size=500)\n>>> cov = EllipticEnvelope(random_state=0).fit(X)\n>>> # predict returns 1 for an inlier and -1 for an outlier\n>>> cov.predict([[0, 0],\n...              [3, 3]])\narray([ 1, -1])\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n       [0.2535..., 0.3053...]])\n>>> cov.location_\narray([0.0813... , 0.0427...])\n\nSee Also\n--------\nEmpiricalCovariance, MinCovDet\n\nNotes\n-----\nOutlier detection from covariance estimation may break or not\nperform well in high-dimensional settings. In particular, one will\nalways take care to work with ``n_samples > n_features ** 2``.\n\nReferences\n----------\n.. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n   minimum covariance determinant estimator\" Technometrics 41(3), 212\n   (1999)",
      "code": "class EllipticEnvelope(OutlierMixin, MinCovDet):\n    \"\"\"An object for detecting outliers in a Gaussian distributed dataset.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of robust location and covariance estimates\n        is computed, and a covariance estimate is recomputed from it,\n        without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. If None, the minimum value of support_fraction will\n        be used within the algorithm: `[n_sample + n_features + 1] / 2`.\n        Range is (0, 1).\n\n    contamination : float, default=0.1\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Range is (0, 0.5).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling\n        the data. Pass an int for reproducible results across multiple function\n        calls. See :term: `Glossary <random_state>`.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute the\n        robust estimates of location and shape.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        The offset depends on the contamination parameter and is defined in\n        such a way we obtain the expected number of outliers (samples with\n        decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n\n    See Also\n    --------\n    EmpiricalCovariance, MinCovDet\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, contamination=0.1,\n                 random_state=None):\n        super().__init__(\n            store_precision=store_precision,\n            assume_centered=assume_centered,\n            support_fraction=support_fraction,\n            random_state=random_state)\n        self.contamination = contamination\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n        \"\"\"\n        super().fit(X)\n        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)\n        return self\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.\n        \"\"\"\n        check_is_fitted(self)\n        negative_mahal_dist = self.score_samples(X)\n        return negative_mahal_dist - self.offset_\n\n    def score_samples(self, X):\n        \"\"\"Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.\n        \"\"\"\n        check_is_fitted(self)\n        return -self.mahalanobis(X)\n\n    def predict(self, X):\n        \"\"\"\n        Predict the labels (1 inlier, -1 outlier) of X according to the\n        fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            Returns -1 for anomalies/outliers and +1 for inliers.\n        \"\"\"\n        X = check_array(X)\n        is_inlier = np.full(X.shape[0], -1, dtype=int)\n        values = self.decision_function(X)\n        is_inlier[values >= 0] = 1\n\n        return is_inlier\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)",
      "instance_attributes": [
        {
          "name": "contamination",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "offset_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance",
      "name": "EmpiricalCovariance",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance",
      "decorators": [],
      "superclasses": [
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/_set_covariance",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/get_precision",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm",
        "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.",
      "docstring": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specifies if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo-inverse matrix.\n    (stored only if store_precision is True)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import EmpiricalCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> cov = EmpiricalCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7569..., 0.2818...],\n       [0.2818..., 0.3928...]])\n>>> cov.location_\narray([0.0622..., 0.0193...])",
      "code": "class EmpiricalCovariance(BaseEstimator):\n    \"\"\"Maximum likelihood covariance estimator\n\n    Read more in the :ref:`User Guide <covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specifies if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo-inverse matrix.\n        (stored only if store_precision is True)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EmpiricalCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> cov = EmpiricalCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7569..., 0.2818...],\n           [0.2818..., 0.3928...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n\n    def _set_covariance(self, covariance):\n        \"\"\"Saves the covariance and precision estimates\n\n        Storage is done accordingly to `self.store_precision`.\n        Precision stored only if invertible.\n\n        Parameters\n        ----------\n        covariance : array-like of shape (n_features, n_features)\n            Estimated covariance matrix to be stored, and from which precision\n            is computed.\n        \"\"\"\n        covariance = check_array(covariance)\n        # set covariance\n        self.covariance_ = covariance\n        # set precision\n        if self.store_precision:\n            self.precision_ = linalg.pinvh(covariance, check_finite=False)\n        else:\n            self.precision_ = None\n\n    def get_precision(self):\n        \"\"\"Getter for the precision matrix.\n\n        Returns\n        -------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.\n        \"\"\"\n        if self.store_precision:\n            precision = self.precision_\n        else:\n            precision = linalg.pinvh(self.covariance_, check_finite=False)\n        return precision\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the Maximum Likelihood Estimator covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n          Training data, where n_samples is the number of samples and\n          n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self._set_covariance(covariance)\n\n        return self\n\n    def score(self, X_test, y=None):\n        \"\"\"Computes the log-likelihood of a Gaussian data set with\n        `self.covariance_` as an estimator of its covariance matrix.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where n_samples is\n            the number of samples and n_features is the number of features.\n            X_test is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        res : float\n            The likelihood of the data set with `self.covariance_` as an\n            estimator of its covariance matrix.\n        \"\"\"\n        # compute empirical covariance of the test set\n        test_cov = empirical_covariance(\n            X_test - self.location_, assume_centered=True)\n        # compute log likelihood\n        res = log_likelihood(test_cov, self.get_precision())\n\n        return res\n\n    def error_norm(self, comp_cov, norm='frobenius', scaling=True,\n                   squared=True):\n        \"\"\"Computes the Mean Squared Error between two covariance estimators.\n        (In the sense of the Frobenius norm).\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.\n\n        Returns\n        -------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.\n        \"\"\"\n        # compute the error\n        error = comp_cov - self.covariance_\n        # compute the error norm\n        if norm == \"frobenius\":\n            squared_norm = np.sum(error ** 2)\n        elif norm == \"spectral\":\n            squared_norm = np.amax(linalg.svdvals(np.dot(error.T, error)))\n        else:\n            raise NotImplementedError(\n                \"Only spectral and frobenius norms are implemented\")\n        # optionally scale the error norm\n        if scaling:\n            squared_norm = squared_norm / error.shape[0]\n        # finally get either the squared norm or the norm\n        if squared:\n            result = squared_norm\n        else:\n            result = np.sqrt(squared_norm)\n\n        return result\n\n    def mahalanobis(self, X):\n        \"\"\"Computes the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.\n\n        Returns\n        -------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.\n        \"\"\"\n        precision = self.get_precision()\n        # compute mahalanobis distances\n        dist = pairwise_distances(X, self.location_[np.newaxis, :],\n                                  metric='mahalanobis', VI=precision)\n\n        return np.reshape(dist, (len(X),)) ** 2",
      "instance_attributes": [
        {
          "name": "store_precision",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "assume_centered",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "covariance_",
          "types": null
        },
        {
          "name": "precision_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso",
      "name": "GraphicalLasso",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLasso",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso",
      "docstring": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso\n\nParameters\n----------\nalpha : float, default=0.01\n    The regularization parameter: the higher alpha, the more\n    regularization, the sparser the inverse covariance.\n    Range is (0, inf].\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    The maximum number of iterations.\n\nverbose : bool, default=False\n    If verbose is True, the objective function and dual gap are\n    plotted at each iteration.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLasso\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLasso().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.049, 0.218, 0.019],\n       [0.049, 0.364, 0.017, 0.034],\n       [0.218, 0.017, 0.322, 0.093],\n       [0.019, 0.034, 0.093, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)\narray([0.073, 0.04 , 0.038, 0.143])\n\nSee Also\n--------\ngraphical_lasso, GraphicalLassoCV",
      "code": "class GraphicalLasso(EmpiricalCovariance):\n    \"\"\"Sparse inverse covariance estimation with an l1-penalized estimator.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLasso has been renamed to GraphicalLasso\n\n    Parameters\n    ----------\n    alpha : float, default=0.01\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n        Range is (0, inf].\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        The maximum number of iterations.\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and dual gap are\n        plotted at each iteration.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLasso\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLasso().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.049, 0.218, 0.019],\n           [0.049, 0.364, 0.017, 0.034],\n           [0.218, 0.017, 0.322, 0.093],\n           [0.019, 0.034, 0.093, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])\n\n    See Also\n    --------\n    graphical_lasso, GraphicalLassoCV\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, alpha=.01, *, mode='cd', tol=1e-4, enet_tol=1e-4,\n                 max_iter=100, verbose=False, assume_centered=False):\n        super().__init__(assume_centered=assume_centered)\n        self.alpha = alpha\n        self.mode = mode\n        self.tol = tol\n        self.enet_tol = enet_tol\n        self.max_iter = max_iter\n        self.verbose = verbose\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, ensure_min_samples=2,\n                                estimator=self)\n\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=self.verbose, return_n_iter=True)\n        return self",
      "instance_attributes": [
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "mode",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "enet_tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "covariance_",
          "types": null
        },
        {
          "name": "precision_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "tuple"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV",
      "name": "GraphicalLassoCV",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV",
      "decorators": [],
      "superclasses": [
        "GraphicalLasso"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/grid_scores_@getter",
        "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/cv_alphas_@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV",
      "docstring": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV\n\nParameters\n----------\nalphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n    If an integer is given, it fixes the number of points on the\n    grids of alpha to be used. If a list is given, it gives the\n    grid to be used. See the notes in the class docstring for\n    more details. Range is (0, inf] when floats given.\n\nn_refinements : int, default=4\n    The number of times the grid is refined. Not used if explicit\n    values of alphas are passed. Range is [1, inf).\n\ncv : int, cross-validation generator or iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.20\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    Maximum number of iterations.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where number of features is greater\n    than number of samples. Elsewhere prefer cd which is more numerically\n    stable.\n\nn_jobs : int, default=None\n    number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nverbose : bool, default=False\n    If verbose is True, the objective function and duality gap are\n    printed at each iteration.\n\nassume_centered : bool, default=False\n    If True, data are not centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data are centered before computation.\n\nAttributes\n----------\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated precision matrix (inverse covariance).\n\nalpha_ : float\n    Penalization parameter selected.\n\ncv_alphas_ : list of shape (n_alphas,), dtype=float\n    All penalization parameters explored.\n\n    .. deprecated:: 0.24\n        The `cv_alphas_` attribute is deprecated in version 0.24 in favor\n        of `cv_results_['alphas']` and will be removed in version\n        1.1 (renaming of 0.26).\n\ngrid_scores_ : ndarray of shape (n_alphas, n_folds)\n    Log-likelihood score on left-out data across folds.\n\n    .. deprecated:: 0.24\n        The `grid_scores_` attribute is deprecated in version 0.24 in favor\n        of `cv_results_` and will be removed in version\n        1.1 (renaming of 0.26).\n\ncv_results_ : dict of ndarrays\n    A dict with keys:\n\n    alphas : ndarray of shape (n_alphas,)\n        All penalization parameters explored.\n\n    split(k)_score : ndarray of shape (n_alphas,)\n        Log-likelihood score on left-out data across (k)th fold.\n\n    mean_score : ndarray of shape (n_alphas,)\n        Mean of scores over the folds.\n\n    std_score : ndarray of shape (n_alphas,)\n        Standard deviation of scores over the folds.\n\n    .. versionadded:: 0.24\n\nn_iter_ : int\n    Number of iterations run for the optimal alpha.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import GraphicalLassoCV\n>>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n...                      [0.0, 0.4, 0.0, 0.0],\n...                      [0.2, 0.0, 0.3, 0.1],\n...                      [0.0, 0.0, 0.1, 0.7]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n...                                   cov=true_cov,\n...                                   size=200)\n>>> cov = GraphicalLassoCV().fit(X)\n>>> np.around(cov.covariance_, decimals=3)\narray([[0.816, 0.051, 0.22 , 0.017],\n       [0.051, 0.364, 0.018, 0.036],\n       [0.22 , 0.018, 0.322, 0.094],\n       [0.017, 0.036, 0.094, 0.69 ]])\n>>> np.around(cov.location_, decimals=3)\narray([0.073, 0.04 , 0.038, 0.143])\n\nSee Also\n--------\ngraphical_lasso, GraphicalLasso\n\nNotes\n-----\nThe search for the optimal penalization parameter (alpha) is done on an\niteratively refined grid: first the cross-validated scores on a grid are\ncomputed, then a new refined grid is centered around the maximum, and so\non.\n\nOne of the challenges which is faced here is that the solvers can\nfail to converge to a well-conditioned estimate. The corresponding\nvalues of alpha then come out as missing values, but the optimum may\nbe close to these missing values.",
      "code": "class GraphicalLassoCV(GraphicalLasso):\n    \"\"\"Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        GraphLassoCV has been renamed to GraphicalLassoCV\n\n    Parameters\n    ----------\n    alphas : int or array-like of shape (n_alphas,), dtype=float, default=4\n        If an integer is given, it fixes the number of points on the\n        grids of alpha to be used. If a list is given, it gives the\n        grid to be used. See the notes in the class docstring for\n        more details. Range is (0, inf] when floats given.\n\n    n_refinements : int, default=4\n        The number of times the grid is refined. Not used if explicit\n        values of alphas are passed. Range is [1, inf).\n\n    cv : int, cross-validation generator or iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        Maximum number of iterations.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where number of features is greater\n        than number of samples. Elsewhere prefer cd which is more numerically\n        stable.\n\n    n_jobs : int, default=None\n        number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and duality gap are\n        printed at each iteration.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated precision matrix (inverse covariance).\n\n    alpha_ : float\n        Penalization parameter selected.\n\n    cv_alphas_ : list of shape (n_alphas,), dtype=float\n        All penalization parameters explored.\n\n        .. deprecated:: 0.24\n            The `cv_alphas_` attribute is deprecated in version 0.24 in favor\n            of `cv_results_['alphas']` and will be removed in version\n            1.1 (renaming of 0.26).\n\n    grid_scores_ : ndarray of shape (n_alphas, n_folds)\n        Log-likelihood score on left-out data across folds.\n\n        .. deprecated:: 0.24\n            The `grid_scores_` attribute is deprecated in version 0.24 in favor\n            of `cv_results_` and will be removed in version\n            1.1 (renaming of 0.26).\n\n    cv_results_ : dict of ndarrays\n        A dict with keys:\n\n        alphas : ndarray of shape (n_alphas,)\n            All penalization parameters explored.\n\n        split(k)_score : ndarray of shape (n_alphas,)\n            Log-likelihood score on left-out data across (k)th fold.\n\n        mean_score : ndarray of shape (n_alphas,)\n            Mean of scores over the folds.\n\n        std_score : ndarray of shape (n_alphas,)\n            Standard deviation of scores over the folds.\n\n        .. versionadded:: 0.24\n\n    n_iter_ : int\n        Number of iterations run for the optimal alpha.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import GraphicalLassoCV\n    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],\n    ...                      [0.0, 0.4, 0.0, 0.0],\n    ...                      [0.2, 0.0, 0.3, 0.1],\n    ...                      [0.0, 0.0, 0.1, 0.7]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0, 0, 0],\n    ...                                   cov=true_cov,\n    ...                                   size=200)\n    >>> cov = GraphicalLassoCV().fit(X)\n    >>> np.around(cov.covariance_, decimals=3)\n    array([[0.816, 0.051, 0.22 , 0.017],\n           [0.051, 0.364, 0.018, 0.036],\n           [0.22 , 0.018, 0.322, 0.094],\n           [0.017, 0.036, 0.094, 0.69 ]])\n    >>> np.around(cov.location_, decimals=3)\n    array([0.073, 0.04 , 0.038, 0.143])\n\n    See Also\n    --------\n    graphical_lasso, GraphicalLasso\n\n    Notes\n    -----\n    The search for the optimal penalization parameter (alpha) is done on an\n    iteratively refined grid: first the cross-validated scores on a grid are\n    computed, then a new refined grid is centered around the maximum, and so\n    on.\n\n    One of the challenges which is faced here is that the solvers can\n    fail to converge to a well-conditioned estimate. The corresponding\n    values of alpha then come out as missing values, but the optimum may\n    be close to these missing values.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, alphas=4, n_refinements=4, cv=None, tol=1e-4,\n                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=None,\n                 verbose=False, assume_centered=False):\n        super().__init__(\n            mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,\n            max_iter=max_iter, assume_centered=assume_centered)\n        self.alphas = alphas\n        self.n_refinements = n_refinements\n        self.cv = cv\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, estimator=self)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n\n        cv = check_cv(self.cv, y, classifier=False)\n\n        # List of (alpha, scores, covs)\n        path = list()\n        n_alphas = self.alphas\n        inner_verbose = max(0, self.verbose - 1)\n\n        if isinstance(n_alphas, Sequence):\n            alphas = self.alphas\n            n_refinements = 1\n        else:\n            n_refinements = self.n_refinements\n            alpha_1 = alpha_max(emp_cov)\n            alpha_0 = 1e-2 * alpha_1\n            alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1),\n                                 n_alphas)[::-1]\n\n        t0 = time.time()\n        for i in range(n_refinements):\n            with warnings.catch_warnings():\n                # No need to see the convergence warnings on this grid:\n                # they will always be points that will not converge\n                # during the cross-validation\n                warnings.simplefilter('ignore', ConvergenceWarning)\n                # Compute the cross-validated loss on the current grid\n\n                # NOTE: Warm-restarting graphical_lasso_path has been tried,\n                # and this did not allow to gain anything\n                # (same execution time with or without).\n                this_path = Parallel(\n                    n_jobs=self.n_jobs,\n                    verbose=self.verbose\n                )(delayed(graphical_lasso_path)(X[train], alphas=alphas,\n                                                X_test=X[test], mode=self.mode,\n                                                tol=self.tol,\n                                                enet_tol=self.enet_tol,\n                                                max_iter=int(.1 *\n                                                             self.max_iter),\n                                                verbose=inner_verbose)\n                  for train, test in cv.split(X, y))\n\n            # Little danse to transform the list in what we need\n            covs, _, scores = zip(*this_path)\n            covs = zip(*covs)\n            scores = zip(*scores)\n            path.extend(zip(alphas, scores, covs))\n            path = sorted(path, key=operator.itemgetter(0), reverse=True)\n\n            # Find the maximum (avoid using built in 'max' function to\n            # have a fully-reproducible selection of the smallest alpha\n            # in case of equality)\n            best_score = -np.inf\n            last_finite_idx = 0\n            for index, (alpha, scores, _) in enumerate(path):\n                this_score = np.mean(scores)\n                if this_score >= .1 / np.finfo(np.float64).eps:\n                    this_score = np.nan\n                if np.isfinite(this_score):\n                    last_finite_idx = index\n                if this_score >= best_score:\n                    best_score = this_score\n                    best_index = index\n\n            # Refine the grid\n            if best_index == 0:\n                # We do not need to go back: we have chosen\n                # the highest value of alpha for which there are\n                # non-zero coefficients\n                alpha_1 = path[0][0]\n                alpha_0 = path[1][0]\n            elif (best_index == last_finite_idx\n                    and not best_index == len(path) - 1):\n                # We have non-converged models on the upper bound of the\n                # grid, we need to refine the grid there\n                alpha_1 = path[best_index][0]\n                alpha_0 = path[best_index + 1][0]\n            elif best_index == len(path) - 1:\n                alpha_1 = path[best_index][0]\n                alpha_0 = 0.01 * path[best_index][0]\n            else:\n                alpha_1 = path[best_index - 1][0]\n                alpha_0 = path[best_index + 1][0]\n\n            if not isinstance(n_alphas, Sequence):\n                alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0),\n                                     n_alphas + 2)\n                alphas = alphas[1:-1]\n\n            if self.verbose and n_refinements > 1:\n                print('[GraphicalLassoCV] Done refinement % 2i out of'\n                      ' %i: % 3is' % (i + 1, n_refinements, time.time() - t0))\n\n        path = list(zip(*path))\n        grid_scores = list(path[1])\n        alphas = list(path[0])\n        # Finally, compute the score with alpha = 0\n        alphas.append(0)\n        grid_scores.append(cross_val_score(EmpiricalCovariance(), X,\n                                           cv=cv, n_jobs=self.n_jobs,\n                                           verbose=inner_verbose))\n        grid_scores = np.array(grid_scores)\n        self.cv_results_ = {'alphas': np.array(alphas)}\n        for i in range(grid_scores.shape[1]):\n            key = \"split{}_score\".format(i)\n            self.cv_results_[key] = grid_scores[:, i]\n\n        self.cv_results_[\"mean_score\"] = np.mean(grid_scores, axis=1)\n        self.cv_results_[\"std_score\"] = np.std(grid_scores, axis=1)\n\n        best_alpha = alphas[best_index]\n        self.alpha_ = best_alpha\n\n        # Finally fit the model with the selected alpha\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=inner_verbose, return_n_iter=True)\n        return self\n\n    # TODO: Remove in 1.1 when grid_scores_ is deprecated\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"The grid_scores_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_ and will be removed in version 1.1 (renaming of 0.26).\"\n    )\n    @property\n    def grid_scores_(self):\n        # remove 3 for mean_score, std_score, and alphas\n        n_alphas = len(self.cv_results_) - 3\n        return np.asarray(\n            [self.cv_results_[\"split{}_score\".format(i)]\n             for i in range(n_alphas)]).T\n\n    # TODO: Remove in 1.1 when cv_alphas_ is deprecated\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"The cv_alphas_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_['alpha'] and will be removed in version 1.1 \"\n        \"(renaming of 0.26).\"\n    )\n    @property\n    def cv_alphas_(self):\n        return self.cv_results_['alphas'].tolist()",
      "instance_attributes": [
        {
          "name": "alphas",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_refinements",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "cv",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "cv_results_",
          "types": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "name": "alpha_",
          "types": null
        },
        {
          "name": "covariance_",
          "types": null
        },
        {
          "name": "precision_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "tuple"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet",
      "name": "MinCovDet",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__",
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit",
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance",
        "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.",
      "docstring": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, the support of the robust location and the covariance\n    estimates is computed, and a covariance estimate is recomputed from\n    it, without centering the data.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, the robust location and covariance are directly computed\n    with the FastMCD algorithm without additional treatment.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is None, which implies that the minimum\n    value of support_fraction will be used within the algorithm:\n    `(n_sample + n_features + 1) / 2`. The parameter must be in the range\n    (0, 1).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nAttributes\n----------\nraw_location_ : ndarray of shape (n_features,)\n    The raw robust estimated location before correction and re-weighting.\n\nraw_covariance_ : ndarray of shape (n_features, n_features)\n    The raw robust estimated covariance before correction and re-weighting.\n\nraw_support_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the raw robust estimates of location and shape, before correction\n    and re-weighting.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated robust location.\n\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated robust covariance matrix.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nsupport_ : ndarray of shape (n_samples,)\n    A mask of the observations that have been used to compute\n    the robust estimates of location and shape.\n\ndist_ : ndarray of shape (n_samples,)\n    Mahalanobis distances of the training set (on which :meth:`fit` is\n    called) observations.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import MinCovDet\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = MinCovDet(random_state=0).fit(X)\n>>> cov.covariance_\narray([[0.7411..., 0.2535...],\n       [0.2535..., 0.3053...]])\n>>> cov.location_\narray([0.0813... , 0.0427...])\n\nReferences\n----------\n\n.. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n    J. Am Stat Ass, 79:871, 1984.\n.. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n    Estimator, 1999, American Statistical Association and the American\n    Society for Quality, TECHNOMETRICS\n.. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
      "code": "class MinCovDet(EmpiricalCovariance):\n    \"\"\"Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\n    The Minimum Covariance Determinant covariance estimator is to be applied\n    on Gaussian-distributed data, but could still be relevant on data\n    drawn from a unimodal, symmetric distribution. It is not meant to be used\n    with multi-modal data (the algorithm used to fit a MinCovDet object is\n    likely to fail in such a case).\n    One should consider projection pursuit methods to deal with multi-modal\n    datasets.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, the support of the robust location and the covariance\n        estimates is computed, and a covariance estimate is recomputed from\n        it, without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. Default is None, which implies that the minimum\n        value of support_fraction will be used within the algorithm:\n        `(n_sample + n_features + 1) / 2`. The parameter must be in the range\n        (0, 1).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term: `Glossary <random_state>`.\n\n    Attributes\n    ----------\n    raw_location_ : ndarray of shape (n_features,)\n        The raw robust estimated location before correction and re-weighting.\n\n    raw_covariance_ : ndarray of shape (n_features, n_features)\n        The raw robust estimated covariance before correction and re-weighting.\n\n    raw_support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the raw robust estimates of location and shape, before correction\n        and re-weighting.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated robust location.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated robust covariance matrix.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : ndarray of shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the robust estimates of location and shape.\n\n    dist_ : ndarray of shape (n_samples,)\n        Mahalanobis distances of the training set (on which :meth:`fit` is\n        called) observations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import MinCovDet\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = MinCovDet(random_state=0).fit(X)\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n\n    References\n    ----------\n\n    .. [Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression.\n        J. Am Stat Ass, 79:871, 1984.\n    .. [Rousseeuw] A Fast Algorithm for the Minimum Covariance Determinant\n        Estimator, 1999, American Statistical Association and the American\n        Society for Quality, TECHNOMETRICS\n    .. [ButlerDavies] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400\n    \"\"\"\n    _nonrobust_covariance = staticmethod(empirical_covariance)\n\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, random_state=None):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n        self.support_fraction = support_fraction\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator='MinCovDet')\n        random_state = check_random_state(self.random_state)\n        n_samples, n_features = X.shape\n        # check that the empirical covariance is full rank\n        if (linalg.svdvals(np.dot(X.T, X)) > 1e-8).sum() != n_features:\n            warnings.warn(\"The covariance matrix associated to your dataset \"\n                          \"is not full rank\")\n        # compute and store raw estimates\n        raw_location, raw_covariance, raw_support, raw_dist = fast_mcd(\n            X, support_fraction=self.support_fraction,\n            cov_computation_method=self._nonrobust_covariance,\n            random_state=random_state)\n        if self.assume_centered:\n            raw_location = np.zeros(n_features)\n            raw_covariance = self._nonrobust_covariance(X[raw_support],\n                                                        assume_centered=True)\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(raw_covariance)\n            raw_dist = np.sum(np.dot(X, precision) * X, 1)\n        self.raw_location_ = raw_location\n        self.raw_covariance_ = raw_covariance\n        self.raw_support_ = raw_support\n        self.location_ = raw_location\n        self.support_ = raw_support\n        self.dist_ = raw_dist\n        # obtain consistency at normal models\n        self.correct_covariance(X)\n        # re-weight estimator\n        self.reweight_covariance(X)\n\n        return self\n\n    def correct_covariance(self, data):\n        \"\"\"Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n\n        # Check that the covariance of the support data is not equal to 0.\n        # Otherwise self.dist_ = 0 and thus correction = 0.\n        n_samples = len(self.dist_)\n        n_support = np.sum(self.support_)\n        if n_support < n_samples and np.allclose(self.raw_covariance_, 0):\n            raise ValueError('The covariance matrix of the support data '\n                             'is equal to 0, try to increase support_fraction')\n        correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5)\n        covariance_corrected = self.raw_covariance_ * correction\n        self.dist_ /= correction\n        return covariance_corrected\n\n    def reweight_covariance(self, data):\n        \"\"\"Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n        n_samples, n_features = data.shape\n        mask = self.dist_ < chi2(n_features).isf(0.025)\n        if self.assume_centered:\n            location_reweighted = np.zeros(n_features)\n        else:\n            location_reweighted = data[mask].mean(0)\n        covariance_reweighted = self._nonrobust_covariance(\n            data[mask], assume_centered=self.assume_centered)\n        support_reweighted = np.zeros(n_samples, dtype=bool)\n        support_reweighted[mask] = True\n        self._set_covariance(covariance_reweighted)\n        self.location_ = location_reweighted\n        self.support_ = support_reweighted\n        X_centered = data - self.location_\n        self.dist_ = np.sum(\n            np.dot(X_centered, self.get_precision()) * X_centered, 1)\n        return location_reweighted, covariance_reweighted, support_reweighted",
      "instance_attributes": [
        {
          "name": "store_precision",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "assume_centered",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "support_fraction",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "raw_location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "raw_covariance_",
          "types": null
        },
        {
          "name": "raw_support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "dist_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf",
      "name": "LedoitWolf",
      "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__",
        "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split\n    during its Ledoit-Wolf estimation. This is purely a memory\n    optimization and does not affect results.\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nshrinkage_ : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import LedoitWolf\n>>> real_cov = np.array([[.4, .2],\n...                      [.2, .8]])\n>>> np.random.seed(0)\n>>> X = np.random.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=50)\n>>> cov = LedoitWolf().fit(X)\n>>> cov.covariance_\narray([[0.4406..., 0.1616...],\n       [0.1616..., 0.8022...]])\n>>> cov.location_\narray([ 0.0595... , -0.0075...])\n\nNotes\n-----\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\nand shrinkage is given by the Ledoit and Wolf formula (see References)\n\nReferences\n----------\n\"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\nLedoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\nFebruary 2004, pages 365-411.",
      "code": "class LedoitWolf(EmpiricalCovariance):\n    \"\"\"LedoitWolf Estimator\n\n    Ledoit-Wolf is a particular form of shrinkage, where the shrinkage\n    coefficient is computed using O. Ledoit and M. Wolf's formula as\n    described in \"A Well-Conditioned Estimator for Large-Dimensional\n    Covariance Matrices\", Ledoit and Wolf, Journal of Multivariate\n    Analysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split\n        during its Ledoit-Wolf estimation. This is purely a memory\n        optimization and does not affect results.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import LedoitWolf\n    >>> real_cov = np.array([[.4, .2],\n    ...                      [.2, .8]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=50)\n    >>> cov = LedoitWolf().fit(X)\n    >>> cov.covariance_\n    array([[0.4406..., 0.1616...],\n           [0.1616..., 0.8022...]])\n    >>> cov.location_\n    array([ 0.0595... , -0.0075...])\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the Ledoit and Wolf formula (see References)\n\n    References\n    ----------\n    \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\n    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\n    February 2004, pages 365-411.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 block_size=1000):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.block_size = block_size\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the Ledoit-Wolf shrunk covariance model according to the given\n        training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance, shrinkage = ledoit_wolf(X - self.location_,\n                                            assume_centered=True,\n                                            block_size=self.block_size)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self",
      "instance_attributes": [
        {
          "name": "block_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "shrinkage_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS",
      "name": "OAS",
      "qname": "sklearn.covariance._shrunk_covariance.OAS",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Oracle Approximating Shrinkage Estimator\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nOAS is a particular form of shrinkage described in\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the\narticle. In the original article, formula (23) states that 2/p is\nmultiplied by Trace(cov*cov) in both the numerator and denominator, but\nthis operation is omitted because for a large p, the value of 2/p is\nso small that it doesn't affect the value of the estimator.",
      "docstring": "Oracle Approximating Shrinkage Estimator\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nOAS is a particular form of shrinkage described in\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the\narticle. In the original article, formula (23) states that 2/p is\nmultiplied by Trace(cov*cov) in both the numerator and denominator, but\nthis operation is omitted because for a large p, the value of 2/p is\nso small that it doesn't affect the value of the estimator.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False (default), data will be centered before computation.\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix.\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nshrinkage_ : float\n  coefficient in the convex combination used for the computation\n  of the shrunk estimate. Range is [0, 1].\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import OAS\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                             cov=real_cov,\n...                             size=500)\n>>> oas = OAS().fit(X)\n>>> oas.covariance_\narray([[0.7533..., 0.2763...],\n       [0.2763..., 0.3964...]])\n>>> oas.precision_\narray([[ 1.7833..., -1.2431... ],\n       [-1.2431...,  3.3889...]])\n>>> oas.shrinkage_\n0.0195...\n\nNotes\n-----\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\nand shrinkage is given by the OAS formula (see References)\n\nReferences\n----------\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.",
      "code": "class OAS(EmpiricalCovariance):\n    \"\"\"Oracle Approximating Shrinkage Estimator\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    OAS is a particular form of shrinkage described in\n    \"Shrinkage Algorithms for MMSE Covariance Estimation\"\n    Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\n    The formula used here does not correspond to the one given in the\n    article. In the original article, formula (23) states that 2/p is\n    multiplied by Trace(cov*cov) in both the numerator and denominator, but\n    this operation is omitted because for a large p, the value of 2/p is\n    so small that it doesn't affect the value of the estimator.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float\n      coefficient in the convex combination used for the computation\n      of the shrunk estimate. Range is [0, 1].\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import OAS\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> oas = OAS().fit(X)\n    >>> oas.covariance_\n    array([[0.7533..., 0.2763...],\n           [0.2763..., 0.3964...]])\n    >>> oas.precision_\n    array([[ 1.7833..., -1.2431... ],\n           [-1.2431...,  3.3889...]])\n    >>> oas.shrinkage_\n    0.0195...\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the OAS formula (see References)\n\n    References\n    ----------\n    \"Shrinkage Algorithms for MMSE Covariance Estimation\"\n    Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n    \"\"\"\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the Oracle Approximating Shrinkage covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n\n        covariance, shrinkage = oas(X - self.location_, assume_centered=True)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self",
      "instance_attributes": [
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "shrinkage_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance",
      "name": "ShrunkCovariance",
      "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance",
      "decorators": [],
      "superclasses": [
        "EmpiricalCovariance"
      ],
      "methods": [
        "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__",
        "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nstore_precision : bool, default=True\n    Specify if the estimated precision is stored\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data will be centered before computation.\n\nshrinkage : float, default=0.1\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nAttributes\n----------\ncovariance_ : ndarray of shape (n_features, n_features)\n    Estimated covariance matrix\n\nlocation_ : ndarray of shape (n_features,)\n    Estimated location, i.e. the estimated mean.\n\nprecision_ : ndarray of shape (n_features, n_features)\n    Estimated pseudo inverse matrix.\n    (stored only if store_precision is True)\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.covariance import ShrunkCovariance\n>>> from sklearn.datasets import make_gaussian_quantiles\n>>> real_cov = np.array([[.8, .3],\n...                      [.3, .4]])\n>>> rng = np.random.RandomState(0)\n>>> X = rng.multivariate_normal(mean=[0, 0],\n...                                   cov=real_cov,\n...                                   size=500)\n>>> cov = ShrunkCovariance().fit(X)\n>>> cov.covariance_\narray([[0.7387..., 0.2536...],\n       [0.2536..., 0.4110...]])\n>>> cov.location_\narray([0.0622..., 0.0193...])\n\nNotes\n-----\nThe regularized covariance is given by:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "class ShrunkCovariance(EmpiricalCovariance):\n    \"\"\"Covariance estimator with shrinkage\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data will be centered before computation.\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Attributes\n    ----------\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import ShrunkCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=500)\n    >>> cov = ShrunkCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7387..., 0.2536...],\n           [0.2536..., 0.4110...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])\n\n    Notes\n    -----\n    The regularized covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 shrinkage=0.1):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.shrinkage = shrinkage\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the shrunk covariance model according to the given training data\n        and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid a potential\n        # matrix inversion when setting the precision\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        covariance = shrunk_covariance(covariance, self.shrinkage)\n        self._set_covariance(covariance)\n\n        return self",
      "instance_attributes": [
        {
          "name": "shrinkage",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "location_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA",
      "name": "CCA",
      "qname": "sklearn.cross_decomposition._pls.CCA",
      "decorators": [],
      "superclasses": [
        "_PLS"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.",
      "docstring": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nmax_iter : int, default=500\n    the maximum number of iterations of the power method.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import CCA\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> cca = CCA(n_components=1)\n>>> cca.fit(X, Y)\nCCA(n_components=1)\n>>> X_c, Y_c = cca.transform(X, Y)\n\nSee Also\n--------\nPLSCanonical\nPLSSVD",
      "code": "class CCA(_PLS):\n    \"\"\"Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    max_iter : int, default=500\n        the maximum number of iterations of the power method.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n        .. deprecated:: 0.24\n           `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n        .. deprecated:: 0.24\n           `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_features, n_targets)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_`.\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import CCA\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> cca = CCA(n_components=1)\n    >>> cca.fit(X, Y)\n    CCA(n_components=1)\n    >>> X_c, Y_c = cca.transform(X, Y)\n\n    See Also\n    --------\n    PLSCanonical\n    PLSSVD\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(n_components=n_components, scale=scale,\n                         deflation_mode=\"canonical\", mode=\"B\",\n                         algorithm=\"nipals\", max_iter=max_iter, tol=tol,\n                         copy=copy)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical",
      "name": "PLSCanonical",
      "qname": "sklearn.cross_decomposition._pls.PLSCanonical",
      "decorators": [],
      "superclasses": [
        "_PLS"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nalgorithm : {'nipals', 'svd'}, default='nipals'\n    The algorithm used to estimate the first singular vectors of the\n    cross-covariance matrix. 'nipals' uses the power method while 'svd'\n    will compute the whole SVD.\n\nmax_iter : int, default=500\n    the maximum number of iterations of the power method when\n    `algorithm='nipals'`. Ignored otherwise.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component. Empty if `algorithm='svd'`.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import PLSCanonical\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> plsca = PLSCanonical(n_components=2)\n>>> plsca.fit(X, Y)\nPLSCanonical()\n>>> X_c, Y_c = plsca.transform(X, Y)\n\nSee Also\n--------\nCCA\nPLSSVD",
      "code": "class PLSCanonical(_PLS):\n    \"\"\"Partial Least Squares transformer and regressor.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    algorithm : {'nipals', 'svd'}, default='nipals'\n        The algorithm used to estimate the first singular vectors of the\n        cross-covariance matrix. 'nipals' uses the power method while 'svd'\n        will compute the whole SVD.\n\n    max_iter : int, default=500\n        the maximum number of iterations of the power method when\n        `algorithm='nipals'`. Ignored otherwise.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n        .. deprecated:: 0.24\n           `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n        .. deprecated:: 0.24\n           `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_features, n_targets)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_`.\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component. Empty if `algorithm='svd'`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, Y)\n    PLSCanonical()\n    >>> X_c, Y_c = plsca.transform(X, Y)\n\n    See Also\n    --------\n    CCA\n    PLSSVD\n    \"\"\"\n    # This implementation provides the same results that the \"plspm\" package\n    # provided in the R language (R-project), using the function plsca(X, Y).\n    # Results are equal or collinear with the function\n    # ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The\n    # difference relies in the fact that mixOmics implementation does not\n    # exactly implement the Wold algorithm since it does not normalize\n    # y_weights to one.\n\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, algorithm=\"nipals\",\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"canonical\", mode=\"A\",\n            algorithm=algorithm,\n            max_iter=max_iter, tol=tol, copy=copy)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression",
      "name": "PLSRegression",
      "qname": "sklearn.cross_decomposition._pls.PLSRegression",
      "decorators": [],
      "superclasses": [
        "_PLS"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    Number of components to keep. Should be in `[1, min(n_samples,\n    n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\nmax_iter : int, default=500\n    The maximum number of iterations of the power method when\n    `algorithm='nipals'`. Ignored otherwise.\n\ntol : float, default=1e-06\n    The tolerance used as convergence criteria in the power method: the\n    algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n    than `tol`, where `u` corresponds to the left singular vector.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the cross-covariance matrices of each\n    iteration.\n\ny_weights_ : ndarray of shape (n_targets, n_components)\n    The right singular vectors of the cross-covariance matrices of each\n    iteration.\n\nx_loadings_ : ndarray of shape (n_features, n_components)\n    The loadings of `X`.\n\ny_loadings_ : ndarray of shape (n_targets, n_components)\n    The loadings of `Y`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\nx_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `X`.\n\ny_rotations_ : ndarray of shape (n_features, n_components)\n    The projection matrix used to transform `Y`.\n\ncoef_ : ndarray of shape (n_features, n_targets)\n    The coefficients of the linear model such that `Y` is approximated as\n    `Y = X @ coef_`.\n\nn_iter_ : list of shape (n_components,)\n    Number of iterations of the power method, for each\n    component.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> from sklearn.cross_decomposition import PLSRegression\n>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n>>> pls2 = PLSRegression(n_components=2)\n>>> pls2.fit(X, Y)\nPLSRegression()\n>>> Y_pred = pls2.predict(X)",
      "code": "class PLSRegression(_PLS):\n    \"\"\"PLS regression\n\n    PLSRegression is also known as PLS2 or PLS1, depending on the number of\n    targets.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of components to keep. Should be in `[1, min(n_samples,\n        n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    max_iter : int, default=500\n        The maximum number of iterations of the power method when\n        `algorithm='nipals'`. Ignored otherwise.\n\n    tol : float, default=1e-06\n        The tolerance used as convergence criteria in the power method: the\n        algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\n        than `tol`, where `u` corresponds to the left singular vector.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    y_weights_ : ndarray of shape (n_targets, n_components)\n        The right singular vectors of the cross-covariance matrices of each\n        iteration.\n\n    x_loadings_ : ndarray of shape (n_features, n_components)\n        The loadings of `X`.\n\n    y_loadings_ : ndarray of shape (n_targets, n_components)\n        The loadings of `Y`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n    x_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `X`.\n\n    y_rotations_ : ndarray of shape (n_features, n_components)\n        The projection matrix used to transform `Y`.\n\n    coef_ : ndarray of shape (n_features, n_targets)\n        The coefficients of the linear model such that `Y` is approximated as\n        `Y = X @ coef_`.\n\n    n_iter_ : list of shape (n_components,)\n        Number of iterations of the power method, for each\n        component.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSRegression\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> pls2 = PLSRegression(n_components=2)\n    >>> pls2.fit(X, Y)\n    PLSRegression()\n    >>> Y_pred = pls2.predict(X)\n    \"\"\"\n\n    # This implementation provides the same results that 3 PLS packages\n    # provided in the R language (R-project):\n    #     - \"mixOmics\" with function pls(X, Y, mode = \"regression\")\n    #     - \"plspm \" with function plsreg2(X, Y)\n    #     - \"pls\" with function oscorespls.fit(X, Y)\n\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"regression\", mode=\"A\",\n            algorithm='nipals', max_iter=max_iter,\n            tol=tol, copy=copy)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD",
      "name": "PLSSVD",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_scores_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_scores_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_mean_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_mean_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_std_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_std_@getter",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform",
        "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cross_decomposition"
      ],
      "description": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8\n\nParameters\n----------\nn_components : int, default=2\n    The number of components to keep. Should be in `[1,\n    min(n_samples, n_features, n_targets)]`.\n\nscale : bool, default=True\n    Whether to scale `X` and `Y`.\n\ncopy : bool, default=True\n    Whether to copy `X` and `Y` in fit before applying centering, and\n    potentially scaling. If False, these operations will be done inplace,\n    modifying both arrays.\n\nAttributes\n----------\nx_weights_ : ndarray of shape (n_features, n_components)\n    The left singular vectors of the SVD of the cross-covariance matrix.\n    Used to project `X` in `transform`.\n\ny_weights_ : ndarray of (n_targets, n_components)\n    The right singular vectors of the SVD of the cross-covariance matrix.\n    Used to project `X` in `transform`.\n\nx_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training samples.\n\n    .. deprecated:: 0.24\n       `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\ny_scores_ : ndarray of shape (n_samples, n_components)\n    The transformed training targets.\n\n    .. deprecated:: 0.24\n       `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n       (renaming of 0.26). You can just call `transform` on the training\n       data instead.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.cross_decomposition import PLSSVD\n>>> X = np.array([[0., 0., 1.],\n...               [1., 0., 0.],\n...               [2., 2., 2.],\n...               [2., 5., 4.]])\n>>> Y = np.array([[0.1, -0.2],\n...               [0.9, 1.1],\n...               [6.2, 5.9],\n...               [11.9, 12.3]])\n>>> pls = PLSSVD(n_components=2).fit(X, Y)\n>>> X_c, Y_c = pls.transform(X, Y)\n>>> X_c.shape, Y_c.shape\n((4, 2), (4, 2))\n\nSee Also\n--------\nPLSCanonical\nCCA",
      "code": "class PLSSVD(TransformerMixin, BaseEstimator):\n    \"\"\"Partial Least Square SVD.\n\n    This transformer simply performs a SVD on the crosscovariance matrix X'Y.\n    It is able to project both the training data `X` and the targets `Y`. The\n    training data X is projected on the left singular vectors, while the\n    targets are projected on the right singular vectors.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    .. versionadded:: 0.8\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        The number of components to keep. Should be in `[1,\n        min(n_samples, n_features, n_targets)]`.\n\n    scale : bool, default=True\n        Whether to scale `X` and `Y`.\n\n    copy : bool, default=True\n        Whether to copy `X` and `Y` in fit before applying centering, and\n        potentially scaling. If False, these operations will be done inplace,\n        modifying both arrays.\n\n    Attributes\n    ----------\n    x_weights_ : ndarray of shape (n_features, n_components)\n        The left singular vectors of the SVD of the cross-covariance matrix.\n        Used to project `X` in `transform`.\n\n    y_weights_ : ndarray of (n_targets, n_components)\n        The right singular vectors of the SVD of the cross-covariance matrix.\n        Used to project `X` in `transform`.\n\n    x_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training samples.\n\n        .. deprecated:: 0.24\n           `x_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    y_scores_ : ndarray of shape (n_samples, n_components)\n        The transformed training targets.\n\n        .. deprecated:: 0.24\n           `y_scores_` is deprecated in 0.24 and will be removed in 1.1\n           (renaming of 0.26). You can just call `transform` on the training\n           data instead.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.cross_decomposition import PLSSVD\n    >>> X = np.array([[0., 0., 1.],\n    ...               [1., 0., 0.],\n    ...               [2., 2., 2.],\n    ...               [2., 5., 4.]])\n    >>> Y = np.array([[0.1, -0.2],\n    ...               [0.9, 1.1],\n    ...               [6.2, 5.9],\n    ...               [11.9, 12.3]])\n    >>> pls = PLSSVD(n_components=2).fit(X, Y)\n    >>> X_c, Y_c = pls.transform(X, Y)\n    >>> X_c.shape, Y_c.shape\n    ((4, 2), (4, 2))\n\n    See Also\n    --------\n    PLSCanonical\n    CCA\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, copy=True):\n        self.n_components = n_components\n        self.scale = scale\n        self.copy = copy\n\n    def fit(self, X, Y):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Targets.\n        \"\"\"\n        check_consistent_length(X, Y)\n        X = self._validate_data(X, dtype=np.float64, copy=self.copy,\n                                ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)\n        # This matrix rank is at most min(n_samples, n_features, n_targets) so\n        # n_components cannot be bigger than that.\n        n_components = self.n_components\n        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])\n        if not 1 <= n_components <= rank_upper_bound:\n            # TODO: raise an error in 1.1\n            warnings.warn(\n                f\"As of version 0.24, n_components({n_components}) should be \"\n                f\"in [1, min(n_features, n_samples, n_targets)] = \"\n                f\"[1, {rank_upper_bound}]. \"\n                f\"n_components={rank_upper_bound} will be used instead. \"\n                f\"In version 1.1 (renaming of 0.26), an error will be raised.\",\n                FutureWarning\n            )\n            n_components = rank_upper_bound\n\n        X, Y, self._x_mean, self._y_mean, self._x_std, self._y_std = (\n            _center_scale_xy(X, Y, self.scale))\n\n        # Compute SVD of cross-covariance matrix\n        C = np.dot(X.T, Y)\n        U, s, Vt = svd(C, full_matrices=False)\n        U = U[:, :n_components]\n        Vt = Vt[:n_components]\n        U, Vt = svd_flip(U, Vt)\n        V = Vt.T\n\n        self._x_scores = np.dot(X, U)  # TODO: remove in 1.1\n        self._y_scores = np.dot(Y, V)  # TODO: remove in 1.1\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self\n\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"Attribute x_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X) on \"\n        \"the training data instead.\"\n    )\n    @property\n    def x_scores_(self):\n        return self._x_scores\n\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"Attribute y_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X, Y) \"\n        \"on the training data instead.\"\n    )\n    @property\n    def y_scores_(self):\n        return self._y_scores\n\n    @deprecated(  # type: ignore\n        \"Attribute x_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_mean_(self):\n        return self._x_mean\n\n    @deprecated(  # type: ignore\n        \"Attribute y_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_mean_(self):\n        return self._y_mean\n\n    @deprecated(  # type: ignore\n        \"Attribute x_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_std_(self):\n        return self._x_std\n\n    @deprecated(  # type: ignore\n        \"Attribute y_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_std_(self):\n        return self._y_std\n\n    def transform(self, X, Y=None):\n        \"\"\"\n        Apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to be transformed.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, dtype=np.float64)\n        Xr = (X - self._x_mean) / self._x_std\n        x_scores = np.dot(Xr, self.x_weights_)\n        if Y is not None:\n            Y = check_array(Y, ensure_2d=False, dtype=np.float64)\n            if Y.ndim == 1:\n                Y = Y.reshape(-1, 1)\n            Yr = (Y - self._y_mean) / self._y_std\n            y_scores = np.dot(Yr, self.y_weights_)\n            return x_scores, y_scores\n        return x_scores\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "scale",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "_x_mean",
          "types": null
        },
        {
          "name": "_y_mean",
          "types": null
        },
        {
          "name": "_x_std",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_y_std",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_x_scores",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "_y_scores",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "x_weights_",
          "types": null
        },
        {
          "name": "y_weights_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning",
      "name": "DictionaryLearning",
      "qname": "sklearn.decomposition._dict_learning.DictionaryLearning",
      "decorators": [],
      "superclasses": [
        "_BaseSparseCoding",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__",
        "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n    (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.",
      "docstring": "Dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n    (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nn_components : int, default=n_features\n    Number of dictionary elements to extract.\n\nalpha : float, default=1.0\n    Sparsity controlling parameter.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for numerical error.\n\nfit_algorithm : {'lars', 'cd'}, default='lars'\n    * `'lars'`: uses the least angle regression method to solve the lasso\n      problem (:func:`~sklearn.linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n      faster if the estimated components are sparse.\n\n    .. versionadded:: 0.17\n       *cd* coordinate descent method to improve speed.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (:func:`~sklearn.linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n      will be faster if the estimated components are sparse.\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution.\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\n    .. versionadded:: 0.17\n       *lasso_cd* coordinate descent method to improve speed.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.0\n\nn_jobs : int or None, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ncode_init : ndarray of shape (n_samples, n_components), default=None\n    Initial value for the code, for warm restart. Only used if `code_init`\n    and `dict_init` are not None.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial values for the dictionary, for warm restart. Only used if\n    `code_init` and `dict_init` are not None.\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    dictionary atoms extracted from the data\n\nerror_ : array\n    vector of errors at each iteration\n\nn_iter_ : int\n    Number of iterations run.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import DictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42,\n... )\n>>> dict_learner = DictionaryLearning(\n...     n_components=15, transform_algorithm='lasso_lars', random_state=42,\n... )\n>>> X_transformed = dict_learner.fit_transform(X)\n\nWe can check the level of sparsity of `X_transformed`:\n\n>>> np.mean(X_transformed == 0)\n0.88...\n\nWe can compare the average squared euclidean norm of the reconstruction\nerror of the sparse coded signal relative to the squared euclidean norm of\nthe original signal:\n\n>>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n0.07...\n\nNotes\n-----\n**References:**\n\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\nfor sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\nSee Also\n--------\nSparseCoder\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA",
      "code": "class DictionaryLearning(_BaseSparseCoding, BaseEstimator):\n    \"\"\"Dictionary learning\n\n    Finds a dictionary (a set of atoms) that can best be used to represent data\n    using a sparse code.\n\n    Solves the optimization problem::\n\n        (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                    (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=n_features\n        Number of dictionary elements to extract.\n\n    alpha : float, default=1.0\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for numerical error.\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        * `'lars'`: uses the least angle regression method to solve the lasso\n          problem (:func:`~sklearn.linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n          faster if the estimated components are sparse.\n\n        .. versionadded:: 0.17\n           *cd* coordinate descent method to improve speed.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n            'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (:func:`~sklearn.linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n          will be faster if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n        .. versionadded:: 0.17\n           *lasso_cd* coordinate descent method to improve speed.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.0\n\n    n_jobs : int or None, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the code, for warm restart. Only used if `code_init`\n        and `dict_init` are not None.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the dictionary, for warm restart. Only used if\n        `code_init` and `dict_init` are not None.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        dictionary atoms extracted from the data\n\n    error_ : array\n        vector of errors at each iteration\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import DictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> dict_learner = DictionaryLearning(\n    ...     n_components=15, transform_algorithm='lasso_lars', random_state=42,\n    ... )\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0)\n    0.88...\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    0.07...\n\n    Notes\n    -----\n    **References:**\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\n    See Also\n    --------\n    SparseCoder\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, max_iter=1000, tol=1e-8,\n                 fit_algorithm='lars', transform_algorithm='omp',\n                 transform_n_nonzero_coefs=None, transform_alpha=None,\n                 n_jobs=None, code_init=None, dict_init=None, verbose=False,\n                 split_sign=False, random_state=None, positive_code=False,\n                 positive_dict=False, transform_max_iter=1000):\n\n        super().__init__(\n            transform_algorithm, transform_n_nonzero_coefs,\n            transform_alpha, split_sign, n_jobs, positive_code,\n            transform_max_iter\n        )\n        self.n_components = n_components\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.fit_algorithm = fit_algorithm\n        self.code_init = code_init\n        self.dict_init = dict_init\n        self.verbose = verbose\n        self.random_state = random_state\n        self.positive_dict = positive_dict\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` in the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the object itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n        if self.n_components is None:\n            n_components = X.shape[1]\n        else:\n            n_components = self.n_components\n\n        V, U, E, self.n_iter_ = dict_learning(\n            X, n_components, alpha=self.alpha,\n            tol=self.tol, max_iter=self.max_iter,\n            method=self.fit_algorithm,\n            method_max_iter=self.transform_max_iter,\n            n_jobs=self.n_jobs,\n            code_init=self.code_init,\n            dict_init=self.dict_init,\n            verbose=self.verbose,\n            random_state=random_state,\n            return_n_iter=True,\n            positive_dict=self.positive_dict,\n            positive_code=self.positive_code)\n        self.components_ = U\n        self.error_ = E\n        return self",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "fit_algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "code_init",
          "types": null
        },
        {
          "name": "dict_init",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "positive_dict",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "components_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "matrix"
              },
              {
                "kind": "NamedType",
                "name": "ndarray"
              }
            ]
          }
        },
        {
          "name": "error_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning",
      "name": "MiniBatchDictionaryLearning",
      "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning",
      "decorators": [],
      "superclasses": [
        "_BaseSparseCoding",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__",
        "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/fit",
        "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/partial_fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Mini-batch dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n   (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.",
      "docstring": "Mini-batch dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n   (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of dictionary elements to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter.\n\nn_iter : int, default=1000\n    Total number of iterations to perform.\n\nfit_algorithm : {'lars', 'cd'}, default='lars'\n    The algorithm used:\n\n    - `'lars'`: uses the least angle regression method to solve the lasso\n      problem (`linear_model.lars_path`)\n    - `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nbatch_size : int, default=3\n    Number of samples in each mini-batch.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples before forming batches.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    initial value of the dictionary for warm restart scenarios\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n      if the estimated components are sparse.\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution.\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components extracted from the data.\n\ninner_stats_ : tuple of (A, B) ndarrays\n    Internal sufficient statistics that are kept by the algorithm.\n    Keeping them is useful in online settings, to avoid losing the\n    history of the evolution, but they shouldn't have any use for the\n    end user.\n    `A` `(n_components, n_components)` is the dictionary covariance matrix.\n    `B` `(n_features, n_components)` is the data approximation matrix.\n\nn_iter_ : int\n    Number of iterations run.\n\niter_offset_ : int\n    The number of iteration on data batches that has been\n    performed before.\n\nrandom_state_ : RandomState instance\n    RandomState instance that is generated either from a seed, the random\n    number generattor or by `np.random`.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_sparse_coded_signal\n>>> from sklearn.decomposition import MiniBatchDictionaryLearning\n>>> X, dictionary, code = make_sparse_coded_signal(\n...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n...     random_state=42)\n>>> dict_learner = MiniBatchDictionaryLearning(\n...     n_components=15, transform_algorithm='lasso_lars', random_state=42,\n... )\n>>> X_transformed = dict_learner.fit_transform(X)\n\nWe can check the level of sparsity of `X_transformed`:\n\n>>> np.mean(X_transformed == 0)\n0.87...\n\nWe can compare the average squared euclidean norm of the reconstruction\nerror of the sparse coded signal relative to the squared euclidean norm of\nthe original signal:\n\n>>> X_hat = X_transformed @ dict_learner.components_\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n0.10...\n\nNotes\n-----\n**References:**\n\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\nfor sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\nSee Also\n--------\nSparseCoder\nDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA",
      "code": "class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\n    \"\"\"Mini-batch dictionary learning\n\n    Finds a dictionary (a set of atoms) that can best be used to represent data\n    using a sparse code.\n\n    Solves the optimization problem::\n\n       (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                    (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    n_iter : int, default=1000\n        Total number of iterations to perform.\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        The algorithm used:\n\n        - `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`)\n        - `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    batch_size : int, default=3\n        Number of samples in each mini-batch.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples before forming batches.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        initial value of the dictionary for warm restart scenarios\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n            'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n          if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components extracted from the data.\n\n    inner_stats_ : tuple of (A, B) ndarrays\n        Internal sufficient statistics that are kept by the algorithm.\n        Keeping them is useful in online settings, to avoid losing the\n        history of the evolution, but they shouldn't have any use for the\n        end user.\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\n        `B` `(n_features, n_components)` is the data approximation matrix.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    iter_offset_ : int\n        The number of iteration on data batches that has been\n        performed before.\n\n    random_state_ : RandomState instance\n        RandomState instance that is generated either from a seed, the random\n        number generattor or by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42)\n    >>> dict_learner = MiniBatchDictionaryLearning(\n    ...     n_components=15, transform_algorithm='lasso_lars', random_state=42,\n    ... )\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0)\n    0.87...\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    0.10...\n\n    Notes\n    -----\n    **References:**\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\n    See Also\n    --------\n    SparseCoder\n    DictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, n_iter=1000,\n                 fit_algorithm='lars', n_jobs=None, batch_size=3, shuffle=True,\n                 dict_init=None, transform_algorithm='omp',\n                 transform_n_nonzero_coefs=None, transform_alpha=None,\n                 verbose=False, split_sign=False, random_state=None,\n                 positive_code=False, positive_dict=False,\n                 transform_max_iter=1000):\n\n        super().__init__(\n            transform_algorithm, transform_n_nonzero_coefs, transform_alpha,\n            split_sign, n_jobs, positive_code, transform_max_iter\n        )\n        self.n_components = n_components\n        self.alpha = alpha\n        self.n_iter = n_iter\n        self.fit_algorithm = fit_algorithm\n        self.dict_init = dict_init\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.split_sign = split_sign\n        self.random_state = random_state\n        self.positive_dict = positive_dict\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n\n        U, (A, B), self.n_iter_ = dict_learning_online(\n            X, self.n_components, alpha=self.alpha,\n            n_iter=self.n_iter, return_code=False,\n            method=self.fit_algorithm,\n            method_max_iter=self.transform_max_iter,\n            n_jobs=self.n_jobs, dict_init=self.dict_init,\n            batch_size=self.batch_size, shuffle=self.shuffle,\n            verbose=self.verbose, random_state=random_state,\n            return_inner_stats=True,\n            return_n_iter=True,\n            positive_dict=self.positive_dict,\n            positive_code=self.positive_code)\n        self.components_ = U\n        # Keep track of the state of the algorithm to be able to do\n        # some online fitting (partial_fit)\n        self.inner_stats_ = (A, B)\n        self.iter_offset_ = self.n_iter\n        self.random_state_ = random_state\n        return self\n\n    def partial_fit(self, X, y=None, iter_offset=None):\n        \"\"\"Updates the model using the data in X as a mini-batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        iter_offset : int, default=None\n            The number of iteration on data batches that has been\n            performed before this call to partial_fit. This is optional:\n            if no number is passed, the memory of the object is\n            used.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        if not hasattr(self, 'random_state_'):\n            self.random_state_ = check_random_state(self.random_state)\n        if hasattr(self, 'components_'):\n            dict_init = self.components_\n        else:\n            dict_init = self.dict_init\n        inner_stats = getattr(self, 'inner_stats_', None)\n        if iter_offset is None:\n            iter_offset = getattr(self, 'iter_offset_', 0)\n        X = self._validate_data(X, reset=(iter_offset == 0))\n        U, (A, B) = dict_learning_online(\n            X, self.n_components, alpha=self.alpha,\n            n_iter=1, method=self.fit_algorithm,\n            method_max_iter=self.transform_max_iter,\n            n_jobs=self.n_jobs, dict_init=dict_init,\n            batch_size=len(X), shuffle=False,\n            verbose=self.verbose, return_code=False,\n            iter_offset=iter_offset, random_state=self.random_state_,\n            return_inner_stats=True, inner_stats=inner_stats,\n            positive_dict=self.positive_dict,\n            positive_code=self.positive_code)\n        self.components_ = U\n\n        # Keep track of the state of the algorithm to be able to do\n        # some online fitting (partial_fit)\n        self.inner_stats_ = (A, B)\n        self.iter_offset_ = iter_offset + 1\n        return self",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "fit_algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "dict_init",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "shuffle",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "batch_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "split_sign",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "positive_dict",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "inner_stats_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "iter_offset_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder",
      "name": "SparseCoder",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder",
      "decorators": [],
      "superclasses": [
        "_BaseSparseCoding",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__",
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/fit",
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/components_@getter",
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/transform",
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/_more_tags",
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/n_components_@getter",
        "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/n_features_in_@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Sparse coding\n\nFinds a sparse representation of data against a fixed, precomputed\ndictionary.\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.",
      "docstring": "Sparse coding\n\nFinds a sparse representation of data against a fixed, precomputed\ndictionary.\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n\nParameters\n----------\ndictionary : ndarray of shape (n_components, n_features)\n    The dictionary atoms used for sparse coding. Lines are assumed to be\n    normalized to unit norm.\n\ntransform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n    Algorithm used to transform the data:\n\n    - `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    - `'lasso_lars'`: uses Lars to compute the Lasso solution;\n    - `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n      the estimated components are sparse;\n    - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution;\n    - `'threshold'`: squashes to zero all coefficients less than alpha from\n      the projection ``dictionary * X'``.\n\ntransform_n_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `transform_n_nonzero_coefs=int(n_features / 10)`.\n\ntransform_alpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\nsplit_sign : bool, default=False\n    Whether to split the sparse feature vector into the concatenation of\n    its negative part and its positive part. This can improve the\n    performance of downstream classifiers.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\ntransform_max_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `lasso_lars`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The unchanged dictionary atoms.\n\n    .. deprecated:: 0.24\n       This attribute is deprecated in 0.24 and will be removed in\n       1.1 (renaming of 0.26). Use `dictionary` instead.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import SparseCoder\n>>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n>>> dictionary = np.array(\n...     [[0, 1, 0],\n...      [-1, -1, 2],\n...      [1, 1, 1],\n...      [0, 1, 1],\n...      [0, 2, 1]],\n...    dtype=np.float64\n... )\n>>> coder = SparseCoder(\n...     dictionary=dictionary, transform_algorithm='lasso_lars',\n...     transform_alpha=1e-10,\n... )\n>>> coder.transform(X)\narray([[ 0.,  0., -1.,  0.,  0.],\n       [ 0.,  1.,  1.,  0.,  0.]])\n\nSee Also\n--------\nDictionaryLearning\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA\nsparse_encode",
      "code": "class SparseCoder(_BaseSparseCoding, BaseEstimator):\n    \"\"\"Sparse coding\n\n    Finds a sparse representation of data against a fixed, precomputed\n    dictionary.\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    dictionary : ndarray of shape (n_components, n_features)\n        The dictionary atoms used for sparse coding. Lines are assumed to be\n        normalized to unit norm.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n            'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n          the estimated components are sparse;\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution;\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `lasso_lars`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The unchanged dictionary atoms.\n\n        .. deprecated:: 0.24\n           This attribute is deprecated in 0.24 and will be removed in\n           1.1 (renaming of 0.26). Use `dictionary` instead.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import SparseCoder\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> coder = SparseCoder(\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\n    ...     transform_alpha=1e-10,\n    ... )\n    >>> coder.transform(X)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])\n\n    See Also\n    --------\n    DictionaryLearning\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n    sparse_encode\n    \"\"\"\n    _required_parameters = [\"dictionary\"]\n\n    @_deprecate_positional_args\n    def __init__(self, dictionary, *, transform_algorithm='omp',\n                 transform_n_nonzero_coefs=None, transform_alpha=None,\n                 split_sign=False, n_jobs=None, positive_code=False,\n                 transform_max_iter=1000):\n        super().__init__(\n            transform_algorithm, transform_n_nonzero_coefs,\n            transform_alpha, split_sign, n_jobs, positive_code,\n            transform_max_iter\n        )\n        self.dictionary = dictionary\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n\n        Parameters\n        ----------\n        X : Ignored\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        return self\n\n    @deprecated(\"The attribute 'components_' is deprecated \"  # type: ignore\n                \"in 0.24 and will be removed in 1.1 (renaming of 0.26). Use \"\n                \"the 'dictionary' instead.\")\n    @property\n    def components_(self):\n        return self.dictionary\n\n    def transform(self, X, y=None):\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\n\n        Coding method is determined by the object parameter\n        `transform_algorithm`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        return super()._transform(X, self.dictionary)\n\n    def _more_tags(self):\n        return {\"requires_fit\": False}\n\n    @property\n    def n_components_(self):\n        return self.dictionary.shape[0]\n\n    @property\n    def n_features_in_(self):\n        return self.dictionary.shape[1]",
      "instance_attributes": [
        {
          "name": "dictionary",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis",
      "name": "FactorAnalysis",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/fit",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/transform",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/get_covariance",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/get_precision",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score_samples",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score",
        "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/_rotate"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Factor Analysis (FA).\n\nA simple linear generative model with Gaussian latent variables.\n\nThe observations are assumed to be caused by a linear transformation of\nlower dimensional latent factors and added Gaussian noise.\nWithout loss of generality the factors are distributed according to a\nGaussian with zero mean and unit covariance. The noise is also zero mean\nand has an arbitrary diagonal covariance matrix.\n\nIf we would restrict the model further, by assuming that the Gaussian\nnoise is even isotropic (all diagonal entries are the same) we would obtain\n:class:`PPCA`.\n\nFactorAnalysis performs a maximum likelihood estimate of the so-called\n`loading` matrix, the transformation of the latent variables to the\nobserved ones, using SVD based approach.\n\nRead more in the :ref:`User Guide <FA>`.\n\n.. versionadded:: 0.13",
      "docstring": "Factor Analysis (FA).\n\nA simple linear generative model with Gaussian latent variables.\n\nThe observations are assumed to be caused by a linear transformation of\nlower dimensional latent factors and added Gaussian noise.\nWithout loss of generality the factors are distributed according to a\nGaussian with zero mean and unit covariance. The noise is also zero mean\nand has an arbitrary diagonal covariance matrix.\n\nIf we would restrict the model further, by assuming that the Gaussian\nnoise is even isotropic (all diagonal entries are the same) we would obtain\n:class:`PPCA`.\n\nFactorAnalysis performs a maximum likelihood estimate of the so-called\n`loading` matrix, the transformation of the latent variables to the\nobserved ones, using SVD based approach.\n\nRead more in the :ref:`User Guide <FA>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_components : int, default=None\n    Dimensionality of latent space, the number of components\n    of ``X`` that are obtained after ``transform``.\n    If None, n_components is set to the number of features.\n\ntol : float, defaul=1e-2\n    Stopping tolerance for log-likelihood increase.\n\ncopy : bool, default=True\n    Whether to make a copy of X. If ``False``, the input X gets overwritten\n    during fitting.\n\nmax_iter : int, default=1000\n    Maximum number of iterations.\n\nnoise_variance_init : ndarray of shape (n_features,), default=None\n    The initial guess of the noise variance for each feature.\n    If None, it defaults to np.ones(n_features).\n\nsvd_method : {'lapack', 'randomized'}, default='randomized'\n    Which SVD method to use. If 'lapack' use standard SVD from\n    scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n    Defaults to 'randomized'. For most applications 'randomized' will\n    be sufficiently precise while providing significant speed gains.\n    Accuracy can also be improved by setting higher values for\n    `iterated_power`. If this is not sufficient, for maximum precision\n    you should choose 'lapack'.\n\niterated_power : int, default=3\n    Number of iterations for the power method. 3 by default. Only used\n    if ``svd_method`` equals 'randomized'.\n\nrotation : {'varimax', 'quartimax'}, default=None\n    If not None, apply the indicated rotation. Currently, varimax and\n    quartimax are implemented. See\n    `\"The varimax criterion for analytic rotation in factor analysis\"\n    <https://link.springer.com/article/10.1007%2FBF02289233>`_\n    H. F. Kaiser, 1958.\n\n    .. versionadded:: 0.24\n\nrandom_state : int or RandomState instance, default=0\n    Only used when ``svd_method`` equals 'randomized'. Pass an int for\n    reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components with maximum variance.\n\nloglike_ : list of shape (n_iterations,)\n    The log likelihood at each iteration.\n\nnoise_variance_ : ndarray of shape (n_features,)\n    The estimated noise variance for each feature.\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FactorAnalysis\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FactorAnalysis(n_components=7, random_state=0)\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)\n\nReferences\n----------\n- David Barber, Bayesian Reasoning and Machine Learning,\n  Algorithm 21.1.\n\n- Christopher M. Bishop: Pattern Recognition and Machine Learning,\n  Chapter 12.2.4.\n\nSee Also\n--------\nPCA: Principal component analysis is also a latent linear variable model\n    which however assumes equal noise variance for each feature.\n    This extra assumption makes probabilistic PCA faster as it can be\n    computed in closed form.\nFastICA: Independent component analysis, a latent variable model with\n    non-Gaussian latent variables.",
      "code": "class FactorAnalysis(TransformerMixin, BaseEstimator):\n    \"\"\"Factor Analysis (FA).\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PPCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float, defaul=1e-2\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool, default=True\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int, default=1000\n        Maximum number of iterations.\n\n    noise_variance_init : ndarray of shape (n_features,), default=None\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features).\n\n    svd_method : {'lapack', 'randomized'}, default='randomized'\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, default=3\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'.\n\n    rotation : {'varimax', 'quartimax'}, default=None\n        If not None, apply the indicated rotation. Currently, varimax and\n        quartimax are implemented. See\n        `\"The varimax criterion for analytic rotation in factor analysis\"\n        <https://link.springer.com/article/10.1007%2FBF02289233>`_\n        H. F. Kaiser, 1958.\n\n        .. versionadded:: 0.24\n\n    random_state : int or RandomState instance, default=0\n        Only used when ``svd_method`` equals 'randomized'. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    loglike_ : list of shape (n_iterations,)\n        The log likelihood at each iteration.\n\n    noise_variance_ : ndarray of shape (n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.\n\n    See Also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, tol=1e-2, copy=True,\n                 max_iter=1000,\n                 noise_variance_init=None, svd_method='randomized',\n                 iterated_power=3, rotation=None, random_state=0):\n        self.n_components = n_components\n        self.copy = copy\n        self.tol = tol\n        self.max_iter = max_iter\n        if svd_method not in ['lapack', 'randomized']:\n            raise ValueError('SVD method %s is not supported. Please consider'\n                             ' the documentation' % svd_method)\n        self.svd_method = svd_method\n\n        self.noise_variance_init = noise_variance_init\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n        self.rotation = rotation\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the FactorAnalysis model to X using SVD based approach\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, copy=self.copy, dtype=np.float64)\n\n        n_samples, n_features = X.shape\n        n_components = self.n_components\n        if n_components is None:\n            n_components = n_features\n\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        # some constant terms\n        nsqrt = sqrt(n_samples)\n        llconst = n_features * log(2. * np.pi) + n_components\n        var = np.var(X, axis=0)\n\n        if self.noise_variance_init is None:\n            psi = np.ones(n_features, dtype=X.dtype)\n        else:\n            if len(self.noise_variance_init) != n_features:\n                raise ValueError(\"noise_variance_init dimension does not \"\n                                 \"with number of features : %d != %d\" %\n                                 (len(self.noise_variance_init), n_features))\n            psi = np.array(self.noise_variance_init)\n\n        loglike = []\n        old_ll = -np.inf\n        SMALL = 1e-12\n\n        # we'll modify svd outputs to return unexplained variance\n        # to allow for unified computation of loglikelihood\n        if self.svd_method == 'lapack':\n            def my_svd(X):\n                _, s, Vt = linalg.svd(X,\n                                      full_matrices=False,\n                                      check_finite=False)\n                return (s[:n_components], Vt[:n_components],\n                        squared_norm(s[n_components:]))\n        elif self.svd_method == 'randomized':\n            random_state = check_random_state(self.random_state)\n\n            def my_svd(X):\n                _, s, Vt = randomized_svd(X, n_components,\n                                          random_state=random_state,\n                                          n_iter=self.iterated_power)\n                return s, Vt, squared_norm(X) - squared_norm(s)\n        else:\n            raise ValueError('SVD method %s is not supported. Please consider'\n                             ' the documentation' % self.svd_method)\n\n        for i in range(self.max_iter):\n            # SMALL helps numerics\n            sqrt_psi = np.sqrt(psi) + SMALL\n            s, Vt, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n            s **= 2\n            # Use 'maximum' here to avoid sqrt problems.\n            W = np.sqrt(np.maximum(s - 1., 0.))[:, np.newaxis] * Vt\n            del Vt\n            W *= sqrt_psi\n\n            # loglikelihood\n            ll = llconst + np.sum(np.log(s))\n            ll += unexp_var + np.sum(np.log(psi))\n            ll *= -n_samples / 2.\n            loglike.append(ll)\n            if (ll - old_ll) < self.tol:\n                break\n            old_ll = ll\n\n            psi = np.maximum(var - np.sum(W ** 2, axis=0), SMALL)\n        else:\n            warnings.warn('FactorAnalysis did not converge.' +\n                          ' You might want' +\n                          ' to increase the number of iterations.',\n                          ConvergenceWarning)\n\n        self.components_ = W\n        if self.rotation is not None:\n            self.components_ = self._rotate(W)\n        self.noise_variance_ = psi\n        self.loglike_ = loglike\n        self.n_iter_ = i + 1\n        return self\n\n    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            The latent variables of X.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, reset=False)\n        Ih = np.eye(len(self.components_))\n\n        X_transformed = X - self.mean_\n\n        Wpsi = self.components_ / self.noise_variance_\n        cov_z = linalg.inv(Ih + np.dot(Wpsi, self.components_.T))\n        tmp = np.dot(X_transformed, Wpsi.T)\n        X_transformed = np.dot(tmp, cov_z)\n\n        return X_transformed\n\n    def get_covariance(self):\n        \"\"\"Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``\n\n        Returns\n        -------\n        cov : ndarray of shape (n_features, n_features)\n            Estimated covariance of data.\n        \"\"\"\n        check_is_fitted(self)\n\n        cov = np.dot(self.components_.T, self.components_)\n        cov.flat[::len(cov) + 1] += self.noise_variance_  # modify diag inplace\n        return cov\n\n    def get_precision(self):\n        \"\"\"Compute data precision matrix with the FactorAnalysis model.\n\n        Returns\n        -------\n        precision : ndarray of shape (n_features, n_features)\n            Estimated precision of data.\n        \"\"\"\n        check_is_fitted(self)\n\n        n_features = self.components_.shape[1]\n\n        # handle corner cases first\n        if self.n_components == 0:\n            return np.diag(1. / self.noise_variance_)\n        if self.n_components == n_features:\n            return linalg.inv(self.get_covariance())\n\n        # Get precision using matrix inversion lemma\n        components_ = self.components_\n        precision = np.dot(components_ / self.noise_variance_, components_.T)\n        precision.flat[::len(precision) + 1] += 1.\n        precision = np.dot(components_.T,\n                           np.dot(linalg.inv(precision), components_))\n        precision /= self.noise_variance_[:, np.newaxis]\n        precision /= -self.noise_variance_[np.newaxis, :]\n        precision.flat[::len(precision) + 1] += 1. / self.noise_variance_\n        return precision\n\n    def score_samples(self, X):\n        \"\"\"Compute the log-likelihood of each sample\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        Xr = X - self.mean_\n        precision = self.get_precision()\n        n_features = X.shape[1]\n        log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= .5 * (n_features * log(2. * np.pi)\n                          - fast_logdet(precision))\n        return log_like\n\n    def score(self, X, y=None):\n        \"\"\"Compute the average log-likelihood of the samples\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model\n        \"\"\"\n        return np.mean(self.score_samples(X))\n\n    def _rotate(self, components, n_components=None, tol=1e-6):\n        \"Rotate the factor analysis solution.\"\n        # note that tol is not exposed\n        implemented = (\"varimax\", \"quartimax\")\n        method = self.rotation\n        if method in implemented:\n            return _ortho_rotation(components.T, method=method,\n                                   tol=tol)[:self.n_components]\n        else:\n            raise ValueError(\"'method' must be in %s, not %s\"\n                             % (implemented, method))",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "svd_method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "noise_variance_init",
          "types": null
        },
        {
          "name": "iterated_power",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "rotation",
          "types": null
        },
        {
          "name": "mean_",
          "types": null
        },
        {
          "name": "components_",
          "types": null
        },
        {
          "name": "noise_variance_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "loglike_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "n_iter_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/FastICA",
      "name": "FastICA",
      "qname": "sklearn.decomposition._fastica.FastICA",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__",
        "scikit-learn/sklearn.decomposition._fastica/FastICA/_fit",
        "scikit-learn/sklearn.decomposition._fastica/FastICA/fit_transform",
        "scikit-learn/sklearn.decomposition._fastica/FastICA/fit",
        "scikit-learn/sklearn.decomposition._fastica/FastICA/transform",
        "scikit-learn/sklearn.decomposition._fastica/FastICA/inverse_transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "FastICA: a fast algorithm for Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.",
      "docstring": "FastICA: a fast algorithm for Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components to use. If None is passed, all are used.\n\nalgorithm : {'parallel', 'deflation'}, default='parallel'\n    Apply parallel or deflational algorithm for FastICA.\n\nwhiten : bool, default=True\n    If whiten is false, the data is already considered to be\n    whitened, and no whitening is performed.\n\nfun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n    The functional form of the G function used in the\n    approximation to neg-entropy. Could be either 'logcosh', 'exp',\n    or 'cube'.\n    You can also provide your own function. It should return a tuple\n    containing the value of the function, and of its derivative, in the\n    point. Example::\n\n        def my_g(x):\n            return x ** 3, (3 * x ** 2).mean(axis=-1)\n\nfun_args : dict, default=None\n    Arguments to send to the functional form.\n    If empty and if fun='logcosh', fun_args will take value\n    {'alpha' : 1.0}.\n\nmax_iter : int, default=200\n    Maximum number of iterations during fit.\n\ntol : float, default=1e-4\n    Tolerance on update at each iteration.\n\nw_init : ndarray of shape (n_components, n_components), default=None\n    The mixing matrix to be used to initialize the algorithm.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to initialize ``w_init`` when not specified, with a\n    normal distribution. Pass an int, for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    The linear operator to apply to the data to get the independent\n    sources. This is equal to the unmixing matrix when ``whiten`` is\n    False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n    ``whiten`` is True.\n\nmixing_ : ndarray of shape (n_features, n_components)\n    The pseudo-inverse of ``components_``. It is the linear operator\n    that maps independent sources to the data.\n\nmean_ : ndarray of shape(n_features,)\n    The mean over features. Only set if `self.whiten` is True.\n\nn_iter_ : int\n    If the algorithm is \"deflation\", n_iter is the\n    maximum number of iterations run across all components. Else\n    they are just the number of iterations taken to converge.\n\nwhitening_ : ndarray of shape (n_components, n_features)\n    Only set if whiten is 'True'. This is the pre-whitening matrix\n    that projects data onto the first `n_components` principal components.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import FastICA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = FastICA(n_components=7,\n...         random_state=0)\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)\n\nNotes\n-----\nImplementation based on\n*A. Hyvarinen and E. Oja, Independent Component Analysis:\nAlgorithms and Applications, Neural Networks, 13(4-5), 2000,\npp. 411-430*",
      "code": "class FastICA(TransformerMixin, BaseEstimator):\n    \"\"\"FastICA: a fast algorithm for Independent Component Analysis.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components to use. If None is passed, all are used.\n\n    algorithm : {'parallel', 'deflation'}, default='parallel'\n        Apply parallel or deflational algorithm for FastICA.\n\n    whiten : bool, default=True\n        If whiten is false, the data is already considered to be\n        whitened, and no whitening is performed.\n\n    fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. Example::\n\n            def my_g(x):\n                return x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations during fit.\n\n    tol : float, default=1e-4\n        Tolerance on update at each iteration.\n\n    w_init : ndarray of shape (n_components, n_components), default=None\n        The mixing matrix to be used to initialize the algorithm.\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The linear operator to apply to the data to get the independent\n        sources. This is equal to the unmixing matrix when ``whiten`` is\n        False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n        ``whiten`` is True.\n\n    mixing_ : ndarray of shape (n_features, n_components)\n        The pseudo-inverse of ``components_``. It is the linear operator\n        that maps independent sources to the data.\n\n    mean_ : ndarray of shape(n_features,)\n        The mean over features. Only set if `self.whiten` is True.\n\n    n_iter_ : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge.\n\n    whitening_ : ndarray of shape (n_components, n_features)\n        Only set if whiten is 'True'. This is the pre-whitening matrix\n        that projects data onto the first `n_components` principal components.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FastICA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FastICA(n_components=7,\n    ...         random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    Notes\n    -----\n    Implementation based on\n    *A. Hyvarinen and E. Oja, Independent Component Analysis:\n    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n    pp. 411-430*\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, algorithm='parallel', whiten=True,\n                 fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,\n                 w_init=None, random_state=None):\n        super().__init__()\n        if max_iter < 1:\n            raise ValueError(\"max_iter should be greater than 1, got \"\n                             \"(max_iter={})\".format(max_iter))\n        self.n_components = n_components\n        self.algorithm = algorithm\n        self.whiten = whiten\n        self.fun = fun\n        self.fun_args = fun_args\n        self.max_iter = max_iter\n        self.tol = tol\n        self.w_init = w_init\n        self.random_state = random_state\n\n    def _fit(self, X, compute_sources=False):\n        \"\"\"Fit the model\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        compute_sources : bool, default=False\n            If False, sources are not computes but only the rotation matrix.\n            This can save memory when working with big data. Defaults to False.\n\n        Returns\n        -------\n            X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n\n        X = self._validate_data(X, copy=self.whiten, dtype=FLOAT_DTYPES,\n                                ensure_min_samples=2).T\n        fun_args = {} if self.fun_args is None else self.fun_args\n        random_state = check_random_state(self.random_state)\n\n        alpha = fun_args.get('alpha', 1.0)\n        if not 1 <= alpha <= 2:\n            raise ValueError('alpha must be in [1,2]')\n\n        if self.fun == 'logcosh':\n            g = _logcosh\n        elif self.fun == 'exp':\n            g = _exp\n        elif self.fun == 'cube':\n            g = _cube\n        elif callable(self.fun):\n            def g(x, fun_args):\n                return self.fun(x, **fun_args)\n        else:\n            exc = ValueError if isinstance(self.fun, str) else TypeError\n            raise exc(\n                \"Unknown function %r;\"\n                \" should be one of 'logcosh', 'exp', 'cube' or callable\"\n                % self.fun\n            )\n\n        n_samples, n_features = X.shape\n\n        n_components = self.n_components\n        if not self.whiten and n_components is not None:\n            n_components = None\n            warnings.warn('Ignoring n_components with whiten=False.')\n\n        if n_components is None:\n            n_components = min(n_samples, n_features)\n        if (n_components > min(n_samples, n_features)):\n            n_components = min(n_samples, n_features)\n            warnings.warn(\n                'n_components is too large: it will be set to %s'\n                % n_components\n            )\n\n        if self.whiten:\n            # Centering the columns (ie the variables)\n            X_mean = X.mean(axis=-1)\n            X -= X_mean[:, np.newaxis]\n\n            # Whitening and preprocessing by PCA\n            u, d, _ = linalg.svd(X, full_matrices=False, check_finite=False)\n\n            del _\n            K = (u / d).T[:n_components]  # see (6.33) p.140\n            del u, d\n            X1 = np.dot(K, X)\n            # see (13.6) p.267 Here X1 is white and data\n            # in X has been projected onto a subspace by PCA\n            X1 *= np.sqrt(n_features)\n        else:\n            # X must be casted to floats to avoid typing issues with numpy\n            # 2.0 and the line below\n            X1 = as_float_array(X, copy=False)  # copy has been taken care of\n\n        w_init = self.w_init\n        if w_init is None:\n            w_init = np.asarray(random_state.normal(\n                size=(n_components, n_components)), dtype=X1.dtype)\n\n        else:\n            w_init = np.asarray(w_init)\n            if w_init.shape != (n_components, n_components):\n                raise ValueError(\n                    'w_init has invalid shape -- should be %(shape)s'\n                    % {'shape': (n_components, n_components)})\n\n        kwargs = {'tol': self.tol,\n                  'g': g,\n                  'fun_args': fun_args,\n                  'max_iter': self.max_iter,\n                  'w_init': w_init}\n\n        if self.algorithm == 'parallel':\n            W, n_iter = _ica_par(X1, **kwargs)\n        elif self.algorithm == 'deflation':\n            W, n_iter = _ica_def(X1, **kwargs)\n        else:\n            raise ValueError('Invalid algorithm: must be either `parallel` or'\n                             ' `deflation`.')\n        del X1\n\n        if compute_sources:\n            if self.whiten:\n                S = np.linalg.multi_dot([W, K, X]).T\n            else:\n                S = np.dot(W, X).T\n        else:\n            S = None\n\n        self.n_iter_ = n_iter\n\n        if self.whiten:\n            self.components_ = np.dot(W, K)\n            self.mean_ = X_mean\n            self.whitening_ = K\n        else:\n            self.components_ = W\n\n        self.mixing_ = linalg.pinv(self.components_, check_finite=False)\n        self._unmixing = W\n\n        return S\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model and recover the sources from X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        return self._fit(X, compute_sources=True)\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._fit(X, compute_sources=False)\n        return self\n\n    def transform(self, X, copy=True):\n        \"\"\"Recover the sources from X (apply the unmixing matrix).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to transform, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        copy : bool, default=True\n            If False, data passed to fit can be overwritten. Defaults to True.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, copy=(copy and self.whiten),\n                                dtype=FLOAT_DTYPES, reset=False)\n        if self.whiten:\n            X -= self.mean_\n\n        return np.dot(X, self.components_.T)\n\n    def inverse_transform(self, X, copy=True):\n        \"\"\"Transform the sources back to the mixed data (apply mixing matrix).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            Sources, where n_samples is the number of samples\n            and n_components is the number of components.\n        copy : bool, default=True\n            If False, data passed to fit are overwritten. Defaults to True.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_features)\n        \"\"\"\n        check_is_fitted(self)\n\n        X = check_array(X, copy=(copy and self.whiten), dtype=FLOAT_DTYPES)\n        X = np.dot(X, self.mixing_.T)\n        if self.whiten:\n            X += self.mean_\n\n        return X",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "whiten",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "fun",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "fun_args",
          "types": null
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "w_init",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "n_iter_",
          "types": null
        },
        {
          "name": "components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "mean_",
          "types": null
        },
        {
          "name": "whitening_",
          "types": null
        },
        {
          "name": "mixing_",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "_unmixing",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA",
      "name": "IncrementalPCA",
      "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA",
      "decorators": [],
      "superclasses": [
        "_BasePCA"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__",
        "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/fit",
        "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/partial_fit",
        "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Incremental principal components analysis (IPCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of\nthe data, keeping only the most significant singular vectors to\nproject the data to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nDepending on the size of the input data, this algorithm can be much more\nmemory efficient than a PCA, and allows sparse input.\n\nThis algorithm has constant memory complexity, on the order\nof ``batch_size * n_features``, enabling use of np.memmap files without\nloading the entire file into memory. For sparse matrices, the input\nis converted to dense in batches (in order to be able to subtract the\nmean) which avoids storing the entire dense matrix at any one time.\n\nThe computational overhead of each SVD is\n``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\nremain in memory at a time. There will be ``n_samples / batch_size`` SVD\ncomputations to get the principal components, versus 1 large SVD of\ncomplexity ``O(n_samples * n_features ** 2)`` for PCA.\n\nRead more in the :ref:`User Guide <IncrementalPCA>`.\n\n.. versionadded:: 0.16",
      "docstring": "Incremental principal components analysis (IPCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of\nthe data, keeping only the most significant singular vectors to\nproject the data to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nDepending on the size of the input data, this algorithm can be much more\nmemory efficient than a PCA, and allows sparse input.\n\nThis algorithm has constant memory complexity, on the order\nof ``batch_size * n_features``, enabling use of np.memmap files without\nloading the entire file into memory. For sparse matrices, the input\nis converted to dense in batches (in order to be able to subtract the\nmean) which avoids storing the entire dense matrix at any one time.\n\nThe computational overhead of each SVD is\n``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\nremain in memory at a time. There will be ``n_samples / batch_size`` SVD\ncomputations to get the principal components, versus 1 large SVD of\ncomplexity ``O(n_samples * n_features ** 2)`` for PCA.\n\nRead more in the :ref:`User Guide <IncrementalPCA>`.\n\n.. versionadded:: 0.16\n\nParameters\n----------\nn_components : int, default=None\n    Number of components to keep. If ``n_components`` is ``None``,\n    then ``n_components`` is set to ``min(n_samples, n_features)``.\n\nwhiten : bool, default=False\n    When True (False by default) the ``components_`` vectors are divided\n    by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n    with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometimes\n    improve the predictive accuracy of the downstream estimators by\n    making data respect some hard-wired assumptions.\n\ncopy : bool, default=True\n    If False, X will be overwritten. ``copy=False`` can be used to\n    save memory but is unsafe for general use.\n\nbatch_size : int, default=None\n    The number of samples to use for each batch. Only used when calling\n    ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n    is inferred from the data and set to ``5 * n_features``, to provide a\n    balance between approximation accuracy and memory consumption.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Components with maximum variance.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    Variance explained by each of the selected components.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n    If all components are stored, the sum of explained variances is equal\n    to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\nvar_ : ndarray of shape (n_features,)\n    Per-feature empirical variance, aggregate over calls to\n    ``partial_fit``.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf.\n\nn_components_ : int\n    The estimated number of components. Relevant when\n    ``n_components=None``.\n\nn_samples_seen_ : int\n    The number of samples processed by the estimator. Will be reset on\n    new calls to fit, but increments across ``partial_fit`` calls.\n\nbatch_size_ : int\n    Inferred batch size from ``batch_size``.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import IncrementalPCA\n>>> from scipy import sparse\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n>>> # either partially fit on smaller batches of data\n>>> transformer.partial_fit(X[:100, :])\nIncrementalPCA(batch_size=200, n_components=7)\n>>> # or let the fit function itself divide the data into batches\n>>> X_sparse = sparse.csr_matrix(X)\n>>> X_transformed = transformer.fit_transform(X_sparse)\n>>> X_transformed.shape\n(1797, 7)\n\nNotes\n-----\nImplements the incremental PCA model from:\n*D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\npp. 125-141, May 2008.*\nSee https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\nThis model is an extension of the Sequential Karhunen-Loeve Transform from:\n*A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\nits Application to Images, IEEE Transactions on Image Processing, Volume 9,\nNumber 8, pp. 1371-1374, August 2000.*\nSee https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf\n\nWe have specifically abstained from an optimization used by authors of both\npapers, a QR decomposition used in specific situations to reduce the\nalgorithmic complexity of the SVD. The source for this technique is\n*Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\nsection 5.4.4, pp 252-253.*. This technique has been omitted because it is\nadvantageous only when decomposing a matrix with ``n_samples`` (rows)\n>= 5/3 * ``n_features`` (columns), and hurts the readability of the\nimplemented algorithm. This would be a good opportunity for future\noptimization, if it is deemed necessary.\n\nReferences\n----------\nD. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\nTracking, International Journal of Computer Vision, Volume 77,\nIssue 1-3, pp. 125-141, May 2008.\n\nG. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\nSection 5.4.4, pp. 252-253.\n\nSee Also\n--------\nPCA\nKernelPCA\nSparsePCA\nTruncatedSVD",
      "code": "class IncrementalPCA(_BasePCA):\n    \"\"\"Incremental principal components analysis (IPCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of\n    the data, keeping only the most significant singular vectors to\n    project the data to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    Depending on the size of the input data, this algorithm can be much more\n    memory efficient than a PCA, and allows sparse input.\n\n    This algorithm has constant memory complexity, on the order\n    of ``batch_size * n_features``, enabling use of np.memmap files without\n    loading the entire file into memory. For sparse matrices, the input\n    is converted to dense in batches (in order to be able to subtract the\n    mean) which avoids storing the entire dense matrix at any one time.\n\n    The computational overhead of each SVD is\n    ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\n    remain in memory at a time. There will be ``n_samples / batch_size`` SVD\n    computations to get the principal components, versus 1 large SVD of\n    complexity ``O(n_samples * n_features ** 2)`` for PCA.\n\n    Read more in the :ref:`User Guide <IncrementalPCA>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components to keep. If ``n_components`` is ``None``,\n        then ``n_components`` is set to ``min(n_samples, n_features)``.\n\n    whiten : bool, default=False\n        When True (False by default) the ``components_`` vectors are divided\n        by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n        with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometimes\n        improve the predictive accuracy of the downstream estimators by\n        making data respect some hard-wired assumptions.\n\n    copy : bool, default=True\n        If False, X will be overwritten. ``copy=False`` can be used to\n        save memory but is unsafe for general use.\n\n    batch_size : int, default=None\n        The number of samples to use for each batch. Only used when calling\n        ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n        is inferred from the data and set to ``5 * n_features``, to provide a\n        balance between approximation accuracy and memory consumption.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        Variance explained by each of the selected components.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n        If all components are stored, the sum of explained variances is equal\n        to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\n    var_ : ndarray of shape (n_features,)\n        Per-feature empirical variance, aggregate over calls to\n        ``partial_fit``.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf.\n\n    n_components_ : int\n        The estimated number of components. Relevant when\n        ``n_components=None``.\n\n    n_samples_seen_ : int\n        The number of samples processed by the estimator. Will be reset on\n        new calls to fit, but increments across ``partial_fit`` calls.\n\n    batch_size_ : int\n        Inferred batch size from ``batch_size``.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import IncrementalPCA\n    >>> from scipy import sparse\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n    >>> # either partially fit on smaller batches of data\n    >>> transformer.partial_fit(X[:100, :])\n    IncrementalPCA(batch_size=200, n_components=7)\n    >>> # or let the fit function itself divide the data into batches\n    >>> X_sparse = sparse.csr_matrix(X)\n    >>> X_transformed = transformer.fit_transform(X_sparse)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    Notes\n    -----\n    Implements the incremental PCA model from:\n    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n    pp. 125-141, May 2008.*\n    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\n    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n    *A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n    Number 8, pp. 1371-1374, August 2000.*\n    See https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf\n\n    We have specifically abstained from an optimization used by authors of both\n    papers, a QR decomposition used in specific situations to reduce the\n    algorithmic complexity of the SVD. The source for this technique is\n    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n    implemented algorithm. This would be a good opportunity for future\n    optimization, if it is deemed necessary.\n\n    References\n    ----------\n    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77,\n    Issue 1-3, pp. 125-141, May 2008.\n\n    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n    Section 5.4.4, pp. 252-253.\n\n    See Also\n    --------\n    PCA\n    KernelPCA\n    SparsePCA\n    TruncatedSVD\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, whiten=False, copy=True,\n                 batch_size=None):\n        self.n_components = n_components\n        self.whiten = whiten\n        self.copy = copy\n        self.batch_size = batch_size\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model with X, using minibatches of size batch_size.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self.components_ = None\n        self.n_samples_seen_ = 0\n        self.mean_ = .0\n        self.var_ = .0\n        self.singular_values_ = None\n        self.explained_variance_ = None\n        self.explained_variance_ratio_ = None\n        self.noise_variance_ = None\n\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'lil'],\n                                copy=self.copy, dtype=[np.float64, np.float32])\n        n_samples, n_features = X.shape\n\n        if self.batch_size is None:\n            self.batch_size_ = 5 * n_features\n        else:\n            self.batch_size_ = self.batch_size\n\n        for batch in gen_batches(n_samples, self.batch_size_,\n                                 min_batch_size=self.n_components or 0):\n            X_batch = X[batch]\n            if sparse.issparse(X_batch):\n                X_batch = X_batch.toarray()\n            self.partial_fit(X_batch, check_input=False)\n\n        return self\n\n    def partial_fit(self, X, y=None, check_input=True):\n        \"\"\"Incremental fit with X. All of X is processed as a single batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        check_input : bool, default=True\n            Run check_array on X.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        first_pass = not hasattr(self, \"components_\")\n        if check_input:\n            if sparse.issparse(X):\n                raise TypeError(\n                    \"IncrementalPCA.partial_fit does not support \"\n                    \"sparse input. Either convert data to dense \"\n                    \"or use IncrementalPCA.fit to do so in batches.\")\n            X = self._validate_data(\n                X, copy=self.copy, dtype=[np.float64, np.float32],\n                reset=first_pass)\n        n_samples, n_features = X.shape\n        if first_pass:\n            self.components_ = None\n\n        if self.n_components is None:\n            if self.components_ is None:\n                self.n_components_ = min(n_samples, n_features)\n            else:\n                self.n_components_ = self.components_.shape[0]\n        elif not 1 <= self.n_components <= n_features:\n            raise ValueError(\"n_components=%r invalid for n_features=%d, need \"\n                             \"more rows than columns for IncrementalPCA \"\n                             \"processing\" % (self.n_components, n_features))\n        elif not self.n_components <= n_samples:\n            raise ValueError(\"n_components=%r must be less or equal to \"\n                             \"the batch number of samples \"\n                             \"%d.\" % (self.n_components, n_samples))\n        else:\n            self.n_components_ = self.n_components\n\n        if (self.components_ is not None) and (self.components_.shape[0] !=\n                                               self.n_components_):\n            raise ValueError(\"Number of input features has changed from %i \"\n                             \"to %i between calls to partial_fit! Try \"\n                             \"setting n_components to a fixed value.\" %\n                             (self.components_.shape[0], self.n_components_))\n\n        # This is the first partial_fit\n        if not hasattr(self, 'n_samples_seen_'):\n            self.n_samples_seen_ = 0\n            self.mean_ = .0\n            self.var_ = .0\n\n        # Update stats - they are 0 if this is the first step\n        col_mean, col_var, n_total_samples = \\\n            _incremental_mean_and_var(\n                X, last_mean=self.mean_, last_variance=self.var_,\n                last_sample_count=np.repeat(self.n_samples_seen_, X.shape[1]))\n        n_total_samples = n_total_samples[0]\n\n        # Whitening\n        if self.n_samples_seen_ == 0:\n            # If it is the first step, simply whiten X\n            X -= col_mean\n        else:\n            col_batch_mean = np.mean(X, axis=0)\n            X -= col_batch_mean\n            # Build matrix of combined previous basis and new data\n            mean_correction = \\\n                np.sqrt((self.n_samples_seen_ / n_total_samples) *\n                        n_samples) * (self.mean_ - col_batch_mean)\n            X = np.vstack((self.singular_values_.reshape((-1, 1)) *\n                           self.components_, X, mean_correction))\n\n        U, S, Vt = linalg.svd(X, full_matrices=False, check_finite=False)\n        U, Vt = svd_flip(U, Vt, u_based_decision=False)\n        explained_variance = S ** 2 / (n_total_samples - 1)\n        explained_variance_ratio = S ** 2 / np.sum(col_var * n_total_samples)\n\n        self.n_samples_seen_ = n_total_samples\n        self.components_ = Vt[:self.n_components_]\n        self.singular_values_ = S[:self.n_components_]\n        self.mean_ = col_mean\n        self.var_ = col_var\n        self.explained_variance_ = explained_variance[:self.n_components_]\n        self.explained_variance_ratio_ = \\\n            explained_variance_ratio[:self.n_components_]\n        if self.n_components_ < n_features:\n            self.noise_variance_ = \\\n                explained_variance[self.n_components_:].mean()\n        else:\n            self.noise_variance_ = 0.\n        return self\n\n    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set, using minibatches of size batch_size if X is\n        sparse.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> from sklearn.decomposition import IncrementalPCA\n        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n        ...               [1, 1], [2, 1], [3, 2]])\n        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n        >>> ipca.fit(X)\n        IncrementalPCA(batch_size=3, n_components=2)\n        >>> ipca.transform(X) # doctest: +SKIP\n        \"\"\"\n        if sparse.issparse(X):\n            n_samples = X.shape[0]\n            output = []\n            for batch in gen_batches(n_samples, self.batch_size_,\n                                     min_batch_size=self.n_components or 0):\n                output.append(super().transform(X[batch].toarray()))\n            return np.vstack(output)\n        else:\n            return super().transform(X)",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "whiten",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "batch_size",
          "types": null
        },
        {
          "name": "components_",
          "types": null
        },
        {
          "name": "n_samples_seen_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "mean_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "var_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "singular_values_",
          "types": null
        },
        {
          "name": "explained_variance_",
          "types": null
        },
        {
          "name": "explained_variance_ratio_",
          "types": null
        },
        {
          "name": "noise_variance_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "batch_size_",
          "types": null
        },
        {
          "name": "n_components_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA",
      "name": "KernelPCA",
      "qname": "sklearn.decomposition._kernel_pca.KernelPCA",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/_pairwise@getter",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/_get_kernel",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/_fit_transform",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/_fit_inverse_transform",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit_transform",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/transform",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/inverse_transform",
        "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Kernel Principal component analysis (KPCA).\n\nNon-linear dimensionality reduction through the use of kernels (see\n:ref:`metrics`).\n\nRead more in the :ref:`User Guide <kernel_PCA>`.",
      "docstring": "Kernel Principal component analysis (KPCA).\n\nNon-linear dimensionality reduction through the use of kernels (see\n:ref:`metrics`).\n\nRead more in the :ref:`User Guide <kernel_PCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components. If None, all non-zero components are kept.\n\nkernel : {'linear', 'poly',             'rbf', 'sigmoid', 'cosine', 'precomputed'}, default='linear'\n    Kernel used for PCA.\n\ngamma : float, default=None\n    Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n    kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\ndegree : int, default=3\n    Degree for poly kernels. Ignored by other kernels.\n\ncoef0 : float, default=1\n    Independent term in poly and sigmoid kernels.\n    Ignored by other kernels.\n\nkernel_params : dict, default=None\n    Parameters (keyword arguments) and\n    values for kernel passed as callable object.\n    Ignored by other kernels.\n\nalpha : float, default=1.0\n    Hyperparameter of the ridge regression that learns the\n    inverse transform (when fit_inverse_transform=True).\n\nfit_inverse_transform : bool, default=False\n    Learn the inverse transform for non-precomputed kernels.\n    (i.e. learn to find the pre-image of a point)\n\neigen_solver : {'auto', 'dense', 'arpack'}, default='auto'\n    Select eigensolver to use. If n_components is much less than\n    the number of training samples, arpack may be more efficient\n    than the dense eigensolver.\n\ntol : float, default=0\n    Convergence tolerance for arpack.\n    If 0, optimal value will be chosen by arpack.\n\nmax_iter : int, default=None\n    Maximum number of iterations for arpack.\n    If None, optimal value will be chosen by arpack.\n\nremove_zero_eig : bool, default=False\n    If True, then all components with zero eigenvalues are removed, so\n    that the number of components in the output may be < n_components\n    (and sometimes even zero due to numerical instability).\n    When n_components is None, this parameter is ignored and components\n    with zero eigenvalues are removed regardless.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18\n\ncopy_X : bool, default=True\n    If True, input X is copied and stored by the model in the `X_fit_`\n    attribute. If no further changes will be done to X, setting\n    `copy_X=False` saves memory by storing a reference.\n\n    .. versionadded:: 0.18\n\nn_jobs : int, default=None\n    The number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nAttributes\n----------\nlambdas_ : ndarray of shape (n_components,)\n    Eigenvalues of the centered kernel matrix in decreasing order.\n    If `n_components` and `remove_zero_eig` are not set,\n    then all values are stored.\n\nalphas_ : ndarray of shape (n_samples, n_components)\n    Eigenvectors of the centered kernel matrix. If `n_components` and\n    `remove_zero_eig` are not set, then all components are stored.\n\ndual_coef_ : ndarray of shape (n_samples, n_features)\n    Inverse transform matrix. Only available when\n    ``fit_inverse_transform`` is True.\n\nX_transformed_fit_ : ndarray of shape (n_samples, n_components)\n    Projection of the fitted data on the kernel principal components.\n    Only available when ``fit_inverse_transform`` is True.\n\nX_fit_ : ndarray of shape (n_samples, n_features)\n    The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n    a reference. This attribute is used for the calls to transform.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.decomposition import KernelPCA\n>>> X, _ = load_digits(return_X_y=True)\n>>> transformer = KernelPCA(n_components=7, kernel='linear')\n>>> X_transformed = transformer.fit_transform(X)\n>>> X_transformed.shape\n(1797, 7)\n\nReferences\n----------\nKernel PCA was introduced in:\n    Bernhard Schoelkopf, Alexander J. Smola,\n    and Klaus-Robert Mueller. 1999. Kernel principal\n    component analysis. In Advances in kernel methods,\n    MIT Press, Cambridge, MA, USA 327-352.",
      "code": "class KernelPCA(TransformerMixin, BaseEstimator):\n    \"\"\"Kernel Principal component analysis (KPCA).\n\n    Non-linear dimensionality reduction through the use of kernels (see\n    :ref:`metrics`).\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : {'linear', 'poly', \\\n            'rbf', 'sigmoid', 'cosine', 'precomputed'}, default='linear'\n        Kernel used for PCA.\n\n    gamma : float, default=None\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\n    degree : int, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Parameters (keyword arguments) and\n        values for kernel passed as callable object.\n        Ignored by other kernels.\n\n    alpha : float, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels.\n        (i.e. learn to find the pre-image of a point)\n\n    eigen_solver : {'auto', 'dense', 'arpack'}, default='auto'\n        Select eigensolver to use. If n_components is much less than\n        the number of training samples, arpack may be more efficient\n        than the dense eigensolver.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    remove_zero_eig : bool, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, default=None\n        Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18\n\n    copy_X : bool, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    lambdas_ : ndarray of shape (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    alphas_ : ndarray of shape (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : ndarray of shape (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : ndarray of shape (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : ndarray of shape (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    Kernel PCA was introduced in:\n        Bernhard Schoelkopf, Alexander J. Smola,\n        and Klaus-Robert Mueller. 1999. Kernel principal\n        component analysis. In Advances in kernel methods,\n        MIT Press, Cambridge, MA, USA 327-352.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, kernel=\"linear\",\n                 gamma=None, degree=3, coef0=1, kernel_params=None,\n                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',\n                 tol=0, max_iter=None, remove_zero_eig=False,\n                 random_state=None, copy_X=True, n_jobs=None):\n        if fit_inverse_transform and kernel == 'precomputed':\n            raise ValueError(\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.remove_zero_eig = remove_zero_eig\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute _pairwise was deprecated in \"  # type: ignore\n                \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def _pairwise(self):\n        return self.kernel == \"precomputed\"\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {\"gamma\": self.gamma,\n                      \"degree\": self.degree,\n                      \"coef0\": self.coef0}\n        return pairwise_kernels(X, Y, metric=self.kernel,\n                                filter_params=True, n_jobs=self.n_jobs,\n                                **params)\n\n    def _fit_transform(self, K):\n        \"\"\" Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            v0 = _init_arpack_v0(K.shape[0], self.random_state)\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # make sure that the eigenvalues are ok and fix numerical issues\n        self.lambdas_ = _check_psd_eigenvalues(self.lambdas_,\n                                               enable_warnings=False)\n\n        # flip eigenvectors' sign to enforce deterministic output\n        self.alphas_, _ = svd_flip(self.alphas_,\n                                   np.zeros_like(self.alphas_).T)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue (null space) if required\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        # Maintenance note on Eigenvectors normalization\n        # ----------------------------------------------\n        # there is a link between\n        # the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)'\n        # if v is an eigenvector of K\n        #     then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'\n        # if u is an eigenvector of Phi(X)Phi(X)'\n        #     then Phi(X)'u is an eigenvector of Phi(X)'Phi(X)\n        #\n        # At this stage our self.alphas_ (the v) have norm 1, we need to scale\n        # them so that eigenvectors in kernel feature space (the u) have norm=1\n        # instead\n        #\n        # We COULD scale them here:\n        #       self.alphas_ = self.alphas_ / np.sqrt(self.lambdas_)\n        #\n        # But choose to perform that LATER when needed, in `fit()` and in\n        # `transform()`.\n\n        return K\n\n    def _fit_inverse_transform(self, X_transformed, X):\n        if hasattr(X, \"tocsr\"):\n            raise NotImplementedError(\"Inverse transform not implemented for \"\n                                      \"sparse matrices!\")\n\n        n_samples = X_transformed.shape[0]\n        K = self._get_kernel(X_transformed)\n        K.flat[::n_samples + 1] += self.alpha\n        self.dual_coef_ = linalg.solve(K, X, sym_pos=True, overwrite_a=True)\n        self.X_transformed_fit_ = X_transformed\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr', copy=self.copy_X)\n        self._centerer = KernelCenterer()\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n\n        if self.fit_inverse_transform:\n            # no need to use the kernel to transform X, use shortcut expression\n            X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n            self._fit_inverse_transform(X_transformed, X)\n\n        self.X_fit_ = X\n        return self\n\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        # no need to use the kernel to transform X, use shortcut expression\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n\n        # Compute centered gram matrix between X and training data X_fit_\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n\n        # scale eigenvectors (properly account for null-space for dot product)\n        non_zeros = np.flatnonzero(self.lambdas_)\n        scaled_alphas = np.zeros_like(self.alphas_)\n        scaled_alphas[:, non_zeros] = (self.alphas_[:, non_zeros]\n                                       / np.sqrt(self.lambdas_[non_zeros]))\n\n        # Project with a scalar product between K and the scaled eigenvectors\n        return np.dot(K, scaled_alphas)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to original space.\n\n        ``inverse_transform`` approximates the inverse transformation using\n        a learned pre-image. The pre-image is learned by kernel ridge\n        regression of the original data on their low-dimensional representation\n        vectors.\n\n        .. note:\n            :meth:`~sklearn.decomposition.fit` internally uses a centered\n            kernel. As the centered kernel no longer contains the information\n            of the mean of kernel features, such information is not taken into\n            account in reconstruction.\n\n        .. note::\n            When users want to compute inverse transformation for 'linear'\n            kernel, it is recommended that they use\n            :class:`~sklearn.decomposition.PCA` instead. Unlike\n            :class:`~sklearn.decomposition.PCA`,\n            :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n            does not reconstruct the mean of data when 'linear' kernel is used\n            due to the use of centered kernel.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_components)\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_features)\n\n        References\n        ----------\n        \"Learning to Find Pre-Images\", G BakIr et al, 2004.\n        \"\"\"\n        if not self.fit_inverse_transform:\n            raise NotFittedError(\"The fit_inverse_transform parameter was not\"\n                                 \" set to True when instantiating and hence \"\n                                 \"the inverse transform is not available.\")\n\n        K = self._get_kernel(X, self.X_transformed_fit_)\n        return np.dot(K, self.dual_coef_)\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32],\n                'pairwise': self.kernel == 'precomputed'}",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "kernel",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "kernel_params",
          "types": null
        },
        {
          "name": "gamma",
          "types": null
        },
        {
          "name": "degree",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "coef0",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "fit_inverse_transform",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "eigen_solver",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "remove_zero_eig",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "max_iter",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "copy_X",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "lambdas_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "alphas_",
          "types": null
        },
        {
          "name": "dual_coef_",
          "types": null
        },
        {
          "name": "X_transformed_fit_",
          "types": null
        },
        {
          "name": "_centerer",
          "types": {
            "kind": "NamedType",
            "name": "KernelCenterer"
          }
        },
        {
          "name": "X_fit_",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation",
      "name": "LatentDirichletAllocation",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_check_params",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_init_latent_vars",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_e_step",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_em_step",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_more_tags",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_check_non_neg_array",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/partial_fit",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/fit",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_unnormalized_transform",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/transform",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_approx_bound",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/score",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/_perplexity_precomp_distr",
        "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/perplexity"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Latent Dirichlet Allocation with online variational Bayes algorithm\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <LatentDirichletAllocation>`.",
      "docstring": "Latent Dirichlet Allocation with online variational Bayes algorithm\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\nParameters\n----------\nn_components : int, default=10\n    Number of topics.\n\n    .. versionchanged:: 0.19\n        ``n_topics`` was renamed to ``n_components``\n\ndoc_topic_prior : float, default=None\n    Prior of document topic distribution `theta`. If the value is None,\n    defaults to `1 / n_components`.\n    In [1]_, this is called `alpha`.\n\ntopic_word_prior : float, default=None\n    Prior of topic word distribution `beta`. If the value is None, defaults\n    to `1 / n_components`.\n    In [1]_, this is called `eta`.\n\nlearning_method : {'batch', 'online'}, default='batch'\n    Method used to update `_component`. Only used in :meth:`fit` method.\n    In general, if the data size is large, the online update will be much\n    faster than the batch update.\n\n    Valid options::\n\n        'batch': Batch variational Bayes method. Use all training data in\n            each EM update.\n            Old `components_` will be overwritten in each iteration.\n        'online': Online variational Bayes method. In each EM update, use\n            mini-batch of training data to update the ``components_``\n            variable incrementally. The learning rate is controlled by the\n            ``learning_decay`` and the ``learning_offset`` parameters.\n\n    .. versionchanged:: 0.20\n        The default learning method is now ``\"batch\"``.\n\nlearning_decay : float, default=0.7\n    It is a parameter that control learning rate in the online learning\n    method. The value should be set between (0.5, 1.0] to guarantee\n    asymptotic convergence. When the value is 0.0 and batch_size is\n    ``n_samples``, the update method is same as batch learning. In the\n    literature, this is called kappa.\n\nlearning_offset : float, default=10.\n    A (positive) parameter that downweights early iterations in online\n    learning.  It should be greater than 1.0. In the literature, this is\n    called tau_0.\n\nmax_iter : int, default=10\n    The maximum number of iterations.\n\nbatch_size : int, default=128\n    Number of documents to use in each EM iteration. Only used in online\n    learning.\n\nevaluate_every : int, default=-1\n    How often to evaluate perplexity. Only used in `fit` method.\n    set it to 0 or negative number to not evaluate perplexity in\n    training at all. Evaluating perplexity can help you check convergence\n    in training process, but it will also increase total training time.\n    Evaluating perplexity in every iteration might increase training time\n    up to two-fold.\n\ntotal_samples : int, default=1e6\n    Total number of documents. Only used in the :meth:`partial_fit` method.\n\nperp_tol : float, default=1e-1\n    Perplexity tolerance in batch learning. Only used when\n    ``evaluate_every`` is greater than 0.\n\nmean_change_tol : float, default=1e-3\n    Stopping tolerance for updating document topic distribution in E-step.\n\nmax_doc_update_iter : int, default=100\n    Max number of iterations for updating document topic distribution in\n    the E-step.\n\nn_jobs : int, default=None\n    The number of jobs to use in the E-step.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : int, default=0\n    Verbosity level.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Variational parameters for topic word distribution. Since the complete\n    conditional for topic word distribution is a Dirichlet,\n    ``components_[i, j]`` can be viewed as pseudocount that represents the\n    number of times word `j` was assigned to topic `i`.\n    It can also be viewed as distribution over the words for each topic\n    after normalization:\n    ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\nexp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n    Exponential value of expectation of log topic word distribution.\n    In the literature, this is `exp(E[log(beta)])`.\n\nn_batch_iter_ : int\n    Number of iterations of the EM step.\n\nn_iter_ : int\n    Number of passes over the dataset.\n\nbound_ : float\n    Final perplexity score on training set.\n\ndoc_topic_prior_ : float\n    Prior of document topic distribution `theta`. If the value is None,\n    it is `1 / n_components`.\n\nrandom_state_ : RandomState instance\n    RandomState instance that is generated either from a seed, the random\n    number generator or by `np.random`.\n\ntopic_word_prior_ : float\n    Prior of topic word distribution `beta`. If the value is None, it is\n    `1 / n_components`.\n\nExamples\n--------\n>>> from sklearn.decomposition import LatentDirichletAllocation\n>>> from sklearn.datasets import make_multilabel_classification\n>>> # This produces a feature matrix of token counts, similar to what\n>>> # CountVectorizer would produce on text.\n>>> X, _ = make_multilabel_classification(random_state=0)\n>>> lda = LatentDirichletAllocation(n_components=5,\n...     random_state=0)\n>>> lda.fit(X)\nLatentDirichletAllocation(...)\n>>> # get topics for some given samples:\n>>> lda.transform(X[-2:])\narray([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n\nReferences\n----------\n.. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n    Hoffman, David M. Blei, Francis Bach, 2010\n\n[2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n    Chong Wang, John Paisley, 2013\n\n[3] Matthew D. Hoffman's onlineldavb code. Link:\n    https://github.com/blei-lab/onlineldavb",
      "code": "class LatentDirichletAllocation(TransformerMixin, BaseEstimator):\n    \"\"\"Latent Dirichlet Allocation with online variational Bayes algorithm\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\n    Parameters\n    ----------\n    n_components : int, default=10\n        Number of topics.\n\n        .. versionchanged:: 0.19\n            ``n_topics`` was renamed to ``n_components``\n\n    doc_topic_prior : float, default=None\n        Prior of document topic distribution `theta`. If the value is None,\n        defaults to `1 / n_components`.\n        In [1]_, this is called `alpha`.\n\n    topic_word_prior : float, default=None\n        Prior of topic word distribution `beta`. If the value is None, defaults\n        to `1 / n_components`.\n        In [1]_, this is called `eta`.\n\n    learning_method : {'batch', 'online'}, default='batch'\n        Method used to update `_component`. Only used in :meth:`fit` method.\n        In general, if the data size is large, the online update will be much\n        faster than the batch update.\n\n        Valid options::\n\n            'batch': Batch variational Bayes method. Use all training data in\n                each EM update.\n                Old `components_` will be overwritten in each iteration.\n            'online': Online variational Bayes method. In each EM update, use\n                mini-batch of training data to update the ``components_``\n                variable incrementally. The learning rate is controlled by the\n                ``learning_decay`` and the ``learning_offset`` parameters.\n\n        .. versionchanged:: 0.20\n            The default learning method is now ``\"batch\"``.\n\n    learning_decay : float, default=0.7\n        It is a parameter that control learning rate in the online learning\n        method. The value should be set between (0.5, 1.0] to guarantee\n        asymptotic convergence. When the value is 0.0 and batch_size is\n        ``n_samples``, the update method is same as batch learning. In the\n        literature, this is called kappa.\n\n    learning_offset : float, default=10.\n        A (positive) parameter that downweights early iterations in online\n        learning.  It should be greater than 1.0. In the literature, this is\n        called tau_0.\n\n    max_iter : int, default=10\n        The maximum number of iterations.\n\n    batch_size : int, default=128\n        Number of documents to use in each EM iteration. Only used in online\n        learning.\n\n    evaluate_every : int, default=-1\n        How often to evaluate perplexity. Only used in `fit` method.\n        set it to 0 or negative number to not evaluate perplexity in\n        training at all. Evaluating perplexity can help you check convergence\n        in training process, but it will also increase total training time.\n        Evaluating perplexity in every iteration might increase training time\n        up to two-fold.\n\n    total_samples : int, default=1e6\n        Total number of documents. Only used in the :meth:`partial_fit` method.\n\n    perp_tol : float, default=1e-1\n        Perplexity tolerance in batch learning. Only used when\n        ``evaluate_every`` is greater than 0.\n\n    mean_change_tol : float, default=1e-3\n        Stopping tolerance for updating document topic distribution in E-step.\n\n    max_doc_update_iter : int, default=100\n        Max number of iterations for updating document topic distribution in\n        the E-step.\n\n    n_jobs : int, default=None\n        The number of jobs to use in the E-step.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    random_state : int, RandomState instance or None, default=None\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Variational parameters for topic word distribution. Since the complete\n        conditional for topic word distribution is a Dirichlet,\n        ``components_[i, j]`` can be viewed as pseudocount that represents the\n        number of times word `j` was assigned to topic `i`.\n        It can also be viewed as distribution over the words for each topic\n        after normalization:\n        ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\n    exp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n        Exponential value of expectation of log topic word distribution.\n        In the literature, this is `exp(E[log(beta)])`.\n\n    n_batch_iter_ : int\n        Number of iterations of the EM step.\n\n    n_iter_ : int\n        Number of passes over the dataset.\n\n    bound_ : float\n        Final perplexity score on training set.\n\n    doc_topic_prior_ : float\n        Prior of document topic distribution `theta`. If the value is None,\n        it is `1 / n_components`.\n\n    random_state_ : RandomState instance\n        RandomState instance that is generated either from a seed, the random\n        number generator or by `np.random`.\n\n    topic_word_prior_ : float\n        Prior of topic word distribution `beta`. If the value is None, it is\n        `1 / n_components`.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import LatentDirichletAllocation\n    >>> from sklearn.datasets import make_multilabel_classification\n    >>> # This produces a feature matrix of token counts, similar to what\n    >>> # CountVectorizer would produce on text.\n    >>> X, _ = make_multilabel_classification(random_state=0)\n    >>> lda = LatentDirichletAllocation(n_components=5,\n    ...     random_state=0)\n    >>> lda.fit(X)\n    LatentDirichletAllocation(...)\n    >>> # get topics for some given samples:\n    >>> lda.transform(X[-2:])\n    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n\n    References\n    ----------\n    .. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n        Hoffman, David M. Blei, Francis Bach, 2010\n\n    [2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n        Chong Wang, John Paisley, 2013\n\n    [3] Matthew D. Hoffman's onlineldavb code. Link:\n        https://github.com/blei-lab/onlineldavb\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=10, *, doc_topic_prior=None,\n                 topic_word_prior=None, learning_method='batch',\n                 learning_decay=.7, learning_offset=10., max_iter=10,\n                 batch_size=128, evaluate_every=-1, total_samples=1e6,\n                 perp_tol=1e-1, mean_change_tol=1e-3, max_doc_update_iter=100,\n                 n_jobs=None, verbose=0, random_state=None):\n        self.n_components = n_components\n        self.doc_topic_prior = doc_topic_prior\n        self.topic_word_prior = topic_word_prior\n        self.learning_method = learning_method\n        self.learning_decay = learning_decay\n        self.learning_offset = learning_offset\n        self.max_iter = max_iter\n        self.batch_size = batch_size\n        self.evaluate_every = evaluate_every\n        self.total_samples = total_samples\n        self.perp_tol = perp_tol\n        self.mean_change_tol = mean_change_tol\n        self.max_doc_update_iter = max_doc_update_iter\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def _check_params(self):\n        \"\"\"Check model parameters.\"\"\"\n        if self.n_components <= 0:\n            raise ValueError(\"Invalid 'n_components' parameter: %r\"\n                             % self.n_components)\n\n        if self.total_samples <= 0:\n            raise ValueError(\"Invalid 'total_samples' parameter: %r\"\n                             % self.total_samples)\n\n        if self.learning_offset < 0:\n            raise ValueError(\"Invalid 'learning_offset' parameter: %r\"\n                             % self.learning_offset)\n\n        if self.learning_method not in (\"batch\", \"online\"):\n            raise ValueError(\"Invalid 'learning_method' parameter: %r\"\n                             % self.learning_method)\n\n    def _init_latent_vars(self, n_features):\n        \"\"\"Initialize latent variables.\"\"\"\n\n        self.random_state_ = check_random_state(self.random_state)\n        self.n_batch_iter_ = 1\n        self.n_iter_ = 0\n\n        if self.doc_topic_prior is None:\n            self.doc_topic_prior_ = 1. / self.n_components\n        else:\n            self.doc_topic_prior_ = self.doc_topic_prior\n\n        if self.topic_word_prior is None:\n            self.topic_word_prior_ = 1. / self.n_components\n        else:\n            self.topic_word_prior_ = self.topic_word_prior\n\n        init_gamma = 100.\n        init_var = 1. / init_gamma\n        # In the literature, this is called `lambda`\n        self.components_ = self.random_state_.gamma(\n            init_gamma, init_var, (self.n_components, n_features))\n\n        # In the literature, this is `exp(E[log(beta)])`\n        self.exp_dirichlet_component_ = np.exp(\n            _dirichlet_expectation_2d(self.components_))\n\n    def _e_step(self, X, cal_sstats, random_init, parallel=None):\n        \"\"\"E-step in EM update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        cal_sstats : bool\n            Parameter that indicate whether to calculate sufficient statistics\n            or not. Set ``cal_sstats`` to True when we need to run M-step.\n\n        random_init : bool\n            Parameter that indicate whether to initialize document topic\n            distribution randomly in the E-step. Set it to True in training\n            steps.\n\n        parallel : joblib.Parallel, default=None\n            Pre-initialized instance of joblib.Parallel.\n\n        Returns\n        -------\n        (doc_topic_distr, suff_stats) :\n            `doc_topic_distr` is unnormalized topic distribution for each\n            document. In the literature, this is called `gamma`.\n            `suff_stats` is expected sufficient statistics for the M-step.\n            When `cal_sstats == False`, it will be None.\n\n        \"\"\"\n\n        # Run e-step in parallel\n        random_state = self.random_state_ if random_init else None\n\n        # TODO: make Parallel._effective_n_jobs public instead?\n        n_jobs = effective_n_jobs(self.n_jobs)\n        if parallel is None:\n            parallel = Parallel(n_jobs=n_jobs, verbose=max(0,\n                                                           self.verbose - 1))\n        results = parallel(\n            delayed(_update_doc_distribution)(X[idx_slice, :],\n                                              self.exp_dirichlet_component_,\n                                              self.doc_topic_prior_,\n                                              self.max_doc_update_iter,\n                                              self.mean_change_tol, cal_sstats,\n                                              random_state)\n            for idx_slice in gen_even_slices(X.shape[0], n_jobs))\n\n        # merge result\n        doc_topics, sstats_list = zip(*results)\n        doc_topic_distr = np.vstack(doc_topics)\n\n        if cal_sstats:\n            # This step finishes computing the sufficient statistics for the\n            # M-step.\n            suff_stats = np.zeros(self.components_.shape)\n            for sstats in sstats_list:\n                suff_stats += sstats\n            suff_stats *= self.exp_dirichlet_component_\n        else:\n            suff_stats = None\n\n        return (doc_topic_distr, suff_stats)\n\n    def _em_step(self, X, total_samples, batch_update, parallel=None):\n        \"\"\"EM update for 1 iteration.\n\n        update `_component` by batch VB or online VB.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        total_samples : int\n            Total number of documents. It is only used when\n            batch_update is `False`.\n\n        batch_update : bool\n            Parameter that controls updating method.\n            `True` for batch learning, `False` for online learning.\n\n        parallel : joblib.Parallel, default=None\n            Pre-initialized instance of joblib.Parallel\n\n        Returns\n        -------\n        doc_topic_distr : ndarray of shape (n_samples, n_components)\n            Unnormalized document topic distribution.\n        \"\"\"\n\n        # E-step\n        _, suff_stats = self._e_step(X, cal_sstats=True, random_init=True,\n                                     parallel=parallel)\n\n        # M-step\n        if batch_update:\n            self.components_ = self.topic_word_prior_ + suff_stats\n        else:\n            # online update\n            # In the literature, the weight is `rho`\n            weight = np.power(self.learning_offset + self.n_batch_iter_,\n                              -self.learning_decay)\n            doc_ratio = float(total_samples) / X.shape[0]\n            self.components_ *= (1 - weight)\n            self.components_ += (weight * (self.topic_word_prior_\n                                           + doc_ratio * suff_stats))\n\n        # update `component_` related variables\n        self.exp_dirichlet_component_ = np.exp(\n            _dirichlet_expectation_2d(self.components_))\n        self.n_batch_iter_ += 1\n        return\n\n    def _more_tags(self):\n        return {'requires_positive_X': True}\n\n    def _check_non_neg_array(self, X, reset_n_features, whom):\n        \"\"\"check X format\n\n        check X format and make sure no negative value in X.\n\n        Parameters\n        ----------\n        X :  array-like or sparse matrix\n\n        \"\"\"\n        X = self._validate_data(X, reset=reset_n_features,\n                                accept_sparse='csr')\n        check_non_negative(X, whom)\n        return X\n\n    def partial_fit(self, X, y=None):\n        \"\"\"Online VB with Mini-Batch update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._check_params()\n        first_time = not hasattr(self, 'components_')\n        X = self._check_non_neg_array(\n            X, reset_n_features=first_time,\n            whom=\"LatentDirichletAllocation.partial_fit\")\n        n_samples, n_features = X.shape\n        batch_size = self.batch_size\n\n        # initialize parameters or check\n        if first_time:\n            self._init_latent_vars(n_features)\n\n        if n_features != self.components_.shape[1]:\n            raise ValueError(\n                \"The provided data has %d dimensions while \"\n                \"the model was trained with feature size %d.\" %\n                (n_features, self.components_.shape[1]))\n\n        n_jobs = effective_n_jobs(self.n_jobs)\n        with Parallel(n_jobs=n_jobs,\n                      verbose=max(0, self.verbose - 1)) as parallel:\n            for idx_slice in gen_batches(n_samples, batch_size):\n                self._em_step(X[idx_slice, :],\n                              total_samples=self.total_samples,\n                              batch_update=False,\n                              parallel=parallel)\n\n        return self\n\n    def fit(self, X, y=None):\n        \"\"\"Learn model for the data X with variational Bayes method.\n\n        When `learning_method` is 'online', use mini-batch update.\n        Otherwise, use batch update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._check_params()\n        X = self._check_non_neg_array(X, reset_n_features=True,\n                                      whom=\"LatentDirichletAllocation.fit\")\n        n_samples, n_features = X.shape\n        max_iter = self.max_iter\n        evaluate_every = self.evaluate_every\n        learning_method = self.learning_method\n\n        batch_size = self.batch_size\n\n        # initialize parameters\n        self._init_latent_vars(n_features)\n        # change to perplexity later\n        last_bound = None\n        n_jobs = effective_n_jobs(self.n_jobs)\n        with Parallel(n_jobs=n_jobs,\n                      verbose=max(0, self.verbose - 1)) as parallel:\n            for i in range(max_iter):\n                if learning_method == 'online':\n                    for idx_slice in gen_batches(n_samples, batch_size):\n                        self._em_step(X[idx_slice, :], total_samples=n_samples,\n                                      batch_update=False, parallel=parallel)\n                else:\n                    # batch update\n                    self._em_step(X, total_samples=n_samples,\n                                  batch_update=True, parallel=parallel)\n\n                # check perplexity\n                if evaluate_every > 0 and (i + 1) % evaluate_every == 0:\n                    doc_topics_distr, _ = self._e_step(X, cal_sstats=False,\n                                                       random_init=False,\n                                                       parallel=parallel)\n                    bound = self._perplexity_precomp_distr(X, doc_topics_distr,\n                                                           sub_sampling=False)\n                    if self.verbose:\n                        print('iteration: %d of max_iter: %d, perplexity: %.4f'\n                              % (i + 1, max_iter, bound))\n\n                    if last_bound and abs(last_bound - bound) < self.perp_tol:\n                        break\n                    last_bound = bound\n\n                elif self.verbose:\n                    print('iteration: %d of max_iter: %d' % (i + 1, max_iter))\n                self.n_iter_ += 1\n\n        # calculate final perplexity value on train set\n        doc_topics_distr, _ = self._e_step(X, cal_sstats=False,\n                                           random_init=False,\n                                           parallel=parallel)\n        self.bound_ = self._perplexity_precomp_distr(X, doc_topics_distr,\n                                                     sub_sampling=False)\n\n        return self\n\n    def _unnormalized_transform(self, X):\n        \"\"\"Transform data X according to fitted model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        Returns\n        -------\n        doc_topic_distr : ndarray of shape (n_samples, n_components)\n            Document topic distribution for X.\n        \"\"\"\n        check_is_fitted(self)\n\n        # make sure feature size is the same in fitted model and in X\n        X = self._check_non_neg_array(\n            X, reset_n_features=True,\n            whom=\"LatentDirichletAllocation.transform\")\n        n_samples, n_features = X.shape\n        if n_features != self.components_.shape[1]:\n            raise ValueError(\n                \"The provided data has %d dimensions while \"\n                \"the model was trained with feature size %d.\" %\n                (n_features, self.components_.shape[1]))\n\n        doc_topic_distr, _ = self._e_step(X, cal_sstats=False,\n                                          random_init=False)\n\n        return doc_topic_distr\n\n    def transform(self, X):\n        \"\"\"Transform data X according to the fitted model.\n\n           .. versionchanged:: 0.18\n              *doc_topic_distr* is now normalized\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        Returns\n        -------\n        doc_topic_distr : ndarray of shape (n_samples, n_components)\n            Document topic distribution for X.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_non_neg_array(\n            X, reset_n_features=False,\n            whom=\"LatentDirichletAllocation.transform\")\n        doc_topic_distr = self._unnormalized_transform(X)\n        doc_topic_distr /= doc_topic_distr.sum(axis=1)[:, np.newaxis]\n        return doc_topic_distr\n\n    def _approx_bound(self, X, doc_topic_distr, sub_sampling):\n        \"\"\"Estimate the variational bound.\n\n        Estimate the variational bound over \"all documents\" using only the\n        documents passed in as X. Since log-likelihood of each word cannot\n        be computed directly, we use this bound to estimate it.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        doc_topic_distr : ndarray of shape (n_samples, n_components)\n            Document topic distribution. In the literature, this is called\n            gamma.\n\n        sub_sampling : bool, default=False\n            Compensate for subsampling of documents.\n            It is used in calculate bound in online learning.\n\n        Returns\n        -------\n        score : float\n\n        \"\"\"\n\n        def _loglikelihood(prior, distr, dirichlet_distr, size):\n            # calculate log-likelihood\n            score = np.sum((prior - distr) * dirichlet_distr)\n            score += np.sum(gammaln(distr) - gammaln(prior))\n            score += np.sum(gammaln(prior * size) - gammaln(np.sum(distr, 1)))\n            return score\n\n        is_sparse_x = sp.issparse(X)\n        n_samples, n_components = doc_topic_distr.shape\n        n_features = self.components_.shape[1]\n        score = 0\n\n        dirichlet_doc_topic = _dirichlet_expectation_2d(doc_topic_distr)\n        dirichlet_component_ = _dirichlet_expectation_2d(self.components_)\n        doc_topic_prior = self.doc_topic_prior_\n        topic_word_prior = self.topic_word_prior_\n\n        if is_sparse_x:\n            X_data = X.data\n            X_indices = X.indices\n            X_indptr = X.indptr\n\n        # E[log p(docs | theta, beta)]\n        for idx_d in range(0, n_samples):\n            if is_sparse_x:\n                ids = X_indices[X_indptr[idx_d]:X_indptr[idx_d + 1]]\n                cnts = X_data[X_indptr[idx_d]:X_indptr[idx_d + 1]]\n            else:\n                ids = np.nonzero(X[idx_d, :])[0]\n                cnts = X[idx_d, ids]\n            temp = (dirichlet_doc_topic[idx_d, :, np.newaxis]\n                    + dirichlet_component_[:, ids])\n            norm_phi = logsumexp(temp, axis=0)\n            score += np.dot(cnts, norm_phi)\n\n        # compute E[log p(theta | alpha) - log q(theta | gamma)]\n        score += _loglikelihood(doc_topic_prior, doc_topic_distr,\n                                dirichlet_doc_topic, self.n_components)\n\n        # Compensate for the subsampling of the population of documents\n        if sub_sampling:\n            doc_ratio = float(self.total_samples) / n_samples\n            score *= doc_ratio\n\n        # E[log p(beta | eta) - log q (beta | lambda)]\n        score += _loglikelihood(topic_word_prior, self.components_,\n                                dirichlet_component_, n_features)\n\n        return score\n\n    def score(self, X, y=None):\n        \"\"\"Calculate approximate log-likelihood as score.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n\n        Returns\n        -------\n        score : float\n            Use approximate bound as score.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_non_neg_array(X, reset_n_features=False,\n                                      whom=\"LatentDirichletAllocation.score\")\n\n        doc_topic_distr = self._unnormalized_transform(X)\n        score = self._approx_bound(X, doc_topic_distr, sub_sampling=False)\n        return score\n\n    def _perplexity_precomp_distr(self, X, doc_topic_distr=None,\n                                  sub_sampling=False):\n        \"\"\"Calculate approximate perplexity for data X with ability to accept\n        precomputed doc_topic_distr\n\n        Perplexity is defined as exp(-1. * log-likelihood per word)\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        doc_topic_distr : ndarray of shape (n_samples, n_components), \\\n                default=None\n            Document topic distribution.\n            If it is None, it will be generated by applying transform on X.\n\n        Returns\n        -------\n        score : float\n            Perplexity score.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_non_neg_array(\n            X, reset_n_features=True,\n            whom=\"LatentDirichletAllocation.perplexity\")\n\n        if doc_topic_distr is None:\n            doc_topic_distr = self._unnormalized_transform(X)\n        else:\n            n_samples, n_components = doc_topic_distr.shape\n            if n_samples != X.shape[0]:\n                raise ValueError(\"Number of samples in X and doc_topic_distr\"\n                                 \" do not match.\")\n\n            if n_components != self.n_components:\n                raise ValueError(\"Number of topics does not match.\")\n\n        current_samples = X.shape[0]\n        bound = self._approx_bound(X, doc_topic_distr, sub_sampling)\n\n        if sub_sampling:\n            word_cnt = X.sum() * (float(self.total_samples) / current_samples)\n        else:\n            word_cnt = X.sum()\n        perword_bound = bound / word_cnt\n\n        return np.exp(-1.0 * perword_bound)\n\n    def perplexity(self, X, sub_sampling=False):\n        \"\"\"Calculate approximate perplexity for data X.\n\n        Perplexity is defined as exp(-1. * log-likelihood per word)\n\n        .. versionchanged:: 0.19\n           *doc_topic_distr* argument has been deprecated and is ignored\n           because user no longer has access to unnormalized distribution\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        sub_sampling : bool\n            Do sub-sampling or not.\n\n        Returns\n        -------\n        score : float\n            Perplexity score.\n        \"\"\"\n        return self._perplexity_precomp_distr(X, sub_sampling=sub_sampling)",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "doc_topic_prior",
          "types": null
        },
        {
          "name": "topic_word_prior",
          "types": null
        },
        {
          "name": "learning_method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "learning_decay",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "learning_offset",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "batch_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "evaluate_every",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "total_samples",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "perp_tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "mean_change_tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_doc_update_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "random_state_",
          "types": null
        },
        {
          "name": "n_batch_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "doc_topic_prior_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "topic_word_prior_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "components_",
          "types": null
        },
        {
          "name": "exp_dirichlet_component_",
          "types": null
        },
        {
          "name": "bound_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/NMF",
      "name": "NMF",
      "qname": "sklearn.decomposition._nmf.NMF",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._nmf/NMF/__init__",
        "scikit-learn/sklearn.decomposition._nmf/NMF/_more_tags",
        "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform",
        "scikit-learn/sklearn.decomposition._nmf/NMF/fit",
        "scikit-learn/sklearn.decomposition._nmf/NMF/transform",
        "scikit-learn/sklearn.decomposition._nmf/NMF/inverse_transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nRead more in the :ref:`User Guide <NMF>`.",
      "docstring": "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nRead more in the :ref:`User Guide <NMF>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n    Default: None.\n    Valid options:\n\n    - `None`: 'nndsvd' if n_components <= min(n_samples, n_features),\n      otherwise random.\n\n    - `'random'`: non-negative random matrices, scaled with:\n      sqrt(X.mean() / n_components)\n\n    - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n      initialization (better for sparseness)\n\n    - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n      (better when sparsity is not desired)\n\n    - `'nndsvdar'` NNDSVD with zeros filled with small random values\n      (generally faster, less accurate alternative to NNDSVDa\n      for when sparsity is not desired)\n\n    - `'custom'`: use custom matrices W and H\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n    'cd' is a Coordinate Descent solver.\n    'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nalpha : float, default=0.\n    Constant that multiplies the regularization terms. Set it to zero to\n    have no regularization.\n\n    .. versionadded:: 0.17\n       *alpha* used in the Coordinate Descent solver.\n\nl1_ratio : float, default=0.\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    .. versionadded:: 0.17\n       Regularization parameter *l1_ratio* used in the Coordinate Descent\n       solver.\n\nverbose : int, default=0\n    Whether to be verbose.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\n    .. versionadded:: 0.17\n       *shuffle* parameter used in the Coordinate Descent solver.\n\nregularization : {'both', 'components', 'transformation', None},                      default='both'\n    Select whether the regularization affects the components (H), the\n    transformation (W), both or none of them.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Factorization matrix, sometimes called 'dictionary'.\n\nn_components_ : int\n    The number of components. It is same as the `n_components` parameter\n    if it was given. Otherwise, it will be same as the number of\n    features.\n\nreconstruction_err_ : float\n    Frobenius norm of the matrix difference, or beta-divergence, between\n    the training data ``X`` and the reconstructed data ``WH`` from\n    the fitted model.\n\nn_iter_ : int\n    Actual number of iterations.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_\n\nReferences\n----------\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\nlarge scale nonnegative matrix and tensor factorizations.\"\nIEICE transactions on fundamentals of electronics, communications and\ncomputer sciences 92.3: 708-721, 2009.\n\nFevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\nfactorization with the beta-divergence. Neural Computation, 23(9).",
      "code": "class NMF(TransformerMixin, BaseEstimator):\n    \"\"\"Non-Negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n            + alpha * l1_{ratio} * ||vec(H)||_1\n\n            + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n            + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Default: None.\n        Valid options:\n\n        - `None`: 'nndsvd' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          sqrt(X.mean() / n_components)\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: use custom matrices W and H\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n        'cd' is a Coordinate Descent solver.\n        'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\\n            'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha : float, default=0.\n        Constant that multiplies the regularization terms. Set it to zero to\n        have no regularization.\n\n        .. versionadded:: 0.17\n           *alpha* used in the Coordinate Descent solver.\n\n    l1_ratio : float, default=0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    regularization : {'both', 'components', 'transformation', None}, \\\n                     default='both'\n        Select whether the regularization affects the components (H), the\n        transformation (W), both or none of them.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n\n    References\n    ----------\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\n    factorization with the beta-divergence. Neural Computation, 23(9).\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, init='warn', solver='cd',\n                 beta_loss='frobenius', tol=1e-4, max_iter=200,\n                 random_state=None, alpha=0., l1_ratio=0., verbose=0,\n                 shuffle=False, regularization='both'):\n        self.n_components = n_components\n        self.init = init\n        self.solver = solver\n        self.beta_loss = beta_loss\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.alpha = alpha\n        self.l1_ratio = l1_ratio\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.regularization = regularization\n\n    def _more_tags(self):\n        return {'requires_positive_X': True}\n\n    def fit_transform(self, X, y=None, W=None, H=None):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        y : Ignored\n\n        W : array-like of shape (n_samples, n_components)\n            If init='custom', it is used as initial guess for the solution.\n\n        H : array-like of shape (n_components, n_features)\n            If init='custom', it is used as initial guess for the solution.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n                                dtype=[np.float64, np.float32])\n\n        with config_context(assume_finite=True):\n            W, H, n_iter_ = non_negative_factorization(\n                X=X, W=W, H=H, n_components=self.n_components, init=self.init,\n                update_H=True, solver=self.solver, beta_loss=self.beta_loss,\n                tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,\n                l1_ratio=self.l1_ratio, regularization=self.regularization,\n                random_state=self.random_state, verbose=self.verbose,\n                shuffle=self.shuffle)\n\n        self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n                                                    square_root=True)\n\n        self.n_components_ = H.shape[0]\n        self.components_ = H\n        self.n_iter_ = n_iter_\n\n        return W\n\n    def fit(self, X, y=None, **params):\n        \"\"\"Learn a NMF model for the data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.fit_transform(X, **params)\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform the data X according to the fitted NMF model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be transformed by the model.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n                                dtype=[np.float64, np.float32],\n                                reset=False)\n\n        with config_context(assume_finite=True):\n            W, _, n_iter_ = non_negative_factorization(\n                X=X, W=None, H=self.components_,\n                n_components=self.n_components_,\n                init=self.init, update_H=False, solver=self.solver,\n                beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,\n                alpha=self.alpha, l1_ratio=self.l1_ratio,\n                regularization=self.regularization,\n                random_state=self.random_state,\n                verbose=self.verbose, shuffle=self.shuffle)\n\n        return W\n\n    def inverse_transform(self, W):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        W : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Returns\n        -------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Data matrix of original shape.\n\n        .. versionadded:: 0.18\n        \"\"\"\n        check_is_fitted(self)\n        return np.dot(W, self.components_)",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "init",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "solver",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "beta_loss",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "l1_ratio",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "shuffle",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "regularization",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "reconstruction_err_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_components_",
          "types": null
        },
        {
          "name": "components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_iter_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._pca/PCA",
      "name": "PCA",
      "qname": "sklearn.decomposition._pca.PCA",
      "decorators": [],
      "superclasses": [
        "_BasePCA"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._pca/PCA/__init__",
        "scikit-learn/sklearn.decomposition._pca/PCA/fit",
        "scikit-learn/sklearn.decomposition._pca/PCA/fit_transform",
        "scikit-learn/sklearn.decomposition._pca/PCA/_fit",
        "scikit-learn/sklearn.decomposition._pca/PCA/_fit_full",
        "scikit-learn/sklearn.decomposition._pca/PCA/_fit_truncated",
        "scikit-learn/sklearn.decomposition._pca/PCA/score_samples",
        "scikit-learn/sklearn.decomposition._pca/PCA/score",
        "scikit-learn/sklearn.decomposition._pca/PCA/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nRead more in the :ref:`User Guide <PCA>`.",
      "docstring": "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nRead more in the :ref:`User Guide <PCA>`.\n\nParameters\n----------\nn_components : int, float or 'mle', default=None\n    Number of components to keep.\n    if n_components is not set all components are kept::\n\n        n_components == min(n_samples, n_features)\n\n    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n    number of components such that the amount of variance that needs to be\n    explained is greater than the percentage specified by n_components.\n\n    If ``svd_solver == 'arpack'``, the number of components must be\n    strictly less than the minimum of n_features and n_samples.\n\n    Hence, the None case results in::\n\n        n_components == min(n_samples, n_features) - 1\n\ncopy : bool, default=True\n    If False, data passed to fit are overwritten and running\n    fit(X).transform(X) will not yield the expected results,\n    use fit_transform(X) instead.\n\nwhiten : bool, default=False\n    When True (False by default) the `components_` vectors are multiplied\n    by the square root of n_samples and then divided by the singular values\n    to ensure uncorrelated outputs with unit component-wise variances.\n\n    Whitening will remove some information from the transformed signal\n    (the relative variance scales of the components) but can sometime\n    improve the predictive accuracy of the downstream estimators by\n    making their data respect some hard-wired assumptions.\n\nsvd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n    If auto :\n        The solver is selected by a default policy based on `X.shape` and\n        `n_components`: if the input data is larger than 500x500 and the\n        number of components to extract is lower than 80% of the smallest\n        dimension of the data, then the more efficient 'randomized'\n        method is enabled. Otherwise the exact full SVD is computed and\n        optionally truncated afterwards.\n    If full :\n        run exact full SVD calling the standard LAPACK solver via\n        `scipy.linalg.svd` and select the components by postprocessing\n    If arpack :\n        run SVD truncated to n_components calling ARPACK solver via\n        `scipy.sparse.linalg.svds`. It requires strictly\n        0 < n_components < min(X.shape)\n    If randomized :\n        run randomized SVD by the method of Halko et al.\n\n    .. versionadded:: 0.18.0\n\ntol : float, default=0.0\n    Tolerance for singular values computed by svd_solver == 'arpack'.\n    Must be of range [0.0, infinity).\n\n    .. versionadded:: 0.18.0\n\niterated_power : int or 'auto', default='auto'\n    Number of iterations for the power method computed by\n    svd_solver == 'randomized'.\n    Must be of range [0, infinity).\n\n    .. versionadded:: 0.18.0\n\nrandom_state : int, RandomState instance or None, default=None\n    Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n    for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.18.0\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of\n    maximum variance in the data. The components are sorted by\n    ``explained_variance_``.\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The amount of variance explained by each of the selected components.\n    The variance estimation uses `n_samples - 1` degrees of freedom.\n\n    Equal to n_components largest eigenvalues\n    of the covariance matrix of X.\n\n    .. versionadded:: 0.18\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\n    If ``n_components`` is not set then all components are stored and the\n    sum of the ratios is equal to 1.0.\n\nsingular_values_ : ndarray of shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\n    .. versionadded:: 0.19\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n\n    Equal to `X.mean(axis=0)`.\n\nn_components_ : int\n    The estimated number of components. When n_components is set\n    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n    number is estimated from input data. Otherwise it equals the parameter\n    n_components, or the lesser value of n_features and n_samples\n    if n_components is None.\n\nn_features_ : int\n    Number of features in the training data.\n\nn_samples_ : int\n    Number of samples in the training data.\n\nnoise_variance_ : float\n    The estimated noise covariance following the Probabilistic PCA model\n    from Tipping and Bishop 1999. See \"Pattern Recognition and\n    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n    compute the estimated data covariance and score samples.\n\n    Equal to the average of (min(n_features, n_samples) - n_components)\n    smallest eigenvalues of the covariance matrix of X.\n\nSee Also\n--------\nKernelPCA : Kernel Principal Component Analysis.\nSparsePCA : Sparse Principal Component Analysis.\nTruncatedSVD : Dimensionality reduction using truncated SVD.\nIncrementalPCA : Incremental Principal Component Analysis.\n\nReferences\n----------\nFor n_components == 'mle', this class uses the method from:\n`Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\nIn NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\nImplements the probabilistic PCA model from:\n`Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\ncomponent analysis\". Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 61(3), 611-622.\n<http://www.miketipping.com/papers/met-mppca.pdf>`_\nvia the score and score_samples methods.\n\nFor svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\nFor svd_solver == 'randomized', see:\n`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n\"Finding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions\".\nSIAM review, 53(2), 217-288.\n<https://doi.org/10.1137/090771806>`_\nand also\n`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n\"A randomized algorithm for the decomposition of matrices\".\nApplied and Computational Harmonic Analysis, 30(1), 47-68\n<https://doi.org/10.1016/j.acha.2010.02.003>`_.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.decomposition import PCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> pca = PCA(n_components=2)\n>>> pca.fit(X)\nPCA(n_components=2)\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.0075...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=2, svd_solver='full')\n>>> pca.fit(X)\nPCA(n_components=2, svd_solver='full')\n>>> print(pca.explained_variance_ratio_)\n[0.9924... 0.00755...]\n>>> print(pca.singular_values_)\n[6.30061... 0.54980...]\n\n>>> pca = PCA(n_components=1, svd_solver='arpack')\n>>> pca.fit(X)\nPCA(n_components=1, svd_solver='arpack')\n>>> print(pca.explained_variance_ratio_)\n[0.99244...]\n>>> print(pca.singular_values_)\n[6.30061...]",
      "code": "class PCA(_BasePCA):\n    \"\"\"Principal component analysis (PCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    It can also use the scipy.sparse.linalg ARPACK implementation of the\n    truncated SVD.\n\n    Notice that this class does not support sparse input. See\n    :class:`TruncatedSVD` for an alternative with sparse data.\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float or 'mle', default=None\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool, default=True\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, default=False\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n        If auto :\n            The solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        If full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        If arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 < n_components < min(X.shape)\n        If randomized :\n            run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n    tol : float, default=0.0\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n        Must be of range [0.0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n        Must be of range [0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    random_state : int, RandomState instance or None, default=None\n        Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. The components are sorted by\n        ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The amount of variance explained by each of the selected components.\n        The variance estimation uses `n_samples - 1` degrees of freedom.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    n_features_ : int\n        Number of features in the training data.\n\n    n_samples_ : int\n        Number of samples in the training data.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    See Also\n    --------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method from:\n    `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n    In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    <http://www.miketipping.com/papers/met-mppca.pdf>`_\n    via the score and score_samples methods.\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    `Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.\n    <https://doi.org/10.1137/090771806>`_\n    and also\n    `Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68\n    <https://doi.org/10.1016/j.acha.2010.02.003>`_.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, copy=True, whiten=False,\n                 svd_solver='auto', tol=0.0, iterated_power='auto',\n                 random_state=None):\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n        self.svd_solver = svd_solver\n        self.tol = tol\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model with X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._fit(X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed values.\n\n        Notes\n        -----\n        This method returns a Fortran-ordered array. To convert it to a\n        C-ordered array, use 'np.ascontiguousarray'.\n        \"\"\"\n        U, S, Vt = self._fit(X)\n        U = U[:, :self.n_components_]\n\n        if self.whiten:\n            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n            U *= sqrt(X.shape[0] - 1)\n        else:\n            # X_new = X * V = U * S * Vt * V = U * S\n            U *= S[:self.n_components_]\n\n        return U\n\n    def _fit(self, X):\n        \"\"\"Dispatch to the right submethod depending on the chosen solver.\"\"\"\n\n        # Raise an error for sparse input.\n        # This is more informative than the generic one raised by check_array.\n        if issparse(X):\n            raise TypeError('PCA does not support sparse input. See '\n                            'TruncatedSVD for a possible alternative.')\n\n        X = self._validate_data(X, dtype=[np.float64, np.float32],\n                                ensure_2d=True, copy=self.copy)\n\n        # Handle n_components==None\n        if self.n_components is None:\n            if self.svd_solver != 'arpack':\n                n_components = min(X.shape)\n            else:\n                n_components = min(X.shape) - 1\n        else:\n            n_components = self.n_components\n\n        # Handle svd_solver\n        self._fit_svd_solver = self.svd_solver\n        if self._fit_svd_solver == 'auto':\n            # Small problem or n_components == 'mle', just call full PCA\n            if max(X.shape) <= 500 or n_components == 'mle':\n                self._fit_svd_solver = 'full'\n            elif n_components >= 1 and n_components < .8 * min(X.shape):\n                self._fit_svd_solver = 'randomized'\n            # This is also the case of n_components in (0,1)\n            else:\n                self._fit_svd_solver = 'full'\n\n        # Call different fits for either full or truncated SVD\n        if self._fit_svd_solver == 'full':\n            return self._fit_full(X, n_components)\n        elif self._fit_svd_solver in ['arpack', 'randomized']:\n            return self._fit_truncated(X, n_components, self._fit_svd_solver)\n        else:\n            raise ValueError(\"Unrecognized svd_solver='{0}'\"\n                             \"\".format(self._fit_svd_solver))\n\n    def _fit_full(self, X, n_components):\n        \"\"\"Fit the model by computing full SVD on X.\"\"\"\n        n_samples, n_features = X.shape\n\n        if n_components == 'mle':\n            if n_samples < n_features:\n                raise ValueError(\"n_components='mle' is only supported \"\n                                 \"if n_samples >= n_features\")\n        elif not 0 <= n_components <= min(n_samples, n_features):\n            raise ValueError(\"n_components=%r must be between 0 and \"\n                             \"min(n_samples, n_features)=%r with \"\n                             \"svd_solver='full'\"\n                             % (n_components, min(n_samples, n_features)))\n        elif n_components >= 1:\n            if not isinstance(n_components, numbers.Integral):\n                raise ValueError(\"n_components=%r must be of type int \"\n                                 \"when greater than or equal to 1, \"\n                                 \"was of type=%r\"\n                                 % (n_components, type(n_components)))\n\n        # Center data\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        U, S, Vt = linalg.svd(X, full_matrices=False)\n        # flip eigenvectors' sign to enforce deterministic output\n        U, Vt = svd_flip(U, Vt)\n\n        components_ = Vt\n\n        # Get variance explained by singular values\n        explained_variance_ = (S ** 2) / (n_samples - 1)\n        total_var = explained_variance_.sum()\n        explained_variance_ratio_ = explained_variance_ / total_var\n        singular_values_ = S.copy()  # Store the singular values.\n\n        # Postprocess the number of components required\n        if n_components == 'mle':\n            n_components = \\\n                _infer_dimension(explained_variance_, n_samples)\n        elif 0 < n_components < 1.0:\n            # number of components for which the cumulated explained\n            # variance percentage is superior to the desired threshold\n            # side='right' ensures that number of features selected\n            # their variance is always greater than n_components float\n            # passed. More discussion in issue: #15669\n            ratio_cumsum = stable_cumsum(explained_variance_ratio_)\n            n_components = np.searchsorted(ratio_cumsum, n_components,\n                                           side='right') + 1\n        # Compute noise covariance using Probabilistic PCA model\n        # The sigma2 maximum likelihood (cf. eq. 12.46)\n        if n_components < min(n_features, n_samples):\n            self.noise_variance_ = explained_variance_[n_components:].mean()\n        else:\n            self.noise_variance_ = 0.\n\n        self.n_samples_, self.n_features_ = n_samples, n_features\n        self.components_ = components_[:n_components]\n        self.n_components_ = n_components\n        self.explained_variance_ = explained_variance_[:n_components]\n        self.explained_variance_ratio_ = \\\n            explained_variance_ratio_[:n_components]\n        self.singular_values_ = singular_values_[:n_components]\n\n        return U, S, Vt\n\n    def _fit_truncated(self, X, n_components, svd_solver):\n        \"\"\"Fit the model by computing truncated SVD (by ARPACK or randomized)\n        on X.\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        if isinstance(n_components, str):\n            raise ValueError(\"n_components=%r cannot be a string \"\n                             \"with svd_solver='%s'\"\n                             % (n_components, svd_solver))\n        elif not 1 <= n_components <= min(n_samples, n_features):\n            raise ValueError(\"n_components=%r must be between 1 and \"\n                             \"min(n_samples, n_features)=%r with \"\n                             \"svd_solver='%s'\"\n                             % (n_components, min(n_samples, n_features),\n                                svd_solver))\n        elif not isinstance(n_components, numbers.Integral):\n            raise ValueError(\"n_components=%r must be of type int \"\n                             \"when greater than or equal to 1, was of type=%r\"\n                             % (n_components, type(n_components)))\n        elif svd_solver == 'arpack' and n_components == min(n_samples,\n                                                            n_features):\n            raise ValueError(\"n_components=%r must be strictly less than \"\n                             \"min(n_samples, n_features)=%r with \"\n                             \"svd_solver='%s'\"\n                             % (n_components, min(n_samples, n_features),\n                                svd_solver))\n\n        random_state = check_random_state(self.random_state)\n\n        # Center data\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        if svd_solver == 'arpack':\n            v0 = _init_arpack_v0(min(X.shape), random_state)\n            U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0)\n            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n            # conventions, so reverse its outputs.\n            S = S[::-1]\n            # flip eigenvectors' sign to enforce deterministic output\n            U, Vt = svd_flip(U[:, ::-1], Vt[::-1])\n\n        elif svd_solver == 'randomized':\n            # sign flipping is done inside\n            U, S, Vt = randomized_svd(X, n_components=n_components,\n                                      n_iter=self.iterated_power,\n                                      flip_sign=True,\n                                      random_state=random_state)\n\n        self.n_samples_, self.n_features_ = n_samples, n_features\n        self.components_ = Vt\n        self.n_components_ = n_components\n\n        # Get variance explained by singular values\n        self.explained_variance_ = (S ** 2) / (n_samples - 1)\n        total_var = np.var(X, ddof=1, axis=0)\n        self.explained_variance_ratio_ = \\\n            self.explained_variance_ / total_var.sum()\n        self.singular_values_ = S.copy()  # Store the singular values.\n\n        if self.n_components_ < min(n_features, n_samples):\n            self.noise_variance_ = (total_var.sum() -\n                                    self.explained_variance_.sum())\n            self.noise_variance_ /= min(n_features, n_samples) - n_components\n        else:\n            self.noise_variance_ = 0.\n\n        return U, S, Vt\n\n    def score_samples(self, X):\n        \"\"\"Return the log-likelihood of each sample.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, dtype=[np.float64, np.float32], reset=False)\n        Xr = X - self.mean_\n        n_features = X.shape[1]\n        precision = self.get_precision()\n        log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= .5 * (n_features * log(2. * np.pi) -\n                          fast_logdet(precision))\n        return log_like\n\n    def score(self, X, y=None):\n        \"\"\"Return the average log-likelihood of all samples.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model.\n        \"\"\"\n        return np.mean(self.score_samples(X))\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32]}",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "copy",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "whiten",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "svd_solver",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "iterated_power",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "_fit_svd_solver",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "mean_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "noise_variance_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "n_samples_",
          "types": null
        },
        {
          "name": "n_features_",
          "types": null
        },
        {
          "name": "components_",
          "types": null
        },
        {
          "name": "n_components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "explained_variance_",
          "types": null
        },
        {
          "name": "explained_variance_ratio_",
          "types": null
        },
        {
          "name": "singular_values_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA",
      "name": "MiniBatchSparsePCA",
      "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA",
      "decorators": [],
      "superclasses": [
        "SparsePCA"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__",
        "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Mini-batch Sparse Principal Components Analysis\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.",
      "docstring": "Mini-batch Sparse Principal Components Analysis\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    number of sparse atoms to extract\n\nalpha : int, default=1\n    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n\nridge_alpha : float, default=0.01\n    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n\nn_iter : int, default=100\n    number of iterations to perform for each mini batch\n\ncallback : callable, default=None\n    callable that gets invoked every five iterations\n\nbatch_size : int, default=3\n    the number of features to take in each mini batch\n\nverbose : int or bool, default=False\n    Controls the verbosity; the higher, the more messages. Defaults to 0.\n\nshuffle : bool, default=True\n    whether to shuffle the data before splitting it in batches\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmethod : {'lars', 'cd'}, default='lars'\n    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for random shuffling when ``shuffle`` is set to ``True``,\n    during online dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Sparse components extracted from the data.\n\nn_components_ : int\n    Estimated number of components.\n\n    .. versionadded:: 0.23\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to ``X.mean(axis=0)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import MiniBatchSparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n...                                  random_state=0)\n>>> transformer.fit(X)\nMiniBatchSparsePCA(...)\n>>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\n0.94\n\nSee Also\n--------\nPCA\nSparsePCA\nDictionaryLearning",
      "code": "class MiniBatchSparsePCA(SparsePCA):\n    \"\"\"Mini-batch Sparse Principal Components Analysis\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        number of sparse atoms to extract\n\n    alpha : int, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    n_iter : int, default=100\n        number of iterations to perform for each mini batch\n\n    callback : callable, default=None\n        callable that gets invoked every five iterations\n\n    batch_size : int, default=3\n        the number of features to take in each mini batch\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    shuffle : bool, default=True\n        whether to shuffle the data before splitting it in batches\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for random shuffling when ``shuffle`` is set to ``True``,\n        during online dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import MiniBatchSparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n    ...                                  random_state=0)\n    >>> transformer.fit(X)\n    MiniBatchSparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    0.94\n\n    See Also\n    --------\n    PCA\n    SparsePCA\n    DictionaryLearning\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01,\n                 n_iter=100, callback=None, batch_size=3, verbose=False,\n                 shuffle=True, n_jobs=None, method='lars', random_state=None):\n        super().__init__(\n            n_components=n_components, alpha=alpha, verbose=verbose,\n            ridge_alpha=ridge_alpha, n_jobs=n_jobs, method=method,\n            random_state=random_state)\n        self.n_iter = n_iter\n        self.callback = callback\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n\n        self.mean_ = X.mean(axis=0)\n        X = X - self.mean_\n\n        if self.n_components is None:\n            n_components = X.shape[1]\n        else:\n            n_components = self.n_components\n        Vt, _, self.n_iter_ = dict_learning_online(\n            X.T, n_components, alpha=self.alpha,\n            n_iter=self.n_iter, return_code=True,\n            dict_init=None, verbose=self.verbose,\n            callback=self.callback,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            n_jobs=self.n_jobs, method=self.method,\n            random_state=random_state,\n            return_n_iter=True)\n        self.components_ = Vt.T\n\n        components_norm = np.linalg.norm(\n            self.components_, axis=1)[:, np.newaxis]\n        components_norm[components_norm == 0] = 1\n        self.components_ /= components_norm\n        self.n_components_ = len(self.components_)\n\n        return self",
      "instance_attributes": [
        {
          "name": "n_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "callback",
          "types": null
        },
        {
          "name": "batch_size",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "shuffle",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "mean_",
          "types": null
        },
        {
          "name": "n_iter_",
          "types": null
        },
        {
          "name": "components_",
          "types": null
        },
        {
          "name": "n_components_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA",
      "name": "SparsePCA",
      "qname": "sklearn.decomposition._sparse_pca.SparsePCA",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__",
        "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/fit",
        "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Sparse Principal Components Analysis (SparsePCA).\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.",
      "docstring": "Sparse Principal Components Analysis (SparsePCA).\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n\nParameters\n----------\nn_components : int, default=None\n    Number of sparse atoms to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter. Higher values lead to sparser\n    components.\n\nridge_alpha : float, default=0.01\n    Amount of ridge shrinkage to apply in order to improve\n    conditioning when calling the transform method.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for the stopping condition.\n\nmethod : {'lars', 'cd'}, default='lars'\n    lars: uses the least angle regression method to solve the lasso problem\n    (linear_model.lars_path)\n    cd: uses the coordinate descent method to compute the\n    Lasso solution (linear_model.Lasso). Lars will be faster if\n    the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nU_init : ndarray of shape (n_samples, n_components), default=None\n    Initial values for the loadings for warm restart scenarios. Only used\n    if `U_init` and `V_init` are not None.\n\nV_init : ndarray of shape (n_components, n_features), default=None\n    Initial values for the components for warm restart scenarios. Only used\n    if `U_init` and `V_init` are not None.\n\nverbose : int or bool, default=False\n    Controls the verbosity; the higher, the more messages. Defaults to 0.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used during dictionary learning. Pass an int for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n    Sparse components extracted from the data.\n\nerror_ : ndarray\n    Vector of errors at each iteration.\n\nn_components_ : int\n    Estimated number of components.\n\n    .. versionadded:: 0.23\n\nn_iter_ : int\n    Number of iterations run.\n\nmean_ : ndarray of shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to ``X.mean(axis=0)``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.decomposition import SparsePCA\n>>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n>>> transformer = SparsePCA(n_components=5, random_state=0)\n>>> transformer.fit(X)\nSparsePCA(...)\n>>> X_transformed = transformer.transform(X)\n>>> X_transformed.shape\n(200, 5)\n>>> # most values in the components_ are zero (sparsity)\n>>> np.mean(transformer.components_ == 0)\n0.9666...\n\nSee Also\n--------\nPCA\nMiniBatchSparsePCA\nDictionaryLearning",
      "code": "class SparsePCA(TransformerMixin, BaseEstimator):\n    \"\"\"Sparse Principal Components Analysis (SparsePCA).\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    U_init : ndarray of shape (n_samples, n_components), default=None\n        Initial values for the loadings for warm restart scenarios. Only used\n        if `U_init` and `V_init` are not None.\n\n    V_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the components for warm restart scenarios. Only used\n        if `U_init` and `V_init` are not None.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    random_state : int, RandomState instance or None, default=None\n        Used during dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    error_ : ndarray\n        Vector of errors at each iteration.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import SparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = SparsePCA(n_components=5, random_state=0)\n    >>> transformer.fit(X)\n    SparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    0.9666...\n\n    See Also\n    --------\n    PCA\n    MiniBatchSparsePCA\n    DictionaryLearning\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01,\n                 max_iter=1000, tol=1e-8, method='lars', n_jobs=None,\n                 U_init=None, V_init=None, verbose=False, random_state=None):\n        self.n_components = n_components\n        self.alpha = alpha\n        self.ridge_alpha = ridge_alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.method = method\n        self.n_jobs = n_jobs\n        self.U_init = U_init\n        self.V_init = V_init\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n\n        self.mean_ = X.mean(axis=0)\n        X = X - self.mean_\n\n        if self.n_components is None:\n            n_components = X.shape[1]\n        else:\n            n_components = self.n_components\n        code_init = self.V_init.T if self.V_init is not None else None\n        dict_init = self.U_init.T if self.U_init is not None else None\n        Vt, _, E, self.n_iter_ = dict_learning(X.T, n_components,\n                                               alpha=self.alpha,\n                                               tol=self.tol,\n                                               max_iter=self.max_iter,\n                                               method=self.method,\n                                               n_jobs=self.n_jobs,\n                                               verbose=self.verbose,\n                                               random_state=random_state,\n                                               code_init=code_init,\n                                               dict_init=dict_init,\n                                               return_n_iter=True)\n        self.components_ = Vt.T\n        components_norm = np.linalg.norm(\n            self.components_, axis=1)[:, np.newaxis]\n        components_norm[components_norm == 0] = 1\n        self.components_ /= components_norm\n        self.n_components_ = len(self.components_)\n\n        self.error_ = E\n        return self\n\n    def transform(self, X):\n        \"\"\"Least Squares projection of the data onto the sparse components.\n\n        To avoid instability issues in case the system is under-determined,\n        regularization can be applied (Ridge regression) via the\n        `ridge_alpha` parameter.\n\n        Note that Sparse PCA components orthogonality is not enforced as in PCA\n        hence one cannot use a simple linear projection.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, reset=False)\n        X = X - self.mean_\n\n        U = ridge_regression(self.components_.T, X.T, self.ridge_alpha,\n                             solver='cholesky')\n\n        return U",
      "instance_attributes": [
        {
          "name": "n_components",
          "types": null
        },
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "ridge_alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "method",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "U_init",
          "types": null
        },
        {
          "name": "V_init",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "mean_",
          "types": null
        },
        {
          "name": "n_iter_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "components_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_components_",
          "types": null
        },
        {
          "name": "error_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD",
      "name": "TruncatedSVD",
      "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__",
        "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit",
        "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit_transform",
        "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/transform",
        "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/inverse_transform",
        "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Dimensionality reduction using truncated SVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of\ntruncated singular value decomposition (SVD). Contrary to PCA, this\nestimator does not center the data before computing the singular value\ndecomposition. This means it can work with sparse matrices\nefficiently.\n\nIn particular, truncated SVD works on term count/tf-idf matrices as\nreturned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\nthat context, it is known as latent semantic analysis (LSA).\n\nThis estimator supports two algorithms: a fast randomized SVD solver, and\na \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n`X.T * X`, whichever is more efficient.\n\nRead more in the :ref:`User Guide <LSA>`.",
      "docstring": "Dimensionality reduction using truncated SVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of\ntruncated singular value decomposition (SVD). Contrary to PCA, this\nestimator does not center the data before computing the singular value\ndecomposition. This means it can work with sparse matrices\nefficiently.\n\nIn particular, truncated SVD works on term count/tf-idf matrices as\nreturned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\nthat context, it is known as latent semantic analysis (LSA).\n\nThis estimator supports two algorithms: a fast randomized SVD solver, and\na \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n`X.T * X`, whichever is more efficient.\n\nRead more in the :ref:`User Guide <LSA>`.\n\nParameters\n----------\nn_components : int, default=2\n    Desired dimensionality of output data.\n    Must be strictly less than the number of features.\n    The default value is useful for visualisation. For LSA, a value of\n    100 is recommended.\n\nalgorithm : {'arpack', 'randomized'}, default='randomized'\n    SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n    (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n    algorithm due to Halko (2009).\n\nn_iter : int, default=5\n    Number of iterations for randomized SVD solver. Not used by ARPACK. The\n    default is larger than the default in\n    :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n    matrices that may have large slowly decaying spectrum.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used during randomized svd. Pass an int for reproducible results across\n    multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ntol : float, default=0.\n    Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n    SVD solver.\n\nAttributes\n----------\ncomponents_ : ndarray of shape (n_components, n_features)\n\nexplained_variance_ : ndarray of shape (n_components,)\n    The variance of the training samples transformed by a projection to\n    each component.\n\nexplained_variance_ratio_ : ndarray of shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n\nsingular_values_ : ndarray od shape (n_components,)\n    The singular values corresponding to each of the selected components.\n    The singular values are equal to the 2-norms of the ``n_components``\n    variables in the lower-dimensional space.\n\nExamples\n--------\n>>> from sklearn.decomposition import TruncatedSVD\n>>> from scipy.sparse import random as sparse_random\n>>> X = sparse_random(100, 100, density=0.01, format='csr',\n...                   random_state=42)\n>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> svd.fit(X)\nTruncatedSVD(n_components=5, n_iter=7, random_state=42)\n>>> print(svd.explained_variance_ratio_)\n[0.0646... 0.0633... 0.0639... 0.0535... 0.0406...]\n>>> print(svd.explained_variance_ratio_.sum())\n0.286...\n>>> print(svd.singular_values_)\n[1.553... 1.512...  1.510... 1.370... 1.199...]\n\nSee Also\n--------\nPCA\n\nReferences\n----------\nFinding structure with randomness: Stochastic algorithms for constructing\napproximate matrix decompositions\nHalko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\nNotes\n-----\nSVD suffers from a problem called \"sign indeterminacy\", which means the\nsign of the ``components_`` and the output from transform depend on the\nalgorithm and random state. To work around this, fit instances of this\nclass to data once, then keep the instance around to do transformations.",
      "code": "class TruncatedSVD(TransformerMixin, BaseEstimator):\n    \"\"\"Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\n    that context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n    `X.T * X`, whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Desired dimensionality of output data.\n        Must be strictly less than the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : {'arpack', 'randomized'}, default='randomized'\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, default=5\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n        matrices that may have large slowly decaying spectrum.\n\n    random_state : int, RandomState instance or None, default=None\n        Used during randomized svd. Pass an int for reproducible results across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : ndarray od shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import random as sparse_random\n    >>> X = sparse_random(100, 100, density=0.01, format='csr',\n    ...                   random_state=42)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0646... 0.0633... 0.0639... 0.0535... 0.0406...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.286...\n    >>> print(svd.singular_values_)\n    [1.553... 1.512...  1.510... 1.370... 1.199...]\n\n    See Also\n    --------\n    PCA\n\n    References\n    ----------\n    Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, algorithm=\"randomized\", n_iter=5,\n                 random_state=None, tol=0.):\n        self.algorithm = algorithm\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.random_state = random_state\n        self.tol = tol\n\n    def fit(self, X, y=None):\n        \"\"\"Fit model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the transformer object.\n        \"\"\"\n        self.fit_transform(X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Fit model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'],\n                                ensure_min_features=2)\n        random_state = check_random_state(self.random_state)\n\n        if self.algorithm == \"arpack\":\n            v0 = _init_arpack_v0(min(X.shape), random_state)\n            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol, v0=v0)\n            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n            # conventions, so reverse its outputs.\n            Sigma = Sigma[::-1]\n            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n\n        elif self.algorithm == \"randomized\":\n            k = self.n_components\n            n_features = X.shape[1]\n            if k >= n_features:\n                raise ValueError(\"n_components must be < n_features;\"\n                                 \" got %d >= %d\" % (k, n_features))\n            U, Sigma, VT = randomized_svd(X, self.n_components,\n                                          n_iter=self.n_iter,\n                                          random_state=random_state)\n        else:\n            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n\n        self.components_ = VT\n\n        # As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\n        # X @ V is not the same as U @ Sigma\n        if self.algorithm == \"randomized\" or \\\n                (self.algorithm == \"arpack\" and self.tol > 0):\n            X_transformed = safe_sparse_dot(X, self.components_.T)\n        else:\n            X_transformed = U * Sigma\n\n        # Calculate explained variance & explained variance ratio\n        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n        if sp.issparse(X):\n            _, full_var = mean_variance_axis(X, axis=0)\n            full_var = full_var.sum()\n        else:\n            full_var = np.var(X, axis=0).sum()\n        self.explained_variance_ratio_ = exp_var / full_var\n        self.singular_values_ = Sigma  # Store the singular values.\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'], reset=False)\n        return safe_sparse_dot(X, self.components_.T)\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Note that this is always a dense array.\n        \"\"\"\n        X = check_array(X)\n        return np.dot(X, self.components_)\n\n    def _more_tags(self):\n        return {'preserves_dtype': [np.float64, np.float32]}",
      "instance_attributes": [
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_components",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_iter",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "random_state",
          "types": null
        },
        {
          "name": "tol",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "components_",
          "types": null
        },
        {
          "name": "explained_variance_",
          "types": null
        },
        {
          "name": "explained_variance_ratio_",
          "types": null
        },
        {
          "name": "singular_values_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier",
      "name": "BaggingClassifier",
      "qname": "sklearn.ensemble._bagging.BaggingClassifier",
      "decorators": [],
      "superclasses": [
        "ClassifierMixin",
        "BaseBagging"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/_validate_estimator",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/_set_oob_score",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/_validate_y",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_proba",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_log_proba",
        "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/decision_function"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "A Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base\nclassifiers each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15",
      "docstring": "A Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base\nclassifiers each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a\n    :class:`~sklearn.tree.DecisionTreeClassifier`.\n\nn_estimators : int, default=10\n    The number of base estimators in the ensemble.\n\nmax_samples : int or float, default=1.0\n    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n\nbootstrap : bool, default=True\n    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n\nbootstrap_features : bool, default=False\n    Whether features are drawn with replacement.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization error. Only available if bootstrap=True.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.17\n       *warm_start* constructor parameter.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nn_features_ : int\n    The number of features when :meth:`fit` is performed.\n\nestimators_ : list of estimators\n    The collection of fitted base estimators.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\nestimators_features_ : list of arrays\n    The subset of drawn features for each base estimator.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_classes_ : int or list\n    The number of classes.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nExamples\n--------\n>>> from sklearn.svm import SVC\n>>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=100, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = BaggingClassifier(base_estimator=SVC(),\n...                         n_estimators=10, random_state=0).fit(X, y)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])\n\nReferences\n----------\n\n.. [1] L. Breiman, \"Pasting small votes for classification in large\n       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n       1996.\n\n.. [3] T. Ho, \"The random subspace method for constructing decision\n       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n       1998.\n\n.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n       Learning and Knowledge Discovery in Databases, 346-361, 2012.",
      "code": "class BaggingClassifier(ClassifierMixin, BaseBagging):\n    \"\"\"A Bagging classifier.\n\n    A Bagging classifier is an ensemble meta-estimator that fits base\n    classifiers each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    base_estimator : object, default=None\n        The base estimator to fit on random subsets of the dataset.\n        If None, then the base estimator is a\n        :class:`~sklearn.tree.DecisionTreeClassifier`.\n\n    n_estimators : int, default=10\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to train each base estimator (with\n        replacement by default, see `bootstrap` for more details).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator (\n        without replacement by default, see `bootstrap_features` for more\n        details).\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max_features * X.shape[1]` features.\n\n    bootstrap : bool, default=True\n        Whether samples are drawn with replacement. If False, sampling\n        without replacement is performed.\n\n    bootstrap_features : bool, default=False\n        Whether features are drawn with replacement.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate\n        the generalization error. Only available if bootstrap=True.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* constructor parameter.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random resampling of the original dataset\n        (sample wise and feature wise).\n        If the base estimator accepts a `random_state` attribute, a different\n        seed is generated for each instance in the ensemble.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    n_features_ : int\n        The number of features when :meth:`fit` is performed.\n\n    estimators_ : list of estimators\n        The collection of fitted base estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n    estimators_features_ : list of arrays\n        The subset of drawn features for each base estimator.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int or list\n        The number of classes.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.ensemble import BaggingClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=100, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = BaggingClassifier(base_estimator=SVC(),\n    ...                         n_estimators=10, random_state=0).fit(X, y)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Pasting small votes for classification in large\n           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n           1996.\n\n    .. [3] T. Ho, \"The random subspace method for constructing decision\n           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n           1998.\n\n    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None,\n                 n_estimators=10, *,\n                 max_samples=1.0,\n                 max_features=1.0,\n                 bootstrap=True,\n                 bootstrap_features=False,\n                 oob_score=False,\n                 warm_start=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0):\n\n        super().__init__(\n            base_estimator,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            bootstrap=bootstrap,\n            bootstrap_features=bootstrap_features,\n            oob_score=oob_score,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n        super()._validate_estimator(\n            default=DecisionTreeClassifier())\n\n    def _set_oob_score(self, X, y):\n        n_samples = y.shape[0]\n        n_classes_ = self.n_classes_\n\n        predictions = np.zeros((n_samples, n_classes_))\n\n        for estimator, samples, features in zip(self.estimators_,\n                                                self.estimators_samples_,\n                                                self.estimators_features_):\n            # Create mask for OOB samples\n            mask = ~indices_to_mask(samples, n_samples)\n\n            if hasattr(estimator, \"predict_proba\"):\n                predictions[mask, :] += estimator.predict_proba(\n                    (X[mask, :])[:, features])\n\n            else:\n                p = estimator.predict((X[mask, :])[:, features])\n                j = 0\n\n                for i in range(n_samples):\n                    if mask[i]:\n                        predictions[i, p[j]] += 1\n                        j += 1\n\n        if (predictions.sum(axis=1) == 0).any():\n            warn(\"Some inputs do not have OOB scores. \"\n                 \"This probably means too few estimators were used \"\n                 \"to compute any reliable oob estimates.\")\n\n        oob_decision_function = (predictions /\n                                 predictions.sum(axis=1)[:, np.newaxis])\n        oob_score = accuracy_score(y, np.argmax(predictions, axis=1))\n\n        self.oob_decision_function_ = oob_decision_function\n        self.oob_score_ = oob_score\n\n    def _validate_y(self, y):\n        y = column_or_1d(y, warn=True)\n        check_classification_targets(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        self.n_classes_ = len(self.classes_)\n\n        return y\n\n    def predict(self, X):\n        \"\"\"Predict class for X.\n\n        The predicted class of an input sample is computed as the class with\n        the highest mean predicted probability. If base estimators do not\n        implement a ``predict_proba`` method, then it resorts to voting.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        predicted_probabilitiy = self.predict_proba(X)\n        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n                                  axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the mean predicted class probabilities of the base estimators in the\n        ensemble. If base estimators do not implement a ``predict_proba``\n        method, then it resorts to voting and the predicted class probabilities\n        of an input sample represents the proportion of estimators predicting\n        each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = check_array(\n            X, accept_sparse=['csr', 'csc'], dtype=None,\n            force_all_finite=False\n        )\n\n        if self.n_features_ != X.shape[1]:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features is {0} and \"\n                             \"input n_features is {1}.\"\n                             \"\".format(self.n_features_, X.shape[1]))\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n                                                             self.n_jobs)\n\n        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n                             **self._parallel_args())(\n            delayed(_parallel_predict_proba)(\n                self.estimators_[starts[i]:starts[i + 1]],\n                self.estimators_features_[starts[i]:starts[i + 1]],\n                X,\n                self.n_classes_)\n            for i in range(n_jobs))\n\n        # Reduce\n        proba = sum(all_proba) / self.n_estimators\n\n        return proba\n\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the base\n        estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n            # Check data\n            X = check_array(\n                X, accept_sparse=['csr', 'csc'], dtype=None,\n                force_all_finite=False\n            )\n\n            if self.n_features_ != X.shape[1]:\n                raise ValueError(\"Number of features of the model must \"\n                                 \"match the input. Model n_features is {0} \"\n                                 \"and input n_features is {1} \"\n                                 \"\".format(self.n_features_, X.shape[1]))\n\n            # Parallel loop\n            n_jobs, n_estimators, starts = _partition_estimators(\n                self.n_estimators, self.n_jobs)\n\n            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n                delayed(_parallel_predict_log_proba)(\n                    self.estimators_[starts[i]:starts[i + 1]],\n                    self.estimators_features_[starts[i]:starts[i + 1]],\n                    X,\n                    self.n_classes_)\n                for i in range(n_jobs))\n\n            # Reduce\n            log_proba = all_log_proba[0]\n\n            for j in range(1, len(all_log_proba)):\n                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n\n            log_proba -= np.log(self.n_estimators)\n\n            return log_proba\n\n        else:\n            return np.log(self.predict_proba(X))\n\n    @if_delegate_has_method(delegate='base_estimator')\n    def decision_function(self, X):\n        \"\"\"Average of the decision functions of the base classifiers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        score : ndarray of shape (n_samples, k)\n            The decision function of the input samples. The columns correspond\n            to the classes in sorted order, as they appear in the attribute\n            ``classes_``. Regression and binary classification are special\n            cases with ``k == 1``, otherwise ``k==n_classes``.\n\n        \"\"\"\n        check_is_fitted(self)\n\n        # Check data\n        X = check_array(\n            X, accept_sparse=['csr', 'csc'], dtype=None,\n            force_all_finite=False\n        )\n\n        if self.n_features_ != X.shape[1]:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features is {0} and \"\n                             \"input n_features is {1} \"\n                             \"\".format(self.n_features_, X.shape[1]))\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n                                                             self.n_jobs)\n\n        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n            delayed(_parallel_decision_function)(\n                self.estimators_[starts[i]:starts[i + 1]],\n                self.estimators_features_[starts[i]:starts[i + 1]],\n                X)\n            for i in range(n_jobs))\n\n        # Reduce\n        decisions = sum(all_decisions) / self.n_estimators\n\n        return decisions",
      "instance_attributes": [
        {
          "name": "oob_decision_function_",
          "types": null
        },
        {
          "name": "oob_score_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "tuple"
              },
              {
                "kind": "NamedType",
                "name": "ndarray"
              }
            ]
          }
        },
        {
          "name": "classes_",
          "types": null
        },
        {
          "name": "n_classes_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor",
      "name": "BaggingRegressor",
      "qname": "sklearn.ensemble._bagging.BaggingRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "BaseBagging"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__",
        "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/predict",
        "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/_validate_estimator",
        "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/_set_oob_score"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "A Bagging regressor.\n\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15",
      "docstring": "A Bagging regressor.\n\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator to fit on random subsets of the dataset.\n    If None, then the base estimator is a\n    :class:`~sklearn.tree.DecisionTreeRegressor`.\n\nn_estimators : int, default=10\n    The number of base estimators in the ensemble.\n\nmax_samples : int or float, default=1.0\n    The number of samples to draw from X to train each base estimator (with\n    replacement by default, see `bootstrap` for more details).\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator (\n    without replacement by default, see `bootstrap_features` for more\n    details).\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.\n\nbootstrap : bool, default=True\n    Whether samples are drawn with replacement. If False, sampling\n    without replacement is performed.\n\nbootstrap_features : bool, default=False\n    Whether features are drawn with replacement.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate\n    the generalization error. Only available if bootstrap=True.\n\nwarm_start : bool, default=False\n    When set to True, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit\n    a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random resampling of the original dataset\n    (sample wise and feature wise).\n    If the base estimator accepts a `random_state` attribute, a different\n    seed is generated for each instance in the ensemble.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nn_features_ : int\n    The number of features when :meth:`fit` is performed.\n\nestimators_ : list of estimators\n    The collection of fitted sub-estimators.\n\nestimators_samples_ : list of arrays\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator. Each subset is defined by an array of the indices selected.\n\nestimators_features_ : list of arrays\n    The subset of drawn features for each base estimator.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_prediction_` might contain NaN. This attribute exists only\n    when ``oob_score`` is True.\n\nExamples\n--------\n>>> from sklearn.svm import SVR\n>>> from sklearn.ensemble import BaggingRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_samples=100, n_features=4,\n...                        n_informative=2, n_targets=1,\n...                        random_state=0, shuffle=False)\n>>> regr = BaggingRegressor(base_estimator=SVR(),\n...                         n_estimators=10, random_state=0).fit(X, y)\n>>> regr.predict([[0, 0, 0, 0]])\narray([-2.8720...])\n\nReferences\n----------\n\n.. [1] L. Breiman, \"Pasting small votes for classification in large\n       databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n       1996.\n\n.. [3] T. Ho, \"The random subspace method for constructing decision\n       forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n       1998.\n\n.. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n       Learning and Knowledge Discovery in Databases, 346-361, 2012.",
      "code": "class BaggingRegressor(RegressorMixin, BaseBagging):\n    \"\"\"A Bagging regressor.\n\n    A Bagging regressor is an ensemble meta-estimator that fits base\n    regressors each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    base_estimator : object, default=None\n        The base estimator to fit on random subsets of the dataset.\n        If None, then the base estimator is a\n        :class:`~sklearn.tree.DecisionTreeRegressor`.\n\n    n_estimators : int, default=10\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to train each base estimator (with\n        replacement by default, see `bootstrap` for more details).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator (\n        without replacement by default, see `bootstrap_features` for more\n        details).\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max_features * X.shape[1]` features.\n\n    bootstrap : bool, default=True\n        Whether samples are drawn with replacement. If False, sampling\n        without replacement is performed.\n\n    bootstrap_features : bool, default=False\n        Whether features are drawn with replacement.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate\n        the generalization error. Only available if bootstrap=True.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random resampling of the original dataset\n        (sample wise and feature wise).\n        If the base estimator accepts a `random_state` attribute, a different\n        seed is generated for each instance in the ensemble.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    n_features_ : int\n        The number of features when :meth:`fit` is performed.\n\n    estimators_ : list of estimators\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n    estimators_features_ : list of arrays\n        The subset of drawn features for each base estimator.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_prediction_` might contain NaN. This attribute exists only\n        when ``oob_score`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVR\n    >>> from sklearn.ensemble import BaggingRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=100, n_features=4,\n    ...                        n_informative=2, n_targets=1,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = BaggingRegressor(base_estimator=SVR(),\n    ...                         n_estimators=10, random_state=0).fit(X, y)\n    >>> regr.predict([[0, 0, 0, 0]])\n    array([-2.8720...])\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Pasting small votes for classification in large\n           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n           1996.\n\n    .. [3] T. Ho, \"The random subspace method for constructing decision\n           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n           1998.\n\n    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None,\n                 n_estimators=10, *,\n                 max_samples=1.0,\n                 max_features=1.0,\n                 bootstrap=True,\n                 bootstrap_features=False,\n                 oob_score=False,\n                 warm_start=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0):\n        super().__init__(\n            base_estimator,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            bootstrap=bootstrap,\n            bootstrap_features=bootstrap_features,\n            oob_score=oob_score,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = check_array(\n            X, accept_sparse=['csr', 'csc'], dtype=None,\n            force_all_finite=False\n        )\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n                                                             self.n_jobs)\n\n        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n            delayed(_parallel_predict_regression)(\n                self.estimators_[starts[i]:starts[i + 1]],\n                self.estimators_features_[starts[i]:starts[i + 1]],\n                X)\n            for i in range(n_jobs))\n\n        # Reduce\n        y_hat = sum(all_y_hat) / self.n_estimators\n\n        return y_hat\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n        super()._validate_estimator(\n            default=DecisionTreeRegressor())\n\n    def _set_oob_score(self, X, y):\n        n_samples = y.shape[0]\n\n        predictions = np.zeros((n_samples,))\n        n_predictions = np.zeros((n_samples,))\n\n        for estimator, samples, features in zip(self.estimators_,\n                                                self.estimators_samples_,\n                                                self.estimators_features_):\n            # Create mask for OOB samples\n            mask = ~indices_to_mask(samples, n_samples)\n\n            predictions[mask] += estimator.predict((X[mask, :])[:, features])\n            n_predictions[mask] += 1\n\n        if (n_predictions == 0).any():\n            warn(\"Some inputs do not have OOB scores. \"\n                 \"This probably means too few estimators were used \"\n                 \"to compute any reliable oob estimates.\")\n            n_predictions[n_predictions == 0] = 1\n\n        predictions /= n_predictions\n\n        self.oob_prediction_ = predictions\n        self.oob_score_ = r2_score(y, predictions)",
      "instance_attributes": [
        {
          "name": "oob_prediction_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "oob_score_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble",
      "name": "BaseEnsemble",
      "qname": "sklearn.ensemble._base.BaseEnsemble",
      "decorators": [],
      "superclasses": [
        "MetaEstimatorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__init__",
        "scikit-learn/sklearn.ensemble._base/BaseEnsemble/_validate_estimator",
        "scikit-learn/sklearn.ensemble._base/BaseEnsemble/_make_estimator",
        "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__len__",
        "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__getitem__",
        "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__iter__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Base class for all ensemble classes.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.",
      "docstring": "Base class for all ensemble classes.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n\nParameters\n----------\nbase_estimator : object\n    The base estimator from which the ensemble is built.\n\nn_estimators : int, default=10\n    The number of estimators in the ensemble.\n\nestimator_params : list of str, default=tuple()\n    The list of attributes to use as parameters when instantiating a\n    new base estimator. If none are given, default parameters are used.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nestimators_ : list of estimators\n    The collection of fitted base estimators.",
      "code": "class BaseEnsemble(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Base class for all ensemble classes.\n\n    Warning: This class should not be used directly. Use derived classes\n    instead.\n\n    Parameters\n    ----------\n    base_estimator : object\n        The base estimator from which the ensemble is built.\n\n    n_estimators : int, default=10\n        The number of estimators in the ensemble.\n\n    estimator_params : list of str, default=tuple()\n        The list of attributes to use as parameters when instantiating a\n        new base estimator. If none are given, default parameters are used.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    estimators_ : list of estimators\n        The collection of fitted base estimators.\n    \"\"\"\n\n    # overwrite _required_parameters from MetaEstimatorMixin\n    _required_parameters: List[str] = []\n\n    @abstractmethod\n    def __init__(self, base_estimator, *, n_estimators=10,\n                 estimator_params=tuple()):\n        # Set parameters\n        self.base_estimator = base_estimator\n        self.n_estimators = n_estimators\n        self.estimator_params = estimator_params\n\n        # Don't instantiate estimators now! Parameters of base_estimator might\n        # still change. Eg., when grid-searching with the nested object syntax.\n        # self.estimators_ needs to be filled by the derived classes in fit.\n\n    def _validate_estimator(self, default=None):\n        \"\"\"Check the estimator and the n_estimator attribute.\n\n        Sets the base_estimator_` attributes.\n        \"\"\"\n        if not isinstance(self.n_estimators, numbers.Integral):\n            raise ValueError(\"n_estimators must be an integer, \"\n                             \"got {0}.\".format(type(self.n_estimators)))\n\n        if self.n_estimators <= 0:\n            raise ValueError(\"n_estimators must be greater than zero, \"\n                             \"got {0}.\".format(self.n_estimators))\n\n        if self.base_estimator is not None:\n            self.base_estimator_ = self.base_estimator\n        else:\n            self.base_estimator_ = default\n\n        if self.base_estimator_ is None:\n            raise ValueError(\"base_estimator cannot be None\")\n\n    def _make_estimator(self, append=True, random_state=None):\n        \"\"\"Make and configure a copy of the `base_estimator_` attribute.\n\n        Warning: This method should be used to properly instantiate new\n        sub-estimators.\n        \"\"\"\n        estimator = clone(self.base_estimator_)\n        estimator.set_params(**{p: getattr(self, p)\n                                for p in self.estimator_params})\n\n        if random_state is not None:\n            _set_random_states(estimator, random_state)\n\n        if append:\n            self.estimators_.append(estimator)\n\n        return estimator\n\n    def __len__(self):\n        \"\"\"Return the number of estimators in the ensemble.\"\"\"\n        return len(self.estimators_)\n\n    def __getitem__(self, index):\n        \"\"\"Return the index'th estimator in the ensemble.\"\"\"\n        return self.estimators_[index]\n\n    def __iter__(self):\n        \"\"\"Return iterator over estimators in the ensemble.\"\"\"\n        return iter(self.estimators_)",
      "instance_attributes": [
        {
          "name": "base_estimator",
          "types": null
        },
        {
          "name": "n_estimators",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "estimator_params",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "base_estimator_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier",
      "name": "ExtraTreesClassifier",
      "qname": "sklearn.ensemble._forest.ExtraTreesClassifier",
      "decorators": [],
      "superclasses": [
        "ForestClassifier"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=False\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    Only available if bootstrap=True.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls 3 sources of randomness:\n\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : ExtraTreesClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nSee Also\n--------\nsklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\nRandomForestClassifier : Ensemble Classifier based on trees with optimal\n    splits.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n       trees\", Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_features=4, random_state=0)\n>>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n>>> clf.fit(X, y)\nExtraTreesClassifier(random_state=0)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])",
      "code": "class ExtraTreesClassifier(ForestClassifier):\n    \"\"\"\n    An extra-trees classifier.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `round(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    bootstrap : bool, default=False\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        Only available if bootstrap=True.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n            default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : ExtraTreesClassifier\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    See Also\n    --------\n    sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n    RandomForestClassifier : Ensemble Classifier based on trees with optimal\n        splits.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    References\n    ----------\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n           trees\", Machine Learning, 63(1), 3-42, 2006.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import ExtraTreesClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    ExtraTreesClassifier(random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"gini\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=False,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 class_weight=None,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=ExtraTreeClassifier(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            class_weight=class_weight,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha",
      "instance_attributes": [
        {
          "name": "criterion",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_depth",
          "types": null
        },
        {
          "name": "min_samples_split",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_samples_leaf",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_weight_fraction_leaf",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_features",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_leaf_nodes",
          "types": null
        },
        {
          "name": "min_impurity_decrease",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_impurity_split",
          "types": null
        },
        {
          "name": "ccp_alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor",
      "name": "ExtraTreesRegressor",
      "qname": "sklearn.ensemble._forest.ExtraTreesRegressor",
      "decorators": [],
      "superclasses": [
        "ForestRegressor"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "An extra-trees regressor.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "An extra-trees regressor.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"mse\", \"mae\"}, default=\"mse\"\n    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion, and \"mae\" for the mean\n    absolute error.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=False\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    Only available if bootstrap=True.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls 3 sources of randomness:\n\n    - the bootstrapping of the samples used when building trees\n      (if ``bootstrap=True``)\n    - the sampling of the features to consider when looking for the best\n      split at each node (if ``max_features < n_features``)\n    - the draw of the splits for each of the `max_features`\n\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : ExtraTreeRegressor\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeRegressor\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features.\n\nn_outputs_ : int\n    The number of outputs.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training set.\n    This attribute exists only when ``oob_score`` is True.\n\nSee Also\n--------\nsklearn.tree.ExtraTreeRegressor : Base estimator for this ensemble.\nRandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.ensemble import ExtraTreesRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n...    X_train, y_train)\n>>> reg.score(X_test, y_test)\n0.2708...",
      "code": "class ExtraTreesRegressor(ForestRegressor):\n    \"\"\"\n    An extra-trees regressor.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"mse\", \"mae\"}, default=\"mse\"\n        The function to measure the quality of a split. Supported criteria\n        are \"mse\" for the mean squared error, which is equal to variance\n        reduction as feature selection criterion, and \"mae\" for the mean\n        absolute error.\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `round(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    bootstrap : bool, default=False\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        Only available if bootstrap=True.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : ExtraTreeRegressor\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeRegressor\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_ : int\n        The number of features.\n\n    n_outputs_ : int\n        The number of outputs.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training set.\n        This attribute exists only when ``oob_score`` is True.\n\n    See Also\n    --------\n    sklearn.tree.ExtraTreeRegressor : Base estimator for this ensemble.\n    RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    References\n    ----------\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n           Machine Learning, 63(1), 3-42, 2006.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.ensemble import ExtraTreesRegressor\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n    ...    X_train, y_train)\n    >>> reg.score(X_test, y_test)\n    0.2708...\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"mse\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=False,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha",
      "instance_attributes": [
        {
          "name": "criterion",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_depth",
          "types": null
        },
        {
          "name": "min_samples_split",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_samples_leaf",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_weight_fraction_leaf",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_features",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_leaf_nodes",
          "types": null
        },
        {
          "name": "min_impurity_decrease",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_impurity_split",
          "types": null
        },
        {
          "name": "ccp_alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier",
      "name": "RandomForestClassifier",
      "qname": "sklearn.ensemble._forest.RandomForestClassifier",
      "decorators": [],
      "superclasses": [
        "ForestClassifier"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n    Note: this parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    Only available if bootstrap=True.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : DecisionTreeClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nSee Also\n--------\nDecisionTreeClassifier, ExtraTreesClassifier\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n>>> clf.fit(X, y)\nRandomForestClassifier(...)\n>>> print(clf.predict([[0, 0, 0, 0]]))\n[1]",
      "code": "class RandomForestClassifier(ForestClassifier):\n    \"\"\"\n    A random forest classifier.\n\n    A random forest is a meta estimator that fits a number of decision tree\n    classifiers on various sub-samples of the dataset and uses averaging to\n    improve the predictive accuracy and control over-fitting.\n    The sub-sample size is controlled with the `max_samples` parameter if\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\n    each tree.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n        Note: this parameter is tree-specific.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `round(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    bootstrap : bool, default=True\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        Only available if bootstrap=True.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls both the randomness of the bootstrapping of the samples used\n        when building trees (if ``bootstrap=True``) and the sampling of the\n        features to consider when looking for the best split at each node\n        (if ``max_features < n_features``).\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n            default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : DecisionTreeClassifier\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    See Also\n    --------\n    DecisionTreeClassifier, ExtraTreesClassifier\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    References\n    ----------\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n    >>> clf.fit(X, y)\n    RandomForestClassifier(...)\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"gini\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=True,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 class_weight=None,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=DecisionTreeClassifier(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            class_weight=class_weight,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha",
      "instance_attributes": [
        {
          "name": "criterion",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_depth",
          "types": null
        },
        {
          "name": "min_samples_split",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_samples_leaf",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_weight_fraction_leaf",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_features",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_leaf_nodes",
          "types": null
        },
        {
          "name": "min_impurity_decrease",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_impurity_split",
          "types": null
        },
        {
          "name": "ccp_alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor",
      "name": "RandomForestRegressor",
      "qname": "sklearn.ensemble._forest.RandomForestRegressor",
      "decorators": [],
      "superclasses": [
        "ForestRegressor"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "A random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and uses averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "A random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and uses averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"mse\", \"mae\"}, default=\"mse\"\n    The function to measure the quality of a split. Supported criteria\n    are \"mse\" for the mean squared error, which is equal to variance\n    reduction as feature selection criterion, and \"mae\" for the mean\n    absolute error.\n\n    .. versionadded:: 0.18\n       Mean Absolute Error (MAE) criterion.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `round(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    Only available if bootstrap=True.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features < n_features``).\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0, 1)`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nbase_estimator_ : DecisionTreeRegressor\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeRegressor\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_prediction_ : ndarray of shape (n_samples,)\n    Prediction computed with out-of-bag estimate on the training set.\n    This attribute exists only when ``oob_score`` is True.\n\nSee Also\n--------\nDecisionTreeRegressor, ExtraTreesRegressor\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nThe default value ``max_features=\"auto\"`` uses ``n_features``\nrather than ``n_features / 3``. The latter was originally suggested in\n[1], whereas the former was more recently justified empirically in [2].\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n.. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n       trees\", Machine Learning, 63(1), 3-42, 2006.\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, n_informative=2,\n...                        random_state=0, shuffle=False)\n>>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n>>> regr.fit(X, y)\nRandomForestRegressor(...)\n>>> print(regr.predict([[0, 0, 0, 0]]))\n[-8.32987858]",
      "code": "class RandomForestRegressor(ForestRegressor):\n    \"\"\"\n    A random forest regressor.\n\n    A random forest is a meta estimator that fits a number of classifying\n    decision trees on various sub-samples of the dataset and uses averaging\n    to improve the predictive accuracy and control over-fitting.\n    The sub-sample size is controlled with the `max_samples` parameter if\n    `bootstrap=True` (default), otherwise the whole dataset is used to build\n    each tree.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"mse\", \"mae\"}, default=\"mse\"\n        The function to measure the quality of a split. Supported criteria\n        are \"mse\" for the mean squared error, which is equal to variance\n        reduction as feature selection criterion, and \"mae\" for the mean\n        absolute error.\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `round(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    bootstrap : bool, default=True\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        Only available if bootstrap=True.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls both the randomness of the bootstrapping of the samples used\n        when building trees (if ``bootstrap=True``) and the sampling of the\n        features to consider when looking for the best split at each node\n        (if ``max_features < n_features``).\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : DecisionTreeRegressor\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeRegressor\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training set.\n        This attribute exists only when ``oob_score`` is True.\n\n    See Also\n    --------\n    DecisionTreeRegressor, ExtraTreesRegressor\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    The default value ``max_features=\"auto\"`` uses ``n_features``\n    rather than ``n_features / 3``. The latter was originally suggested in\n    [1], whereas the former was more recently justified empirically in [2].\n\n    References\n    ----------\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n           trees\", Machine Learning, 63(1), 3-42, 2006.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=4, n_informative=2,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n    >>> regr.fit(X, y)\n    RandomForestRegressor(...)\n    >>> print(regr.predict([[0, 0, 0, 0]]))\n    [-8.32987858]\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"mse\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=True,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=DecisionTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha",
      "instance_attributes": [
        {
          "name": "criterion",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_depth",
          "types": null
        },
        {
          "name": "min_samples_split",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_samples_leaf",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_weight_fraction_leaf",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_features",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_leaf_nodes",
          "types": null
        },
        {
          "name": "min_impurity_decrease",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_impurity_split",
          "types": null
        },
        {
          "name": "ccp_alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding",
      "name": "RandomTreesEmbedding",
      "qname": "sklearn.ensemble._forest.RandomTreesEmbedding",
      "decorators": [],
      "superclasses": [
        "BaseForest"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__",
        "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/_set_oob_score",
        "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit",
        "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit_transform",
        "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "An ensemble of totally random trees.\n\nAn unsupervised transformation of a dataset to a high-dimensional\nsparse representation. A datapoint is coded according to which leaf of\neach tree it is sorted into. Using a one-hot encoding of the leaves,\nthis leads to a binary coding with as many ones as there are trees in\nthe forest.\n\nThe dimensionality of the resulting representation is\n``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\nthe number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\nRead more in the :ref:`User Guide <random_trees_embedding>`.",
      "docstring": "An ensemble of totally random trees.\n\nAn unsupervised transformation of a dataset to a high-dimensional\nsparse representation. A datapoint is coded according to which leaf of\neach tree it is sorted into. Using a one-hot encoding of the leaves,\nthis leads to a binary coding with as many ones as there are trees in\nthe forest.\n\nThe dimensionality of the resulting representation is\n``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\nthe number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\nRead more in the :ref:`User Guide <random_trees_embedding>`.\n\nParameters\n----------\nn_estimators : int, default=100\n    Number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\nmax_depth : int, default=5\n    The maximum depth of each tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` is the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` is the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\nsparse_output : bool, default=True\n    Whether or not to return a sparse CSR matrix, as default behavior,\n    or to return a dense array compatible with dense pipeline operators.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the generation of the random `y` used to fit the trees\n    and the draw of the splits for each feature at the trees' nodes.\n    See :term:`Glossary <random_state>` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\nAttributes\n----------\nbase_estimator_ : DecisionTreeClassifier instance\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\nestimators_ : list of DecisionTreeClassifier instances\n    The collection of fitted sub-estimators.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The feature importances (the higher, the more important the feature).\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\none_hot_encoder_ : OneHotEncoder instance\n    One-hot encoder used to create the sparse embedding.\n\nReferences\n----------\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n       Machine Learning, 63(1), 3-42, 2006.\n.. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n       visual codebooks using randomized clustering forests\"\n       NIPS 2007\n\nExamples\n--------\n>>> from sklearn.ensemble import RandomTreesEmbedding\n>>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n>>> random_trees = RandomTreesEmbedding(\n...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n>>> X_sparse_embedding = random_trees.transform(X)\n>>> X_sparse_embedding.toarray()\narray([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])",
      "code": "class RandomTreesEmbedding(BaseForest):\n    \"\"\"\n    An ensemble of totally random trees.\n\n    An unsupervised transformation of a dataset to a high-dimensional\n    sparse representation. A datapoint is coded according to which leaf of\n    each tree it is sorted into. Using a one-hot encoding of the leaves,\n    this leads to a binary coding with as many ones as there are trees in\n    the forest.\n\n    The dimensionality of the resulting representation is\n    ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n    the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\n    Read more in the :ref:`User Guide <random_trees_embedding>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        Number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    max_depth : int, default=5\n        The maximum depth of each tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` is the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` is the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    sparse_output : bool, default=True\n        Whether or not to return a sparse CSR matrix, as default behavior,\n        or to return a dense array compatible with dense pipeline operators.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the generation of the random `y` used to fit the trees\n        and the draw of the splits for each feature at the trees' nodes.\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    Attributes\n    ----------\n    base_estimator_ : DecisionTreeClassifier instance\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeClassifier instances\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    one_hot_encoder_ : OneHotEncoder instance\n        One-hot encoder used to create the sparse embedding.\n\n    References\n    ----------\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n           Machine Learning, 63(1), 3-42, 2006.\n    .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n           visual codebooks using randomized clustering forests\"\n           NIPS 2007\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomTreesEmbedding\n    >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n    >>> random_trees = RandomTreesEmbedding(\n    ...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n    >>> X_sparse_embedding = random_trees.transform(X)\n    >>> X_sparse_embedding.toarray()\n    array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n           [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n           [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n           [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n           [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])\n    \"\"\"\n\n    criterion = 'mse'\n    max_features = 1\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 max_depth=5,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 sparse_output=True,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\"),\n            bootstrap=False,\n            oob_score=False,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=None)\n\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.sparse_output = sparse_output\n\n    def _set_oob_score(self, X, y):\n        raise NotImplementedError(\"OOB score not supported by tree embedding\")\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        self.fit_transform(X, y, sample_weight=sample_weight)\n        return self\n\n    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator and transform dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data used to build forests. Use ``dtype=np.float32`` for\n            maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        Returns\n        -------\n        X_transformed : sparse matrix of shape (n_samples, n_out)\n            Transformed dataset.\n        \"\"\"\n        X = check_array(X, accept_sparse=['csc'])\n        if issparse(X):\n            # Pre-sort indices to avoid that each individual tree of the\n            # ensemble sorts the indices.\n            X.sort_indices()\n\n        rnd = check_random_state(self.random_state)\n        y = rnd.uniform(size=X.shape[0])\n        super().fit(X, y, sample_weight=sample_weight)\n\n        self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output)\n        return self.one_hot_encoder_.fit_transform(self.apply(X))\n\n    def transform(self, X):\n        \"\"\"\n        Transform dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data to be transformed. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csr_matrix`` for maximum efficiency.\n\n        Returns\n        -------\n        X_transformed : sparse matrix of shape (n_samples, n_out)\n            Transformed dataset.\n        \"\"\"\n        check_is_fitted(self)\n        return self.one_hot_encoder_.transform(self.apply(X))",
      "instance_attributes": [
        {
          "name": "max_depth",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_samples_split",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_samples_leaf",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "min_weight_fraction_leaf",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "max_leaf_nodes",
          "types": null
        },
        {
          "name": "min_impurity_decrease",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_impurity_split",
          "types": null
        },
        {
          "name": "sparse_output",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "one_hot_encoder_",
          "types": {
            "kind": "NamedType",
            "name": "OneHotEncoder"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier",
      "name": "GradientBoostingClassifier",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier",
      "decorators": [],
      "superclasses": [
        "ClassifierMixin",
        "BaseGradientBoosting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/_validate_y",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/_warn_mae_for_criterion",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/decision_function",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_decision_function",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_proba",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_log_proba",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict_proba"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Gradient Boosting for classification.\n\nGB builds an additive model in a\nforward stage-wise fashion; it allows for the optimization of\narbitrary differentiable loss functions. In each stage ``n_classes_``\nregression trees are fit on the negative gradient of the\nbinomial or multinomial deviance loss function. Binary classification\nis a special case where only a single regression tree is induced.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.",
      "docstring": "Gradient Boosting for classification.\n\nGB builds an additive model in a\nforward stage-wise fashion; it allows for the optimization of\narbitrary differentiable loss functions. In each stage ``n_classes_``\nregression trees are fit on the negative gradient of the\nbinomial or multinomial deviance loss function. Binary classification\nis a special case where only a single regression tree is induced.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n\nParameters\n----------\nloss : {'deviance', 'exponential'}, default='deviance'\n    The loss function to be optimized. 'deviance' refers to\n    deviance (= logistic regression) for classification\n    with probabilistic outputs. For loss 'exponential' gradient\n    boosting recovers the AdaBoost algorithm.\n\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n\nn_estimators : int, default=100\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n\nsubsample : float, default=1.0\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n\ncriterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n    The function to measure the quality of a split. Supported criteria\n    are 'friedman_mse' for the mean squared error with improvement\n    score by Friedman, 'mse' for mean squared error, and 'mae' for\n    the mean absolute error. The default value of 'friedman_mse' is\n    generally the best as it can provide a better approximation in\n    some cases.\n\n    .. versionadded:: 0.18\n    .. deprecated:: 0.24\n        `criterion='mae'` is deprecated and will be removed in version\n        1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n        instead, as trees should use a least-square criterion in\n        Gradient Boosting.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_depth : int, default=3\n    The maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\ninit : estimator or 'zero', default=None\n    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n    'zero', the initial raw predictions are set to zero. By default, a\n    ``DummyEstimator`` predicting the classes priors is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If 'auto', then `max_features=sqrt(n_features)`.\n    - If 'sqrt', then `max_features=sqrt(n_features)`.\n    - If 'log2', then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=None\n    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations. The split is stratified.\n\n    .. versionadded:: 0.20\n\ntol : float, default=1e-4\n    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n\n    .. versionadded:: 0.20\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nn_estimators_ : int\n    The number of estimators as selected by early stopping (if\n    ``n_iter_no_change`` is specified). Otherwise it is set to\n    ``n_estimators``.\n\n    .. versionadded:: 0.20\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_improvement_ : ndarray of shape (n_estimators,)\n    The improvement in loss (= deviance) on the out-of-bag samples\n    relative to the previous iteration.\n    ``oob_improvement_[0]`` is the improvement in\n    loss of the first stage over the ``init`` estimator.\n    Only available if ``subsample < 1.0``\n\ntrain_score_ : ndarray of shape (n_estimators,)\n    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n    model at iteration ``i`` on the in-bag sample.\n    If ``subsample == 1`` this is the deviance on the training data.\n\nloss_ : LossFunction\n    The concrete ``LossFunction`` object.\n\ninit_ : estimator\n    The estimator that provides the initial predictions.\n    Set via the ``init`` argument or ``loss.init_estimator``.\n\nestimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\n    The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n    classification, otherwise n_classes.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_features_ : int\n    The number of data features.\n\nn_classes_ : int\n    The number of classes.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nSee Also\n--------\nHistGradientBoostingClassifier : Histogram-based Gradient Boosting\n    Classification Tree.\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nRandomForestClassifier : A meta-estimator that fits a number of decision\n    tree classifiers on various sub-samples of the dataset and uses\n    averaging to improve the predictive accuracy and control over-fitting.\nAdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n    on the original dataset and then fits additional copies of the\n    classifier on the same dataset where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers\n    focus more on difficult cases.\n\nNotes\n-----\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data and\n``max_features=n_features``, if the improvement of the criterion is\nidentical for several splits enumerated during the search of the best\nsplit. To obtain a deterministic behaviour during fitting,\n``random_state`` has to be fixed.\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\nJ. Friedman, Stochastic Gradient Boosting, 1999\n\nT. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009.\n\nExamples\n--------\nThe following example shows how to fit a gradient boosting classifier with\n100 decision stumps as weak learners.\n\n>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)\n0.913...",
      "code": "class GradientBoostingClassifier(ClassifierMixin, BaseGradientBoosting):\n    \"\"\"Gradient Boosting for classification.\n\n    GB builds an additive model in a\n    forward stage-wise fashion; it allows for the optimization of\n    arbitrary differentiable loss functions. In each stage ``n_classes_``\n    regression trees are fit on the negative gradient of the\n    binomial or multinomial deviance loss function. Binary classification\n    is a special case where only a single regression tree is induced.\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'deviance', 'exponential'}, default='deviance'\n        The loss function to be optimized. 'deviance' refers to\n        deviance (= logistic regression) for classification\n        with probabilistic outputs. For loss 'exponential' gradient\n        boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n\n    criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria\n        are 'friedman_mse' for the mean squared error with improvement\n        score by Friedman, 'mse' for mean squared error, and 'mae' for\n        the mean absolute error. The default value of 'friedman_mse' is\n        generally the best as it can provide a better approximation in\n        some cases.\n\n        .. versionadded:: 0.18\n        .. deprecated:: 0.24\n            `criterion='mae'` is deprecated and will be removed in version\n            1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n            instead, as trees should use a least-square criterion in\n            Gradient Boosting.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_depth : int, default=3\n        The maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n        'zero', the initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random spliting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If 'auto', then `max_features=sqrt(n_features)`.\n        - If 'sqrt', then `max_features=sqrt(n_features)`.\n        - If 'log2', then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss (= deviance) on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the deviance on the training data.\n\n    loss_ : LossFunction\n        The concrete ``LossFunction`` object.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of \\\nshape (n_estimators, ``loss_.K``)\n        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n        classification, otherwise n_classes.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_ : int\n        The number of data features.\n\n    n_classes_ : int\n        The number of classes.\n\n    max_features_ : int\n        The inferred value of max_features.\n\n    See Also\n    --------\n    HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n        Classification Tree.\n    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n    RandomForestClassifier : A meta-estimator that fits a number of decision\n        tree classifiers on various sub-samples of the dataset and uses\n        averaging to improve the predictive accuracy and control over-fitting.\n    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n        on the original dataset and then fits additional copies of the\n        classifier on the same dataset where the weights of incorrectly\n        classified instances are adjusted such that subsequent classifiers\n        focus more on difficult cases.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n\n    Examples\n    --------\n    The following example shows how to fit a gradient boosting classifier with\n    100 decision stumps as weak learners.\n\n    >>> from sklearn.datasets import make_hastie_10_2\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n\n    >>> X, y = make_hastie_10_2(random_state=0)\n    >>> X_train, X_test = X[:2000], X[2000:]\n    >>> y_train, y_test = y[:2000], y[2000:]\n\n    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    ...     max_depth=1, random_state=0).fit(X_train, y_train)\n    >>> clf.score(X_test, y_test)\n    0.913...\n    \"\"\"\n\n    _SUPPORTED_LOSS = ('deviance', 'exponential')\n\n    @_deprecate_positional_args\n    def __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100,\n                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n                 max_depth=3, min_impurity_decrease=0.,\n                 min_impurity_split=None, init=None,\n                 random_state=None, max_features=None, verbose=0,\n                 max_leaf_nodes=None, warm_start=False,\n                 validation_fraction=0.1, n_iter_no_change=None, tol=1e-4,\n                 ccp_alpha=0.0):\n\n        super().__init__(\n            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n            criterion=criterion, min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_depth=max_depth, init=init, subsample=subsample,\n            max_features=max_features,\n            random_state=random_state, verbose=verbose,\n            max_leaf_nodes=max_leaf_nodes,\n            min_impurity_decrease=min_impurity_decrease,\n            min_impurity_split=min_impurity_split,\n            warm_start=warm_start, validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, ccp_alpha=ccp_alpha)\n\n    def _validate_y(self, y, sample_weight):\n        check_classification_targets(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n        if n_trim_classes < 2:\n            raise ValueError(\"y contains %d class after sample_weight \"\n                             \"trimmed classes with zero weights, while a \"\n                             \"minimum of 2 classes are required.\"\n                             % n_trim_classes)\n        self._n_classes = len(self.classes_)\n        # expose n_classes_ attribute\n        self.n_classes_ = self._n_classes\n        return y\n\n    def _warn_mae_for_criterion(self):\n        # TODO: This should raise an error from 1.1\n        warnings.warn(\"criterion='mae' was deprecated in version 0.24 and \"\n                      \"will be removed in version 1.1 (renaming of 0.26). Use \"\n                      \"criterion='friedman_mse' or 'mse' instead, as trees \"\n                      \"should use a least-square criterion in Gradient \"\n                      \"Boosting.\", FutureWarning)\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            order of the classes corresponds to that in the attribute\n            :term:`classes_`. Regression and binary classification produce an\n            array of shape (n_samples,).\n        \"\"\"\n        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            return raw_predictions.ravel()\n        return raw_predictions\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        yield from self._staged_raw_predict(X)\n\n    def predict(self, X):\n        \"\"\"Predict class for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        raw_predictions = self.decision_function(X)\n        encoded_labels = \\\n            self.loss_._raw_prediction_to_decision(raw_predictions)\n        return self.classes_.take(encoded_labels, axis=0)\n\n    def staged_predict(self, X):\n        \"\"\"Predict class at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            encoded_labels = \\\n                self.loss_._raw_prediction_to_decision(raw_predictions)\n            yield self.classes_.take(encoded_labels, axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        raw_predictions = self.decision_function(X)\n        try:\n            return self.loss_._raw_prediction_to_proba(raw_predictions)\n        except NotFittedError:\n            raise\n        except AttributeError as e:\n            raise AttributeError('loss=%r does not support predict_proba' %\n                                 self.loss) from e\n\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n        return np.log(proba)\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        try:\n            for raw_predictions in self._staged_raw_predict(X):\n                yield self.loss_._raw_prediction_to_proba(raw_predictions)\n        except NotFittedError:\n            raise\n        except AttributeError as e:\n            raise AttributeError('loss=%r does not support predict_proba' %\n                                 self.loss) from e",
      "instance_attributes": [
        {
          "name": "classes_",
          "types": null
        },
        {
          "name": "_n_classes",
          "types": null
        },
        {
          "name": "n_classes_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor",
      "name": "GradientBoostingRegressor",
      "qname": "sklearn.ensemble._gb.GradientBoostingRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "BaseGradientBoosting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/_validate_y",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/_warn_mae_for_criterion",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/predict",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/staged_predict",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/apply",
        "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/n_classes_@getter"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Gradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion;\nit allows for the optimization of arbitrary differentiable loss functions.\nIn each stage a regression tree is fit on the negative gradient of the\ngiven loss function.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.",
      "docstring": "Gradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion;\nit allows for the optimization of arbitrary differentiable loss functions.\nIn each stage a regression tree is fit on the negative gradient of the\ngiven loss function.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n\nParameters\n----------\nloss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n    Loss function to be optimized. 'ls' refers to least squares\n    regression. 'lad' (least absolute deviation) is a highly robust\n    loss function solely based on order information of the input\n    variables. 'huber' is a combination of the two. 'quantile'\n    allows quantile regression (use `alpha` to specify the quantile).\n\nlearning_rate : float, default=0.1\n    Learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n\nn_estimators : int, default=100\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n\nsubsample : float, default=1.0\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n\ncriterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n    The function to measure the quality of a split. Supported criteria\n    are \"friedman_mse\" for the mean squared error with improvement\n    score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n    the mean absolute error. The default value of \"friedman_mse\" is\n    generally the best as it can provide a better approximation in\n    some cases.\n\n    .. versionadded:: 0.18\n    .. deprecated:: 0.24\n        `criterion='mae'` is deprecated and will be removed in version\n        1.1 (renaming of 0.26). The correct way of minimizing the absolute\n        error is to use `loss='lad'` instead.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_depth : int, default=3\n    Maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nmin_impurity_split : float, default=None\n    Threshold for early stopping in tree growth. A node will split\n    if its impurity is above the threshold, otherwise it is a leaf.\n\n    .. deprecated:: 0.19\n       ``min_impurity_split`` has been deprecated in favor of\n       ``min_impurity_decrease`` in 0.19. The default value of\n       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n       will be removed in 1.0 (renaming of 0.25).\n       Use ``min_impurity_decrease`` instead.\n\ninit : estimator or 'zero', default=None\n    An estimator object that is used to compute the initial predictions.\n    ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n    initial raw predictions are set to zero. By default a\n    ``DummyEstimator`` is used, predicting either the average target value\n    (for loss='ls'), or a quantile for the other losses.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given to each Tree estimator at each\n    boosting iteration.\n    In addition, it controls the random permutation of the features at\n    each split (see Notes for more details).\n    It also controls the random spliting of the training data to obtain a\n    validation set if `n_iter_no_change` is not None.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nmax_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `int(max_features * n_features)` features are considered at each\n      split.\n    - If \"auto\", then `max_features=n_features`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    Choosing `max_features < n_features` leads to a reduction of variance\n    and an increase in bias.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nalpha : float, default=0.9\n    The alpha-quantile of the huber loss function and the quantile\n    loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n\nverbose : int, default=0\n    Enable verbose output. If 1 then it prints progress and performance\n    once in a while (the more trees the lower the frequency). If greater\n    than 1 then it prints progress and performance for every tree.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just erase the\n    previous solution. See :term:`the Glossary <warm_start>`.\n\nvalidation_fraction : float, default=0.1\n    The proportion of training data to set aside as validation set for\n    early stopping. Must be between 0 and 1.\n    Only used if ``n_iter_no_change`` is set to an integer.\n\n    .. versionadded:: 0.20\n\nn_iter_no_change : int, default=None\n    ``n_iter_no_change`` is used to decide if early stopping will be used\n    to terminate training when validation score is not improving. By\n    default it is set to None to disable early stopping. If set to a\n    number, it will set aside ``validation_fraction`` size of the training\n    data as validation and terminate training when validation score is not\n    improving in all of the previous ``n_iter_no_change`` numbers of\n    iterations.\n\n    .. versionadded:: 0.20\n\ntol : float, default=1e-4\n    Tolerance for the early stopping. When the loss is not improving\n    by at least tol for ``n_iter_no_change`` iterations (if set to a\n    number), the training stops.\n\n    .. versionadded:: 0.20\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_improvement_ : ndarray of shape (n_estimators,)\n    The improvement in loss (= deviance) on the out-of-bag samples\n    relative to the previous iteration.\n    ``oob_improvement_[0]`` is the improvement in\n    loss of the first stage over the ``init`` estimator.\n    Only available if ``subsample < 1.0``\n\ntrain_score_ : ndarray of shape (n_estimators,)\n    The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n    model at iteration ``i`` on the in-bag sample.\n    If ``subsample == 1`` this is the deviance on the training data.\n\nloss_ : LossFunction\n    The concrete ``LossFunction`` object.\n\ninit_ : estimator\n    The estimator that provides the initial predictions.\n    Set via the ``init`` argument or ``loss.init_estimator``.\n\nestimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n    The collection of fitted sub-estimators.\n\nn_classes_ : int\n    The number of classes, set to 1 for regressors.\n\n    .. deprecated:: 0.24\n        Attribute ``n_classes_`` was deprecated in version 0.24 and\n        will be removed in 1.1 (renaming of 0.26).\n\nn_estimators_ : int\n    The number of estimators as selected by early stopping (if\n    ``n_iter_no_change`` is specified). Otherwise it is set to\n    ``n_estimators``.\n\nn_features_ : int\n    The number of data features.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nSee Also\n--------\nHistGradientBoostingRegressor : Histogram-based Gradient Boosting\n    Classification Tree.\nsklearn.tree.DecisionTreeRegressor : A decision tree regressor.\nsklearn.tree.RandomForestRegressor : A random forest regressor.\n\nNotes\n-----\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data and\n``max_features=n_features``, if the improvement of the criterion is\nidentical for several splits enumerated during the search of the best\nsplit. To obtain a deterministic behaviour during fitting,\n``random_state`` has to be fixed.\n\nExamples\n--------\n>>> from sklearn.datasets import make_regression\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = make_regression(random_state=0)\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=0)\n>>> reg = GradientBoostingRegressor(random_state=0)\n>>> reg.fit(X_train, y_train)\nGradientBoostingRegressor(random_state=0)\n>>> reg.predict(X_test[1:2])\narray([-61...])\n>>> reg.score(X_test, y_test)\n0.4...\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\nJ. Friedman, Stochastic Gradient Boosting, 1999\n\nT. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009.",
      "code": "class GradientBoostingRegressor(RegressorMixin, BaseGradientBoosting):\n    \"\"\"Gradient Boosting for regression.\n\n    GB builds an additive model in a forward stage-wise fashion;\n    it allows for the optimization of arbitrary differentiable loss functions.\n    In each stage a regression tree is fit on the negative gradient of the\n    given loss function.\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n        Loss function to be optimized. 'ls' refers to least squares\n        regression. 'lad' (least absolute deviation) is a highly robust\n        loss function solely based on order information of the input\n        variables. 'huber' is a combination of the two. 'quantile'\n        allows quantile regression (use `alpha` to specify the quantile).\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n\n    criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria\n        are \"friedman_mse\" for the mean squared error with improvement\n        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n        the mean absolute error. The default value of \"friedman_mse\" is\n        generally the best as it can provide a better approximation in\n        some cases.\n\n        .. versionadded:: 0.18\n        .. deprecated:: 0.24\n            `criterion='mae'` is deprecated and will be removed in version\n            1.1 (renaming of 0.26). The correct way of minimizing the absolute\n            error is to use `loss='lad'` instead.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_depth : int, default=3\n        Maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=None\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n           will be removed in 1.0 (renaming of 0.25).\n           Use ``min_impurity_decrease`` instead.\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n        initial raw predictions are set to zero. By default a\n        ``DummyEstimator`` is used, predicting either the average target value\n        (for loss='ls'), or a quantile for the other losses.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random spliting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    alpha : float, default=0.9\n        The alpha-quantile of the huber loss function and the quantile\n        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss (= deviance) on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the deviance on the training data.\n\n    loss_ : LossFunction\n        The concrete ``LossFunction`` object.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n        The collection of fitted sub-estimators.\n\n    n_classes_ : int\n        The number of classes, set to 1 for regressors.\n\n        .. deprecated:: 0.24\n            Attribute ``n_classes_`` was deprecated in version 0.24 and\n            will be removed in 1.1 (renaming of 0.26).\n\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n    n_features_ : int\n        The number of data features.\n\n    max_features_ : int\n        The inferred value of max_features.\n\n    See Also\n    --------\n    HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n        Classification Tree.\n    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n    sklearn.tree.RandomForestRegressor : A random forest regressor.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_regression(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> reg = GradientBoostingRegressor(random_state=0)\n    >>> reg.fit(X_train, y_train)\n    GradientBoostingRegressor(random_state=0)\n    >>> reg.predict(X_test[1:2])\n    array([-61...])\n    >>> reg.score(X_test, y_test)\n    0.4...\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n    \"\"\"\n\n    _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')\n\n    @_deprecate_positional_args\n    def __init__(self, *, loss='ls', learning_rate=0.1, n_estimators=100,\n                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n                 max_depth=3, min_impurity_decrease=0.,\n                 min_impurity_split=None, init=None, random_state=None,\n                 max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n                 warm_start=False, validation_fraction=0.1,\n                 n_iter_no_change=None, tol=1e-4, ccp_alpha=0.0):\n\n        super().__init__(\n            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n            criterion=criterion, min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_depth=max_depth, init=init, subsample=subsample,\n            max_features=max_features,\n            min_impurity_decrease=min_impurity_decrease,\n            min_impurity_split=min_impurity_split,\n            random_state=random_state, alpha=alpha, verbose=verbose,\n            max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, ccp_alpha=ccp_alpha)\n\n    def _validate_y(self, y, sample_weight=None):\n        if y.dtype.kind == 'O':\n            y = y.astype(DOUBLE)\n        return y\n\n    def _warn_mae_for_criterion(self):\n        # TODO: This should raise an error from 1.1\n        warnings.warn(\"criterion='mae' was deprecated in version 0.24 and \"\n                      \"will be removed in version 1.1 (renaming of 0.26). The \"\n                      \"correct way of minimizing the absolute error is to use \"\n                      \" loss='lad' instead.\", FutureWarning)\n\n    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n        # In regression we can directly return the raw value from the trees.\n        return self._raw_predict(X).ravel()\n\n    def staged_predict(self, X):\n        \"\"\"Predict regression target at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield raw_predictions.ravel()\n\n    def apply(self, X):\n        \"\"\"Apply trees in the ensemble to X, return leaf indices.\n\n        .. versionadded:: 0.17\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will\n            be converted to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        X_leaves : array-like of shape (n_samples, n_estimators)\n            For each datapoint x in X and for each tree in the ensemble,\n            return the index of the leaf x ends up in each estimator.\n        \"\"\"\n\n        leaves = super().apply(X)\n        leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n        return leaves\n\n    # FIXME: to be removed in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute n_classes_ was deprecated \"  # type: ignore\n                \"in version 0.24 and will be removed in 1.1 \"\n                \"(renaming of 0.26).\")\n    @property\n    def n_classes_(self):\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_classes_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n        return 1",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier",
      "name": "HistGradientBoostingClassifier",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier",
      "decorators": [],
      "superclasses": [
        "ClassifierMixin",
        "BaseHistGradientBoosting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict_proba",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict_proba",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/decision_function",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_decision_function",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/_encode_y",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/_get_loss"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Histogram-based Gradient Boosting Classification Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21",
      "docstring": "Histogram-based Gradient Boosting Classification Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nloss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'},             default='auto'\n    The loss function to use in the boosting process. 'binary_crossentropy'\n    (also known as logistic loss) is used for binary classification and\n    generalizes to 'categorical_crossentropy' for multiclass\n    classification. 'auto' will automatically choose either loss depending\n    on the nature of the problem.\nlearning_rate : float, default=0.1\n    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\nmax_iter : int, default=100\n    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees for binary classification. For multiclass\n    classification, `n_classes` trees per iteration are built.\nmax_leaf_nodes : int or None, default=31\n    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\nmax_depth : int or None, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\nl2_regularization : float, default=0\n    The L2 regularization parameter. Use 0 for no regularization.\nmax_bins : int, default=255\n    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\ncategorical_features : array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None.\n    Indicates the categorical features.\n\n    - None : no feature will be considered categorical.\n    - boolean array-like : boolean mask indicating categorical features.\n    - integer array-like : integer indices indicating categorical\n      features.\n\n    For each categorical feature, there must be at most `max_bins` unique\n    categories, and each categorical value must be in [0, max_bins -1].\n\n    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n    .. versionadded:: 0.24\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a negative constraint, positive\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 0.23\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\nearly_stopping : 'auto' or bool, default='auto'\n    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n\n    .. versionadded:: 0.23\n\nscoring : str or callable or None, default='loss'\n    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer\n    is used. If ``scoring='loss'``, early stopping is checked\n    w.r.t the loss value. Only used if early stopping is performed.\nvalidation_fraction : int or float or None, default=0.1\n    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\nn_iter_no_change : int, default=10\n    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\ntol : float or None, default=1e-7\n    The absolute tolerance to use when comparing scores. The higher the\n    tolerance, the more likely we are to early stop: higher tolerance\n    means that it will be harder for subsequent iterations to be\n    considered an improvement upon the reference score.\nverbose : int, default=0\n    The verbosity level. If not zero, print some information about the\n    fitting process.\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nclasses_ : array, shape = (n_classes,)\n    Class labels.\ndo_early_stopping_ : bool\n    Indicates whether early stopping is used during training.\nn_iter_ : int\n    The number of iterations as selected by early stopping, depending on\n    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\nn_trees_per_iteration_ : int\n    The number of tree that are built at each iteration. This is equal to 1\n    for binary classification, and to ``n_classes`` for multiclass\n    classification.\ntrain_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the training data. The first entry\n    is the score of the ensemble before the first iteration. Scores are\n    computed according to the ``scoring`` parameter. If ``scoring`` is\n    not 'loss', scores are computed on a subset of at most 10 000\n    samples. Empty if no early stopping.\nvalidation_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the held-out validation data. The\n    first entry is the score of the ensemble before the first iteration.\n    Scores are computed according to the ``scoring`` parameter. Empty if\n    no early stopping or if ``validation_fraction`` is None.\nis_categorical_ : ndarray, shape (n_features, ) or None\n    Boolean mask for the categorical features. ``None`` if there are no\n    categorical features.\n\nExamples\n--------\n>>> # To use this experimental feature, we need to explicitly ask for it:\n>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n>>> from sklearn.ensemble import HistGradientBoostingClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> clf = HistGradientBoostingClassifier().fit(X, y)\n>>> clf.score(X, y)\n1.0",
      "code": "class HistGradientBoostingClassifier(ClassifierMixin,\n                                     BaseHistGradientBoosting):\n    \"\"\"Histogram-based Gradient Boosting Classification Tree.\n\n    This estimator is much faster than\n    :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n    for big datasets (n_samples >= 10 000).\n\n    This estimator has native support for missing values (NaNs). During\n    training, the tree grower learns at each split point whether samples\n    with missing values should go to the left or right child, based on the\n    potential gain. When predicting, samples with missing values are\n    assigned to the left or right child consequently. If no missing values\n    were encountered for a given feature during training, then samples with\n    missing values are mapped to whichever child has the most samples.\n\n    This implementation is inspired by\n    `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n    .. note::\n\n      This estimator is still **experimental** for now: the predictions\n      and the API might change without any deprecation cycle. To use it,\n      you need to explicitly import ``enable_hist_gradient_boosting``::\n\n        >>> # explicitly require this experimental feature\n        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n        >>> # now you can import normally from ensemble\n        >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\n    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n    .. versionadded:: 0.21\n\n    Parameters\n    ----------\n    loss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'}, \\\n            default='auto'\n        The loss function to use in the boosting process. 'binary_crossentropy'\n        (also known as logistic loss) is used for binary classification and\n        generalizes to 'categorical_crossentropy' for multiclass\n        classification. 'auto' will automatically choose either loss depending\n        on the nature of the problem.\n    learning_rate : float, default=0.1\n        The learning rate, also known as *shrinkage*. This is used as a\n        multiplicative factor for the leaves values. Use ``1`` for no\n        shrinkage.\n    max_iter : int, default=100\n        The maximum number of iterations of the boosting process, i.e. the\n        maximum number of trees for binary classification. For multiclass\n        classification, `n_classes` trees per iteration are built.\n    max_leaf_nodes : int or None, default=31\n        The maximum number of leaves for each tree. Must be strictly greater\n        than 1. If None, there is no maximum limit.\n    max_depth : int or None, default=None\n        The maximum depth of each tree. The depth of a tree is the number of\n        edges to go from the root to the deepest leaf.\n        Depth isn't constrained by default.\n    min_samples_leaf : int, default=20\n        The minimum number of samples per leaf. For small datasets with less\n        than a few hundred samples, it is recommended to lower this value\n        since only very shallow trees would be built.\n    l2_regularization : float, default=0\n        The L2 regularization parameter. Use 0 for no regularization.\n    max_bins : int, default=255\n        The maximum number of bins to use for non-missing values. Before\n        training, each feature of the input array `X` is binned into\n        integer-valued bins, which allows for a much faster training stage.\n        Features with a small number of unique values may use less than\n        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n        is always reserved for missing values. Must be no larger than 255.\n    categorical_features : array-like of {bool, int} of shape (n_features) \\\n            or shape (n_categorical_features,), default=None.\n        Indicates the categorical features.\n\n        - None : no feature will be considered categorical.\n        - boolean array-like : boolean mask indicating categorical features.\n        - integer array-like : integer indices indicating categorical\n          features.\n\n        For each categorical feature, there must be at most `max_bins` unique\n        categories, and each categorical value must be in [0, max_bins -1].\n\n        Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n        .. versionadded:: 0.24\n\n    monotonic_cst : array-like of int of shape (n_features), default=None\n        Indicates the monotonic constraint to enforce on each feature. -1, 1\n        and 0 respectively correspond to a negative constraint, positive\n        constraint and no constraint. Read more in the :ref:`User Guide\n        <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 0.23\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble. For results to be valid, the\n        estimator should be re-trained on the same data only.\n        See :term:`the Glossary <warm_start>`.\n    early_stopping : 'auto' or bool, default='auto'\n        If 'auto', early stopping is enabled if the sample size is larger than\n        10000. If True, early stopping is enabled, otherwise early stopping is\n        disabled.\n\n        .. versionadded:: 0.23\n\n    scoring : str or callable or None, default='loss'\n        Scoring parameter to use for early stopping. It can be a single\n        string (see :ref:`scoring_parameter`) or a callable (see\n        :ref:`scoring`). If None, the estimator's default scorer\n        is used. If ``scoring='loss'``, early stopping is checked\n        w.r.t the loss value. Only used if early stopping is performed.\n    validation_fraction : int or float or None, default=0.1\n        Proportion (or absolute size) of training data to set aside as\n        validation data for early stopping. If None, early stopping is done on\n        the training data. Only used if early stopping is performed.\n    n_iter_no_change : int, default=10\n        Used to determine when to \"early stop\". The fitting process is\n        stopped when none of the last ``n_iter_no_change`` scores are better\n        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n        tolerance. Only used if early stopping is performed.\n    tol : float or None, default=1e-7\n        The absolute tolerance to use when comparing scores. The higher the\n        tolerance, the more likely we are to early stop: higher tolerance\n        means that it will be harder for subsequent iterations to be\n        considered an improvement upon the reference score.\n    verbose : int, default=0\n        The verbosity level. If not zero, print some information about the\n        fitting process.\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the subsampling in the\n        binning process, and the train/validation data split if early stopping\n        is enabled.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    classes_ : array, shape = (n_classes,)\n        Class labels.\n    do_early_stopping_ : bool\n        Indicates whether early stopping is used during training.\n    n_iter_ : int\n        The number of iterations as selected by early stopping, depending on\n        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n    n_trees_per_iteration_ : int\n        The number of tree that are built at each iteration. This is equal to 1\n        for binary classification, and to ``n_classes`` for multiclass\n        classification.\n    train_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the training data. The first entry\n        is the score of the ensemble before the first iteration. Scores are\n        computed according to the ``scoring`` parameter. If ``scoring`` is\n        not 'loss', scores are computed on a subset of at most 10 000\n        samples. Empty if no early stopping.\n    validation_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the held-out validation data. The\n        first entry is the score of the ensemble before the first iteration.\n        Scores are computed according to the ``scoring`` parameter. Empty if\n        no early stopping or if ``validation_fraction`` is None.\n    is_categorical_ : ndarray, shape (n_features, ) or None\n        Boolean mask for the categorical features. ``None`` if there are no\n        categorical features.\n\n    Examples\n    --------\n    >>> # To use this experimental feature, we need to explicitly ask for it:\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = HistGradientBoostingClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    1.0\n    \"\"\"\n\n    _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy',\n                     'auto')\n\n    @_deprecate_positional_args\n    def __init__(self, loss='auto', *, learning_rate=0.1, max_iter=100,\n                 max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,\n                 l2_regularization=0., max_bins=255,\n                 categorical_features=None,  monotonic_cst=None,\n                 warm_start=False, early_stopping='auto', scoring='loss',\n                 validation_fraction=0.1, n_iter_no_change=10, tol=1e-7,\n                 verbose=0, random_state=None):\n        super(HistGradientBoostingClassifier, self).__init__(\n            loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n            max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n            min_samples_leaf=min_samples_leaf,\n            l2_regularization=l2_regularization, max_bins=max_bins,\n            categorical_features=categorical_features,\n            monotonic_cst=monotonic_cst,\n            warm_start=warm_start,\n            early_stopping=early_stopping, scoring=scoring,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n            random_state=random_state)\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        # TODO: This could be done in parallel\n        encoded_classes = np.argmax(self.predict_proba(X), axis=1)\n        return self.classes_[encoded_classes]\n\n    def staged_predict(self, X):\n        \"\"\"Predict classes at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes of the input samples, for each iteration.\n        \"\"\"\n        for proba in self.staged_predict_proba(X):\n            encoded_classes = np.argmax(proba, axis=1)\n            yield self.classes_.take(encoded_classes, axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        p : ndarray, shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        raw_predictions = self._raw_predict(X)\n        return self._loss.predict_proba(raw_predictions)\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted class probabilities of the input samples,\n            for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield self._loss.predict_proba(raw_predictions)\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        decision : ndarray, shape (n_samples,) or \\\n                (n_samples, n_trees_per_iteration)\n            The raw predicted values (i.e. the sum of the trees leaves) for\n            each sample. n_trees_per_iteration is equal to the number of\n            classes in multiclass classification.\n        \"\"\"\n        decision = self._raw_predict(X)\n        if decision.shape[0] == 1:\n            decision = decision.ravel()\n        return decision.T\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        decision : generator of ndarray of shape (n_samples,) or \\\n                (n_samples, n_trees_per_iteration)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        for staged_decision in self._staged_raw_predict(X):\n            if staged_decision.shape[0] == 1:\n                staged_decision = staged_decision.ravel()\n            yield staged_decision.T\n\n    def _encode_y(self, y):\n        # encode classes into 0 ... n_classes - 1 and sets attributes classes_\n        # and n_trees_per_iteration_\n        check_classification_targets(y)\n\n        label_encoder = LabelEncoder()\n        encoded_y = label_encoder.fit_transform(y)\n        self.classes_ = label_encoder.classes_\n        n_classes = self.classes_.shape[0]\n        # only 1 tree for binary classification. For multiclass classification,\n        # we build 1 tree per class.\n        self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes\n        encoded_y = encoded_y.astype(Y_DTYPE, copy=False)\n        return encoded_y\n\n    def _get_loss(self, sample_weight):\n        if (self.loss == 'categorical_crossentropy' and\n                self.n_trees_per_iteration_ == 1):\n            raise ValueError(\"'categorical_crossentropy' is not suitable for \"\n                             \"a binary classification problem. Please use \"\n                             \"'auto' or 'binary_crossentropy' instead.\")\n\n        if self.loss == 'auto':\n            if self.n_trees_per_iteration_ == 1:\n                return _LOSSES['binary_crossentropy'](\n                    sample_weight=sample_weight)\n            else:\n                return _LOSSES['categorical_crossentropy'](\n                    sample_weight=sample_weight)\n\n        return _LOSSES[self.loss](sample_weight=sample_weight)",
      "instance_attributes": [
        {
          "name": "classes_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_trees_per_iteration_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor",
      "name": "HistGradientBoostingRegressor",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "BaseHistGradientBoosting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/predict",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/staged_predict",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/_encode_y",
        "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/_get_loss"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Histogram-based Gradient Boosting Regression Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingRegressor\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21",
      "docstring": "Histogram-based Gradient Boosting Regression Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingRegressor\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nloss : {'least_squares', 'least_absolute_deviation', 'poisson'},             default='least_squares'\n    The loss function to use in the boosting process. Note that the\n    \"least squares\" and \"poisson\" losses actually implement\n    \"half least squares loss\" and \"half poisson deviance\" to simplify the\n    computation of the gradient. Furthermore, \"poisson\" loss internally\n    uses a log-link and requires ``y >= 0``\n\n    .. versionchanged:: 0.23\n       Added option 'poisson'.\n\nlearning_rate : float, default=0.1\n    The learning rate, also known as *shrinkage*. This is used as a\n    multiplicative factor for the leaves values. Use ``1`` for no\n    shrinkage.\nmax_iter : int, default=100\n    The maximum number of iterations of the boosting process, i.e. the\n    maximum number of trees.\nmax_leaf_nodes : int or None, default=31\n    The maximum number of leaves for each tree. Must be strictly greater\n    than 1. If None, there is no maximum limit.\nmax_depth : int or None, default=None\n    The maximum depth of each tree. The depth of a tree is the number of\n    edges to go from the root to the deepest leaf.\n    Depth isn't constrained by default.\nmin_samples_leaf : int, default=20\n    The minimum number of samples per leaf. For small datasets with less\n    than a few hundred samples, it is recommended to lower this value\n    since only very shallow trees would be built.\nl2_regularization : float, default=0\n    The L2 regularization parameter. Use ``0`` for no regularization\n    (default).\nmax_bins : int, default=255\n    The maximum number of bins to use for non-missing values. Before\n    training, each feature of the input array `X` is binned into\n    integer-valued bins, which allows for a much faster training stage.\n    Features with a small number of unique values may use less than\n    ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n    is always reserved for missing values. Must be no larger than 255.\ncategorical_features : array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,), default=None.\n    Indicates the categorical features.\n\n    - None : no feature will be considered categorical.\n    - boolean array-like : boolean mask indicating categorical features.\n    - integer array-like : integer indices indicating categorical\n      features.\n\n    For each categorical feature, there must be at most `max_bins` unique\n    categories, and each categorical value must be in [0, max_bins -1].\n\n    Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n    .. versionadded:: 0.24\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonic constraint to enforce on each feature. -1, 1\n    and 0 respectively correspond to a negative constraint, positive\n    constraint and no constraint. Read more in the :ref:`User Guide\n    <monotonic_cst_gbdt>`.\n\n    .. versionadded:: 0.23\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble. For results to be valid, the\n    estimator should be re-trained on the same data only.\n    See :term:`the Glossary <warm_start>`.\nearly_stopping : 'auto' or bool, default='auto'\n    If 'auto', early stopping is enabled if the sample size is larger than\n    10000. If True, early stopping is enabled, otherwise early stopping is\n    disabled.\n\n    .. versionadded:: 0.23\n\nscoring : str or callable or None, default='loss'\n    Scoring parameter to use for early stopping. It can be a single\n    string (see :ref:`scoring_parameter`) or a callable (see\n    :ref:`scoring`). If None, the estimator's default scorer is used. If\n    ``scoring='loss'``, early stopping is checked w.r.t the loss value.\n    Only used if early stopping is performed.\nvalidation_fraction : int or float or None, default=0.1\n    Proportion (or absolute size) of training data to set aside as\n    validation data for early stopping. If None, early stopping is done on\n    the training data. Only used if early stopping is performed.\nn_iter_no_change : int, default=10\n    Used to determine when to \"early stop\". The fitting process is\n    stopped when none of the last ``n_iter_no_change`` scores are better\n    than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n    tolerance. Only used if early stopping is performed.\ntol : float or None, default=1e-7\n    The absolute tolerance to use when comparing scores during early\n    stopping. The higher the tolerance, the more likely we are to early\n    stop: higher tolerance means that it will be harder for subsequent\n    iterations to be considered an improvement upon the reference score.\nverbose : int, default=0\n    The verbosity level. If not zero, print some information about the\n    fitting process.\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the subsampling in the\n    binning process, and the train/validation data split if early stopping\n    is enabled.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\ndo_early_stopping_ : bool\n    Indicates whether early stopping is used during training.\nn_iter_ : int\n    The number of iterations as selected by early stopping, depending on\n    the `early_stopping` parameter. Otherwise it corresponds to max_iter.\nn_trees_per_iteration_ : int\n    The number of tree that are built at each iteration. For regressors,\n    this is always 1.\ntrain_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the training data. The first entry\n    is the score of the ensemble before the first iteration. Scores are\n    computed according to the ``scoring`` parameter. If ``scoring`` is\n    not 'loss', scores are computed on a subset of at most 10 000\n    samples. Empty if no early stopping.\nvalidation_score_ : ndarray, shape (n_iter_+1,)\n    The scores at each iteration on the held-out validation data. The\n    first entry is the score of the ensemble before the first iteration.\n    Scores are computed according to the ``scoring`` parameter. Empty if\n    no early stopping or if ``validation_fraction`` is None.\nis_categorical_ : ndarray, shape (n_features, ) or None\n    Boolean mask for the categorical features. ``None`` if there are no\n    categorical features.\n\nExamples\n--------\n>>> # To use this experimental feature, we need to explicitly ask for it:\n>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n>>> from sklearn.ensemble import HistGradientBoostingRegressor\n>>> from sklearn.datasets import load_diabetes\n>>> X, y = load_diabetes(return_X_y=True)\n>>> est = HistGradientBoostingRegressor().fit(X, y)\n>>> est.score(X, y)\n0.92...",
      "code": "class HistGradientBoostingRegressor(RegressorMixin, BaseHistGradientBoosting):\n    \"\"\"Histogram-based Gradient Boosting Regression Tree.\n\n    This estimator is much faster than\n    :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\n    for big datasets (n_samples >= 10 000).\n\n    This estimator has native support for missing values (NaNs). During\n    training, the tree grower learns at each split point whether samples\n    with missing values should go to the left or right child, based on the\n    potential gain. When predicting, samples with missing values are\n    assigned to the left or right child consequently. If no missing values\n    were encountered for a given feature during training, then samples with\n    missing values are mapped to whichever child has the most samples.\n\n    This implementation is inspired by\n    `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n    .. note::\n\n      This estimator is still **experimental** for now: the predictions\n      and the API might change without any deprecation cycle. To use it,\n      you need to explicitly import ``enable_hist_gradient_boosting``::\n\n        >>> # explicitly require this experimental feature\n        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n        >>> # now you can import normally from ensemble\n        >>> from sklearn.ensemble import HistGradientBoostingRegressor\n\n    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n    .. versionadded:: 0.21\n\n    Parameters\n    ----------\n    loss : {'least_squares', 'least_absolute_deviation', 'poisson'}, \\\n            default='least_squares'\n        The loss function to use in the boosting process. Note that the\n        \"least squares\" and \"poisson\" losses actually implement\n        \"half least squares loss\" and \"half poisson deviance\" to simplify the\n        computation of the gradient. Furthermore, \"poisson\" loss internally\n        uses a log-link and requires ``y >= 0``\n\n        .. versionchanged:: 0.23\n           Added option 'poisson'.\n\n    learning_rate : float, default=0.1\n        The learning rate, also known as *shrinkage*. This is used as a\n        multiplicative factor for the leaves values. Use ``1`` for no\n        shrinkage.\n    max_iter : int, default=100\n        The maximum number of iterations of the boosting process, i.e. the\n        maximum number of trees.\n    max_leaf_nodes : int or None, default=31\n        The maximum number of leaves for each tree. Must be strictly greater\n        than 1. If None, there is no maximum limit.\n    max_depth : int or None, default=None\n        The maximum depth of each tree. The depth of a tree is the number of\n        edges to go from the root to the deepest leaf.\n        Depth isn't constrained by default.\n    min_samples_leaf : int, default=20\n        The minimum number of samples per leaf. For small datasets with less\n        than a few hundred samples, it is recommended to lower this value\n        since only very shallow trees would be built.\n    l2_regularization : float, default=0\n        The L2 regularization parameter. Use ``0`` for no regularization\n        (default).\n    max_bins : int, default=255\n        The maximum number of bins to use for non-missing values. Before\n        training, each feature of the input array `X` is binned into\n        integer-valued bins, which allows for a much faster training stage.\n        Features with a small number of unique values may use less than\n        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n        is always reserved for missing values. Must be no larger than 255.\n    categorical_features : array-like of {bool, int} of shape (n_features) \\\n            or shape (n_categorical_features,), default=None.\n        Indicates the categorical features.\n\n        - None : no feature will be considered categorical.\n        - boolean array-like : boolean mask indicating categorical features.\n        - integer array-like : integer indices indicating categorical\n          features.\n\n        For each categorical feature, there must be at most `max_bins` unique\n        categories, and each categorical value must be in [0, max_bins -1].\n\n        Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n        .. versionadded:: 0.24\n\n    monotonic_cst : array-like of int of shape (n_features), default=None\n        Indicates the monotonic constraint to enforce on each feature. -1, 1\n        and 0 respectively correspond to a negative constraint, positive\n        constraint and no constraint. Read more in the :ref:`User Guide\n        <monotonic_cst_gbdt>`.\n\n        .. versionadded:: 0.23\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble. For results to be valid, the\n        estimator should be re-trained on the same data only.\n        See :term:`the Glossary <warm_start>`.\n    early_stopping : 'auto' or bool, default='auto'\n        If 'auto', early stopping is enabled if the sample size is larger than\n        10000. If True, early stopping is enabled, otherwise early stopping is\n        disabled.\n\n        .. versionadded:: 0.23\n\n    scoring : str or callable or None, default='loss'\n        Scoring parameter to use for early stopping. It can be a single\n        string (see :ref:`scoring_parameter`) or a callable (see\n        :ref:`scoring`). If None, the estimator's default scorer is used. If\n        ``scoring='loss'``, early stopping is checked w.r.t the loss value.\n        Only used if early stopping is performed.\n    validation_fraction : int or float or None, default=0.1\n        Proportion (or absolute size) of training data to set aside as\n        validation data for early stopping. If None, early stopping is done on\n        the training data. Only used if early stopping is performed.\n    n_iter_no_change : int, default=10\n        Used to determine when to \"early stop\". The fitting process is\n        stopped when none of the last ``n_iter_no_change`` scores are better\n        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n        tolerance. Only used if early stopping is performed.\n    tol : float or None, default=1e-7\n        The absolute tolerance to use when comparing scores during early\n        stopping. The higher the tolerance, the more likely we are to early\n        stop: higher tolerance means that it will be harder for subsequent\n        iterations to be considered an improvement upon the reference score.\n    verbose : int, default=0\n        The verbosity level. If not zero, print some information about the\n        fitting process.\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the subsampling in the\n        binning process, and the train/validation data split if early stopping\n        is enabled.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    do_early_stopping_ : bool\n        Indicates whether early stopping is used during training.\n    n_iter_ : int\n        The number of iterations as selected by early stopping, depending on\n        the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n    n_trees_per_iteration_ : int\n        The number of tree that are built at each iteration. For regressors,\n        this is always 1.\n    train_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the training data. The first entry\n        is the score of the ensemble before the first iteration. Scores are\n        computed according to the ``scoring`` parameter. If ``scoring`` is\n        not 'loss', scores are computed on a subset of at most 10 000\n        samples. Empty if no early stopping.\n    validation_score_ : ndarray, shape (n_iter_+1,)\n        The scores at each iteration on the held-out validation data. The\n        first entry is the score of the ensemble before the first iteration.\n        Scores are computed according to the ``scoring`` parameter. Empty if\n        no early stopping or if ``validation_fraction`` is None.\n    is_categorical_ : ndarray, shape (n_features, ) or None\n        Boolean mask for the categorical features. ``None`` if there are no\n        categorical features.\n\n    Examples\n    --------\n    >>> # To use this experimental feature, we need to explicitly ask for it:\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> from sklearn.ensemble import HistGradientBoostingRegressor\n    >>> from sklearn.datasets import load_diabetes\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> est = HistGradientBoostingRegressor().fit(X, y)\n    >>> est.score(X, y)\n    0.92...\n    \"\"\"\n\n    _VALID_LOSSES = ('least_squares', 'least_absolute_deviation',\n                     'poisson')\n\n    @_deprecate_positional_args\n    def __init__(self, loss='least_squares', *, learning_rate=0.1,\n                 max_iter=100, max_leaf_nodes=31, max_depth=None,\n                 min_samples_leaf=20, l2_regularization=0., max_bins=255,\n                 categorical_features=None, monotonic_cst=None,\n                 warm_start=False, early_stopping='auto',\n                 scoring='loss', validation_fraction=0.1,\n                 n_iter_no_change=10, tol=1e-7,\n                 verbose=0, random_state=None):\n        super(HistGradientBoostingRegressor, self).__init__(\n            loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n            max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n            min_samples_leaf=min_samples_leaf,\n            l2_regularization=l2_regularization, max_bins=max_bins,\n            monotonic_cst=monotonic_cst,\n            categorical_features=categorical_features,\n            early_stopping=early_stopping,\n            warm_start=warm_start, scoring=scoring,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n            random_state=random_state)\n\n    def predict(self, X):\n        \"\"\"Predict values for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        # Return inverse link of raw predictions after converting\n        # shape (n_samples, 1) to (n_samples,)\n        return self._loss.inverse_link_function(self._raw_predict(X).ravel())\n\n    def staged_predict(self, X):\n        \"\"\"Predict regression target for each iteration\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted values of the input samples, for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield self._loss.inverse_link_function(raw_predictions.ravel())\n\n    def _encode_y(self, y):\n        # Just convert y to the expected dtype\n        self.n_trees_per_iteration_ = 1\n        y = y.astype(Y_DTYPE, copy=False)\n        if self.loss == 'poisson':\n            # Ensure y >= 0 and sum(y) > 0\n            if not (np.all(y >= 0) and np.sum(y) > 0):\n                raise ValueError(\"loss='poisson' requires non-negative y and \"\n                                 \"sum(y) > 0.\")\n        return y\n\n    def _get_loss(self, sample_weight):\n        return _LOSSES[self.loss](sample_weight=sample_weight)",
      "instance_attributes": [
        {
          "name": "n_trees_per_iteration_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest",
      "name": "IsolationForest",
      "qname": "sklearn.ensemble._iforest.IsolationForest",
      "decorators": [],
      "superclasses": [
        "OutlierMixin",
        "BaseBagging"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/_set_oob_score",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/_parallel_args",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/fit",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/predict",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/decision_function",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/score_samples",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/_compute_chunked_score_samples",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/_compute_score_samples",
        "scikit-learn/sklearn.ensemble._iforest/IsolationForest/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Isolation Forest Algorithm.\n\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest 'isolates' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nRead more in the :ref:`User Guide <isolation_forest>`.\n\n.. versionadded:: 0.18",
      "docstring": "Isolation Forest Algorithm.\n\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest 'isolates' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nRead more in the :ref:`User Guide <isolation_forest>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of base estimators in the ensemble.\n\nmax_samples : \"auto\", int or float, default=\"auto\"\n    The number of samples to draw from X to train each base estimator.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n        - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n    If max_samples is larger than the number of samples provided,\n    all samples will be used for all trees (no sampling).\n\ncontamination : 'auto' or float, default='auto'\n    The amount of contamination of the data set, i.e. the proportion\n    of outliers in the data set. Used when fitting to define the threshold\n    on the scores of the samples.\n\n        - If 'auto', the threshold is determined as in the\n          original paper.\n        - If float, the contamination should be in the range [0, 0.5].\n\n    .. versionchanged:: 0.22\n       The default value of ``contamination`` changed from 0.1\n       to ``'auto'``.\n\nmax_features : int or float, default=1.0\n    The number of features to draw from X to train each base estimator.\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max_features * X.shape[1]` features.\n\nbootstrap : bool, default=False\n    If True, individual trees are fit on random subsets of the training\n    data sampled with replacement. If False, sampling without replacement\n    is performed.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for both :meth:`fit` and\n    :meth:`predict`. ``None`` means 1 unless in a\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\n    processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the pseudo-randomness of the selection of the feature\n    and split values for each branching step and each tree in the forest.\n\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    Controls the verbosity of the tree building process.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`the Glossary <warm_start>`.\n\n    .. versionadded:: 0.21\n\nAttributes\n----------\nbase_estimator_ : ExtraTreeRegressor instance\n    The child estimator template used to create the collection of\n    fitted sub-estimators.\n\nestimators_ : list of ExtraTreeRegressor instances\n    The collection of fitted sub-estimators.\n\nestimators_features_ : list of ndarray\n    The subset of drawn features for each base estimator.\n\nestimators_samples_ : list of ndarray\n    The subset of drawn samples (i.e., the in-bag samples) for each base\n    estimator.\n\nmax_samples_ : int\n    The actual number of samples.\n\noffset_ : float\n    Offset used to define the decision function from the raw scores. We\n    have the relation: ``decision_function = score_samples - offset_``.\n    ``offset_`` is defined as follows. When the contamination parameter is\n    set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n    close to 0 and the scores of outliers are close to -1. When a\n    contamination parameter different than \"auto\" is provided, the offset\n    is defined in such a way we obtain the expected number of outliers\n    (samples with decision function < 0) in training.\n\n    .. versionadded:: 0.20\n\nn_features_ : int\n    The number of features when ``fit`` is performed.\n\nNotes\n-----\nThe implementation is based on an ensemble of ExtraTreeRegressor. The\nmaximum depth of each tree is set to ``ceil(log_2(n))`` where\n:math:`n` is the number of samples used to build the tree\n(see (Liu et al., 2008) for more details).\n\nReferences\n----------\n.. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n.. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n       anomaly detection.\" ACM Transactions on Knowledge Discovery from\n       Data (TKDD) 6.1 (2012): 3.\n\nSee Also\n----------\nsklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n    Gaussian distributed dataset.\nsklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n    Estimate the support of a high-dimensional distribution.\n    The implementation is based on libsvm.\nsklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n    using Local Outlier Factor (LOF).\n\nExamples\n--------\n>>> from sklearn.ensemble import IsolationForest\n>>> X = [[-1.1], [0.3], [0.5], [100]]\n>>> clf = IsolationForest(random_state=0).fit(X)\n>>> clf.predict([[0.1], [0], [90]])\narray([ 1,  1, -1])",
      "code": "class IsolationForest(OutlierMixin, BaseBagging):\n    \"\"\"\n    Isolation Forest Algorithm.\n\n    Return the anomaly score of each sample using the IsolationForest algorithm\n\n    The IsolationForest 'isolates' observations by randomly selecting a feature\n    and then randomly selecting a split value between the maximum and minimum\n    values of the selected feature.\n\n    Since recursive partitioning can be represented by a tree structure, the\n    number of splittings required to isolate a sample is equivalent to the path\n    length from the root node to the terminating node.\n\n    This path length, averaged over a forest of such random trees, is a\n    measure of normality and our decision function.\n\n    Random partitioning produces noticeably shorter paths for anomalies.\n    Hence, when a forest of random trees collectively produce shorter path\n    lengths for particular samples, they are highly likely to be anomalies.\n\n    Read more in the :ref:`User Guide <isolation_forest>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of base estimators in the ensemble.\n\n    max_samples : \"auto\", int or float, default=\"auto\"\n        The number of samples to draw from X to train each base estimator.\n            - If int, then draw `max_samples` samples.\n            - If float, then draw `max_samples * X.shape[0]` samples.\n            - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n        If max_samples is larger than the number of samples provided,\n        all samples will be used for all trees (no sampling).\n\n    contamination : 'auto' or float, default='auto'\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Used when fitting to define the threshold\n        on the scores of the samples.\n\n            - If 'auto', the threshold is determined as in the\n              original paper.\n            - If float, the contamination should be in the range [0, 0.5].\n\n        .. versionchanged:: 0.22\n           The default value of ``contamination`` changed from 0.1\n           to ``'auto'``.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator.\n\n            - If int, then draw `max_features` features.\n            - If float, then draw `max_features * X.shape[1]` features.\n\n    bootstrap : bool, default=False\n        If True, individual trees are fit on random subsets of the training\n        data sampled with replacement. If False, sampling without replacement\n        is performed.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo-randomness of the selection of the feature\n        and split values for each branching step and each tree in the forest.\n\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity of the tree building process.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    base_estimator_ : ExtraTreeRegressor instance\n        The child estimator template used to create the collection of\n        fitted sub-estimators.\n\n    estimators_ : list of ExtraTreeRegressor instances\n        The collection of fitted sub-estimators.\n\n    estimators_features_ : list of ndarray\n        The subset of drawn features for each base estimator.\n\n    estimators_samples_ : list of ndarray\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator.\n\n    max_samples_ : int\n        The actual number of samples.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores. We\n        have the relation: ``decision_function = score_samples - offset_``.\n        ``offset_`` is defined as follows. When the contamination parameter is\n        set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n        close to 0 and the scores of outliers are close to -1. When a\n        contamination parameter different than \"auto\" is provided, the offset\n        is defined in such a way we obtain the expected number of outliers\n        (samples with decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    See Also\n    ----------\n    sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n        Gaussian distributed dataset.\n    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n        Estimate the support of a high-dimensional distribution.\n        The implementation is based on libsvm.\n    sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n        using Local Outlier Factor (LOF).\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import IsolationForest\n    >>> X = [[-1.1], [0.3], [0.5], [100]]\n    >>> clf = IsolationForest(random_state=0).fit(X)\n    >>> clf.predict([[0.1], [0], [90]])\n    array([ 1,  1, -1])\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"auto\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n        self.contamination = contamination\n\n    def _set_oob_score(self, X, y):\n        raise NotImplementedError(\"OOB score not supported by iforest\")\n\n    def _parallel_args(self):\n        # ExtraTreeRegressor releases the GIL, so it's more efficient to use\n        # a thread-based backend rather than a process-based backend so as\n        # to avoid suffering from communication overhead and extra memory\n        # copies.\n        return _joblib_parallel_args(prefer='threads')\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = check_array(X, accept_sparse=['csc'])\n        if issparse(X):\n            # Pre-sort indices to avoid that each individual tree of the\n            # ensemble sorts the indices.\n            X.sort_indices()\n\n        rnd = check_random_state(self.random_state)\n        y = rnd.uniform(size=X.shape[0])\n\n        # ensure that max_sample is in [1, n_samples]:\n        n_samples = X.shape[0]\n\n        if isinstance(self.max_samples, str):\n            if self.max_samples == 'auto':\n                max_samples = min(256, n_samples)\n            else:\n                raise ValueError('max_samples (%s) is not supported.'\n                                 'Valid choices are: \"auto\", int or'\n                                 'float' % self.max_samples)\n\n        elif isinstance(self.max_samples, numbers.Integral):\n            if self.max_samples > n_samples:\n                warn(\"max_samples (%s) is greater than the \"\n                     \"total number of samples (%s). max_samples \"\n                     \"will be set to n_samples for estimation.\"\n                     % (self.max_samples, n_samples))\n                max_samples = n_samples\n            else:\n                max_samples = self.max_samples\n        else:  # float\n            if not 0. < self.max_samples <= 1.:\n                raise ValueError(\"max_samples must be in (0, 1], got %r\"\n                                 % self.max_samples)\n            max_samples = int(self.max_samples * X.shape[0])\n\n        self.max_samples_ = max_samples\n        max_depth = int(np.ceil(np.log2(max(max_samples, 2))))\n        super()._fit(X, y, max_samples,\n                     max_depth=max_depth,\n                     sample_weight=sample_weight)\n\n        if self.contamination == \"auto\":\n            # 0.5 plays a special role as described in the original paper.\n            # we take the opposite as we consider the opposite of their score.\n            self.offset_ = -0.5\n            return self\n\n        # else, define offset_ wrt contamination parameter\n        self.offset_ = np.percentile(self.score_samples(X),\n                                     100. * self.contamination)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict if a particular sample is an outlier or not.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            For each observation, tells whether or not (+1 or -1) it should\n            be considered as an inlier according to the fitted model.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr')\n        is_inlier = np.ones(X.shape[0], dtype=int)\n        is_inlier[self.decision_function(X) < 0] = -1\n        return is_inlier\n\n    def decision_function(self, X):\n        \"\"\"\n        Average anomaly score of X of the base classifiers.\n\n        The anomaly score of an input sample is computed as\n        the mean anomaly score of the trees in the forest.\n\n        The measure of normality of an observation given a tree is the depth\n        of the leaf containing this observation, which is equivalent to\n        the number of splittings required to isolate this point. In case of\n        several observations n_left in the leaf, the average path length of\n        a n_left samples isolation tree is added.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        scores : ndarray of shape (n_samples,)\n            The anomaly score of the input samples.\n            The lower, the more abnormal. Negative scores represent outliers,\n            positive scores represent inliers.\n        \"\"\"\n        # We subtract self.offset_ to make 0 be the threshold value for being\n        # an outlier:\n\n        return self.score_samples(X) - self.offset_\n\n    def score_samples(self, X):\n        \"\"\"\n        Opposite of the anomaly score defined in the original paper.\n\n        The anomaly score of an input sample is computed as\n        the mean anomaly score of the trees in the forest.\n\n        The measure of normality of an observation given a tree is the depth\n        of the leaf containing this observation, which is equivalent to\n        the number of splittings required to isolate this point. In case of\n        several observations n_left in the leaf, the average path length of\n        a n_left samples isolation tree is added.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        scores : ndarray of shape (n_samples,)\n            The anomaly score of the input samples.\n            The lower, the more abnormal.\n        \"\"\"\n        # code structure from ForestClassifier/predict_proba\n\n        check_is_fitted(self)\n\n        # Check data\n        X = check_array(X, accept_sparse='csr')\n        if self.n_features_ != X.shape[1]:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features is {0} and \"\n                             \"input n_features is {1}.\"\n                             \"\".format(self.n_features_, X.shape[1]))\n\n        # Take the opposite of the scores as bigger is better (here less\n        # abnormal)\n        return -self._compute_chunked_score_samples(X)\n\n    def _compute_chunked_score_samples(self, X):\n\n        n_samples = _num_samples(X)\n\n        if self._max_features == X.shape[1]:\n            subsample_features = False\n        else:\n            subsample_features = True\n\n        # We get as many rows as possible within our working_memory budget\n        # (defined by sklearn.get_config()['working_memory']) to store\n        # self._max_features in each row during computation.\n        #\n        # Note:\n        #  - this will get at least 1 row, even if 1 row of score will\n        #    exceed working_memory.\n        #  - this does only account for temporary memory usage while loading\n        #    the data needed to compute the scores -- the returned scores\n        #    themselves are 1D.\n\n        chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features,\n                                        max_n_rows=n_samples)\n        slices = gen_batches(n_samples, chunk_n_rows)\n\n        scores = np.zeros(n_samples, order=\"f\")\n\n        for sl in slices:\n            # compute score on the slices of test samples:\n            scores[sl] = self._compute_score_samples(X[sl], subsample_features)\n\n        return scores\n\n    def _compute_score_samples(self, X, subsample_features):\n        \"\"\"\n        Compute the score of each samples in X going through the extra trees.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix\n            Data matrix.\n\n        subsample_features : bool\n            Whether features should be subsampled.\n        \"\"\"\n        n_samples = X.shape[0]\n\n        depths = np.zeros(n_samples, order=\"f\")\n\n        for tree, features in zip(self.estimators_, self.estimators_features_):\n            X_subset = X[:, features] if subsample_features else X\n\n            leaves_index = tree.apply(X_subset)\n            node_indicator = tree.decision_path(X_subset)\n            n_samples_leaf = tree.tree_.n_node_samples[leaves_index]\n\n            depths += (\n                np.ravel(node_indicator.sum(axis=1))\n                + _average_path_length(n_samples_leaf)\n                - 1.0\n            )\n\n        scores = 2 ** (\n            -depths\n            / (len(self.estimators_)\n               * _average_path_length([self.max_samples_]))\n        )\n        return scores\n\n    def _more_tags(self):\n        return {\n            '_xfail_checks': {\n                'check_sample_weights_invariance':\n                'zero sample_weight is not equivalent to removing samples',\n            }\n        }",
      "instance_attributes": [
        {
          "name": "contamination",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "max_samples_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "name": "offset_",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier",
      "name": "StackingClassifier",
      "qname": "sklearn.ensemble._stacking.StackingClassifier",
      "decorators": [],
      "superclasses": [
        "ClassifierMixin",
        "_BaseStacking"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/_validate_final_estimator",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/fit",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict_proba",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/decision_function",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/transform",
        "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/_sk_visual_block_"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Stack of estimators with a final classifier.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22",
      "docstring": "Stack of estimators with a final classifier.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nestimators : list of (str, estimator)\n    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n\nfinal_estimator : estimator, default=None\n    A classifier which will be used to combine the base estimators.\n    The default classifier is a\n    :class:`~sklearn.linear_model.LogisticRegression`.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used.\n    In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n    These splitters are instantiated with `shuffle=False` so the splits\n    will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n\nstack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n    Methods called for each base estimator. It can be:\n\n    * if 'auto', it will try to invoke, for each estimator,\n      `'predict_proba'`, `'decision_function'` or `'predict'` in that\n      order.\n    * otherwise, one of `'predict_proba'`, `'decision_function'` or\n      `'predict'`. If the method is not implemented by the estimator, it\n      will raise an error.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel all `estimators` `fit`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n\npassthrough : bool, default=False\n    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n\nverbose : int, default=0\n    Verbosity level.\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,)\n    Class labels.\n\nestimators_ : list of estimators\n    The elements of the estimators parameter, having been fitted on the\n    training data. If an estimator has been set to `'drop'`, it\n    will not appear in `estimators_`.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\nfinal_estimator_ : estimator\n    The classifier which predicts given the output of `estimators_`.\n\nstack_method_ : list of str\n    The method used by each base estimator.\n\nNotes\n-----\nWhen `predict_proba` is used by each estimator (i.e. most of the time for\n`stack_method='auto'` or specifically for `stack_method='predict_proba'`),\nThe first column predicted by each estimator will be dropped in the case\nof a binary classification problem. Indeed, both feature will be perfectly\ncollinear.\n\nReferences\n----------\n.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\nExamples\n--------\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.ensemble import StackingClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> estimators = [\n...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n...     ('svr', make_pipeline(StandardScaler(),\n...                           LinearSVC(random_state=42)))\n... ]\n>>> clf = StackingClassifier(\n...     estimators=estimators, final_estimator=LogisticRegression()\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, stratify=y, random_state=42\n... )\n>>> clf.fit(X_train, y_train).score(X_test, y_test)\n0.9...",
      "code": "class StackingClassifier(ClassifierMixin, _BaseStacking):\n    \"\"\"Stack of estimators with a final classifier.\n\n    Stacked generalization consists in stacking the output of individual\n    estimator and use a classifier to compute the final prediction. Stacking\n    allows to use the strength of each individual estimator by using their\n    output as input of a final estimator.\n\n    Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n    is trained using cross-validated predictions of the base estimators using\n    `cross_val_predict`.\n\n    Read more in the :ref:`User Guide <stacking>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator)\n        Base estimators which will be stacked together. Each element of the\n        list is defined as a tuple of string (i.e. name) and an estimator\n        instance. An estimator can be set to 'drop' using `set_params`.\n\n    final_estimator : estimator, default=None\n        A classifier which will be used to combine the base estimators.\n        The default classifier is a\n        :class:`~sklearn.linear_model.LogisticRegression`.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy used in\n        `cross_val_predict` to train `final_estimator`. Possible inputs for\n        cv are:\n\n        * None, to use the default 5-fold cross validation,\n        * integer, to specify the number of folds in a (Stratified) KFold,\n        * An object to be used as a cross-validation generator,\n        * An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and y is\n        either binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used.\n        In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n        These splitters are instantiated with `shuffle=False` so the splits\n        will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. note::\n           A larger number of split will provide no benefits if the number\n           of training samples is large enough. Indeed, the training time\n           will increase. ``cv`` is not used for model evaluation but for\n           prediction.\n\n    stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'}, \\\n            default='auto'\n        Methods called for each base estimator. It can be:\n\n        * if 'auto', it will try to invoke, for each estimator,\n          `'predict_proba'`, `'decision_function'` or `'predict'` in that\n          order.\n        * otherwise, one of `'predict_proba'`, `'decision_function'` or\n          `'predict'`. If the method is not implemented by the estimator, it\n          will raise an error.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel all `estimators` `fit`.\n        `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n        using all processors. See Glossary for more details.\n\n    passthrough : bool, default=False\n        When False, only the predictions of estimators will be used as\n        training data for `final_estimator`. When True, the\n        `final_estimator` is trained on the predictions as well as the\n        original training data.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels.\n\n    estimators_ : list of estimators\n        The elements of the estimators parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it\n        will not appear in `estimators_`.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n    final_estimator_ : estimator\n        The classifier which predicts given the output of `estimators_`.\n\n    stack_method_ : list of str\n        The method used by each base estimator.\n\n    Notes\n    -----\n    When `predict_proba` is used by each estimator (i.e. most of the time for\n    `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n    The first column predicted by each estimator will be dropped in the case\n    of a binary classification problem. Indeed, both feature will be perfectly\n    collinear.\n\n    References\n    ----------\n    .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n       (1992): 241-259.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.ensemble import StackingClassifier\n    >>> X, y = load_iris(return_X_y=True)\n    >>> estimators = [\n    ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ...     ('svr', make_pipeline(StandardScaler(),\n    ...                           LinearSVC(random_state=42)))\n    ... ]\n    >>> clf = StackingClassifier(\n    ...     estimators=estimators, final_estimator=LogisticRegression()\n    ... )\n    >>> from sklearn.model_selection import train_test_split\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, stratify=y, random_state=42\n    ... )\n    >>> clf.fit(X_train, y_train).score(X_test, y_test)\n    0.9...\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimators, final_estimator=None, *, cv=None,\n                 stack_method='auto', n_jobs=None, passthrough=False,\n                 verbose=0):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            stack_method=stack_method,\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose\n        )\n\n    def _validate_final_estimator(self):\n        self._clone_final_estimator(default=LogisticRegression())\n        if not is_classifier(self.final_estimator_):\n            raise ValueError(\n                \"'final_estimator' parameter should be a classifier. Got {}\"\n                .format(self.final_estimator_)\n            )\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        check_classification_targets(y)\n        self._le = LabelEncoder().fit(y)\n        self.classes_ = self._le.classes_\n        return super().fit(X, self._le.transform(y), sample_weight)\n\n    @if_delegate_has_method(delegate='final_estimator_')\n    def predict(self, X, **predict_params):\n        \"\"\"Predict target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        **predict_params : dict of str -> obj\n            Parameters to the `predict` called by the `final_estimator`. Note\n            that this may be used to return uncertainties from some estimators\n            with `return_std` or `return_cov`. Be aware that it will only\n            accounts for uncertainty in the final estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n            Predicted targets.\n        \"\"\"\n        y_pred = super().predict(X, **predict_params)\n        return self._le.inverse_transform(y_pred)\n\n    @if_delegate_has_method(delegate='final_estimator_')\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X using\n        `final_estimator_.predict_proba`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes) or \\\n            list of ndarray of shape (n_output,)\n            The class probabilities of the input samples.\n        \"\"\"\n        check_is_fitted(self)\n        return self.final_estimator_.predict_proba(self.transform(X))\n\n    @if_delegate_has_method(delegate='final_estimator_')\n    def decision_function(self, X):\n        \"\"\"Predict decision function for samples in X using\n        `final_estimator_.decision_function`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,), (n_samples, n_classes), \\\n            or (n_samples, n_classes * (n_classes-1) / 2)\n            The decision function computed the final estimator.\n        \"\"\"\n        check_is_fitted(self)\n        return self.final_estimator_.decision_function(self.transform(X))\n\n    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators) or \\\n                (n_samples, n_classes * n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._transform(X)\n\n    def _sk_visual_block_(self):\n        # If final_estimator's default changes then this should be\n        # updated.\n        if self.final_estimator is None:\n            final_estimator = LogisticRegression()\n        else:\n            final_estimator = self.final_estimator\n        return super()._sk_visual_block_(final_estimator)",
      "instance_attributes": [
        {
          "name": "_le",
          "types": {
            "kind": "NamedType",
            "name": "LabelEncoder"
          }
        },
        {
          "name": "classes_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor",
      "name": "StackingRegressor",
      "qname": "sklearn.ensemble._stacking.StackingRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "_BaseStacking"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__",
        "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/_validate_final_estimator",
        "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/fit",
        "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/transform",
        "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/_sk_visual_block_"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Stack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a regressor to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22",
      "docstring": "Stack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a regressor to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22\n\nParameters\n----------\nestimators : list of (str, estimator)\n    Base estimators which will be stacked together. Each element of the\n    list is defined as a tuple of string (i.e. name) and an estimator\n    instance. An estimator can be set to 'drop' using `set_params`.\n\nfinal_estimator : estimator, default=None\n    A regressor which will be used to combine the base estimators.\n    The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy used in\n    `cross_val_predict` to train `final_estimator`. Possible inputs for\n    cv are:\n\n    * None, to use the default 5-fold cross validation,\n    * integer, to specify the number of folds in a (Stratified) KFold,\n    * An object to be used as a cross-validation generator,\n    * An iterable yielding train, test splits.\n\n    For integer/None inputs, if the estimator is a classifier and y is\n    either binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used.\n    In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n    These splitters are instantiated with `shuffle=False` so the splits\n    will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. note::\n       A larger number of split will provide no benefits if the number\n       of training samples is large enough. Indeed, the training time\n       will increase. ``cv`` is not used for model evaluation but for\n       prediction.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for `fit` of all `estimators`.\n    `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n    using all processors. See Glossary for more details.\n\npassthrough : bool, default=False\n    When False, only the predictions of estimators will be used as\n    training data for `final_estimator`. When True, the\n    `final_estimator` is trained on the predictions as well as the\n    original training data.\n\nverbose : int, default=0\n    Verbosity level.\n\nAttributes\n----------\nestimators_ : list of estimator\n    The elements of the estimators parameter, having been fitted on the\n    training data. If an estimator has been set to `'drop'`, it\n    will not appear in `estimators_`.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\n\nfinal_estimator_ : estimator\n    The regressor to stacked the base estimators fitted.\n\nReferences\n----------\n.. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import StackingRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> estimators = [\n...     ('lr', RidgeCV()),\n...     ('svr', LinearSVR(random_state=42))\n... ]\n>>> reg = StackingRegressor(\n...     estimators=estimators,\n...     final_estimator=RandomForestRegressor(n_estimators=10,\n...                                           random_state=42)\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42\n... )\n>>> reg.fit(X_train, y_train).score(X_test, y_test)\n0.3...",
      "code": "class StackingRegressor(RegressorMixin, _BaseStacking):\n    \"\"\"Stack of estimators with a final regressor.\n\n    Stacked generalization consists in stacking the output of individual\n    estimator and use a regressor to compute the final prediction. Stacking\n    allows to use the strength of each individual estimator by using their\n    output as input of a final estimator.\n\n    Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n    is trained using cross-validated predictions of the base estimators using\n    `cross_val_predict`.\n\n    Read more in the :ref:`User Guide <stacking>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator)\n        Base estimators which will be stacked together. Each element of the\n        list is defined as a tuple of string (i.e. name) and an estimator\n        instance. An estimator can be set to 'drop' using `set_params`.\n\n    final_estimator : estimator, default=None\n        A regressor which will be used to combine the base estimators.\n        The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy used in\n        `cross_val_predict` to train `final_estimator`. Possible inputs for\n        cv are:\n\n        * None, to use the default 5-fold cross validation,\n        * integer, to specify the number of folds in a (Stratified) KFold,\n        * An object to be used as a cross-validation generator,\n        * An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and y is\n        either binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used.\n        In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n        These splitters are instantiated with `shuffle=False` so the splits\n        will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. note::\n           A larger number of split will provide no benefits if the number\n           of training samples is large enough. Indeed, the training time\n           will increase. ``cv`` is not used for model evaluation but for\n           prediction.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for `fit` of all `estimators`.\n        `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n        using all processors. See Glossary for more details.\n\n    passthrough : bool, default=False\n        When False, only the predictions of estimators will be used as\n        training data for `final_estimator`. When True, the\n        `final_estimator` is trained on the predictions as well as the\n        original training data.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    Attributes\n    ----------\n    estimators_ : list of estimator\n        The elements of the estimators parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it\n        will not appear in `estimators_`.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n\n    final_estimator_ : estimator\n        The regressor to stacked the base estimators fitted.\n\n    References\n    ----------\n    .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n       (1992): 241-259.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.linear_model import RidgeCV\n    >>> from sklearn.svm import LinearSVR\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from sklearn.ensemble import StackingRegressor\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> estimators = [\n    ...     ('lr', RidgeCV()),\n    ...     ('svr', LinearSVR(random_state=42))\n    ... ]\n    >>> reg = StackingRegressor(\n    ...     estimators=estimators,\n    ...     final_estimator=RandomForestRegressor(n_estimators=10,\n    ...                                           random_state=42)\n    ... )\n    >>> from sklearn.model_selection import train_test_split\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=42\n    ... )\n    >>> reg.fit(X_train, y_train).score(X_test, y_test)\n    0.3...\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimators, final_estimator=None, *, cv=None,\n                 n_jobs=None, passthrough=False, verbose=0):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            stack_method=\"predict\",\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose\n        )\n\n    def _validate_final_estimator(self):\n        self._clone_final_estimator(default=RidgeCV())\n        if not is_regressor(self.final_estimator_):\n            raise ValueError(\n                \"'final_estimator' parameter should be a regressor. Got {}\"\n                .format(self.final_estimator_)\n            )\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        return super().fit(X, y, sample_weight)\n\n    def transform(self, X):\n        \"\"\"Return the predictions for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._transform(X)\n\n    def _sk_visual_block_(self):\n        # If final_estimator's default changes then this should be\n        # updated.\n        if self.final_estimator is None:\n            final_estimator = RidgeCV()\n        else:\n            final_estimator = self.final_estimator\n        return super()._sk_visual_block_(final_estimator)",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier",
      "name": "VotingClassifier",
      "qname": "sklearn.ensemble._voting.VotingClassifier",
      "decorators": [],
      "superclasses": [
        "ClassifierMixin",
        "_BaseVoting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__",
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/fit",
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict",
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/_collect_probas",
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/_predict_proba",
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict_proba@getter",
        "scikit-learn/sklearn.ensemble._voting/VotingClassifier/transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Soft Voting/Majority Rule classifier for unfitted estimators.\n\nRead more in the :ref:`User Guide <voting_classifier>`.\n\n.. versionadded:: 0.17",
      "docstring": "Soft Voting/Majority Rule classifier for unfitted estimators.\n\nRead more in the :ref:`User Guide <voting_classifier>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nestimators : list of (str, estimator) tuples\n    Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'``\n    using ``set_params``.\n\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n        support was removed in 0.24.\n\nvoting : {'hard', 'soft'}, default='hard'\n    If 'hard', uses predicted class labels for majority rule voting.\n    Else if 'soft', predicts the class label based on the argmax of\n    the sums of the predicted probabilities, which is recommended for\n    an ensemble of well-calibrated classifiers.\n\nweights : array-like of shape (n_classifiers,), default=None\n    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted class labels (`hard` voting) or class probabilities\n    before averaging (`soft` voting). Uses uniform weights if `None`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nflatten_transform : bool, default=True\n    Affects shape of transform output only when voting='soft'\n    If voting='soft' and flatten_transform=True, transform method returns\n    matrix with shape (n_samples, n_classifiers * n_classes). If\n    flatten_transform=False, it returns\n    (n_classifiers, n_samples, n_classes).\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting will be printed as it\n    is completed.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators as defined in ``estimators``\n    that are not 'drop'.\n\nnamed_estimators_ : :class:`~sklearn.utils.Bunch`\n    Attribute to access any fitted sub-estimators by name.\n\n    .. versionadded:: 0.20\n\nclasses_ : array-like of shape (n_predictions,)\n    The classes labels.\n\nSee Also\n--------\nVotingRegressor : Prediction voting regressor.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n>>> clf3 = GaussianNB()\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> eclf1 = VotingClassifier(estimators=[\n...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n>>> eclf1 = eclf1.fit(X, y)\n>>> print(eclf1.predict(X))\n[1 1 1 2 2 2]\n>>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n...                eclf1.named_estimators_['lr'].predict(X))\nTrue\n>>> eclf2 = VotingClassifier(estimators=[\n...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...         voting='soft')\n>>> eclf2 = eclf2.fit(X, y)\n>>> print(eclf2.predict(X))\n[1 1 1 2 2 2]\n>>> eclf3 = VotingClassifier(estimators=[\n...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...        voting='soft', weights=[2,1,1],\n...        flatten_transform=True)\n>>> eclf3 = eclf3.fit(X, y)\n>>> print(eclf3.predict(X))\n[1 1 1 2 2 2]\n>>> print(eclf3.transform(X).shape)\n(6, 6)",
      "code": "class VotingClassifier(ClassifierMixin, _BaseVoting):\n    \"\"\"Soft Voting/Majority Rule classifier for unfitted estimators.\n\n    Read more in the :ref:`User Guide <voting_classifier>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'``\n        using ``set_params``.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    voting : {'hard', 'soft'}, default='hard'\n        If 'hard', uses predicted class labels for majority rule voting.\n        Else if 'soft', predicts the class label based on the argmax of\n        the sums of the predicted probabilities, which is recommended for\n        an ensemble of well-calibrated classifiers.\n\n    weights : array-like of shape (n_classifiers,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted class labels (`hard` voting) or class probabilities\n        before averaging (`soft` voting). Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    flatten_transform : bool, default=True\n        Affects shape of transform output only when voting='soft'\n        If voting='soft' and flatten_transform=True, transform method returns\n        matrix with shape (n_samples, n_classifiers * n_classes). If\n        flatten_transform=False, it returns\n        (n_classifiers, n_samples, n_classes).\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting will be printed as it\n        is completed.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    classes_ : array-like of shape (n_predictions,)\n        The classes labels.\n\n    See Also\n    --------\n    VotingRegressor : Prediction voting regressor.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n    >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    >>> clf3 = GaussianNB()\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> eclf1 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    >>> eclf1 = eclf1.fit(X, y)\n    >>> print(eclf1.predict(X))\n    [1 1 1 2 2 2]\n    >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n    ...                eclf1.named_estimators_['lr'].predict(X))\n    True\n    >>> eclf2 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...         voting='soft')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> print(eclf2.predict(X))\n    [1 1 1 2 2 2]\n    >>> eclf3 = VotingClassifier(estimators=[\n    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...        voting='soft', weights=[2,1,1],\n    ...        flatten_transform=True)\n    >>> eclf3 = eclf3.fit(X, y)\n    >>> print(eclf3.predict(X))\n    [1 1 1 2 2 2]\n    >>> print(eclf3.transform(X).shape)\n    (6, 6)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimators, *, voting='hard', weights=None,\n                 n_jobs=None, flatten_transform=True, verbose=False):\n        super().__init__(estimators=estimators)\n        self.voting = voting\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.flatten_transform = flatten_transform\n        self.verbose = verbose\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n            .. versionadded:: 0.18\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        check_classification_targets(y)\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        transformed_y = self.le_.transform(y)\n\n        return super().fit(X, transformed_y, sample_weight)\n\n    def predict(self, X):\n        \"\"\"Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        maj : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        check_is_fitted(self)\n        if self.voting == 'soft':\n            maj = np.argmax(self.predict_proba(X), axis=1)\n\n        else:  # 'hard' voting\n            predictions = self._predict(X)\n            maj = np.apply_along_axis(\n                lambda x: np.argmax(\n                    np.bincount(x, weights=self._weights_not_none)),\n                axis=1, arr=predictions)\n\n        maj = self.le_.inverse_transform(maj)\n\n        return maj\n\n    def _collect_probas(self, X):\n        \"\"\"Collect results from clf.predict calls.\"\"\"\n        return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n\n    def _predict_proba(self, X):\n        \"\"\"Predict class probabilities for X in 'soft' voting.\"\"\"\n        check_is_fitted(self)\n        avg = np.average(self._collect_probas(X), axis=0,\n                         weights=self._weights_not_none)\n        return avg\n\n    @property\n    def predict_proba(self):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        avg : array-like of shape (n_samples, n_classes)\n            Weighted average probability for each class per sample.\n        \"\"\"\n        if self.voting == 'hard':\n            raise AttributeError(\"predict_proba is not available when\"\n                                 \" voting=%r\" % self.voting)\n        return self._predict_proba\n\n    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        probabilities_or_labels\n            If `voting='soft'` and `flatten_transform=True`:\n                returns ndarray of shape (n_classifiers, n_samples *\n                n_classes), being class probabilities calculated by each\n                classifier.\n            If `voting='soft' and `flatten_transform=False`:\n                ndarray of shape (n_classifiers, n_samples, n_classes)\n            If `voting='hard'`:\n                ndarray of shape (n_samples, n_classifiers), being\n                class labels predicted by each classifier.\n        \"\"\"\n        check_is_fitted(self)\n\n        if self.voting == 'soft':\n            probas = self._collect_probas(X)\n            if not self.flatten_transform:\n                return probas\n            return np.hstack(probas)\n\n        else:\n            return self._predict(X)",
      "instance_attributes": [
        {
          "name": "voting",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "weights",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "flatten_transform",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "le_",
          "types": {
            "kind": "NamedType",
            "name": "LabelEncoder"
          }
        },
        {
          "name": "classes_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor",
      "name": "VotingRegressor",
      "qname": "sklearn.ensemble._voting.VotingRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "_BaseVoting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__",
        "scikit-learn/sklearn.ensemble._voting/VotingRegressor/fit",
        "scikit-learn/sklearn.ensemble._voting/VotingRegressor/predict",
        "scikit-learn/sklearn.ensemble._voting/VotingRegressor/transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "Prediction voting regressor for unfitted estimators.\n\nA voting regressor is an ensemble meta-estimator that fits several base\nregressors, each on the whole dataset. Then it averages the individual\npredictions to form a final prediction.\n\nRead more in the :ref:`User Guide <voting_regressor>`.\n\n.. versionadded:: 0.21",
      "docstring": "Prediction voting regressor for unfitted estimators.\n\nA voting regressor is an ensemble meta-estimator that fits several base\nregressors, each on the whole dataset. Then it averages the individual\npredictions to form a final prediction.\n\nRead more in the :ref:`User Guide <voting_regressor>`.\n\n.. versionadded:: 0.21\n\nParameters\n----------\nestimators : list of (str, estimator) tuples\n    Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n    of those original estimators that will be stored in the class attribute\n    ``self.estimators_``. An estimator can be set to ``'drop'`` using\n    ``set_params``.\n\n    .. versionchanged:: 0.21\n        ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n        support was removed in 0.24.\n\nweights : array-like of shape (n_regressors,), default=None\n    Sequence of weights (`float` or `int`) to weight the occurrences of\n    predicted values before averaging. Uses uniform weights if `None`.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel for ``fit``.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting will be printed as it\n    is completed.\n\n    .. versionadded:: 0.23\n\nAttributes\n----------\nestimators_ : list of regressors\n    The collection of fitted sub-estimators as defined in ``estimators``\n    that are not 'drop'.\n\nnamed_estimators_ : Bunch\n    Attribute to access any fitted sub-estimators by name.\n\n    .. versionadded:: 0.20\n\nSee Also\n--------\nVotingClassifier : Soft Voting/Majority Rule classifier.\n\nExamples\n--------\n>>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import VotingRegressor\n>>> r1 = LinearRegression()\n>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n>>> y = np.array([2, 6, 12, 20, 30, 42])\n>>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n>>> print(er.fit(X, y).predict(X))\n[ 3.3  5.7 11.8 19.7 28.  40.3]",
      "code": "class VotingRegressor(RegressorMixin, _BaseVoting):\n    \"\"\"Prediction voting regressor for unfitted estimators.\n\n    A voting regressor is an ensemble meta-estimator that fits several base\n    regressors, each on the whole dataset. Then it averages the individual\n    predictions to form a final prediction.\n\n    Read more in the :ref:`User Guide <voting_regressor>`.\n\n    .. versionadded:: 0.21\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'`` using\n        ``set_params``.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    weights : array-like of shape (n_regressors,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted values before averaging. Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting will be printed as it\n        is completed.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    estimators_ : list of regressors\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : Bunch\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    See Also\n    --------\n    VotingClassifier : Soft Voting/Majority Rule classifier.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from sklearn.ensemble import VotingRegressor\n    >>> r1 = LinearRegression()\n    >>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    >>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    >>> y = np.array([2, 6, 12, 20, 30, 42])\n    >>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n    >>> print(er.fit(X, y).predict(X))\n    [ 3.3  5.7 11.8 19.7 28.  40.3]\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimators, *, weights=None, n_jobs=None,\n                 verbose=False):\n        super().__init__(estimators=estimators)\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        return super().fit(X, y, sample_weight)\n\n    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        return np.average(self._predict(X), axis=1,\n                          weights=self._weights_not_none)\n\n    def transform(self, X):\n        \"\"\"Return predictions for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        predictions: ndarray of shape (n_samples, n_classifiers)\n            Values predicted by each regressor.\n        \"\"\"\n        check_is_fitted(self)\n        return self._predict(X)",
      "instance_attributes": [
        {
          "name": "weights",
          "types": null
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier",
      "name": "AdaBoostClassifier",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier",
      "decorators": [],
      "superclasses": [
        "ClassifierMixin",
        "BaseWeightBoosting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/fit",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/_validate_estimator",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/_boost",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/_boost_real",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/_boost_discrete",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/decision_function",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_decision_function",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/_compute_proba_from_decision",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_proba",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict_proba",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_log_proba"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "An AdaBoost classifier.\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14",
      "docstring": "An AdaBoost classifier.\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator from which the boosted ensemble is built.\n    Support for sample weighting is required, as well as proper\n    ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n    the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n    initialized with `max_depth=1`.\n\nn_estimators : int, default=50\n    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n\nlearning_rate : float, default=1.\n    Weight applied to each classifier at each boosting iteration. A higher\n    learning rate increases the contribution of each classifier. There is\n    a trade-off between the `learning_rate` and `n_estimators` parameters.\n\nalgorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n    If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n    ``base_estimator`` must support calculation of class probabilities.\n    If 'SAMME' then use the SAMME discrete boosting algorithm.\n    The SAMME.R algorithm typically converges faster than SAMME,\n    achieving a lower test error with fewer boosting iterations.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels.\n\nn_classes_ : int\n    The number of classes.\n\nestimator_weights_ : ndarray of floats\n    Weights for each estimator in the boosted ensemble.\n\nestimator_errors_ : ndarray of floats\n    Classification error for each estimator in the boosted\n    ensemble.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances if supported by the\n    ``base_estimator`` (when based on decision trees).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nSee Also\n--------\nAdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n    regressor on the original dataset and then fits additional copies of\n    the regressor on the same dataset but where the weights of instances\n    are adjusted according to the error of the current prediction.\n\nGradientBoostingClassifier : GB builds an additive model in a forward\n    stage-wise fashion. Regression trees are fit on the negative gradient\n    of the binomial or multinomial deviance loss function. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\nsklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n    method used for classification.\n    Creates a model that predicts the value of a target variable by\n    learning simple decision rules inferred from the data features.\n\nReferences\n----------\n.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n       on-Line Learning and an Application to Boosting\", 1995.\n\n.. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\nExamples\n--------\n>>> from sklearn.ensemble import AdaBoostClassifier\n>>> from sklearn.datasets import make_classification\n>>> X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n>>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n>>> clf.fit(X, y)\nAdaBoostClassifier(n_estimators=100, random_state=0)\n>>> clf.predict([[0, 0, 0, 0]])\narray([1])\n>>> clf.score(X, y)\n0.983...",
      "code": "class AdaBoostClassifier(ClassifierMixin, BaseWeightBoosting):\n    \"\"\"An AdaBoost classifier.\n\n    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifier on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost-SAMME [2].\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    base_estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n    n_estimators : int, default=50\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n\n    learning_rate : float, default=1.\n        Weight applied to each classifier at each boosting iteration. A higher\n        learning rate increases the contribution of each classifier. There is\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\n\n    algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n        ``base_estimator`` must support calculation of class probabilities.\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\n        The SAMME.R algorithm typically converges faster than SAMME,\n        achieving a lower test error with fewer boosting iterations.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given at each `base_estimator` at each\n        boosting iteration.\n        Thus, it is only used when `base_estimator` exposes a `random_state`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int\n        The number of classes.\n\n    estimator_weights_ : ndarray of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : ndarray of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances if supported by the\n        ``base_estimator`` (when based on decision trees).\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    See Also\n    --------\n    AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n        regressor on the original dataset and then fits additional copies of\n        the regressor on the same dataset but where the weights of instances\n        are adjusted according to the error of the current prediction.\n\n    GradientBoostingClassifier : GB builds an additive model in a forward\n        stage-wise fashion. Regression trees are fit on the negative gradient\n        of the binomial or multinomial deviance loss function. Binary\n        classification is a special case where only a single regression tree is\n        induced.\n\n    sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n        method used for classification.\n        Creates a model that predicts the value of a target variable by\n        learning simple decision rules inferred from the data features.\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    >>> clf.score(X, y)\n    0.983...\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None, *,\n                 n_estimators=50,\n                 learning_rate=1.,\n                 algorithm='SAMME.R',\n                 random_state=None):\n\n        super().__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state)\n\n        self.algorithm = algorithm\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Build a boosted classifier from the training set (X, y).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels).\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, the sample weights are initialized to\n            ``1 / n_samples``.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        # Check that algorithm is supported\n        if self.algorithm not in ('SAMME', 'SAMME.R'):\n            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n\n        # Fit\n        return super().fit(X, y, sample_weight)\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n        super()._validate_estimator(\n            default=DecisionTreeClassifier(max_depth=1))\n\n        #  SAMME-R requires predict_proba-enabled base estimators\n        if self.algorithm == 'SAMME.R':\n            if not hasattr(self.base_estimator_, 'predict_proba'):\n                raise TypeError(\n                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n                    \"that the weak learner supports the calculation of class \"\n                    \"probabilities with a predict_proba method.\\n\"\n                    \"Please change the base estimator or set \"\n                    \"algorithm='SAMME' instead.\")\n        if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\n            raise ValueError(\"%s doesn't support sample_weight.\"\n                             % self.base_estimator_.__class__.__name__)\n\n    def _boost(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost.\n\n        Perform a single boost according to the real multi-class SAMME.R\n        algorithm or to the discrete SAMME algorithm and return the updated\n        sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels).\n\n        sample_weight : array-like of shape (n_samples,)\n            The current sample weights.\n\n        random_state : RandomState instance\n            The RandomState instance used if the base estimator accepts a\n            `random_state` attribute.\n\n        Returns\n        -------\n        sample_weight : array-like of shape (n_samples,) or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        estimator_weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        estimator_error : float\n            The classification error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        if self.algorithm == 'SAMME.R':\n            return self._boost_real(iboost, X, y, sample_weight, random_state)\n\n        else:  # elif self.algorithm == \"SAMME\":\n            return self._boost_discrete(iboost, X, y, sample_weight,\n                                        random_state)\n\n    def _boost_real(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict_proba = estimator.predict_proba(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, 'classes_', None)\n            self.n_classes_ = len(self.classes_)\n\n        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n                                       axis=0)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(\n            np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1., 0.\n\n        # Construct y coding as described in Zhu et al [2]:\n        #\n        #    y_k = 1 if c == k else -1 / (K - 1)\n        #\n        # where K == n_classes_ and c, k in [0, K) are indices along the second\n        # axis of the y coding with c being the index corresponding to the true\n        # class label.\n        n_classes = self.n_classes_\n        classes = self.classes_\n        y_codes = np.array([-1. / (n_classes - 1), 1.])\n        y_coding = y_codes.take(classes == y[:, np.newaxis])\n\n        # Displace zero probabilities so the log is defined.\n        # Also fix negative elements which may occur with\n        # negative sample weights.\n        proba = y_predict_proba  # alias for readability\n        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n\n        # Boost weight using multi-class AdaBoost SAMME.R alg\n        estimator_weight = (-1. * self.learning_rate\n                            * ((n_classes - 1.) / n_classes)\n                            * xlogy(y_coding, y_predict_proba).sum(axis=1))\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight *= np.exp(estimator_weight *\n                                    ((sample_weight > 0) |\n                                     (estimator_weight < 0)))\n\n        return sample_weight, 1., estimator_error\n\n    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict = estimator.predict(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, 'classes_', None)\n            self.n_classes_ = len(self.classes_)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(\n            np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1., 0.\n\n        n_classes = self.n_classes_\n\n        # Stop if the error is at least as bad as random guessing\n        if estimator_error >= 1. - (1. / n_classes):\n            self.estimators_.pop(-1)\n            if len(self.estimators_) == 0:\n                raise ValueError('BaseClassifier in AdaBoostClassifier '\n                                 'ensemble is worse than random, ensemble '\n                                 'can not be fit.')\n            return None, None, None\n\n        # Boost weight using multi-class AdaBoost SAMME alg\n        estimator_weight = self.learning_rate * (\n            np.log((1. - estimator_error) / estimator_error) +\n            np.log(n_classes - 1.))\n\n        # Only boost the weights if I will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight *= np.exp(estimator_weight * incorrect *\n                                    (sample_weight > 0))\n\n        return sample_weight, estimator_weight, estimator_error\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        X = self._check_X(X)\n\n        pred = self.decision_function(X)\n\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n\n    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_\n\n        if n_classes == 2:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(pred > 0, axis=0))\n\n        else:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(\n                    np.argmax(pred, axis=1), axis=0))\n\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        score : ndarray of shape of (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n\n        if self.algorithm == 'SAMME.R':\n            # The weights are all 1. for SAMME.R\n            pred = sum(_samme_proba(estimator, n_classes, X)\n                       for estimator in self.estimators_)\n        else:  # self.algorithm == \"SAMME\"\n            pred = sum((estimator.predict(X) == classes).T * w\n                       for estimator, w in zip(self.estimators_,\n                                               self.estimator_weights_))\n\n        pred /= self.estimator_weights_.sum()\n        if n_classes == 2:\n            pred[:, 0] *= -1\n            return pred.sum(axis=1)\n        return pred\n\n    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each boosting iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each boosting iteration.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n        pred = None\n        norm = 0.\n\n        for weight, estimator in zip(self.estimator_weights_,\n                                     self.estimators_):\n            norm += weight\n\n            if self.algorithm == 'SAMME.R':\n                # The weights are all 1. for SAMME.R\n                current_pred = _samme_proba(estimator, n_classes, X)\n            else:  # elif self.algorithm == \"SAMME\":\n                current_pred = estimator.predict(X)\n                current_pred = (current_pred == classes).T * weight\n\n            if pred is None:\n                pred = current_pred\n            else:\n                pred += current_pred\n\n            if n_classes == 2:\n                tmp_pred = np.copy(pred)\n                tmp_pred[:, 0] *= -1\n                yield (tmp_pred / norm).sum(axis=1)\n            else:\n                yield pred / norm\n\n    @staticmethod\n    def _compute_proba_from_decision(decision, n_classes):\n        \"\"\"Compute probabilities from the decision function.\n\n        This is based eq. (4) of [1] where:\n            p(y=c|X) = exp((1 / K-1) f_c(X)) / sum_k(exp((1 / K-1) f_k(X)))\n                     = softmax((1 / K-1) * f(X))\n\n        References\n        ----------\n        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",\n               2009.\n        \"\"\"\n        if n_classes == 2:\n            decision = np.vstack([-decision, decision]).T / 2\n        else:\n            decision /= (n_classes - 1)\n        return softmax(decision, copy=False)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n\n        if n_classes == 1:\n            return np.ones((_num_samples(X), 1))\n\n        decision = self.decision_function(X)\n        return self._compute_proba_from_decision(decision, n_classes)\n\n    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        This generator method yields the ensemble predicted class probabilities\n        after each iteration of boosting and therefore allows monitoring, such\n        as to determine the predicted class probabilities on a test set after\n        each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        -------\n        p : generator of ndarray of shape (n_samples,)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n\n        for decision in self.staged_decision_function(X):\n            yield self._compute_proba_from_decision(decision, n_classes)\n\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the weighted mean predicted class log-probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        X = self._check_X(X)\n        return np.log(self.predict_proba(X))",
      "instance_attributes": [
        {
          "name": "algorithm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "classes_",
          "types": null
        },
        {
          "name": "n_classes_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor",
      "name": "AdaBoostRegressor",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor",
      "decorators": [],
      "superclasses": [
        "RegressorMixin",
        "BaseWeightBoosting"
      ],
      "methods": [
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/fit",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/_validate_estimator",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/_boost",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/_get_median_predict",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/predict",
        "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/staged_predict"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.ensemble"
      ],
      "description": "An AdaBoost regressor.\n\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost.R2 [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14",
      "docstring": "An AdaBoost regressor.\n\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost.R2 [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14\n\nParameters\n----------\nbase_estimator : object, default=None\n    The base estimator from which the boosted ensemble is built.\n    If ``None``, then the base estimator is\n    :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n    `max_depth=3`.\n\nn_estimators : int, default=50\n    The maximum number of estimators at which boosting is terminated.\n    In case of perfect fit, the learning procedure is stopped early.\n\nlearning_rate : float, default=1.\n    Weight applied to each classifier at each boosting iteration. A higher\n    learning rate increases the contribution of each classifier. There is\n    a trade-off between the `learning_rate` and `n_estimators` parameters.\n\nloss : {'linear', 'square', 'exponential'}, default='linear'\n    The loss function to use when updating the weights after each\n    boosting iteration.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the random seed given at each `base_estimator` at each\n    boosting iteration.\n    Thus, it is only used when `base_estimator` exposes a `random_state`.\n    In addition, it controls the bootstrap of the weights used to train the\n    `base_estimator` at each boosting iteration.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nAttributes\n----------\nbase_estimator_ : estimator\n    The base estimator from which the ensemble is grown.\n\nestimators_ : list of classifiers\n    The collection of fitted sub-estimators.\n\nestimator_weights_ : ndarray of floats\n    Weights for each estimator in the boosted ensemble.\n\nestimator_errors_ : ndarray of floats\n    Regression error for each estimator in the boosted ensemble.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances if supported by the\n    ``base_estimator`` (when based on decision trees).\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nExamples\n--------\n>>> from sklearn.ensemble import AdaBoostRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(n_features=4, n_informative=2,\n...                        random_state=0, shuffle=False)\n>>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n>>> regr.fit(X, y)\nAdaBoostRegressor(n_estimators=100, random_state=0)\n>>> regr.predict([[0, 0, 0, 0]])\narray([4.7972...])\n>>> regr.score(X, y)\n0.9771...\n\nSee Also\n--------\nAdaBoostClassifier, GradientBoostingRegressor,\nsklearn.tree.DecisionTreeRegressor\n\nReferences\n----------\n.. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n       on-Line Learning and an Application to Boosting\", 1995.\n\n.. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.",
      "code": "class AdaBoostRegressor(RegressorMixin, BaseWeightBoosting):\n    \"\"\"An AdaBoost regressor.\n\n    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n    regressor on the original dataset and then fits additional copies of the\n    regressor on the same dataset but where the weights of instances are\n    adjusted according to the error of the current prediction. As such,\n    subsequent regressors focus more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost.R2 [2].\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    base_estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        If ``None``, then the base estimator is\n        :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n        `max_depth=3`.\n\n    n_estimators : int, default=50\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n\n    learning_rate : float, default=1.\n        Weight applied to each classifier at each boosting iteration. A higher\n        learning rate increases the contribution of each classifier. There is\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\n\n    loss : {'linear', 'square', 'exponential'}, default='linear'\n        The loss function to use when updating the weights after each\n        boosting iteration.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given at each `base_estimator` at each\n        boosting iteration.\n        Thus, it is only used when `base_estimator` exposes a `random_state`.\n        In addition, it controls the bootstrap of the weights used to train the\n        `base_estimator` at each boosting iteration.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    estimator_weights_ : ndarray of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : ndarray of floats\n        Regression error for each estimator in the boosted ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances if supported by the\n        ``base_estimator`` (when based on decision trees).\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=4, n_informative=2,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n    >>> regr.fit(X, y)\n    AdaBoostRegressor(n_estimators=100, random_state=0)\n    >>> regr.predict([[0, 0, 0, 0]])\n    array([4.7972...])\n    >>> regr.score(X, y)\n    0.9771...\n\n    See Also\n    --------\n    AdaBoostClassifier, GradientBoostingRegressor,\n    sklearn.tree.DecisionTreeRegressor\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None, *,\n                 n_estimators=50,\n                 learning_rate=1.,\n                 loss='linear',\n                 random_state=None):\n\n        super().__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state)\n\n        self.loss = loss\n        self.random_state = random_state\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Build a boosted regressor from the training set (X, y).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        y : array-like of shape (n_samples,)\n            The target values (real numbers).\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, the sample weights are initialized to\n            1 / n_samples.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Check loss\n        if self.loss not in ('linear', 'square', 'exponential'):\n            raise ValueError(\n                \"loss must be 'linear', 'square', or 'exponential'\")\n\n        # Fit\n        return super().fit(X, y, sample_weight)\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n        super()._validate_estimator(\n            default=DecisionTreeRegressor(max_depth=3))\n\n    def _boost(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost for regression\n\n        Perform a single boost according to the AdaBoost.R2 algorithm and\n        return the updated sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels in classification, real numbers in\n            regression).\n\n        sample_weight : array-like of shape (n_samples,)\n            The current sample weights.\n\n        random_state : RandomState\n            The RandomState instance used if the base estimator accepts a\n            `random_state` attribute.\n            Controls also the bootstrap of the weights used to train the weak\n            learner.\n            replacement.\n\n        Returns\n        -------\n        sample_weight : array-like of shape (n_samples,) or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        estimator_weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        estimator_error : float\n            The regression error for the current boost.\n            If None then boosting has terminated early.\n        \"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        # Weighted sampling of the training set with replacement\n        bootstrap_idx = random_state.choice(\n            np.arange(_num_samples(X)), size=_num_samples(X), replace=True,\n            p=sample_weight\n        )\n\n        # Fit on the bootstrapped sample and obtain a prediction\n        # for all samples in the training set\n        X_ = _safe_indexing(X, bootstrap_idx)\n        y_ = _safe_indexing(y, bootstrap_idx)\n        estimator.fit(X_, y_)\n        y_predict = estimator.predict(X)\n\n        error_vect = np.abs(y_predict - y)\n        sample_mask = sample_weight > 0\n        masked_sample_weight = sample_weight[sample_mask]\n        masked_error_vector = error_vect[sample_mask]\n\n        error_max = masked_error_vector.max()\n        if error_max != 0:\n            masked_error_vector /= error_max\n\n        if self.loss == 'square':\n            masked_error_vector **= 2\n        elif self.loss == 'exponential':\n            masked_error_vector = 1. - np.exp(-masked_error_vector)\n\n        # Calculate the average loss\n        estimator_error = (masked_sample_weight * masked_error_vector).sum()\n\n        if estimator_error <= 0:\n            # Stop if fit is perfect\n            return sample_weight, 1., 0.\n\n        elif estimator_error >= 0.5:\n            # Discard current estimator only if it isn't the only one\n            if len(self.estimators_) > 1:\n                self.estimators_.pop(-1)\n            return None, None, None\n\n        beta = estimator_error / (1. - estimator_error)\n\n        # Boost weight using AdaBoost.R2 alg\n        estimator_weight = self.learning_rate * np.log(1. / beta)\n\n        if not iboost == self.n_estimators - 1:\n            sample_weight[sample_mask] *= np.power(\n                beta, (1. - masked_error_vector) * self.learning_rate\n            )\n\n        return sample_weight, estimator_weight, estimator_error\n\n    def _get_median_predict(self, X, limit):\n        # Evaluate predictions of all estimators\n        predictions = np.array([\n            est.predict(X) for est in self.estimators_[:limit]]).T\n\n        # Sort the predictions\n        sorted_idx = np.argsort(predictions, axis=1)\n\n        # Find index of median prediction for each sample\n        weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)\n        median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n        median_idx = median_or_above.argmax(axis=1)\n\n        median_estimators = sorted_idx[np.arange(_num_samples(X)), median_idx]\n\n        # Return median predictions\n        return predictions[np.arange(_num_samples(X)), median_estimators]\n\n    def predict(self, X):\n        \"\"\"Predict regression value for X.\n\n        The predicted regression value of an input sample is computed\n        as the weighted median prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted regression values.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        return self._get_median_predict(X, len(self.estimators_))\n\n    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted regression value of an input sample is computed\n        as the weighted median prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted regression values.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        for i, _ in enumerate(self.estimators_, 1):\n            yield self._get_median_predict(X, limit=i)",
      "instance_attributes": [
        {
          "name": "loss",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "random_state",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer",
      "name": "DictVectorizer",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/_add_iterable_element",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/_transform",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit_transform",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/inverse_transform",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/transform",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/get_feature_names",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/restrict",
        "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_extraction"
      ],
      "description": "Transforms lists of feature-value mappings to vectors.\n\nThis transformer turns lists of mappings (dict-like objects) of feature\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\nwith scikit-learn estimators.\n\nWhen feature values are strings, this transformer will do a binary one-hot\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\nof the possible string values that the feature can take on. For instance,\na feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\nfeatures in the output, one signifying \"f=ham\", the other \"f=spam\".\n\nIf a feature value is a sequence or set of strings, this transformer\nwill iterate over the values and will count the occurrences of each string\nvalue.\n\nHowever, note that this transformer will only do a binary one-hot encoding\nwhen feature values are of type string. If categorical features are\nrepresented as numeric values such as int or iterables of strings, the\nDictVectorizer can be followed by\n:class:`~sklearn.preprocessing.OneHotEncoder` to complete\nbinary one-hot encoding.\n\nFeatures that do not occur in a sample (mapping) will have a zero value\nin the resulting array/matrix.\n\nRead more in the :ref:`User Guide <dict_feature_extraction>`.",
      "docstring": "Transforms lists of feature-value mappings to vectors.\n\nThis transformer turns lists of mappings (dict-like objects) of feature\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\nwith scikit-learn estimators.\n\nWhen feature values are strings, this transformer will do a binary one-hot\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\nof the possible string values that the feature can take on. For instance,\na feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\nfeatures in the output, one signifying \"f=ham\", the other \"f=spam\".\n\nIf a feature value is a sequence or set of strings, this transformer\nwill iterate over the values and will count the occurrences of each string\nvalue.\n\nHowever, note that this transformer will only do a binary one-hot encoding\nwhen feature values are of type string. If categorical features are\nrepresented as numeric values such as int or iterables of strings, the\nDictVectorizer can be followed by\n:class:`~sklearn.preprocessing.OneHotEncoder` to complete\nbinary one-hot encoding.\n\nFeatures that do not occur in a sample (mapping) will have a zero value\nin the resulting array/matrix.\n\nRead more in the :ref:`User Guide <dict_feature_extraction>`.\n\nParameters\n----------\ndtype : dtype, default=np.float64\n    The type of feature values. Passed to Numpy array/scipy.sparse matrix\n    constructors as the dtype argument.\nseparator : str, default=\"=\"\n    Separator string used when constructing new features for one-hot\n    coding.\nsparse : bool, default=True\n    Whether transform should produce scipy.sparse matrices.\nsort : bool, default=True\n    Whether ``feature_names_`` and ``vocabulary_`` should be\n    sorted when fitting.\n\nAttributes\n----------\nvocabulary_ : dict\n    A dictionary mapping feature names to feature indices.\n\nfeature_names_ : list\n    A list of length n_features containing the feature names (e.g., \"f=ham\"\n    and \"f=spam\").\n\nExamples\n--------\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> v = DictVectorizer(sparse=False)\n>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n>>> X = v.fit_transform(D)\n>>> X\narray([[2., 0., 1.],\n       [0., 1., 3.]])\n>>> v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},\n...                            {'baz': 1.0, 'foo': 3.0}]\nTrue\n>>> v.transform({'foo': 4, 'unseen_feature': 3})\narray([[0., 0., 4.]])\n\nSee Also\n--------\nFeatureHasher : Performs vectorization using only a hash function.\nsklearn.preprocessing.OrdinalEncoder : Handles nominal/categorical\n    features encoded as columns of arbitrary data types.",
      "code": "class DictVectorizer(TransformerMixin, BaseEstimator):\n    \"\"\"Transforms lists of feature-value mappings to vectors.\n\n    This transformer turns lists of mappings (dict-like objects) of feature\n    names to feature values into Numpy arrays or scipy.sparse matrices for use\n    with scikit-learn estimators.\n\n    When feature values are strings, this transformer will do a binary one-hot\n    (aka one-of-K) coding: one boolean-valued feature is constructed for each\n    of the possible string values that the feature can take on. For instance,\n    a feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\n    features in the output, one signifying \"f=ham\", the other \"f=spam\".\n\n    If a feature value is a sequence or set of strings, this transformer\n    will iterate over the values and will count the occurrences of each string\n    value.\n\n    However, note that this transformer will only do a binary one-hot encoding\n    when feature values are of type string. If categorical features are\n    represented as numeric values such as int or iterables of strings, the\n    DictVectorizer can be followed by\n    :class:`~sklearn.preprocessing.OneHotEncoder` to complete\n    binary one-hot encoding.\n\n    Features that do not occur in a sample (mapping) will have a zero value\n    in the resulting array/matrix.\n\n    Read more in the :ref:`User Guide <dict_feature_extraction>`.\n\n    Parameters\n    ----------\n    dtype : dtype, default=np.float64\n        The type of feature values. Passed to Numpy array/scipy.sparse matrix\n        constructors as the dtype argument.\n    separator : str, default=\"=\"\n        Separator string used when constructing new features for one-hot\n        coding.\n    sparse : bool, default=True\n        Whether transform should produce scipy.sparse matrices.\n    sort : bool, default=True\n        Whether ``feature_names_`` and ``vocabulary_`` should be\n        sorted when fitting.\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A dictionary mapping feature names to feature indices.\n\n    feature_names_ : list\n        A list of length n_features containing the feature names (e.g., \"f=ham\"\n        and \"f=spam\").\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction import DictVectorizer\n    >>> v = DictVectorizer(sparse=False)\n    >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n    >>> X = v.fit_transform(D)\n    >>> X\n    array([[2., 0., 1.],\n           [0., 1., 3.]])\n    >>> v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},\n    ...                            {'baz': 1.0, 'foo': 3.0}]\n    True\n    >>> v.transform({'foo': 4, 'unseen_feature': 3})\n    array([[0., 0., 4.]])\n\n    See Also\n    --------\n    FeatureHasher : Performs vectorization using only a hash function.\n    sklearn.preprocessing.OrdinalEncoder : Handles nominal/categorical\n        features encoded as columns of arbitrary data types.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, dtype=np.float64, separator=\"=\", sparse=True,\n                 sort=True):\n        self.dtype = dtype\n        self.separator = separator\n        self.sparse = sparse\n        self.sort = sort\n\n    def _add_iterable_element(self, f, v, feature_names, vocab, *,\n                              fitting=True, transforming=False,\n                              indices=None, values=None):\n        \"\"\"Add feature names for iterable of strings\"\"\"\n        for vv in v:\n            if isinstance(vv, str):\n                feature_name = \"%s%s%s\" % (f, self.separator, vv)\n                vv = 1\n            else:\n                raise TypeError(f'Unsupported type {type(vv)} in iterable '\n                                'value. Only iterables of string are '\n                                'supported.')\n            if fitting and feature_name not in vocab:\n                vocab[feature_name] = len(feature_names)\n                feature_names.append(feature_name)\n\n            if transforming and feature_name in vocab:\n                indices.append(vocab[feature_name])\n                values.append(self.dtype(vv))\n\n        return\n\n    def fit(self, X, y=None):\n        \"\"\"Learn a list of feature name -> indices mappings.\n\n        Parameters\n        ----------\n        X : Mapping or iterable over Mappings\n            Dict(s) or Mapping(s) from feature names (arbitrary Python\n            objects) to feature values (strings or convertible to dtype).\n\n            .. versionchanged:: 0.24\n               Accepts multiple string values for one categorical feature.\n\n        y : (ignored)\n\n        Returns\n        -------\n        self\n        \"\"\"\n        feature_names = []\n        vocab = {}\n\n        for x in X:\n            for f, v in x.items():\n                if isinstance(v, str):\n                    feature_name = \"%s%s%s\" % (f, self.separator, v)\n                    v = 1\n                elif isinstance(v, Number) or (v is None):\n                    feature_name = f\n                elif isinstance(v, Mapping):\n                    raise TypeError(f'Unsupported value type {type(v)} '\n                                    f'for {f}: {v}.\\n'\n                                    'Mapping objects are not supported.')\n                elif isinstance(v, Iterable):\n                    feature_name = None\n                    self._add_iterable_element(f, v, feature_names, vocab)\n\n                if feature_name is not None:\n                    if feature_name not in vocab:\n                        vocab[feature_name] = len(feature_names)\n                        feature_names.append(feature_name)\n\n        if self.sort:\n            feature_names.sort()\n            vocab = {f: i for i, f in enumerate(feature_names)}\n\n        self.feature_names_ = feature_names\n        self.vocabulary_ = vocab\n\n        return self\n\n    def _transform(self, X, fitting):\n        # Sanity check: Python's array has no way of explicitly requesting the\n        # signed 32-bit integers that scipy.sparse needs, so we use the next\n        # best thing: typecode \"i\" (int). However, if that gives larger or\n        # smaller integers than 32-bit ones, np.frombuffer screws up.\n        assert array(\"i\").itemsize == 4, (\n            \"sizeof(int) != 4 on your platform; please report this at\"\n            \" https://github.com/scikit-learn/scikit-learn/issues and\"\n            \" include the output from platform.platform() in your bug report\")\n\n        dtype = self.dtype\n        if fitting:\n            feature_names = []\n            vocab = {}\n        else:\n            feature_names = self.feature_names_\n            vocab = self.vocabulary_\n\n        transforming = True\n\n        # Process everything as sparse regardless of setting\n        X = [X] if isinstance(X, Mapping) else X\n\n        indices = array(\"i\")\n        indptr = [0]\n        # XXX we could change values to an array.array as well, but it\n        # would require (heuristic) conversion of dtype to typecode...\n        values = []\n\n        # collect all the possible feature names and build sparse matrix at\n        # same time\n        for x in X:\n            for f, v in x.items():\n                if isinstance(v, str):\n                    feature_name = \"%s%s%s\" % (f, self.separator, v)\n                    v = 1\n                elif isinstance(v, Number) or (v is None):\n                    feature_name = f\n                elif isinstance(v, Mapping):\n                    raise TypeError(f'Unsupported value Type {type(v)} '\n                                    f'for {f}: {v}.\\n'\n                                    'Mapping objects are not supported.')\n                elif isinstance(v, Iterable):\n                    feature_name = None\n                    self._add_iterable_element(f, v, feature_names, vocab,\n                                               fitting=fitting,\n                                               transforming=transforming,\n                                               indices=indices, values=values)\n\n                if feature_name is not None:\n                    if fitting and feature_name not in vocab:\n                        vocab[feature_name] = len(feature_names)\n                        feature_names.append(feature_name)\n\n                    if feature_name in vocab:\n                        indices.append(vocab[feature_name])\n                        values.append(self.dtype(v))\n\n            indptr.append(len(indices))\n\n        if len(indptr) == 1:\n            raise ValueError(\"Sample sequence X is empty.\")\n\n        indices = np.frombuffer(indices, dtype=np.intc)\n        shape = (len(indptr) - 1, len(vocab))\n\n        result_matrix = sp.csr_matrix((values, indices, indptr),\n                                      shape=shape, dtype=dtype)\n\n        # Sort everything if asked\n        if fitting and self.sort:\n            feature_names.sort()\n            map_index = np.empty(len(feature_names), dtype=np.int32)\n            for new_val, f in enumerate(feature_names):\n                map_index[new_val] = vocab[f]\n                vocab[f] = new_val\n            result_matrix = result_matrix[:, map_index]\n\n        if self.sparse:\n            result_matrix.sort_indices()\n        else:\n            result_matrix = result_matrix.toarray()\n\n        if fitting:\n            self.feature_names_ = feature_names\n            self.vocabulary_ = vocab\n\n        return result_matrix\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Learn a list of feature name -> indices mappings and transform X.\n\n        Like fit(X) followed by transform(X), but does not require\n        materializing X in memory.\n\n        Parameters\n        ----------\n        X : Mapping or iterable over Mappings\n            Dict(s) or Mapping(s) from feature names (arbitrary Python\n            objects) to feature values (strings or convertible to dtype).\n\n            .. versionchanged:: 0.24\n               Accepts multiple string values for one categorical feature.\n\n        y : (ignored)\n\n        Returns\n        -------\n        Xa : {array, sparse matrix}\n            Feature vectors; always 2-d.\n        \"\"\"\n        return self._transform(X, fitting=True)\n\n    def inverse_transform(self, X, dict_type=dict):\n        \"\"\"Transform array or sparse matrix X back to feature mappings.\n\n        X must have been produced by this DictVectorizer's transform or\n        fit_transform method; it may only have passed through transformers\n        that preserve the number of features and their order.\n\n        In the case of one-hot/one-of-K coding, the constructed feature\n        names and values are returned rather than the original ones.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Sample matrix.\n        dict_type : type, default=dict\n            Constructor for feature mappings. Must conform to the\n            collections.Mapping API.\n\n        Returns\n        -------\n        D : list of dict_type objects of shape (n_samples,)\n            Feature mappings for the samples in X.\n        \"\"\"\n        # COO matrix is not subscriptable\n        X = check_array(X, accept_sparse=['csr', 'csc'])\n        n_samples = X.shape[0]\n\n        names = self.feature_names_\n        dicts = [dict_type() for _ in range(n_samples)]\n\n        if sp.issparse(X):\n            for i, j in zip(*X.nonzero()):\n                dicts[i][names[j]] = X[i, j]\n        else:\n            for i, d in enumerate(dicts):\n                for j, v in enumerate(X[i, :]):\n                    if v != 0:\n                        d[names[j]] = X[i, j]\n\n        return dicts\n\n    def transform(self, X):\n        \"\"\"Transform feature->value dicts to array or sparse matrix.\n\n        Named features not encountered during fit or fit_transform will be\n        silently ignored.\n\n        Parameters\n        ----------\n        X : Mapping or iterable over Mappings of shape (n_samples,)\n            Dict(s) or Mapping(s) from feature names (arbitrary Python\n            objects) to feature values (strings or convertible to dtype).\n\n        Returns\n        -------\n        Xa : {array, sparse matrix}\n            Feature vectors; always 2-d.\n        \"\"\"\n        if self.sparse:\n            return self._transform(X, fitting=False)\n\n        else:\n            dtype = self.dtype\n            vocab = self.vocabulary_\n            X = _tosequence(X)\n            Xa = np.zeros((len(X), len(vocab)), dtype=dtype)\n\n            for i, x in enumerate(X):\n                for f, v in x.items():\n                    if isinstance(v, str):\n                        f = \"%s%s%s\" % (f, self.separator, v)\n                        v = 1\n                    try:\n                        Xa[i, vocab[f]] = dtype(v)\n                    except KeyError:\n                        pass\n\n            return Xa\n\n    def get_feature_names(self):\n        \"\"\"Returns a list of feature names, ordered by their indices.\n\n        If one-of-K coding is applied to categorical features, this will\n        include the constructed feature names but not the original ones.\n        \"\"\"\n        return self.feature_names_\n\n    def restrict(self, support, indices=False):\n        \"\"\"Restrict the features to those in support using feature selection.\n\n        This function modifies the estimator in-place.\n\n        Parameters\n        ----------\n        support : array-like\n            Boolean mask or list of indices (as returned by the get_support\n            member of feature selectors).\n        indices : bool, default=False\n            Whether support is a list of indices.\n\n        Returns\n        -------\n        self\n\n        Examples\n        --------\n        >>> from sklearn.feature_extraction import DictVectorizer\n        >>> from sklearn.feature_selection import SelectKBest, chi2\n        >>> v = DictVectorizer()\n        >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n        >>> X = v.fit_transform(D)\n        >>> support = SelectKBest(chi2, k=2).fit(X, [0, 1])\n        >>> v.get_feature_names()\n        ['bar', 'baz', 'foo']\n        >>> v.restrict(support.get_support())\n        DictVectorizer()\n        >>> v.get_feature_names()\n        ['bar', 'foo']\n        \"\"\"\n        if not indices:\n            support = np.where(support)[0]\n\n        names = self.feature_names_\n        new_vocab = {}\n        for i in support:\n            new_vocab[names[i]] = len(new_vocab)\n\n        self.vocabulary_ = new_vocab\n        self.feature_names_ = [f for f, i in sorted(new_vocab.items(),\n                                                    key=itemgetter(1))]\n\n        return self\n\n    def _more_tags(self):\n        return {'X_types': [\"dict\"]}",
      "instance_attributes": [
        {
          "name": "dtype",
          "types": {
            "kind": "NamedType",
            "name": "type"
          }
        },
        {
          "name": "separator",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "sparse",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "sort",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "feature_names_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "vocabulary_",
          "types": {
            "kind": "NamedType",
            "name": "dict"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher",
      "name": "FeatureHasher",
      "qname": "sklearn.feature_extraction._hash.FeatureHasher",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__",
        "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/_validate_params",
        "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/fit",
        "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/transform",
        "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_extraction"
      ],
      "description": "Implements feature hashing, aka the hashing trick.\n\nThis class turns sequences of symbolic feature names (strings) into\nscipy.sparse matrices, using a hash function to compute the matrix column\ncorresponding to a name. The hash function employed is the signed 32-bit\nversion of Murmurhash3.\n\nFeature names of type byte string are used as-is. Unicode strings are\nconverted to UTF-8 first, but no Unicode normalization is done.\nFeature values must be (finite) numbers.\n\nThis class is a low-memory alternative to DictVectorizer and\nCountVectorizer, intended for large-scale (online) learning and situations\nwhere memory is tight, e.g. when running prediction code on embedded\ndevices.\n\nRead more in the :ref:`User Guide <feature_hashing>`.\n\n.. versionadded:: 0.13",
      "docstring": "Implements feature hashing, aka the hashing trick.\n\nThis class turns sequences of symbolic feature names (strings) into\nscipy.sparse matrices, using a hash function to compute the matrix column\ncorresponding to a name. The hash function employed is the signed 32-bit\nversion of Murmurhash3.\n\nFeature names of type byte string are used as-is. Unicode strings are\nconverted to UTF-8 first, but no Unicode normalization is done.\nFeature values must be (finite) numbers.\n\nThis class is a low-memory alternative to DictVectorizer and\nCountVectorizer, intended for large-scale (online) learning and situations\nwhere memory is tight, e.g. when running prediction code on embedded\ndevices.\n\nRead more in the :ref:`User Guide <feature_hashing>`.\n\n.. versionadded:: 0.13\n\nParameters\n----------\nn_features : int, default=2**20\n    The number of features (columns) in the output matrices. Small numbers\n    of features are likely to cause hash collisions, but large numbers\n    will cause larger coefficient dimensions in linear learners.\ninput_type : {\"dict\", \"pair\", \"string\"}, default=\"dict\"\n    Either \"dict\" (the default) to accept dictionaries over\n    (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n    or \"string\" to accept single strings.\n    feature_name should be a string, while value should be a number.\n    In the case of \"string\", a value of 1 is implied.\n    The feature_name is hashed to find the appropriate column for the\n    feature. The value's sign might be flipped in the output (but see\n    non_negative, below).\ndtype : numpy dtype, default=np.float64\n    The type of feature values. Passed to scipy.sparse matrix constructors\n    as the dtype argument. Do not set this to bool, np.boolean or any\n    unsigned integer type.\nalternate_sign : bool, default=True\n    When True, an alternating sign is added to the features as to\n    approximately conserve the inner product in the hashed space even for\n    small n_features. This approach is similar to sparse random projection.\n\n    .. versionchanged:: 0.19\n        ``alternate_sign`` replaces the now deprecated ``non_negative``\n        parameter.\n\nExamples\n--------\n>>> from sklearn.feature_extraction import FeatureHasher\n>>> h = FeatureHasher(n_features=10)\n>>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n>>> f = h.transform(D)\n>>> f.toarray()\narray([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\nSee Also\n--------\nDictVectorizer : Vectorizes string-valued features using a hash table.\nsklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.",
      "code": "class FeatureHasher(TransformerMixin, BaseEstimator):\n    \"\"\"Implements feature hashing, aka the hashing trick.\n\n    This class turns sequences of symbolic feature names (strings) into\n    scipy.sparse matrices, using a hash function to compute the matrix column\n    corresponding to a name. The hash function employed is the signed 32-bit\n    version of Murmurhash3.\n\n    Feature names of type byte string are used as-is. Unicode strings are\n    converted to UTF-8 first, but no Unicode normalization is done.\n    Feature values must be (finite) numbers.\n\n    This class is a low-memory alternative to DictVectorizer and\n    CountVectorizer, intended for large-scale (online) learning and situations\n    where memory is tight, e.g. when running prediction code on embedded\n    devices.\n\n    Read more in the :ref:`User Guide <feature_hashing>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_features : int, default=2**20\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n    input_type : {\"dict\", \"pair\", \"string\"}, default=\"dict\"\n        Either \"dict\" (the default) to accept dictionaries over\n        (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n        or \"string\" to accept single strings.\n        feature_name should be a string, while value should be a number.\n        In the case of \"string\", a value of 1 is implied.\n        The feature_name is hashed to find the appropriate column for the\n        feature. The value's sign might be flipped in the output (but see\n        non_negative, below).\n    dtype : numpy dtype, default=np.float64\n        The type of feature values. Passed to scipy.sparse matrix constructors\n        as the dtype argument. Do not set this to bool, np.boolean or any\n        unsigned integer type.\n    alternate_sign : bool, default=True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionchanged:: 0.19\n            ``alternate_sign`` replaces the now deprecated ``non_negative``\n            parameter.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction import FeatureHasher\n    >>> h = FeatureHasher(n_features=10)\n    >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n    >>> f = h.transform(D)\n    >>> f.toarray()\n    array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n           [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\n    See Also\n    --------\n    DictVectorizer : Vectorizes string-valued features using a hash table.\n    sklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, n_features=(2 ** 20), *, input_type=\"dict\",\n                 dtype=np.float64, alternate_sign=True):\n        self._validate_params(n_features, input_type)\n\n        self.dtype = dtype\n        self.input_type = input_type\n        self.n_features = n_features\n        self.alternate_sign = alternate_sign\n\n    @staticmethod\n    def _validate_params(n_features, input_type):\n        # strangely, np.int16 instances are not instances of Integral,\n        # while np.int64 instances are...\n        if not isinstance(n_features, numbers.Integral):\n            raise TypeError(\"n_features must be integral, got %r (%s).\"\n                            % (n_features, type(n_features)))\n        elif n_features < 1 or n_features >= np.iinfo(np.int32).max + 1:\n            raise ValueError(\"Invalid number of features (%d).\" % n_features)\n\n        if input_type not in (\"dict\", \"pair\", \"string\"):\n            raise ValueError(\"input_type must be 'dict', 'pair' or 'string',\"\n                             \" got %r.\" % input_type)\n\n    def fit(self, X=None, y=None):\n        \"\"\"No-op.\n\n        This method doesn't do anything. It exists purely for compatibility\n        with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : ndarray\n\n        Returns\n        -------\n        self : FeatureHasher\n\n        \"\"\"\n        # repeat input validation for grid search (which calls set_params)\n        self._validate_params(self.n_features, self.input_type)\n        return self\n\n    def transform(self, raw_X):\n        \"\"\"Transform a sequence of instances to a scipy.sparse matrix.\n\n        Parameters\n        ----------\n        raw_X : iterable over iterable over raw features, length = n_samples\n            Samples. Each sample must be iterable an (e.g., a list or tuple)\n            containing/generating feature names (and optionally values, see\n            the input_type constructor argument) which will be hashed.\n            raw_X need not support the len function, so it can be the result\n            of a generator; n_samples is determined on the fly.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Feature matrix, for use with estimators or further transformers.\n\n        \"\"\"\n        raw_X = iter(raw_X)\n        if self.input_type == \"dict\":\n            raw_X = (_iteritems(d) for d in raw_X)\n        elif self.input_type == \"string\":\n            raw_X = (((f, 1) for f in x) for x in raw_X)\n        indices, indptr, values = \\\n            _hashing_transform(raw_X, self.n_features, self.dtype,\n                               self.alternate_sign, seed=0)\n        n_samples = indptr.shape[0] - 1\n\n        if n_samples == 0:\n            raise ValueError(\"Cannot vectorize empty sequence.\")\n\n        X = sp.csr_matrix((values, indices, indptr), dtype=self.dtype,\n                          shape=(n_samples, self.n_features))\n        X.sum_duplicates()  # also sorts the indices\n\n        return X\n\n    def _more_tags(self):\n        return {'X_types': [self.input_type]}",
      "instance_attributes": [
        {
          "name": "dtype",
          "types": {
            "kind": "NamedType",
            "name": "type"
          }
        },
        {
          "name": "input_type",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "n_features",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "alternate_sign",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor",
      "name": "PatchExtractor",
      "qname": "sklearn.feature_extraction.image.PatchExtractor",
      "decorators": [],
      "superclasses": [
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/__init__",
        "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/fit",
        "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/transform",
        "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [],
      "description": "Extracts patches from a collection of images\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\n.. versionadded:: 0.9",
      "docstring": "Extracts patches from a collection of images\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\n.. versionadded:: 0.9\n\nParameters\n----------\npatch_size : tuple of int (patch_height, patch_width), default=None\n    The dimensions of one patch.\n\nmax_patches : int or float, default=None\n    The maximum number of patches per image to extract. If max_patches is a\n    float in (0, 1), it is taken to mean a proportion of the total number\n    of patches.\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator used for random sampling when\n    `max_patches` is not None. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nExamples\n--------\n>>> from sklearn.datasets import load_sample_images\n>>> from sklearn.feature_extraction import image\n>>> # Use the array data from the second image in this dataset:\n>>> X = load_sample_images().images[1]\n>>> print('Image shape: {}'.format(X.shape))\nImage shape: (427, 640, 3)\n>>> pe = image.PatchExtractor(patch_size=(2, 2))\n>>> pe_fit = pe.fit(X)\n>>> pe_trans = pe.transform(X)\n>>> print('Patches shape: {}'.format(pe_trans.shape))\nPatches shape: (545706, 2, 2)",
      "code": "class PatchExtractor(BaseEstimator):\n    \"\"\"Extracts patches from a collection of images\n\n    Read more in the :ref:`User Guide <image_feature_extraction>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    patch_size : tuple of int (patch_height, patch_width), default=None\n        The dimensions of one patch.\n\n    max_patches : int or float, default=None\n        The maximum number of patches per image to extract. If max_patches is a\n        float in (0, 1), it is taken to mean a proportion of the total number\n        of patches.\n\n    random_state : int, RandomState instance, default=None\n        Determines the random number generator used for random sampling when\n        `max_patches` is not None. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_sample_images\n    >>> from sklearn.feature_extraction import image\n    >>> # Use the array data from the second image in this dataset:\n    >>> X = load_sample_images().images[1]\n    >>> print('Image shape: {}'.format(X.shape))\n    Image shape: (427, 640, 3)\n    >>> pe = image.PatchExtractor(patch_size=(2, 2))\n    >>> pe_fit = pe.fit(X)\n    >>> pe_trans = pe.transform(X)\n    >>> print('Patches shape: {}'.format(pe_trans.shape))\n    Patches shape: (545706, 2, 2)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, patch_size=None, max_patches=None,\n                 random_state=None):\n        self.patch_size = patch_size\n        self.max_patches = max_patches\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        \"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"Transforms the image samples in X into a matrix of patch data.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, image_height, image_width) or \\\n            (n_samples, image_height, image_width, n_channels)\n            Array of images from which to extract patches. For color images,\n            the last dimension specifies the channel: a RGB image would have\n            `n_channels=3`.\n\n        Returns\n        -------\n        patches : array of shape (n_patches, patch_height, patch_width) or \\\n             (n_patches, patch_height, patch_width, n_channels)\n             The collection of patches extracted from the images, where\n             `n_patches` is either `n_samples * max_patches` or the total\n             number of patches that can be extracted.\n        \"\"\"\n        self.random_state = check_random_state(self.random_state)\n        n_images, i_h, i_w = X.shape[:3]\n        X = np.reshape(X, (n_images, i_h, i_w, -1))\n        n_channels = X.shape[-1]\n        if self.patch_size is None:\n            patch_size = i_h // 10, i_w // 10\n        else:\n            patch_size = self.patch_size\n\n        # compute the dimensions of the patches array\n        p_h, p_w = patch_size\n        n_patches = _compute_n_patches(i_h, i_w, p_h, p_w, self.max_patches)\n        patches_shape = (n_images * n_patches,) + patch_size\n        if n_channels > 1:\n            patches_shape += (n_channels,)\n\n        # extract the patches\n        patches = np.empty(patches_shape)\n        for ii, image in enumerate(X):\n            patches[ii * n_patches:(ii + 1) * n_patches] = extract_patches_2d(\n                image, patch_size, max_patches=self.max_patches,\n                random_state=self.random_state)\n        return patches\n\n    def _more_tags(self):\n        return {'X_types': ['3darray']}",
      "instance_attributes": [
        {
          "name": "patch_size",
          "types": null
        },
        {
          "name": "max_patches",
          "types": null
        },
        {
          "name": "random_state",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer",
      "name": "CountVectorizer",
      "qname": "sklearn.feature_extraction.text.CountVectorizer",
      "decorators": [],
      "superclasses": [
        "_VectorizerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/_sort_features",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/_limit_features",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/_count_vocab",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit_transform",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/transform",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/inverse_transform",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/get_feature_names",
        "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [],
      "description": "Convert a collection of text documents to a matrix of token counts\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "Convert a collection of text documents to a matrix of token counts\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : string, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (strip_accents and lowercase) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. max_df can be set to a value\n    in the range [0.7, 1.0) to automatically detect and filter stop\n    words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    max_features ordered by term frequency across the corpus.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : type, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_: boolean\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(vectorizer.get_feature_names())\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> print(vectorizer2.get_feature_names())\n['and this', 'document is', 'first document', 'is the', 'is this',\n'second document', 'the first', 'the second', 'the third', 'third one',\n 'this document', 'this is', 'this the']\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n\nSee Also\n--------\nHashingVectorizer, TfidfVectorizer\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.",
      "code": "class CountVectorizer(_VectorizerMixin, BaseEstimator):\n    r\"\"\"Convert a collection of text documents to a matrix of token counts\n\n    This implementation produces a sparse representation of the counts using\n    scipy.sparse.csr_matrix.\n\n    If you do not provide an a-priori dictionary and you do not use an analyzer\n    that does some kind of feature selection then the number of features will\n    be equal to the vocabulary size found by analyzing the data.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : {'filename', 'file', 'content'}, default='content'\n        - If `'filename'`, the sequence passed as an argument to fit is\n          expected to be a list of filenames that need reading to fetch\n          the raw content to analyze.\n\n        - If `'file'`, the sequence items must have a 'read' method (file-like\n          object) that is called to fetch the bytes in memory.\n\n        - If `'content'`, the input is expected to be a sequence of items that\n          can be of type string or byte.\n\n    encoding : string, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode'}, default=None\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable, default=None\n        Override the preprocessing (strip_accents and lowercase) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer is not callable``.\n\n    tokenizer : callable, default=None\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    stop_words : {'english'}, list, default=None\n        If 'english', a built-in stop word list for English is used.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n        If None, no stop words will be used. max_df can be set to a value\n        in the range [0.7, 1.0) to automatically detect and filter stop\n        words based on intra corpus document frequency of terms.\n\n    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp select tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n        If there is a capturing group in token_pattern then the\n        captured group content, not the entire match, becomes the token.\n        At most one capturing group is permitted.\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        word n-grams or char n-grams to be extracted. All values of n such\n        such that min_n <= n <= max_n will be used. For example an\n        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n        unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n        Only applies if ``analyzer is not callable``.\n\n    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n        Whether the feature should be made of word n-gram or character\n        n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n\n        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n        first read from the file and then passed to the given callable\n        analyzer.\n\n    max_df : float in range [0.0, 1.0] or int, default=1.0\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float in range [0.0, 1.0] or int, default=1\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int, default=None\n        If not None, build a vocabulary that only consider the top\n        max_features ordered by term frequency across the corpus.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, default=None\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents. Indices\n        in the mapping should not be repeated and should not have any gap\n        between 0 and the largest index.\n\n    binary : bool, default=False\n        If True, all non zero counts are set to 1. This is useful for discrete\n        probabilistic models that model binary events rather than integer\n        counts.\n\n    dtype : type, default=np.int64\n        Type of the matrix returned by fit_transform() or transform().\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    fixed_vocabulary_: boolean\n        True if a fixed vocabulary of term to indices mapping\n        is provided by the user\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = CountVectorizer()\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(vectorizer.get_feature_names())\n    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n    >>> print(X.toarray())\n    [[0 1 1 1 0 0 1 0 1]\n     [0 2 0 1 0 1 1 0 1]\n     [1 0 0 1 1 0 1 1 1]\n     [0 1 1 1 0 0 1 0 1]]\n    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n    >>> X2 = vectorizer2.fit_transform(corpus)\n    >>> print(vectorizer2.get_feature_names())\n    ['and this', 'document is', 'first document', 'is the', 'is this',\n    'second document', 'the first', 'the second', 'the third', 'third one',\n     'this document', 'this is', 'this the']\n     >>> print(X2.toarray())\n     [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n     [0 1 0 1 0 1 0 1 0 0 1 0 0]\n     [1 0 0 1 0 0 0 0 1 1 0 1 0]\n     [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n\n    See Also\n    --------\n    HashingVectorizer, TfidfVectorizer\n\n    Notes\n    -----\n    The ``stop_words_`` attribute can get large and increase the model size\n    when pickling. This attribute is provided only for introspection and can\n    be safely removed using delattr or set to None before pickling.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None,\n                 lowercase=True, preprocessor=None, tokenizer=None,\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), analyzer='word',\n                 max_df=1.0, min_df=1, max_features=None,\n                 vocabulary=None, binary=False, dtype=np.int64):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.max_df = max_df\n        self.min_df = min_df\n        if max_df < 0 or min_df < 0:\n            raise ValueError(\"negative value for max_df or min_df\")\n        self.max_features = max_features\n        if max_features is not None:\n            if (not isinstance(max_features, numbers.Integral) or\n                    max_features <= 0):\n                raise ValueError(\n                    \"max_features=%r, neither a positive integer nor None\"\n                    % max_features)\n        self.ngram_range = ngram_range\n        self.vocabulary = vocabulary\n        self.binary = binary\n        self.dtype = dtype\n\n    def _sort_features(self, X, vocabulary):\n        \"\"\"Sort features by name\n\n        Returns a reordered matrix and modifies the vocabulary in place\n        \"\"\"\n        sorted_features = sorted(vocabulary.items())\n        map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)\n        for new_val, (term, old_val) in enumerate(sorted_features):\n            vocabulary[term] = new_val\n            map_index[old_val] = new_val\n\n        X.indices = map_index.take(X.indices, mode='clip')\n        return X\n\n    def _limit_features(self, X, vocabulary, high=None, low=None,\n                        limit=None):\n        \"\"\"Remove too rare or too common features.\n\n        Prune features that are non zero in more samples than high or less\n        documents than low, modifying the vocabulary, and restricting it to\n        at most the limit most frequent.\n\n        This does not prune samples with zero features.\n        \"\"\"\n        if high is None and low is None and limit is None:\n            return X, set()\n\n        # Calculate a mask based on document frequencies\n        dfs = _document_frequency(X)\n        mask = np.ones(len(dfs), dtype=bool)\n        if high is not None:\n            mask &= dfs <= high\n        if low is not None:\n            mask &= dfs >= low\n        if limit is not None and mask.sum() > limit:\n            tfs = np.asarray(X.sum(axis=0)).ravel()\n            mask_inds = (-tfs[mask]).argsort()[:limit]\n            new_mask = np.zeros(len(dfs), dtype=bool)\n            new_mask[np.where(mask)[0][mask_inds]] = True\n            mask = new_mask\n\n        new_indices = np.cumsum(mask) - 1  # maps old indices to new\n        removed_terms = set()\n        for term, old_index in list(vocabulary.items()):\n            if mask[old_index]:\n                vocabulary[term] = new_indices[old_index]\n            else:\n                del vocabulary[term]\n                removed_terms.add(term)\n        kept_indices = np.where(mask)[0]\n        if len(kept_indices) == 0:\n            raise ValueError(\"After pruning, no terms remain. Try a lower\"\n                             \" min_df or a higher max_df.\")\n        return X[:, kept_indices], removed_terms\n\n    def _count_vocab(self, raw_documents, fixed_vocab):\n        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n        \"\"\"\n        if fixed_vocab:\n            vocabulary = self.vocabulary_\n        else:\n            # Add a new value when a new vocabulary item is seen\n            vocabulary = defaultdict()\n            vocabulary.default_factory = vocabulary.__len__\n\n        analyze = self.build_analyzer()\n        j_indices = []\n        indptr = []\n\n        values = _make_int_array()\n        indptr.append(0)\n        for doc in raw_documents:\n            feature_counter = {}\n            for feature in analyze(doc):\n                try:\n                    feature_idx = vocabulary[feature]\n                    if feature_idx not in feature_counter:\n                        feature_counter[feature_idx] = 1\n                    else:\n                        feature_counter[feature_idx] += 1\n                except KeyError:\n                    # Ignore out-of-vocabulary items for fixed_vocab=True\n                    continue\n\n            j_indices.extend(feature_counter.keys())\n            values.extend(feature_counter.values())\n            indptr.append(len(j_indices))\n\n        if not fixed_vocab:\n            # disable defaultdict behaviour\n            vocabulary = dict(vocabulary)\n            if not vocabulary:\n                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n                                 \" contain stop words\")\n\n        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n            if _IS_32BIT:\n                raise ValueError(('sparse CSR array has {} non-zero '\n                                  'elements and requires 64 bit indexing, '\n                                  'which is unsupported with 32 bit Python.')\n                                 .format(indptr[-1]))\n            indices_dtype = np.int64\n\n        else:\n            indices_dtype = np.int32\n        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n        indptr = np.asarray(indptr, dtype=indices_dtype)\n        values = np.frombuffer(values, dtype=np.intc)\n\n        X = sp.csr_matrix((values, j_indices, indptr),\n                          shape=(len(indptr) - 1, len(vocabulary)),\n                          dtype=self.dtype)\n        X.sort_indices()\n        return vocabulary, X\n\n    def fit(self, raw_documents, y=None):\n        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._warn_for_unused_params()\n        self.fit_transform(raw_documents)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        \"\"\"Learn the vocabulary dictionary and return document-term matrix.\n\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : array of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        # We intentionally don't call the transform method to make\n        # fit_transform overridable without unwanted side effects in\n        # TfidfVectorizer.\n        if isinstance(raw_documents, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._validate_params()\n        self._validate_vocabulary()\n        max_df = self.max_df\n        min_df = self.min_df\n        max_features = self.max_features\n\n        vocabulary, X = self._count_vocab(raw_documents,\n                                          self.fixed_vocabulary_)\n\n        if self.binary:\n            X.data.fill(1)\n\n        if not self.fixed_vocabulary_:\n            n_doc = X.shape[0]\n            max_doc_count = (max_df\n                             if isinstance(max_df, numbers.Integral)\n                             else max_df * n_doc)\n            min_doc_count = (min_df\n                             if isinstance(min_df, numbers.Integral)\n                             else min_df * n_doc)\n            if max_doc_count < min_doc_count:\n                raise ValueError(\n                    \"max_df corresponds to < documents than min_df\")\n            if max_features is not None:\n                X = self._sort_features(X, vocabulary)\n            X, self.stop_words_ = self._limit_features(X, vocabulary,\n                                                       max_doc_count,\n                                                       min_doc_count,\n                                                       max_features)\n            if max_features is None:\n                X = self._sort_features(X, vocabulary)\n            self.vocabulary_ = vocabulary\n\n        return X\n\n    def transform(self, raw_documents):\n        \"\"\"Transform documents to document-term matrix.\n\n        Extract token counts out of raw text documents using the vocabulary\n        fitted with fit or the one provided to the constructor.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        if isinstance(raw_documents, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n        self._check_vocabulary()\n\n        # use the same matrix-building strategy as fit_transform\n        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n        if self.binary:\n            X.data.fill(1)\n        return X\n\n    def inverse_transform(self, X):\n        \"\"\"Return terms per document with nonzero entries in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document-term matrix.\n\n        Returns\n        -------\n        X_inv : list of arrays of shape (n_samples,)\n            List of arrays of terms.\n        \"\"\"\n        self._check_vocabulary()\n        # We need CSR format for fast row manipulations.\n        X = check_array(X, accept_sparse='csr')\n        n_samples = X.shape[0]\n\n        terms = np.array(list(self.vocabulary_.keys()))\n        indices = np.array(list(self.vocabulary_.values()))\n        inverse_vocabulary = terms[np.argsort(indices)]\n\n        if sp.issparse(X):\n            return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\n                    for i in range(n_samples)]\n        else:\n            return [inverse_vocabulary[np.flatnonzero(X[i, :])].ravel()\n                    for i in range(n_samples)]\n\n    def get_feature_names(self):\n        \"\"\"Array mapping from feature integer indices to feature name.\n\n        Returns\n        -------\n        feature_names : list\n            A list of feature names.\n        \"\"\"\n\n        self._check_vocabulary()\n\n        return [t for t, i in sorted(self.vocabulary_.items(),\n                                     key=itemgetter(1))]\n\n    def _more_tags(self):\n        return {'X_types': ['string']}",
      "instance_attributes": [
        {
          "name": "input",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "encoding",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "decode_error",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "strip_accents",
          "types": null
        },
        {
          "name": "preprocessor",
          "types": null
        },
        {
          "name": "tokenizer",
          "types": null
        },
        {
          "name": "analyzer",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "lowercase",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "token_pattern",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "stop_words",
          "types": null
        },
        {
          "name": "max_df",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "min_df",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "max_features",
          "types": null
        },
        {
          "name": "ngram_range",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "vocabulary",
          "types": null
        },
        {
          "name": "binary",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "dtype",
          "types": {
            "kind": "NamedType",
            "name": "type"
          }
        },
        {
          "name": "stop_words_",
          "types": {
            "kind": "NamedType",
            "name": "set"
          }
        },
        {
          "name": "vocabulary_",
          "types": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "dict"
              },
              {
                "kind": "NamedType",
                "name": "defaultdict"
              }
            ]
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer",
      "name": "HashingVectorizer",
      "qname": "sklearn.feature_extraction.text.HashingVectorizer",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "_VectorizerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__",
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/partial_fit",
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit",
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/transform",
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit_transform",
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/_get_hasher",
        "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [],
      "description": "Convert a collection of text documents to a matrix of token occurrences\n\nIt turns a collection of text documents into a scipy.sparse matrix holding\ntoken occurrence counts (or binary occurrence information), possibly\nnormalized as token frequencies if norm='l1' or projected on the euclidean\nunit sphere if norm='l2'.\n\nThis text vectorizer implementation uses the hashing trick to find the\ntoken string name to feature integer index mapping.\n\nThis strategy has several advantages:\n\n- it is very low memory scalable to large datasets as there is no need to\n  store a vocabulary dictionary in memory\n\n- it is fast to pickle and un-pickle as it holds no state besides the\n  constructor parameters\n\n- it can be used in a streaming (partial fit) or parallel pipeline as there\n  is no state computed during fit.\n\nThere are also a couple of cons (vs using a CountVectorizer with an\nin-memory vocabulary):\n\n- there is no way to compute the inverse transform (from feature indices to\n  string feature names) which can be a problem when trying to introspect\n  which features are most important to a model.\n\n- there can be collisions: distinct tokens can be mapped to the same\n  feature index. However in practice this is rarely an issue if n_features\n  is large enough (e.g. 2 ** 18 for text classification problems).\n\n- no IDF weighting as this would render the transformer stateful.\n\nThe hash function employed is the signed 32-bit version of Murmurhash3.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "Convert a collection of text documents to a matrix of token occurrences\n\nIt turns a collection of text documents into a scipy.sparse matrix holding\ntoken occurrence counts (or binary occurrence information), possibly\nnormalized as token frequencies if norm='l1' or projected on the euclidean\nunit sphere if norm='l2'.\n\nThis text vectorizer implementation uses the hashing trick to find the\ntoken string name to feature integer index mapping.\n\nThis strategy has several advantages:\n\n- it is very low memory scalable to large datasets as there is no need to\n  store a vocabulary dictionary in memory\n\n- it is fast to pickle and un-pickle as it holds no state besides the\n  constructor parameters\n\n- it can be used in a streaming (partial fit) or parallel pipeline as there\n  is no state computed during fit.\n\nThere are also a couple of cons (vs using a CountVectorizer with an\nin-memory vocabulary):\n\n- there is no way to compute the inverse transform (from feature indices to\n  string feature names) which can be a problem when trying to introspect\n  which features are most important to a model.\n\n- there can be collisions: distinct tokens can be mapped to the same\n  feature index. However in practice this is rarely an issue if n_features\n  is large enough (e.g. 2 ** 18 for text classification problems).\n\n- no IDF weighting as this would render the transformer stateful.\n\nThe hash function employed is the signed 32-bit version of Murmurhash3.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\n\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : string, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n        is first read from the file and then passed to the given callable\n        analyzer.\n\nn_features : int, default=(2 ** 20)\n    The number of features (columns) in the output matrices. Small numbers\n    of features are likely to cause hash collisions, but large numbers\n    will cause larger coefficient dimensions in linear learners.\n\nbinary : bool, default=False.\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\nnorm : {'l1', 'l2'}, default='l2'\n    Norm used to normalize term vectors. None for no normalization.\n\nalternate_sign : bool, default=True\n    When True, an alternating sign is added to the features as to\n    approximately conserve the inner product in the hashed space even for\n    small n_features. This approach is similar to sparse random projection.\n\n    .. versionadded:: 0.19\n\ndtype : type, default=np.float64\n    Type of the matrix returned by fit_transform() or transform().\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = HashingVectorizer(n_features=2**4)\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(X.shape)\n(4, 16)\n\nSee Also\n--------\nCountVectorizer, TfidfVectorizer",
      "code": "class HashingVectorizer(TransformerMixin, _VectorizerMixin, BaseEstimator):\n    r\"\"\"Convert a collection of text documents to a matrix of token occurrences\n\n    It turns a collection of text documents into a scipy.sparse matrix holding\n    token occurrence counts (or binary occurrence information), possibly\n    normalized as token frequencies if norm='l1' or projected on the euclidean\n    unit sphere if norm='l2'.\n\n    This text vectorizer implementation uses the hashing trick to find the\n    token string name to feature integer index mapping.\n\n    This strategy has several advantages:\n\n    - it is very low memory scalable to large datasets as there is no need to\n      store a vocabulary dictionary in memory\n\n    - it is fast to pickle and un-pickle as it holds no state besides the\n      constructor parameters\n\n    - it can be used in a streaming (partial fit) or parallel pipeline as there\n      is no state computed during fit.\n\n    There are also a couple of cons (vs using a CountVectorizer with an\n    in-memory vocabulary):\n\n    - there is no way to compute the inverse transform (from feature indices to\n      string feature names) which can be a problem when trying to introspect\n      which features are most important to a model.\n\n    - there can be collisions: distinct tokens can be mapped to the same\n      feature index. However in practice this is rarely an issue if n_features\n      is large enough (e.g. 2 ** 18 for text classification problems).\n\n    - no IDF weighting as this would render the transformer stateful.\n\n    The hash function employed is the signed 32-bit version of Murmurhash3.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n\n    input : {'filename', 'file', 'content'}, default='content'\n        - If `'filename'`, the sequence passed as an argument to fit is\n          expected to be a list of filenames that need reading to fetch\n          the raw content to analyze.\n\n        - If `'file'`, the sequence items must have a 'read' method (file-like\n          object) that is called to fetch the bytes in memory.\n\n        - If `'content'`, the input is expected to be a sequence of items that\n          can be of type string or byte.\n\n    encoding : string, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode'}, default=None\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable, default=None\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer is not callable``.\n\n    tokenizer : callable, default=None\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    stop_words : {'english'}, list, default=None\n        If 'english', a built-in stop word list for English is used.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n        If there is a capturing group in token_pattern then the\n        captured group content, not the entire match, becomes the token.\n        At most one capturing group is permitted.\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer is not callable``.\n\n    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n            is first read from the file and then passed to the given callable\n            analyzer.\n\n    n_features : int, default=(2 ** 20)\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n\n    binary : bool, default=False.\n        If True, all non zero counts are set to 1. This is useful for discrete\n        probabilistic models that model binary events rather than integer\n        counts.\n\n    norm : {'l1', 'l2'}, default='l2'\n        Norm used to normalize term vectors. None for no normalization.\n\n    alternate_sign : bool, default=True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionadded:: 0.19\n\n    dtype : type, default=np.float64\n        Type of the matrix returned by fit_transform() or transform().\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import HashingVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = HashingVectorizer(n_features=2**4)\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(X.shape)\n    (4, 16)\n\n    See Also\n    --------\n    CountVectorizer, TfidfVectorizer\n\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None,\n                 lowercase=True, preprocessor=None, tokenizer=None,\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), analyzer='word', n_features=(2 ** 20),\n                 binary=False, norm='l2', alternate_sign=True,\n                 dtype=np.float64):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.n_features = n_features\n        self.ngram_range = ngram_range\n        self.binary = binary\n        self.norm = norm\n        self.alternate_sign = alternate_sign\n        self.dtype = dtype\n\n    def partial_fit(self, X, y=None):\n        \"\"\"Does nothing: this transformer is stateless.\n\n        This method is just there to mark the fact that this transformer\n        can work in a streaming setup.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n        \"\"\"\n        return self\n\n    def fit(self, X, y=None):\n        \"\"\"Does nothing: this transformer is stateless.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n        \"\"\"\n        # triggers a parameter validation\n        if isinstance(X, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._warn_for_unused_params()\n        self._validate_params()\n\n        self._get_hasher().fit(X, y=y)\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        if isinstance(X, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._validate_params()\n\n        analyzer = self.build_analyzer()\n        X = self._get_hasher().transform(analyzer(doc) for doc in X)\n        if self.binary:\n            X.data.fill(1)\n        if self.norm is not None:\n            X = normalize(X, norm=self.norm, copy=False)\n        return X\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n        y : any\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        return self.fit(X, y).transform(X)\n\n    def _get_hasher(self):\n        return FeatureHasher(n_features=self.n_features,\n                             input_type='string', dtype=self.dtype,\n                             alternate_sign=self.alternate_sign)\n\n    def _more_tags(self):\n        return {'X_types': ['string']}",
      "instance_attributes": [
        {
          "name": "input",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "encoding",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "decode_error",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "strip_accents",
          "types": null
        },
        {
          "name": "preprocessor",
          "types": null
        },
        {
          "name": "tokenizer",
          "types": null
        },
        {
          "name": "analyzer",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "lowercase",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "token_pattern",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "stop_words",
          "types": null
        },
        {
          "name": "n_features",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "ngram_range",
          "types": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "name": "binary",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "norm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "alternate_sign",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "dtype",
          "types": {
            "kind": "NamedType",
            "name": "type"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer",
      "name": "TfidfTransformer",
      "qname": "sklearn.feature_extraction.text.TfidfTransformer",
      "decorators": [],
      "superclasses": [
        "TransformerMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__",
        "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/fit",
        "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/transform",
        "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@getter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@setter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform a count matrix to a normalized tf or tf-idf representation\n\nTf means term-frequency while tf-idf means term-frequency times inverse\ndocument-frequency. This is a common term weighting scheme in information\nretrieval, that has also found good use in document classification.\n\nThe goal of using tf-idf instead of the raw frequencies of occurrence of a\ntoken in a given document is to scale down the impact of tokens that occur\nvery frequently in a given corpus and that are hence empirically less\ninformative than features that occur in a small fraction of the training\ncorpus.\n\nThe formula that is used to compute the tf-idf for a term t of a document d\nin a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\ncomputed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\nn is the total number of documents in the document set and df(t) is the\ndocument frequency of t; the document frequency is the number of documents\nin the document set that contain the term t. The effect of adding \"1\" to\nthe idf in the equation above is that terms with zero idf, i.e., terms\nthat occur in all documents in a training set, will not be entirely\nignored.\n(Note that the idf formula above differs from the standard textbook\nnotation that defines the idf as\nidf(t) = log [ n / (df(t) + 1) ]).\n\nIf ``smooth_idf=True`` (the default), the constant \"1\" is added to the\nnumerator and denominator of the idf as if an extra document was seen\ncontaining every term in the collection exactly once, which prevents\nzero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n\nFurthermore, the formulas used to compute tf and idf depend\non parameter settings that correspond to the SMART notation used in IR\nas follows:\n\nTf is \"n\" (natural) by default, \"l\" (logarithmic) when\n``sublinear_tf=True``.\nIdf is \"t\" when use_idf is given, \"n\" (none) otherwise.\nNormalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\nwhen ``norm=None``.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "Transform a count matrix to a normalized tf or tf-idf representation\n\nTf means term-frequency while tf-idf means term-frequency times inverse\ndocument-frequency. This is a common term weighting scheme in information\nretrieval, that has also found good use in document classification.\n\nThe goal of using tf-idf instead of the raw frequencies of occurrence of a\ntoken in a given document is to scale down the impact of tokens that occur\nvery frequently in a given corpus and that are hence empirically less\ninformative than features that occur in a small fraction of the training\ncorpus.\n\nThe formula that is used to compute the tf-idf for a term t of a document d\nin a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\ncomputed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\nn is the total number of documents in the document set and df(t) is the\ndocument frequency of t; the document frequency is the number of documents\nin the document set that contain the term t. The effect of adding \"1\" to\nthe idf in the equation above is that terms with zero idf, i.e., terms\nthat occur in all documents in a training set, will not be entirely\nignored.\n(Note that the idf formula above differs from the standard textbook\nnotation that defines the idf as\nidf(t) = log [ n / (df(t) + 1) ]).\n\nIf ``smooth_idf=True`` (the default), the constant \"1\" is added to the\nnumerator and denominator of the idf as if an extra document was seen\ncontaining every term in the collection exactly once, which prevents\nzero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n\nFurthermore, the formulas used to compute tf and idf depend\non parameter settings that correspond to the SMART notation used in IR\nas follows:\n\nTf is \"n\" (natural) by default, \"l\" (logarithmic) when\n``sublinear_tf=True``.\nIdf is \"t\" when use_idf is given, \"n\" (none) otherwise.\nNormalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\nwhen ``norm=None``.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\nnorm : {'l1', 'l2'}, default='l2'\n    Each output row will have unit norm, either:\n    * 'l2': Sum of squares of vector elements is 1. The cosine\n    similarity between two vectors is their dot product when l2 norm has\n    been applied.\n    * 'l1': Sum of absolute values of vector elements is 1.\n    See :func:`preprocessing.normalize`\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nidf_ : array of shape (n_features)\n    The inverse document frequency (IDF) vector; only defined\n    if  ``use_idf`` is True.\n\n    .. versionadded:: 0.20\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfTransformer\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> from sklearn.pipeline import Pipeline\n>>> import numpy as np\n>>> corpus = ['this is the first document',\n...           'this document is the second document',\n...           'and this is the third one',\n...           'is this the first document']\n>>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n...               'and', 'one']\n>>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n...                  ('tfid', TfidfTransformer())]).fit(corpus)\n>>> pipe['count'].transform(corpus).toarray()\narray([[1, 1, 1, 1, 0, 1, 0, 0],\n       [1, 2, 0, 1, 1, 1, 0, 0],\n       [1, 0, 0, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 0, 1, 0, 0]])\n>>> pipe['tfid'].idf_\narray([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n       1.        , 1.91629073, 1.91629073])\n>>> pipe.transform(corpus).shape\n(4, 8)\n\nReferences\n----------\n\n.. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n               Information Retrieval. Addison Wesley, pp. 68-74.\n\n.. [MRS2008] C.D. Manning, P. Raghavan and H. Sch\u00fctze  (2008).\n               Introduction to Information Retrieval. Cambridge University\n               Press, pp. 118-120.",
      "code": "class TfidfTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"Transform a count matrix to a normalized tf or tf-idf representation\n\n    Tf means term-frequency while tf-idf means term-frequency times inverse\n    document-frequency. This is a common term weighting scheme in information\n    retrieval, that has also found good use in document classification.\n\n    The goal of using tf-idf instead of the raw frequencies of occurrence of a\n    token in a given document is to scale down the impact of tokens that occur\n    very frequently in a given corpus and that are hence empirically less\n    informative than features that occur in a small fraction of the training\n    corpus.\n\n    The formula that is used to compute the tf-idf for a term t of a document d\n    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n    n is the total number of documents in the document set and df(t) is the\n    document frequency of t; the document frequency is the number of documents\n    in the document set that contain the term t. The effect of adding \"1\" to\n    the idf in the equation above is that terms with zero idf, i.e., terms\n    that occur in all documents in a training set, will not be entirely\n    ignored.\n    (Note that the idf formula above differs from the standard textbook\n    notation that defines the idf as\n    idf(t) = log [ n / (df(t) + 1) ]).\n\n    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n    numerator and denominator of the idf as if an extra document was seen\n    containing every term in the collection exactly once, which prevents\n    zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n\n    Furthermore, the formulas used to compute tf and idf depend\n    on parameter settings that correspond to the SMART notation used in IR\n    as follows:\n\n    Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n    ``sublinear_tf=True``.\n    Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n    Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n    when ``norm=None``.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    norm : {'l1', 'l2'}, default='l2'\n        Each output row will have unit norm, either:\n        * 'l2': Sum of squares of vector elements is 1. The cosine\n        similarity between two vectors is their dot product when l2 norm has\n        been applied.\n        * 'l1': Sum of absolute values of vector elements is 1.\n        See :func:`preprocessing.normalize`\n\n    use_idf : bool, default=True\n        Enable inverse-document-frequency reweighting.\n\n    smooth_idf : bool, default=True\n        Smooth idf weights by adding one to document frequencies, as if an\n        extra document was seen containing every term in the collection\n        exactly once. Prevents zero divisions.\n\n    sublinear_tf : bool, default=False\n        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n    Attributes\n    ----------\n    idf_ : array of shape (n_features)\n        The inverse document frequency (IDF) vector; only defined\n        if  ``use_idf`` is True.\n\n        .. versionadded:: 0.20\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import TfidfTransformer\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> from sklearn.pipeline import Pipeline\n    >>> import numpy as np\n    >>> corpus = ['this is the first document',\n    ...           'this document is the second document',\n    ...           'and this is the third one',\n    ...           'is this the first document']\n    >>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n    ...               'and', 'one']\n    >>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n    ...                  ('tfid', TfidfTransformer())]).fit(corpus)\n    >>> pipe['count'].transform(corpus).toarray()\n    array([[1, 1, 1, 1, 0, 1, 0, 0],\n           [1, 2, 0, 1, 1, 1, 0, 0],\n           [1, 0, 0, 1, 0, 1, 1, 1],\n           [1, 1, 1, 1, 0, 1, 0, 0]])\n    >>> pipe['tfid'].idf_\n    array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n           1.        , 1.91629073, 1.91629073])\n    >>> pipe.transform(corpus).shape\n    (4, 8)\n\n    References\n    ----------\n\n    .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n                   Information Retrieval. Addison Wesley, pp. 68-74.\n\n    .. [MRS2008] C.D. Manning, P. Raghavan and H. Sch\u00fctze  (2008).\n                   Introduction to Information Retrieval. Cambridge University\n                   Press, pp. 118-120.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, norm='l2', use_idf=True, smooth_idf=True,\n                 sublinear_tf=False):\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n\n    def fit(self, X, y=None):\n        \"\"\"Learn the idf vector (global term weights).\n\n        Parameters\n        ----------\n        X : sparse matrix of shape n_samples, n_features)\n            A matrix of term/token counts.\n        \"\"\"\n        X = check_array(X, accept_sparse=('csr', 'csc'))\n        if not sp.issparse(X):\n            X = sp.csr_matrix(X)\n        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n\n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            df = df.astype(dtype, **_astype_copy_false(df))\n\n            # perform idf smoothing if required\n            df += int(self.smooth_idf)\n            n_samples += int(self.smooth_idf)\n\n            # log+1 instead of log makes sure terms with zero idf don't get\n            # suppressed entirely.\n            idf = np.log(n_samples / df) + 1\n            self._idf_diag = sp.diags(idf, offsets=0,\n                                      shape=(n_features, n_features),\n                                      format='csr',\n                                      dtype=dtype)\n\n        return self\n\n    def transform(self, X, copy=True):\n        \"\"\"Transform a count matrix to a tf or tf-idf representation\n\n        Parameters\n        ----------\n        X : sparse matrix of (n_samples, n_features)\n            a matrix of term/token counts\n\n        copy : bool, default=True\n            Whether to copy X and operate on the copy or perform in-place\n            operations.\n\n        Returns\n        -------\n        vectors : sparse matrix of shape (n_samples, n_features)\n        \"\"\"\n        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n        if not sp.issparse(X):\n            X = sp.csr_matrix(X, dtype=np.float64)\n\n        n_samples, n_features = X.shape\n\n        if self.sublinear_tf:\n            np.log(X.data, X.data)\n            X.data += 1\n\n        if self.use_idf:\n            # idf_ being a property, the automatic attributes detection\n            # does not work as usual and we need to specify the attribute\n            # name:\n            check_is_fitted(self, attributes=[\"idf_\"],\n                            msg='idf vector is not fitted')\n\n            expected_n_features = self._idf_diag.shape[0]\n            if n_features != expected_n_features:\n                raise ValueError(\"Input has n_features=%d while the model\"\n                                 \" has been trained with n_features=%d\" % (\n                                     n_features, expected_n_features))\n            # *= doesn't work\n            X = X * self._idf_diag\n\n        if self.norm:\n            X = normalize(X, norm=self.norm, copy=False)\n\n        return X\n\n    @property\n    def idf_(self):\n        # if _idf_diag is not set, this will raise an attribute error,\n        # which means hasattr(self, \"idf_\") is False\n        return np.ravel(self._idf_diag.sum(axis=0))\n\n    @idf_.setter\n    def idf_(self, value):\n        value = np.asarray(value, dtype=np.float64)\n        n_features = value.shape[0]\n        self._idf_diag = sp.spdiags(value, diags=0, m=n_features,\n                                    n=n_features, format='csr')\n\n    def _more_tags(self):\n        return {'X_types': 'sparse'}",
      "instance_attributes": [
        {
          "name": "norm",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "use_idf",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "smooth_idf",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "sublinear_tf",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "_idf_diag",
          "types": {
            "kind": "NamedType",
            "name": "dia_matrix"
          }
        },
        {
          "name": "idf_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer",
      "name": "TfidfVectorizer",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer",
      "decorators": [],
      "superclasses": [
        "CountVectorizer"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@getter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@setter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@getter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@setter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@getter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@setter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@getter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@setter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@getter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@setter",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/_check_params",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit_transform",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/transform",
        "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [],
      "description": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : {'filename', 'file', 'content'}, default='content'\n    - If `'filename'`, the sequence passed as an argument to fit is\n      expected to be a list of filenames that need reading to fetch\n      the raw content to analyze.\n\n    - If `'file'`, the sequence items must have a 'read' method (file-like\n      object) that is called to fetch the bytes in memory.\n\n    - If `'content'`, the input is expected to be a sequence of items that\n      can be of type string or byte.\n\nencoding : str, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nanalyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n    Whether the feature should be made of word or character n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n        is first read from the file and then passed to the given callable\n        analyzer.\n\nstop_words : {'english'}, list, default=None\n    If a string, it is passed to _check_stop_list and the appropriate stop\n    list is returned. 'english' is currently the only supported string\n    value.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. max_df can be set to a value\n    in the range [0.7, 1.0) to automatically detect and filter stop\n    words based on intra corpus document frequency of terms.\n\ntoken_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\n    If there is a capturing group in token_pattern then the\n    captured group content, not the entire match, becomes the token.\n    At most one capturing group is permitted.\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n    only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nmax_df : float or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float in range [0.0, 1.0], the parameter represents a proportion of\n    documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float in range of [0.0, 1.0], the parameter represents a proportion\n    of documents, integer absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    max_features ordered by term frequency across the corpus.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents.\n\nbinary : bool, default=False\n    If True, all non-zero term counts are set to 1. This does not mean\n    outputs will have only 0/1 values, only that the tf term in tf-idf\n    is binary. (Set idf and normalization to False to get 0/1 outputs).\n\ndtype : dtype, default=float64\n    Type of the matrix returned by fit_transform() or transform().\n\nnorm : {'l1', 'l2'}, default='l2'\n    Each output row will have unit norm, either:\n    * 'l2': Sum of squares of vector elements is 1. The cosine\n    similarity between two vectors is their dot product when l2 norm has\n    been applied.\n    * 'l1': Sum of absolute values of vector elements is 1.\n    See :func:`preprocessing.normalize`.\n\nuse_idf : bool, default=True\n    Enable inverse-document-frequency reweighting.\n\nsmooth_idf : bool, default=True\n    Smooth idf weights by adding one to document frequencies, as if an\n    extra document was seen containing every term in the collection\n    exactly once. Prevents zero divisions.\n\nsublinear_tf : bool, default=False\n    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_: bool\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user\n\nidf_ : array of shape (n_features,)\n    The inverse document frequency (IDF) vector; only defined\n    if ``use_idf`` is True.\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nSee Also\n--------\nCountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\nTfidfTransformer : Performs the TF-IDF transformation from a provided\n    matrix of counts.\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = TfidfVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(vectorizer.get_feature_names())\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n>>> print(X.shape)\n(4, 9)",
      "code": "class TfidfVectorizer(CountVectorizer):\n    r\"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n\n    Equivalent to :class:`CountVectorizer` followed by\n    :class:`TfidfTransformer`.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : {'filename', 'file', 'content'}, default='content'\n        - If `'filename'`, the sequence passed as an argument to fit is\n          expected to be a list of filenames that need reading to fetch\n          the raw content to analyze.\n\n        - If `'file'`, the sequence items must have a 'read' method (file-like\n          object) that is called to fetch the bytes in memory.\n\n        - If `'content'`, the input is expected to be a sequence of items that\n          can be of type string or byte.\n\n    encoding : str, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode'}, default=None\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable, default=None\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer is not callable``.\n\n    tokenizer : callable, default=None\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n            is first read from the file and then passed to the given callable\n            analyzer.\n\n    stop_words : {'english'}, list, default=None\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. 'english' is currently the only supported string\n        value.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n        If None, no stop words will be used. max_df can be set to a value\n        in the range [0.7, 1.0) to automatically detect and filter stop\n        words based on intra corpus document frequency of terms.\n\n    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n        If there is a capturing group in token_pattern then the\n        captured group content, not the entire match, becomes the token.\n        At most one capturing group is permitted.\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer is not callable``.\n\n    max_df : float or int, default=1.0\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float in range [0.0, 1.0], the parameter represents a proportion of\n        documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float or int, default=1\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float in range of [0.0, 1.0], the parameter represents a proportion\n        of documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int, default=None\n        If not None, build a vocabulary that only consider the top\n        max_features ordered by term frequency across the corpus.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, default=None\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents.\n\n    binary : bool, default=False\n        If True, all non-zero term counts are set to 1. This does not mean\n        outputs will have only 0/1 values, only that the tf term in tf-idf\n        is binary. (Set idf and normalization to False to get 0/1 outputs).\n\n    dtype : dtype, default=float64\n        Type of the matrix returned by fit_transform() or transform().\n\n    norm : {'l1', 'l2'}, default='l2'\n        Each output row will have unit norm, either:\n        * 'l2': Sum of squares of vector elements is 1. The cosine\n        similarity between two vectors is their dot product when l2 norm has\n        been applied.\n        * 'l1': Sum of absolute values of vector elements is 1.\n        See :func:`preprocessing.normalize`.\n\n    use_idf : bool, default=True\n        Enable inverse-document-frequency reweighting.\n\n    smooth_idf : bool, default=True\n        Smooth idf weights by adding one to document frequencies, as if an\n        extra document was seen containing every term in the collection\n        exactly once. Prevents zero divisions.\n\n    sublinear_tf : bool, default=False\n        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    fixed_vocabulary_: bool\n        True if a fixed vocabulary of term to indices mapping\n        is provided by the user\n\n    idf_ : array of shape (n_features,)\n        The inverse document frequency (IDF) vector; only defined\n        if ``use_idf`` is True.\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n\n    See Also\n    --------\n    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\n    TfidfTransformer : Performs the TF-IDF transformation from a provided\n        matrix of counts.\n\n    Notes\n    -----\n    The ``stop_words_`` attribute can get large and increase the model size\n    when pickling. This attribute is provided only for introspection and can\n    be safely removed using delattr or set to None before pickling.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = TfidfVectorizer()\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(vectorizer.get_feature_names())\n    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n    >>> print(X.shape)\n    (4, 9)\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, *, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None, lowercase=True,\n                 preprocessor=None, tokenizer=None, analyzer='word',\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n                 max_features=None, vocabulary=None, binary=False,\n                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n                 sublinear_tf=False):\n\n        super().__init__(\n            input=input, encoding=encoding, decode_error=decode_error,\n            strip_accents=strip_accents, lowercase=lowercase,\n            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n            stop_words=stop_words, token_pattern=token_pattern,\n            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n            max_features=max_features, vocabulary=vocabulary, binary=binary,\n            dtype=dtype)\n\n        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,\n                                       smooth_idf=smooth_idf,\n                                       sublinear_tf=sublinear_tf)\n\n    # Broadcast the TF-IDF parameters to the underlying transformer instance\n    # for easy grid search and repr\n\n    @property\n    def norm(self):\n        return self._tfidf.norm\n\n    @norm.setter\n    def norm(self, value):\n        self._tfidf.norm = value\n\n    @property\n    def use_idf(self):\n        return self._tfidf.use_idf\n\n    @use_idf.setter\n    def use_idf(self, value):\n        self._tfidf.use_idf = value\n\n    @property\n    def smooth_idf(self):\n        return self._tfidf.smooth_idf\n\n    @smooth_idf.setter\n    def smooth_idf(self, value):\n        self._tfidf.smooth_idf = value\n\n    @property\n    def sublinear_tf(self):\n        return self._tfidf.sublinear_tf\n\n    @sublinear_tf.setter\n    def sublinear_tf(self, value):\n        self._tfidf.sublinear_tf = value\n\n    @property\n    def idf_(self):\n        return self._tfidf.idf_\n\n    @idf_.setter\n    def idf_(self, value):\n        self._validate_vocabulary()\n        if hasattr(self, 'vocabulary_'):\n            if len(self.vocabulary_) != len(value):\n                raise ValueError(\"idf length = %d must be equal \"\n                                 \"to vocabulary size = %d\" %\n                                 (len(value), len(self.vocabulary)))\n        self._tfidf.idf_ = value\n\n    def _check_params(self):\n        if self.dtype not in FLOAT_DTYPES:\n            warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"\n                          \"be converted to np.float64.\"\n                          .format(FLOAT_DTYPES, self.dtype),\n                          UserWarning)\n\n    def fit(self, raw_documents, y=None):\n        \"\"\"Learn vocabulary and idf from training set.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n        y : None\n            This parameter is not needed to compute tfidf.\n\n        Returns\n        -------\n        self : object\n            Fitted vectorizer.\n        \"\"\"\n        self._check_params()\n        self._warn_for_unused_params()\n        X = super().fit_transform(raw_documents)\n        self._tfidf.fit(X)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        \"\"\"Learn vocabulary and idf, return document-term matrix.\n\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n        y : None\n            This parameter is ignored.\n\n        Returns\n        -------\n        X : sparse matrix of (n_samples, n_features)\n            Tf-idf-weighted document-term matrix.\n        \"\"\"\n        self._check_params()\n        X = super().fit_transform(raw_documents)\n        self._tfidf.fit(X)\n        # X is already a transformed view of raw_documents so\n        # we set copy to False\n        return self._tfidf.transform(X, copy=False)\n\n    def transform(self, raw_documents):\n        \"\"\"Transform documents to document-term matrix.\n\n        Uses the vocabulary and document frequencies (df) learned by fit (or\n        fit_transform).\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix of (n_samples, n_features)\n            Tf-idf-weighted document-term matrix.\n        \"\"\"\n        check_is_fitted(self, msg='The TF-IDF vectorizer is not fitted')\n\n        X = super().transform(raw_documents)\n        return self._tfidf.transform(X, copy=False)\n\n    def _more_tags(self):\n        return {'X_types': ['string'], '_skip_test': True}",
      "instance_attributes": [
        {
          "name": "_tfidf",
          "types": {
            "kind": "NamedType",
            "name": "TfidfTransformer"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin",
      "name": "SelectorMixin",
      "qname": "sklearn.feature_selection._base.SelectorMixin",
      "decorators": [],
      "superclasses": [
        "TransformerMixin"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._base/SelectorMixin/get_support",
        "scikit-learn/sklearn.feature_selection._base/SelectorMixin/_get_support_mask",
        "scikit-learn/sklearn.feature_selection._base/SelectorMixin/transform",
        "scikit-learn/sklearn.feature_selection._base/SelectorMixin/inverse_transform"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Transformer mixin that performs feature selection given a support mask\n\nThis mixin provides a feature selector implementation with `transform` and\n`inverse_transform` functionality given an implementation of\n`_get_support_mask`.",
      "docstring": "Transformer mixin that performs feature selection given a support mask\n\nThis mixin provides a feature selector implementation with `transform` and\n`inverse_transform` functionality given an implementation of\n`_get_support_mask`.",
      "code": "class SelectorMixin(TransformerMixin, metaclass=ABCMeta):\n    \"\"\"\n    Transformer mixin that performs feature selection given a support mask\n\n    This mixin provides a feature selector implementation with `transform` and\n    `inverse_transform` functionality given an implementation of\n    `_get_support_mask`.\n    \"\"\"\n\n    def get_support(self, indices=False):\n        \"\"\"\n        Get a mask, or integer index, of the features selected\n\n        Parameters\n        ----------\n        indices : bool, default=False\n            If True, the return value will be an array of integers, rather\n            than a boolean mask.\n\n        Returns\n        -------\n        support : array\n            An index that selects the retained features from a feature vector.\n            If `indices` is False, this is a boolean array of shape\n            [# input features], in which an element is True iff its\n            corresponding feature is selected for retention. If `indices` is\n            True, this is an integer array of shape [# output features] whose\n            values are indices into the input feature vector.\n        \"\"\"\n        mask = self._get_support_mask()\n        return mask if not indices else np.where(mask)[0]\n\n    @abstractmethod\n    def _get_support_mask(self):\n        \"\"\"\n        Get the boolean mask indicating which features are selected\n\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for\n            retention.\n        \"\"\"\n\n    def transform(self, X):\n        \"\"\"Reduce X to the selected features.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        X_r : array of shape [n_samples, n_selected_features]\n            The input samples with only the selected features.\n        \"\"\"\n        # note: we use _safe_tags instead of _get_tags because this is a\n        # public Mixin.\n        X = check_array(\n            X,\n            dtype=None,\n            accept_sparse=\"csr\",\n            force_all_finite=not _safe_tags(self, key=\"allow_nan\"),\n        )\n        mask = self.get_support()\n        if not mask.any():\n            warn(\"No features were selected: either the data is\"\n                 \" too noisy or the selection test too strict.\",\n                 UserWarning)\n            return np.empty(0).reshape((X.shape[0], 0))\n        if len(mask) != X.shape[1]:\n            raise ValueError(\"X has a different shape than during fitting.\")\n        return X[:, safe_mask(X, mask)]\n\n    def inverse_transform(self, X):\n        \"\"\"\n        Reverse the transformation operation\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_selected_features]\n            The input samples.\n\n        Returns\n        -------\n        X_r : array of shape [n_samples, n_original_features]\n            `X` with columns of zeros inserted where features would have\n            been removed by :meth:`transform`.\n        \"\"\"\n        if issparse(X):\n            X = X.tocsc()\n            # insert additional entries in indptr:\n            # e.g. if transform changed indptr from [0 2 6 7] to [0 2 3]\n            # col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3]\n            it = self.inverse_transform(np.diff(X.indptr).reshape(1, -1))\n            col_nonzeros = it.ravel()\n            indptr = np.concatenate([[0], np.cumsum(col_nonzeros)])\n            Xt = csc_matrix((X.data, X.indices, indptr),\n                            shape=(X.shape[0], len(indptr) - 1), dtype=X.dtype)\n            return Xt\n\n        support = self.get_support()\n        X = check_array(X, dtype=None)\n        if support.sum() != X.shape[1]:\n            raise ValueError(\"X has a different shape than during fitting.\")\n\n        if X.ndim == 1:\n            X = X[None, :]\n        Xt = np.zeros((X.shape[0], support.size), dtype=X.dtype)\n        Xt[:, support] = X\n        return Xt",
      "instance_attributes": []
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel",
      "name": "SelectFromModel",
      "qname": "sklearn.feature_selection._from_model.SelectFromModel",
      "decorators": [],
      "superclasses": [
        "MetaEstimatorMixin",
        "SelectorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__",
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/_get_support_mask",
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/fit",
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/threshold_@getter",
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/partial_fit",
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/n_features_in_@getter",
        "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.",
      "docstring": "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.\n\nParameters\n----------\nestimator : object\n    The base estimator from which the transformer is built.\n    This can be both a fitted (if ``prefit`` is set to True)\n    or a non-fitted estimator. The estimator should have a\n    ``feature_importances_`` or ``coef_`` attribute after fitting.\n    Otherwise, the ``importance_getter`` parameter should be used.\n\nthreshold : string or float, default=None\n    The threshold value to use for feature selection. Features whose\n    importance is greater or equal are kept while the others are\n    discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n    the median (resp. the mean) of the feature importances. A scaling\n    factor (e.g., \"1.25*mean\") may also be used. If None and if the\n    estimator has a parameter penalty set to l1, either explicitly\n    or implicitly (e.g, Lasso), the threshold used is 1e-5.\n    Otherwise, \"mean\" is used by default.\n\nprefit : bool, default=False\n    Whether a prefit model is expected to be passed into the constructor\n    directly or not. If True, ``transform`` must be called directly\n    and SelectFromModel cannot be used with ``cross_val_score``,\n    ``GridSearchCV`` and similar utilities that clone the estimator.\n    Otherwise train the model using ``fit`` and then ``transform`` to do\n    feature selection.\n\nnorm_order : non-zero int, inf, -inf, default=1\n    Order of the norm used to filter the vectors of coefficients below\n    ``threshold`` in the case where the ``coef_`` attribute of the\n    estimator is of dimension 2.\n\nmax_features : int, default=None\n    The maximum number of features to select.\n    To only select based on ``max_features``, set ``threshold=-np.inf``.\n\n    .. versionadded:: 0.20\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a ``coef_``\n    attribute or ``feature_importances_`` attribute of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : an estimator\n    The base estimator from which the transformer is built.\n    This is stored only when a non-fitted estimator is passed to the\n    ``SelectFromModel``, i.e when prefit is False.\n\nthreshold_ : float\n    The threshold value used for feature selection.\n\nNotes\n-----\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nExamples\n--------\n>>> from sklearn.feature_selection import SelectFromModel\n>>> from sklearn.linear_model import LogisticRegression\n>>> X = [[ 0.87, -1.34,  0.31 ],\n...      [-2.79, -0.02, -0.85 ],\n...      [-1.34, -0.48, -2.55 ],\n...      [ 1.92,  1.48,  0.65 ]]\n>>> y = [0, 1, 0, 1]\n>>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n>>> selector.estimator_.coef_\narray([[-0.3252302 ,  0.83462377,  0.49750423]])\n>>> selector.threshold_\n0.55245...\n>>> selector.get_support()\narray([False,  True, False])\n>>> selector.transform(X)\narray([[-1.34],\n       [-0.02],\n       [-0.48],\n       [ 1.48]])\n\nSee Also\n--------\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.",
      "code": "class SelectFromModel(MetaEstimatorMixin, SelectorMixin, BaseEstimator):\n    \"\"\"Meta-transformer for selecting features based on importance weights.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <select_from_model>`.\n\n    Parameters\n    ----------\n    estimator : object\n        The base estimator from which the transformer is built.\n        This can be both a fitted (if ``prefit`` is set to True)\n        or a non-fitted estimator. The estimator should have a\n        ``feature_importances_`` or ``coef_`` attribute after fitting.\n        Otherwise, the ``importance_getter`` parameter should be used.\n\n    threshold : string or float, default=None\n        The threshold value to use for feature selection. Features whose\n        importance is greater or equal are kept while the others are\n        discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n        the median (resp. the mean) of the feature importances. A scaling\n        factor (e.g., \"1.25*mean\") may also be used. If None and if the\n        estimator has a parameter penalty set to l1, either explicitly\n        or implicitly (e.g, Lasso), the threshold used is 1e-5.\n        Otherwise, \"mean\" is used by default.\n\n    prefit : bool, default=False\n        Whether a prefit model is expected to be passed into the constructor\n        directly or not. If True, ``transform`` must be called directly\n        and SelectFromModel cannot be used with ``cross_val_score``,\n        ``GridSearchCV`` and similar utilities that clone the estimator.\n        Otherwise train the model using ``fit`` and then ``transform`` to do\n        feature selection.\n\n    norm_order : non-zero int, inf, -inf, default=1\n        Order of the norm used to filter the vectors of coefficients below\n        ``threshold`` in the case where the ``coef_`` attribute of the\n        estimator is of dimension 2.\n\n    max_features : int, default=None\n        The maximum number of features to select.\n        To only select based on ``max_features``, set ``threshold=-np.inf``.\n\n        .. versionadded:: 0.20\n\n    importance_getter : str or callable, default='auto'\n        If 'auto', uses the feature importance either through a ``coef_``\n        attribute or ``feature_importances_`` attribute of estimator.\n\n        Also accepts a string that specifies an attribute name/path\n        for extracting feature importance (implemented with `attrgetter`).\n        For example, give `regressor_.coef_` in case of\n        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n        `named_steps.clf.feature_importances_` in case of\n        :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n        If `callable`, overrides the default feature importance getter.\n        The callable is passed with the fitted estimator and it should\n        return importance for each feature.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    estimator_ : an estimator\n        The base estimator from which the transformer is built.\n        This is stored only when a non-fitted estimator is passed to the\n        ``SelectFromModel``, i.e when prefit is False.\n\n    threshold_ : float\n        The threshold value used for feature selection.\n\n    Notes\n    -----\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    Examples\n    --------\n    >>> from sklearn.feature_selection import SelectFromModel\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[ 0.87, -1.34,  0.31 ],\n    ...      [-2.79, -0.02, -0.85 ],\n    ...      [-1.34, -0.48, -2.55 ],\n    ...      [ 1.92,  1.48,  0.65 ]]\n    >>> y = [0, 1, 0, 1]\n    >>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n    >>> selector.estimator_.coef_\n    array([[-0.3252302 ,  0.83462377,  0.49750423]])\n    >>> selector.threshold_\n    0.55245...\n    >>> selector.get_support()\n    array([False,  True, False])\n    >>> selector.transform(X)\n    array([[-1.34],\n           [-0.02],\n           [-0.48],\n           [ 1.48]])\n\n    See Also\n    --------\n    RFE : Recursive feature elimination based on importance weights.\n    RFECV : Recursive feature elimination with built-in cross-validated\n        selection of the best number of features.\n    SequentialFeatureSelector : Sequential cross-validation based feature\n        selection. Does not rely on importance weights.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimator, *, threshold=None, prefit=False,\n                 norm_order=1, max_features=None,\n                 importance_getter='auto'):\n        self.estimator = estimator\n        self.threshold = threshold\n        self.prefit = prefit\n        self.importance_getter = importance_getter\n        self.norm_order = norm_order\n        self.max_features = max_features\n\n    def _get_support_mask(self):\n        # SelectFromModel can directly call on transform.\n        if self.prefit:\n            estimator = self.estimator\n        elif hasattr(self, 'estimator_'):\n            estimator = self.estimator_\n        else:\n            raise ValueError('Either fit the model before transform or set'\n                             ' \"prefit=True\" while passing the fitted'\n                             ' estimator to the constructor.')\n        scores = _get_feature_importances(\n            estimator=estimator, getter=self.importance_getter,\n            transform_func='norm', norm_order=self.norm_order)\n        threshold = _calculate_threshold(estimator, scores, self.threshold)\n        if self.max_features is not None:\n            mask = np.zeros_like(scores, dtype=bool)\n            candidate_indices = \\\n                np.argsort(-scores, kind='mergesort')[:self.max_features]\n            mask[candidate_indices] = True\n        else:\n            mask = np.ones_like(scores, dtype=bool)\n        mask[scores < threshold] = False\n        return mask\n\n    def fit(self, X, y=None, **fit_params):\n        \"\"\"Fit the SelectFromModel meta-transformer.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,), default=None\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        **fit_params : Other estimator specific parameters\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        if self.max_features is not None:\n            if not isinstance(self.max_features, numbers.Integral):\n                raise TypeError(\"'max_features' should be an integer between\"\n                                \" 0 and {} features. Got {!r} instead.\"\n                                .format(X.shape[1], self.max_features))\n            elif self.max_features < 0 or self.max_features > X.shape[1]:\n                raise ValueError(\"'max_features' should be 0 and {} features.\"\n                                 \"Got {} instead.\"\n                                 .format(X.shape[1], self.max_features))\n\n        if self.prefit:\n            raise NotFittedError(\n                \"Since 'prefit=True', call transform directly\")\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X, y, **fit_params)\n        return self\n\n    @property\n    def threshold_(self):\n        scores = _get_feature_importances(estimator=self.estimator_,\n                                          getter=self.importance_getter,\n                                          transform_func='norm',\n                                          norm_order=self.norm_order)\n        return _calculate_threshold(self.estimator, scores, self.threshold)\n\n    @if_delegate_has_method('estimator')\n    def partial_fit(self, X, y=None, **fit_params):\n        \"\"\"Fit the SelectFromModel meta-transformer only once.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,), default=None\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        **fit_params : Other estimator specific parameters\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        if self.prefit:\n            raise NotFittedError(\n                \"Since 'prefit=True', call transform directly\")\n        if not hasattr(self, \"estimator_\"):\n            self.estimator_ = clone(self.estimator)\n        self.estimator_.partial_fit(X, y, **fit_params)\n        return self\n\n    @property\n    def n_features_in_(self):\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() fails if the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n\n        return self.estimator_.n_features_in_\n\n    def _more_tags(self):\n        return {\n            'allow_nan': _safe_tags(self.estimator, key=\"allow_nan\")\n        }",
      "instance_attributes": [
        {
          "name": "estimator",
          "types": null
        },
        {
          "name": "threshold",
          "types": null
        },
        {
          "name": "prefit",
          "types": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "name": "importance_getter",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "norm_order",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "max_features",
          "types": null
        },
        {
          "name": "estimator_",
          "types": {
            "kind": "NamedType",
            "name": "Kernel"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE",
      "name": "RFE",
      "qname": "sklearn.feature_selection._rfe.RFE",
      "decorators": [],
      "superclasses": [
        "SelectorMixin",
        "MetaEstimatorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/_estimator_type@getter",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/classes_@getter",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/fit",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/_fit",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/predict",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/score",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/_get_support_mask",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/decision_function",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_proba",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_log_proba",
        "scikit-learn/sklearn.feature_selection._rfe/RFE/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.",
      "docstring": "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.\n\nParameters\n----------\nestimator : ``Estimator`` instance\n    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance\n    (e.g. `coef_`, `feature_importances_`).\n\nn_features_to_select : int or float, default=None\n    The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\n    .. versionchanged:: 0.24\n       Added float values for fractions.\n\nstep : int or float, default=1\n    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n\nverbose : int, default=0\n    Controls verbosity of output.\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance (implemented with `attrgetter`).\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : ``Estimator`` instance\n    The fitted estimator used to select features.\n\nn_features_ : int\n    The number of selected features.\n\nranking_ : ndarray of shape (n_features,)\n    The feature ranking, such that ``ranking_[i]`` corresponds to the\n    ranking position of the i-th feature. Selected (i.e., estimated\n    best) features are assigned rank 1.\n\nsupport_ : ndarray of shape (n_features,)\n    The mask of selected features.\n\nExamples\n--------\nThe following example shows how to retrieve the 5 most informative\nfeatures in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFE\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFE(estimator, n_features_to_select=5, step=1)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\nNotes\n-----\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nSee Also\n--------\nRFECV : Recursive feature elimination with built-in cross-validated\n    selection of the best number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\nSequentialFeatureSelector : Sequential cross-validation based feature\n    selection. Does not rely on importance weights.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.",
      "code": "class RFE(SelectorMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Feature ranking with recursive feature elimination.\n\n    Given an external estimator that assigns weights to features (e.g., the\n    coefficients of a linear model), the goal of recursive feature elimination\n    (RFE) is to select features by recursively considering smaller and smaller\n    sets of features. First, the estimator is trained on the initial set of\n    features and the importance of each feature is obtained either through\n    any specific attribute or callable.\n    Then, the least important features are pruned from current set of features.\n    That procedure is recursively repeated on the pruned set until the desired\n    number of features to select is eventually reached.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : ``Estimator`` instance\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance\n        (e.g. `coef_`, `feature_importances_`).\n\n    n_features_to_select : int or float, default=None\n        The number of features to select. If `None`, half of the features are\n        selected. If integer, the parameter is the absolute number of features\n        to select. If float between 0 and 1, it is the fraction of features to\n        select.\n\n        .. versionchanged:: 0.24\n           Added float values for fractions.\n\n    step : int or float, default=1\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n\n    verbose : int, default=0\n        Controls verbosity of output.\n\n    importance_getter : str or callable, default='auto'\n        If 'auto', uses the feature importance either through a `coef_`\n        or `feature_importances_` attributes of estimator.\n\n        Also accepts a string that specifies an attribute name/path\n        for extracting feature importance (implemented with `attrgetter`).\n        For example, give `regressor_.coef_` in case of\n        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n        `named_steps.clf.feature_importances_` in case of\n        class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n        If `callable`, overrides the default feature importance getter.\n        The callable is passed with the fitted estimator and it should\n        return importance for each feature.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    estimator_ : ``Estimator`` instance\n        The fitted estimator used to select features.\n\n    n_features_ : int\n        The number of selected features.\n\n    ranking_ : ndarray of shape (n_features,)\n        The feature ranking, such that ``ranking_[i]`` corresponds to the\n        ranking position of the i-th feature. Selected (i.e., estimated\n        best) features are assigned rank 1.\n\n    support_ : ndarray of shape (n_features,)\n        The mask of selected features.\n\n    Examples\n    --------\n    The following example shows how to retrieve the 5 most informative\n    features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFE\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFE(estimator, n_features_to_select=5, step=1)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\n    Notes\n    -----\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    See Also\n    --------\n    RFECV : Recursive feature elimination with built-in cross-validated\n        selection of the best number of features.\n    SelectFromModel : Feature selection based on thresholds of importance\n        weights.\n    SequentialFeatureSelector : Sequential cross-validation based feature\n        selection. Does not rely on importance weights.\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimator, *, n_features_to_select=None, step=1,\n                 verbose=0, importance_getter='auto'):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.importance_getter = importance_getter\n        self.verbose = verbose\n\n    @property\n    def _estimator_type(self):\n        return self.estimator._estimator_type\n\n    @property\n    def classes_(self):\n        return self.estimator_.classes_\n\n    def fit(self, X, y):\n        \"\"\"Fit the RFE model and then the underlying estimator on the selected\n           features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values.\n        \"\"\"\n        return self._fit(X, y)\n\n    def _fit(self, X, y, step_score=None):\n        # Parameter step_score controls the calculation of self.scores_\n        # step_score is not exposed to users\n        # and is used when implementing RFECV\n        # self.scores_ will not be calculated when calling _fit through fit\n\n        tags = self._get_tags()\n        X, y = self._validate_data(\n            X, y, accept_sparse=\"csc\",\n            ensure_min_features=2,\n            force_all_finite=not tags.get(\"allow_nan\", True),\n            multi_output=True\n        )\n        error_msg = (\"n_features_to_select must be either None, a \"\n                     \"positive integer representing the absolute \"\n                     \"number of features or a float in (0.0, 1.0] \"\n                     \"representing a percentage of features to \"\n                     f\"select. Got {self.n_features_to_select}\")\n\n        # Initialization\n        n_features = X.shape[1]\n        if self.n_features_to_select is None:\n            n_features_to_select = n_features // 2\n        elif self.n_features_to_select < 0:\n            raise ValueError(error_msg)\n        elif isinstance(self.n_features_to_select, numbers.Integral):  # int\n            n_features_to_select = self.n_features_to_select\n        elif self.n_features_to_select > 1.0:  # float > 1\n            raise ValueError(error_msg)\n        else:  # float\n            n_features_to_select = int(n_features * self.n_features_to_select)\n\n        if 0.0 < self.step < 1.0:\n            step = int(max(1, self.step * n_features))\n        else:\n            step = int(self.step)\n        if step <= 0:\n            raise ValueError(\"Step must be >0\")\n\n        support_ = np.ones(n_features, dtype=bool)\n        ranking_ = np.ones(n_features, dtype=int)\n\n        if step_score:\n            self.scores_ = []\n\n        # Elimination\n        while np.sum(support_) > n_features_to_select:\n            # Remaining features\n            features = np.arange(n_features)[support_]\n\n            # Rank the remaining features\n            estimator = clone(self.estimator)\n            if self.verbose > 0:\n                print(\"Fitting estimator with %d features.\" % np.sum(support_))\n\n            estimator.fit(X[:, features], y)\n\n            # Get importance and rank them\n            importances = _get_feature_importances(\n                estimator, self.importance_getter, transform_func=\"square\",\n            )\n            ranks = np.argsort(importances)\n\n            # for sparse case ranks is matrix\n            ranks = np.ravel(ranks)\n\n            # Eliminate the worse features\n            threshold = min(step, np.sum(support_) - n_features_to_select)\n\n            # Compute step score on the previous selection iteration\n            # because 'estimator' must use features\n            # that have not been eliminated yet\n            if step_score:\n                self.scores_.append(step_score(estimator, features))\n            support_[features[ranks][:threshold]] = False\n            ranking_[np.logical_not(support_)] += 1\n\n        # Set final attributes\n        features = np.arange(n_features)[support_]\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X[:, features], y)\n\n        # Compute step score when only n_features_to_select features left\n        if step_score:\n            self.scores_.append(step_score(self.estimator_, features))\n        self.n_features_ = support_.sum()\n        self.support_ = support_\n        self.ranking_ = ranking_\n\n        return self\n\n    @if_delegate_has_method(delegate='estimator')\n    def predict(self, X):\n        \"\"\"Reduce X to the selected features and then predict using the\n           underlying estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        y : array of shape [n_samples]\n            The predicted target values.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict(self.transform(X))\n\n    @if_delegate_has_method(delegate='estimator')\n    def score(self, X, y):\n        \"\"\"Reduce X to the selected features and then return the score of the\n           underlying estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        y : array of shape [n_samples]\n            The target values.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.score(self.transform(X), y)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.support_\n\n    @if_delegate_has_method(delegate='estimator')\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : array, shape = [n_samples, n_classes] or [n_samples]\n            The decision function of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification produce an array of shape\n            [n_samples].\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.decision_function(self.transform(X))\n\n    @if_delegate_has_method(delegate='estimator')\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_proba(self.transform(X))\n\n    @if_delegate_has_method(delegate='estimator')\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_log_proba(self.transform(X))\n\n    def _more_tags(self):\n        return {\n            'poor_score': True,\n            'allow_nan': _safe_tags(self.estimator, key='allow_nan'),\n            'requires_y': True,\n        }",
      "instance_attributes": [
        {
          "name": "estimator",
          "types": null
        },
        {
          "name": "n_features_to_select",
          "types": null
        },
        {
          "name": "step",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "importance_getter",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "scores_",
          "types": {
            "kind": "NamedType",
            "name": "list"
          }
        },
        {
          "name": "estimator_",
          "types": {
            "kind": "NamedType",
            "name": "Kernel"
          }
        },
        {
          "name": "n_features_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "ranking_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV",
      "name": "RFECV",
      "qname": "sklearn.feature_selection._rfe.RFECV",
      "decorators": [],
      "superclasses": [
        "RFE"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__",
        "scikit-learn/sklearn.feature_selection._rfe/RFECV/fit"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Feature ranking with recursive feature elimination and cross-validated\nselection of the best number of features.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.",
      "docstring": "Feature ranking with recursive feature elimination and cross-validated\nselection of the best number of features.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.\n\nParameters\n----------\nestimator : ``Estimator`` instance\n    A supervised learning estimator with a ``fit`` method that provides\n    information about feature importance either through a ``coef_``\n    attribute or through a ``feature_importances_`` attribute.\n\nstep : int or float, default=1\n    If greater than or equal to 1, then ``step`` corresponds to the\n    (integer) number of features to remove at each iteration.\n    If within (0.0, 1.0), then ``step`` corresponds to the percentage\n    (rounded down) of features to remove at each iteration.\n    Note that the last iteration may remove fewer than ``step`` features in\n    order to reach ``min_features_to_select``.\n\nmin_features_to_select : int, default=1\n    The minimum number of features to be selected. This number of features\n    will always be scored, even if the difference between the original\n    feature count and ``min_features_to_select`` isn't divisible by\n    ``step``.\n\n    .. versionadded:: 0.20\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross-validation,\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n    estimator is a classifier or if ``y`` is neither binary nor multiclass,\n    :class:`~sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value of None changed from 3-fold to 5-fold.\n\nscoring : string, callable or None, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nverbose : int, default=0\n    Controls verbosity of output.\n\nn_jobs : int or None, default=None\n    Number of cores to run in parallel while fitting across folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.18\n\nimportance_getter : str or callable, default='auto'\n    If 'auto', uses the feature importance either through a `coef_`\n    or `feature_importances_` attributes of estimator.\n\n    Also accepts a string that specifies an attribute name/path\n    for extracting feature importance.\n    For example, give `regressor_.coef_` in case of\n    :class:`~sklearn.compose.TransformedTargetRegressor`  or\n    `named_steps.clf.feature_importances_` in case of\n    :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n    If `callable`, overrides the default feature importance getter.\n    The callable is passed with the fitted estimator and it should\n    return importance for each feature.\n\n    .. versionadded:: 0.24\n\nAttributes\n----------\nestimator_ : ``Estimator`` instance\n    The fitted estimator used to select features.\n\ngrid_scores_ : ndarray of shape (n_subsets_of_features,)\n    The cross-validation scores such that\n    ``grid_scores_[i]`` corresponds to\n    the CV score of the i-th subset of features.\n\nn_features_ : int\n    The number of selected features with cross-validation.\n\nranking_ : narray of shape (n_features,)\n    The feature ranking, such that `ranking_[i]`\n    corresponds to the ranking\n    position of the i-th feature.\n    Selected (i.e., estimated best)\n    features are assigned rank 1.\n\nsupport_ : ndarray of shape (n_features,)\n    The mask of selected features.\n\nNotes\n-----\nThe size of ``grid_scores_`` is equal to\n``ceil((n_features - min_features_to_select) / step) + 1``,\nwhere step is the number of features removed at each iteration.\n\nAllows NaN/Inf in the input if the underlying estimator does as well.\n\nExamples\n--------\nThe following example shows how to retrieve the a-priori not known 5\ninformative features in the Friedman #1 dataset.\n\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.feature_selection import RFECV\n>>> from sklearn.svm import SVR\n>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n>>> estimator = SVR(kernel=\"linear\")\n>>> selector = RFECV(estimator, step=1, cv=5)\n>>> selector = selector.fit(X, y)\n>>> selector.support_\narray([ True,  True,  True,  True,  True, False, False, False, False,\n       False])\n>>> selector.ranking_\narray([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\nSee Also\n--------\nRFE : Recursive feature elimination.\n\nReferences\n----------\n\n.. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n       for cancer classification using support vector machines\",\n       Mach. Learn., 46(1-3), 389--422, 2002.",
      "code": "class RFECV(RFE):\n    \"\"\"Feature ranking with recursive feature elimination and cross-validated\n    selection of the best number of features.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : ``Estimator`` instance\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance either through a ``coef_``\n        attribute or through a ``feature_importances_`` attribute.\n\n    step : int or float, default=1\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n        Note that the last iteration may remove fewer than ``step`` features in\n        order to reach ``min_features_to_select``.\n\n    min_features_to_select : int, default=1\n        The minimum number of features to be selected. This number of features\n        will always be scored, even if the difference between the original\n        feature count and ``min_features_to_select`` isn't divisible by\n        ``step``.\n\n        .. versionadded:: 0.20\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n        estimator is a classifier or if ``y`` is neither binary nor multiclass,\n        :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value of None changed from 3-fold to 5-fold.\n\n    scoring : string, callable or None, default=None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    verbose : int, default=0\n        Controls verbosity of output.\n\n    n_jobs : int or None, default=None\n        Number of cores to run in parallel while fitting across folds.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    importance_getter : str or callable, default='auto'\n        If 'auto', uses the feature importance either through a `coef_`\n        or `feature_importances_` attributes of estimator.\n\n        Also accepts a string that specifies an attribute name/path\n        for extracting feature importance.\n        For example, give `regressor_.coef_` in case of\n        :class:`~sklearn.compose.TransformedTargetRegressor`  or\n        `named_steps.clf.feature_importances_` in case of\n        :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\n        If `callable`, overrides the default feature importance getter.\n        The callable is passed with the fitted estimator and it should\n        return importance for each feature.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    estimator_ : ``Estimator`` instance\n        The fitted estimator used to select features.\n\n    grid_scores_ : ndarray of shape (n_subsets_of_features,)\n        The cross-validation scores such that\n        ``grid_scores_[i]`` corresponds to\n        the CV score of the i-th subset of features.\n\n    n_features_ : int\n        The number of selected features with cross-validation.\n\n    ranking_ : narray of shape (n_features,)\n        The feature ranking, such that `ranking_[i]`\n        corresponds to the ranking\n        position of the i-th feature.\n        Selected (i.e., estimated best)\n        features are assigned rank 1.\n\n    support_ : ndarray of shape (n_features,)\n        The mask of selected features.\n\n    Notes\n    -----\n    The size of ``grid_scores_`` is equal to\n    ``ceil((n_features - min_features_to_select) / step) + 1``,\n    where step is the number of features removed at each iteration.\n\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    Examples\n    --------\n    The following example shows how to retrieve the a-priori not known 5\n    informative features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFECV\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFECV(estimator, step=1, cv=5)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\n    See Also\n    --------\n    RFE : Recursive feature elimination.\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, estimator, *, step=1, min_features_to_select=1,\n                 cv=None, scoring=None, verbose=0, n_jobs=None,\n                 importance_getter='auto'):\n        self.estimator = estimator\n        self.step = step\n        self.importance_getter = importance_getter\n        self.cv = cv\n        self.scoring = scoring\n        self.verbose = verbose\n        self.n_jobs = n_jobs\n        self.min_features_to_select = min_features_to_select\n\n    def fit(self, X, y, groups=None):\n        \"\"\"Fit the RFE model and automatically tune the number of selected\n           features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the total number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values (integers for classification, real numbers for\n            regression).\n\n        groups : array-like of shape (n_samples,) or None, default=None\n            Group labels for the samples used while splitting the dataset into\n            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n            instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n            .. versionadded:: 0.20\n        \"\"\"\n        tags = self._get_tags()\n        X, y = self._validate_data(\n            X, y, accept_sparse=\"csr\", ensure_min_features=2,\n            force_all_finite=not tags.get('allow_nan', True),\n            multi_output=True\n        )\n\n        # Initialization\n        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n        scorer = check_scoring(self.estimator, scoring=self.scoring)\n        n_features = X.shape[1]\n\n        if 0.0 < self.step < 1.0:\n            step = int(max(1, self.step * n_features))\n        else:\n            step = int(self.step)\n        if step <= 0:\n            raise ValueError(\"Step must be >0\")\n\n        # Build an RFE object, which will evaluate and score each possible\n        # feature count, down to self.min_features_to_select\n        rfe = RFE(estimator=self.estimator,\n                  n_features_to_select=self.min_features_to_select,\n                  importance_getter=self.importance_getter,\n                  step=self.step, verbose=self.verbose)\n\n        # Determine the number of subsets of features by fitting across\n        # the train folds and choosing the \"features_to_select\" parameter\n        # that gives the least averaged error across all folds.\n\n        # Note that joblib raises a non-picklable error for bound methods\n        # even if n_jobs is set to 1 with the default multiprocessing\n        # backend.\n        # This branching is done so that to\n        # make sure that user code that sets n_jobs to 1\n        # and provides bound methods as scorers is not broken with the\n        # addition of n_jobs parameter in version 0.18.\n\n        if effective_n_jobs(self.n_jobs) == 1:\n            parallel, func = list, _rfe_single_fit\n        else:\n            parallel = Parallel(n_jobs=self.n_jobs)\n            func = delayed(_rfe_single_fit)\n\n        scores = parallel(\n            func(rfe, self.estimator, X, y, train, test, scorer)\n            for train, test in cv.split(X, y, groups))\n\n        scores = np.sum(scores, axis=0)\n        scores_rev = scores[::-1]\n        argmax_idx = len(scores) - np.argmax(scores_rev) - 1\n        n_features_to_select = max(\n            n_features - (argmax_idx * step),\n            self.min_features_to_select)\n\n        # Re-execute an elimination with best_k over the whole set\n        rfe = RFE(estimator=self.estimator,\n                  n_features_to_select=n_features_to_select, step=self.step,\n                  importance_getter=self.importance_getter,\n                  verbose=self.verbose)\n\n        rfe.fit(X, y)\n\n        # Set final attributes\n        self.support_ = rfe.support_\n        self.n_features_ = rfe.n_features_\n        self.ranking_ = rfe.ranking_\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(self.transform(X), y)\n\n        # Fixing a normalization error, n is equal to get_n_splits(X, y) - 1\n        # here, the scores are normalized by get_n_splits(X, y)\n        self.grid_scores_ = scores[::-1] / cv.get_n_splits(X, y, groups)\n        return self",
      "instance_attributes": [
        {
          "name": "estimator",
          "types": null
        },
        {
          "name": "step",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "importance_getter",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "cv",
          "types": null
        },
        {
          "name": "scoring",
          "types": null
        },
        {
          "name": "verbose",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "min_features_to_select",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "n_features_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "ranking_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "name": "estimator_",
          "types": {
            "kind": "NamedType",
            "name": "Kernel"
          }
        },
        {
          "name": "grid_scores_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector",
      "name": "SequentialFeatureSelector",
      "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector",
      "decorators": [],
      "superclasses": [
        "SelectorMixin",
        "MetaEstimatorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__",
        "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/fit",
        "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/_get_best_new_feature",
        "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/_get_support_mask",
        "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator.\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24",
      "docstring": "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator.\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nestimator : estimator instance\n    An unfitted estimator.\n\nn_features_to_select : int or float, default=None\n    The number of features to select. If `None`, half of the features are\n    selected. If integer, the parameter is the absolute number of features\n    to select. If float between 0 and 1, it is the fraction of features to\n    select.\n\ndirection : {'forward', 'backward'}, default='forward'\n    Whether to perform forward selection or backward selection.\n\nscoring : str, callable, list/tuple or dict, default=None\n    A single str (see :ref:`scoring_parameter`) or a callable\n    (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n    NOTE that when using custom scorers, each scorer should return a single\n    value. Metric functions returning a list/array of values can be wrapped\n    into multiple scorers that return one value each.\n\n    If None, the estimator's score method is used.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used. These splitters are instantiated\n    with `shuffle=False` so the splits will be the same across calls.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel. When evaluating a new feature to\n    add or remove, the cross-validation procedure is parallel over the\n    folds.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nAttributes\n----------\nn_features_to_select_ : int\n    The number of features that were selected.\n\nsupport_ : ndarray of shape (n_features,), dtype=bool\n    The mask of selected features.\n\nSee Also\n--------\nRFE : Recursive feature elimination based on importance weights.\nRFECV : Recursive feature elimination based on importance weights, with\n    automatic selection of the number of features.\nSelectFromModel : Feature selection based on thresholds of importance\n    weights.\n\nExamples\n--------\n>>> from sklearn.feature_selection import SequentialFeatureSelector\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.datasets import load_iris\n>>> X, y = load_iris(return_X_y=True)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n>>> sfs.fit(X, y)\nSequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                          n_features_to_select=3)\n>>> sfs.get_support()\narray([ True, False,  True,  True])\n>>> sfs.transform(X).shape\n(150, 3)",
      "code": "class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin,\n                                BaseEstimator):\n    \"\"\"Transformer that performs Sequential Feature Selection.\n\n    This Sequential Feature Selector adds (forward selection) or\n    removes (backward selection) features to form a feature subset in a\n    greedy fashion. At each stage, this estimator chooses the best feature to\n    add or remove based on the cross-validation score of an estimator.\n\n    Read more in the :ref:`User Guide <sequential_feature_selection>`.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An unfitted estimator.\n\n    n_features_to_select : int or float, default=None\n        The number of features to select. If `None`, half of the features are\n        selected. If integer, the parameter is the absolute number of features\n        to select. If float between 0 and 1, it is the fraction of features to\n        select.\n\n    direction : {'forward', 'backward'}, default='forward'\n        Whether to perform forward selection or backward selection.\n\n    scoring : str, callable, list/tuple or dict, default=None\n        A single str (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        If None, the estimator's score method is used.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel. When evaluating a new feature to\n        add or remove, the cross-validation procedure is parallel over the\n        folds.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    n_features_to_select_ : int\n        The number of features that were selected.\n\n    support_ : ndarray of shape (n_features,), dtype=bool\n        The mask of selected features.\n\n    See Also\n    --------\n    RFE : Recursive feature elimination based on importance weights.\n    RFECV : Recursive feature elimination based on importance weights, with\n        automatic selection of the number of features.\n    SelectFromModel : Feature selection based on thresholds of importance\n        weights.\n\n    Examples\n    --------\n    >>> from sklearn.feature_selection import SequentialFeatureSelector\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> X, y = load_iris(return_X_y=True)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n    >>> sfs.fit(X, y)\n    SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n                              n_features_to_select=3)\n    >>> sfs.get_support()\n    array([ True, False,  True,  True])\n    >>> sfs.transform(X).shape\n    (150, 3)\n    \"\"\"\n    def __init__(self, estimator, *, n_features_to_select=None,\n                 direction='forward', scoring=None, cv=5, n_jobs=None):\n\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.direction = direction\n        self.scoring = scoring\n        self.cv = cv\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y):\n        \"\"\"Learn the features to select.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        tags = self._get_tags()\n        X, y = self._validate_data(\n            X, y, accept_sparse=\"csc\",\n            ensure_min_features=2,\n            force_all_finite=not tags.get(\"allow_nan\", True),\n            multi_output=True\n        )\n        n_features = X.shape[1]\n\n        error_msg = (\"n_features_to_select must be either None, an \"\n                     \"integer in [1, n_features - 1] \"\n                     \"representing the absolute \"\n                     \"number of features, or a float in (0, 1] \"\n                     \"representing a percentage of features to \"\n                     f\"select. Got {self.n_features_to_select}\")\n        if self.n_features_to_select is None:\n            self.n_features_to_select_ = n_features // 2\n        elif isinstance(self.n_features_to_select, numbers.Integral):\n            if not 0 < self.n_features_to_select < n_features:\n                raise ValueError(error_msg)\n            self.n_features_to_select_ = self.n_features_to_select\n        elif isinstance(self.n_features_to_select, numbers.Real):\n            if not 0 < self.n_features_to_select <= 1:\n                raise ValueError(error_msg)\n            self.n_features_to_select_ = int(n_features *\n                                             self.n_features_to_select)\n        else:\n            raise ValueError(error_msg)\n\n        if self.direction not in ('forward', 'backward'):\n            raise ValueError(\n                \"direction must be either 'forward' or 'backward'. \"\n                f\"Got {self.direction}.\"\n            )\n\n        cloned_estimator = clone(self.estimator)\n\n        # the current mask corresponds to the set of features:\n        # - that we have already *selected* if we do forward selection\n        # - that we have already *excluded* if we do backward selection\n        current_mask = np.zeros(shape=n_features, dtype=bool)\n        n_iterations = (\n            self.n_features_to_select_ if self.direction == 'forward'\n            else n_features - self.n_features_to_select_\n        )\n        for _ in range(n_iterations):\n            new_feature_idx = self._get_best_new_feature(cloned_estimator, X,\n                                                         y, current_mask)\n            current_mask[new_feature_idx] = True\n\n        if self.direction == 'backward':\n            current_mask = ~current_mask\n        self.support_ = current_mask\n\n        return self\n\n    def _get_best_new_feature(self, estimator, X, y, current_mask):\n        # Return the best new feature to add to the current_mask, i.e. return\n        # the best new feature to add (resp. remove) when doing forward\n        # selection (resp. backward selection)\n        candidate_feature_indices = np.flatnonzero(~current_mask)\n        scores = {}\n        for feature_idx in candidate_feature_indices:\n            candidate_mask = current_mask.copy()\n            candidate_mask[feature_idx] = True\n            if self.direction == 'backward':\n                candidate_mask = ~candidate_mask\n            X_new = X[:, candidate_mask]\n            scores[feature_idx] = cross_val_score(\n                estimator, X_new, y, cv=self.cv, scoring=self.scoring,\n                n_jobs=self.n_jobs).mean()\n        return max(scores, key=lambda feature_idx: scores[feature_idx])\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n        return self.support_\n\n    def _more_tags(self):\n        return {\n            'allow_nan': _safe_tags(self.estimator, key=\"allow_nan\"),\n            'requires_y': True,\n        }",
      "instance_attributes": [
        {
          "name": "estimator",
          "types": null
        },
        {
          "name": "n_features_to_select",
          "types": null
        },
        {
          "name": "direction",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "scoring",
          "types": null
        },
        {
          "name": "cv",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "n_jobs",
          "types": null
        },
        {
          "name": "n_features_to_select_",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "support_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect",
      "name": "GenericUnivariateSelect",
      "qname": "sklearn.feature_selection._univariate_selection.GenericUnivariateSelect",
      "decorators": [],
      "superclasses": [
        "_BaseFilter"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/__init__",
        "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/_make_selector",
        "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/_check_params",
        "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/_get_support_mask"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n    a single array scores.\n\nmode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n    Feature selection mode.\n\nparam : float or int depending on the feature selection mode, default=1e-5\n    Parameter of the corresponding mode.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned scores only.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n>>> X_new = transformer.fit_transform(X, y)\n>>> X_new.shape\n(569, 20)\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.",
      "code": "class GenericUnivariateSelect(_BaseFilter):\n    \"\"\"Univariate feature selector with configurable strategy.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n        a single array scores.\n\n    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n        Feature selection mode.\n\n    param : float or int depending on the feature selection mode, default=1e-5\n        Parameter of the corresponding mode.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned scores only.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n    >>> X_new = transformer.fit_transform(X, y)\n    >>> X_new.shape\n    (569, 20)\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif : Mutual information for a discrete target.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a continuous target.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n    \"\"\"\n\n    _selection_modes = {'percentile': SelectPercentile,\n                        'k_best': SelectKBest,\n                        'fpr': SelectFpr,\n                        'fdr': SelectFdr,\n                        'fwe': SelectFwe}\n\n    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, mode='percentile', param=1e-5):\n        super().__init__(score_func=score_func)\n        self.mode = mode\n        self.param = param\n\n    def _make_selector(self):\n        selector = self._selection_modes[self.mode](score_func=self.score_func)\n\n        # Now perform some acrobatics to set the right named parameter in\n        # the selector\n        possible_params = selector._get_param_names()\n        possible_params.remove('score_func')\n        selector.set_params(**{possible_params[0]: self.param})\n\n        return selector\n\n    def _check_params(self, X, y):\n        if self.mode not in self._selection_modes:\n            raise ValueError(\"The mode passed should be one of %s, %r,\"\n                             \" (type %s) was passed.\"\n                             % (self._selection_modes.keys(), self.mode,\n                                type(self.mode)))\n\n        self._make_selector()._check_params(X, y)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        selector = self._make_selector()\n        selector.pvalues_ = self.pvalues_\n        selector.scores_ = self.scores_\n        return selector._get_support_mask()",
      "instance_attributes": [
        {
          "name": "mode",
          "types": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "name": "param",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr",
      "name": "SelectFdr",
      "qname": "sklearn.feature_selection._univariate_selection.SelectFdr",
      "decorators": [],
      "superclasses": [
        "_BaseFilter"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr/__init__",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr/_get_support_mask"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Filter: Select the p-values for an estimated false discovery rate\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Filter: Select the p-values for an estimated false discovery rate\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest uncorrected p-value for features to keep.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFdr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/False_discovery_rate\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a contnuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
      "code": "class SelectFdr(_BaseFilter):\n    \"\"\"Filter: Select the p-values for an estimated false discovery rate\n\n    This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\n    on the expected false discovery rate.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues).\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n    alpha : float, default=5e-2\n        The highest uncorrected p-value for features to keep.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import SelectFdr, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n    >>> X_new.shape\n    (569, 16)\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/False_discovery_rate\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif : Mutual information for a discrete target.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a contnuous target.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFwe : Select features based on family-wise error rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        n_features = len(self.pvalues_)\n        sv = np.sort(self.pvalues_)\n        selected = sv[sv <= float(self.alpha) / n_features *\n                      np.arange(1, n_features + 1)]\n        if selected.size == 0:\n            return np.zeros_like(self.pvalues_, dtype=bool)\n        return self.pvalues_ <= selected.max()",
      "instance_attributes": [
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr",
      "name": "SelectFpr",
      "qname": "sklearn.feature_selection._univariate_selection.SelectFpr",
      "decorators": [],
      "superclasses": [
        "_BaseFilter"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr/__init__",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr/_get_support_mask"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest p-value for features to be kept.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFpr, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 16)\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nmutual_info_classif: Mutual information for a discrete target.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
      "code": "class SelectFpr(_BaseFilter):\n    \"\"\"Filter: Select the pvalues below alpha based on a FPR test.\n\n    FPR test stands for False Positive Rate test. It controls the total\n    amount of false detections.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues).\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n    alpha : float, default=5e-2\n        The highest p-value for features to be kept.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import SelectFpr, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n    >>> X_new.shape\n    (569, 16)\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    mutual_info_classif: Mutual information for a discrete target.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a continuous target.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        return self.pvalues_ < self.alpha",
      "instance_attributes": [
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe",
      "name": "SelectFwe",
      "qname": "sklearn.feature_selection._univariate_selection.SelectFwe",
      "decorators": [],
      "superclasses": [
        "_BaseFilter"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe/__init__",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe/_get_support_mask"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Filter: Select the p-values corresponding to Family-wise error rate\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Filter: Select the p-values corresponding to Family-wise error rate\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues).\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\nalpha : float, default=5e-2\n    The highest uncorrected p-value for features to keep.\n\nExamples\n--------\n>>> from sklearn.datasets import load_breast_cancer\n>>> from sklearn.feature_selection import SelectFwe, chi2\n>>> X, y = load_breast_cancer(return_X_y=True)\n>>> X.shape\n(569, 30)\n>>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n>>> X_new.shape\n(569, 15)\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
      "code": "class SelectFwe(_BaseFilter):\n    \"\"\"Filter: Select the p-values corresponding to Family-wise error rate\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues).\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n    alpha : float, default=5e-2\n        The highest uncorrected p-value for features to keep.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import SelectFwe, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n    >>> X_new.shape\n    (569, 15)\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores.\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        return (self.pvalues_ < self.alpha / len(self.pvalues_))",
      "instance_attributes": [
        {
          "name": "alpha",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest",
      "name": "SelectKBest",
      "qname": "sklearn.feature_selection._univariate_selection.SelectKBest",
      "decorators": [],
      "superclasses": [
        "_BaseFilter"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/__init__",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/_check_params",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/_get_support_mask"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18\n\nk : int or \"all\", default=10\n    Number of top features to select.\n    The \"all\" option bypasses selection, for use in a parameter search.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned only scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n>>> X_new.shape\n(1797, 20)\n\nNotes\n-----\nTies between features with equal scores will be broken in an unspecified\nway.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectPercentile : Select features based on percentile of the highest\n    scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
      "code": "class SelectKBest(_BaseFilter):\n    \"\"\"Select features according to the k highest scores.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues) or a single array with scores.\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n        .. versionadded:: 0.18\n\n    k : int or \"all\", default=10\n        Number of top features to select.\n        The \"all\" option bypasses selection, for use in a parameter search.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned only scores.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.feature_selection import SelectKBest, chi2\n    >>> X, y = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n    >>> X_new.shape\n    (1797, 20)\n\n    Notes\n    -----\n    Ties between features with equal scores will be broken in an unspecified\n    way.\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif : Mutual information for a discrete target.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a continuous target.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, k=10):\n        super().__init__(score_func=score_func)\n        self.k = k\n\n    def _check_params(self, X, y):\n        if not (self.k == \"all\" or 0 <= self.k <= X.shape[1]):\n            raise ValueError(\"k should be >=0, <= n_features = %d; got %r. \"\n                             \"Use k='all' to return all features.\"\n                             % (X.shape[1], self.k))\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        if self.k == 'all':\n            return np.ones(self.scores_.shape, dtype=bool)\n        elif self.k == 0:\n            return np.zeros(self.scores_.shape, dtype=bool)\n        else:\n            scores = _clean_nans(self.scores_)\n            mask = np.zeros(scores.shape, dtype=bool)\n\n            # Request a stable sort. Mergesort takes more memory (~40MB per\n            # megafeature on x86-64).\n            mask[np.argsort(scores, kind=\"mergesort\")[-self.k:]] = 1\n            return mask",
      "instance_attributes": [
        {
          "name": "k",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile",
      "name": "SelectPercentile",
      "qname": "sklearn.feature_selection._univariate_selection.SelectPercentile",
      "decorators": [],
      "superclasses": [
        "_BaseFilter"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/__init__",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/_check_params",
        "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/_get_support_mask"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nscore_func : callable, default=f_classif\n    Function taking two arrays X and y, and returning a pair of arrays\n    (scores, pvalues) or a single array with scores.\n    Default is f_classif (see below \"See Also\"). The default function only\n    works with classification tasks.\n\n    .. versionadded:: 0.18\n\npercentile : int, default=10\n    Percent of features to keep.\n\nAttributes\n----------\nscores_ : array-like of shape (n_features,)\n    Scores of features.\n\npvalues_ : array-like of shape (n_features,)\n    p-values of feature scores, None if `score_func` returned only scores.\n\nExamples\n--------\n>>> from sklearn.datasets import load_digits\n>>> from sklearn.feature_selection import SelectPercentile, chi2\n>>> X, y = load_digits(return_X_y=True)\n>>> X.shape\n(1797, 64)\n>>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n>>> X_new.shape\n(1797, 7)\n\nNotes\n-----\nTies between features with equal scores will be broken in an unspecified\nway.\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nmutual_info_classif : Mutual information for a discrete target.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.\nmutual_info_regression : Mutual information for a continuous target.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nGenericUnivariateSelect : Univariate feature selector with configurable\n    mode.",
      "code": "class SelectPercentile(_BaseFilter):\n    \"\"\"Select features according to a percentile of the highest scores.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable, default=f_classif\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues) or a single array with scores.\n        Default is f_classif (see below \"See Also\"). The default function only\n        works with classification tasks.\n\n        .. versionadded:: 0.18\n\n    percentile : int, default=10\n        Percent of features to keep.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned only scores.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.feature_selection import SelectPercentile, chi2\n    >>> X, y = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n    >>> X_new.shape\n    (1797, 7)\n\n    Notes\n    -----\n    Ties between features with equal scores will be broken in an unspecified\n    way.\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif : Mutual information for a discrete target.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    mutual_info_regression : Mutual information for a continuous target.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n    GenericUnivariateSelect : Univariate feature selector with configurable\n        mode.\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, percentile=10):\n        super().__init__(score_func=score_func)\n        self.percentile = percentile\n\n    def _check_params(self, X, y):\n        if not 0 <= self.percentile <= 100:\n            raise ValueError(\"percentile should be >=0, <=100; got %r\"\n                             % self.percentile)\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        # Cater for NaNs\n        if self.percentile == 100:\n            return np.ones(len(self.scores_), dtype=bool)\n        elif self.percentile == 0:\n            return np.zeros(len(self.scores_), dtype=bool)\n\n        scores = _clean_nans(self.scores_)\n        threshold = np.percentile(scores, 100 - self.percentile)\n        mask = scores > threshold\n        ties = np.where(scores == threshold)[0]\n        if len(ties):\n            max_feats = int(len(scores) * self.percentile / 100)\n            kept_ties = ties[:max_feats - mask.sum()]\n            mask[kept_ties] = True\n        return mask",
      "instance_attributes": [
        {
          "name": "percentile",
          "types": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "name": "pvalues_",
          "types": null
        },
        {
          "name": "scores_",
          "types": null
        }
      ]
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold",
      "name": "VarianceThreshold",
      "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold",
      "decorators": [],
      "superclasses": [
        "SelectorMixin",
        "BaseEstimator"
      ],
      "methods": [
        "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/__init__",
        "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/fit",
        "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/_get_support_mask",
        "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/_more_tags"
      ],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.",
      "docstring": "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.\n\nParameters\n----------\nthreshold : float, default=0\n    Features with a training-set variance lower than this threshold will\n    be removed. The default is to keep all features with non-zero variance,\n    i.e. remove the features that have the same value in all samples.\n\nAttributes\n----------\nvariances_ : array, shape (n_features,)\n    Variances of individual features.\n\nNotes\n-----\nAllows NaN in the input.\nRaises ValueError if no feature in X meets the variance threshold.\n\nExamples\n--------\nThe following dataset has integer features, two of which are the same\nin every sample. These are removed with the default setting for threshold::\n\n    >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n    >>> selector = VarianceThreshold()\n    >>> selector.fit_transform(X)\n    array([[2, 0],\n           [1, 4],\n           [1, 1]])",
      "code": "class VarianceThreshold(SelectorMixin, BaseEstimator):\n    \"\"\"Feature selector that removes all low-variance features.\n\n    This feature selection algorithm looks only at the features (X), not the\n    desired outputs (y), and can thus be used for unsupervised learning.\n\n    Read more in the :ref:`User Guide <variance_threshold>`.\n\n    Parameters\n    ----------\n    threshold : float, default=0\n        Features with a training-set variance lower than this threshold will\n        be removed. The default is to keep all features with non-zero variance,\n        i.e. remove the features that have the same value in all samples.\n\n    Attributes\n    ----------\n    variances_ : array, shape (n_features,)\n        Variances of individual features.\n\n    Notes\n    -----\n    Allows NaN in the input.\n    Raises ValueError if no feature in X meets the variance threshold.\n\n    Examples\n    --------\n    The following dataset has integer features, two of which are the same\n    in every sample. These are removed with the default setting for threshold::\n\n        >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n        >>> selector = VarianceThreshold()\n        >>> selector.fit_transform(X)\n        array([[2, 0],\n               [1, 4],\n               [1, 1]])\n    \"\"\"\n\n    def __init__(self, threshold=0.):\n        self.threshold = threshold\n\n    def fit(self, X, y=None):\n        \"\"\"Learn empirical variances from X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Sample vectors from which to compute variances.\n\n        y : any, default=None\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n                                dtype=np.float64,\n                                force_all_finite='allow-nan')\n\n        if hasattr(X, \"toarray\"):   # sparse matrix\n            _, self.variances_ = mean_variance_axis(X, axis=0)\n            if self.threshold == 0:\n                mins, maxes = min_max_axis(X, axis=0)\n                peak_to_peaks = maxes - mins\n        else:\n            self.variances_ = np.nanvar(X, axis=0)\n            if self.threshold == 0:\n                peak_to_peaks = np.ptp(X, axis=0)\n\n        if self.threshold == 0:\n            # Use peak-to-peak to avoid numeric precision issues\n            # for constant features\n            compare_arr = np.array([self.variances_, peak_to_peaks])\n            self.variances_ = np.nanmin(compare_arr, axis=0)\n\n        if np.all(~np.isfinite(self.variances_) |\n                  (self.variances_ <= self.threshold)):\n            msg = \"No feature in X meets the variance threshold {0:.5f}\"\n            if X.shape[0] == 1:\n                msg += \" (X contains only one sample)\"\n            raise ValueError(msg.format(self.threshold))\n\n        return self\n\n    def _get_support_mask(self):\n        check_is_fitted(self)\n\n        return self.variances_ > self.threshold\n\n    def _more_tags(self):\n        return {'allow_nan': True}",
      "instance_attributes": [
        {
          "name": "threshold",
          "types": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "name": "variances_",
          "types": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        }
      ]
    }
  ],
  "functions": [
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/damping",
          "name": "damping",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.damping",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "Damping factor (between 0.5 and 1) is the extent to\nwhich the current value is maintained relative to\nincoming values (weighted 1 - damping). This in order\nto avoid numerical oscillations when updating these\nvalues (messages)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/convergence_iter",
          "name": "convergence_iter",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.convergence_iter",
          "default_value": "15",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "15",
            "description": "Number of iterations with no change in the number\nof estimated clusters that stops the convergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Make a copy of input data."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/preference",
          "name": "preference",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.preference",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or float",
            "default_value": "None",
            "description": "Preferences for each point - points with larger values of\npreferences are more likely to be chosen as exemplars. The number\nof exemplars, ie of clusters, is influenced by the input\npreferences value. If the preferences are not passed as arguments,\nthey will be set to the median of the input similarities."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples,)"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'euclidean', 'precomputed'}",
            "default_value": "'euclidean'",
            "description": "Which affinity to use. At the moment 'precomputed' and\n``euclidean`` are supported. 'euclidean' uses the\nnegative squared euclidean distance between points."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "precomputed",
              "euclidean"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to be verbose."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.__init__.random_state",
          "default_value": "'warn'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "0",
            "description": "Pseudo-random number generator to control the starting state.\nUse an int for reproducible results across function calls.\nSee the :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.23\n    this parameter was previously hardcoded as 0."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, damping=.5, max_iter=200, convergence_iter=15,\n                 copy=True, preference=None, affinity='euclidean',\n                 verbose=False, random_state='warn'):\n\n        self.damping = damping\n        self.max_iter = max_iter\n        self.convergence_iter = convergence_iter\n        self.copy = copy\n        self.verbose = verbose\n        self.preference = preference\n        self.affinity = affinity\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit",
      "name": "fit",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse feature matrix\nis provided, it will be converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the clustering from features, or affinity matrix.",
      "docstring": "Fit the clustering from features, or affinity matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse feature matrix\n    is provided, it will be converted into a sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        if self.affinity == \"precomputed\":\n            accept_sparse = False\n        else:\n            accept_sparse = 'csr'\n        X = self._validate_data(X, accept_sparse=accept_sparse)\n        if self.affinity == \"precomputed\":\n            self.affinity_matrix_ = X\n        elif self.affinity == \"euclidean\":\n            self.affinity_matrix_ = -euclidean_distances(X, squared=True)\n        else:\n            raise ValueError(\"Affinity must be 'precomputed' or \"\n                             \"'euclidean'. Got %s instead\"\n                             % str(self.affinity))\n\n        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \\\n            affinity_propagation(\n                self.affinity_matrix_, preference=self.preference,\n                max_iter=self.max_iter,\n                convergence_iter=self.convergence_iter, damping=self.damping,\n                copy=self.copy, verbose=self.verbose, return_n_iter=True,\n                random_state=self.random_state)\n\n        if self.affinity != \"precomputed\":\n            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or similarities / affinities between\ninstances if ``affinity='precomputed'``. If a sparse feature matrix\nis provided, it will be converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the clustering from features or affinity matrix, and return\ncluster labels.",
      "docstring": "Fit the clustering from features or affinity matrix, and return\ncluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or                 array-like of shape (n_samples, n_samples)\n    Training instances to cluster, or similarities / affinities between\n    instances if ``affinity='precomputed'``. If a sparse feature matrix\n    is provided, it will be converted into a sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def fit_predict(self, X, y=None):\n        \"\"\"Fit the clustering from features or affinity matrix, and return\n        cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                array-like of shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict",
      "name": "predict",
      "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/AffinityPropagation/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._affinity_propagation.AffinityPropagation.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict. If a sparse matrix is provided, it will be\nconverted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict. If a sparse matrix is provided, it will be\n    converted into a sparse ``csr_matrix``.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        if not hasattr(self, \"cluster_centers_\"):\n            raise ValueError(\"Predict method is not supported when \"\n                             \"affinity='precomputed'.\")\n\n        if self.cluster_centers_.shape[0] > 0:\n            with config_context(assume_finite=True):\n                return pairwise_distances_argmin(X, self.cluster_centers_)\n        else:\n            warnings.warn(\"This model does not have any cluster centers \"\n                          \"because affinity propagation did not converge. \"\n                          \"Labeling every sample as '-1'.\", ConvergenceWarning)\n            return np.array([-1] * X.shape[0])"
    },
    {
      "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation",
      "name": "affinity_propagation",
      "qname": "sklearn.cluster._affinity_propagation.affinity_propagation",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/S",
          "name": "S",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.S",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "Matrix of similarities between points."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_samples)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/preference",
          "name": "preference",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.preference",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or float",
            "default_value": "None",
            "description": "Preferences for each point - points with larger values of\npreferences are more likely to be chosen as exemplars. The number of\nexemplars, i.e. of clusters, is influenced by the input preferences\nvalue. If the preferences are not passed as arguments, they will be\nset to the median of the input similarities (resulting in a moderate\nnumber of clusters). For a smaller amount of clusters, this can be set\nto the minimum value of the similarities."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples,)"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/convergence_iter",
          "name": "convergence_iter",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.convergence_iter",
          "default_value": "15",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "15",
            "description": "Number of iterations with no change in the number\nof estimated clusters that stops the convergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/damping",
          "name": "damping",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.damping",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "Damping factor between 0.5 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/copy",
          "name": "copy",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If copy is False, the affinity matrix is modified inplace by the\nalgorithm, for memory efficiency."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "The verbosity level."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._affinity_propagation/affinity_propagation/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._affinity_propagation.affinity_propagation.random_state",
          "default_value": "'warn'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "0",
            "description": "Pseudo-random number generator to control the starting state.\nUse an int for reproducible results across function calls.\nSee the :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.23\n    this parameter was previously hardcoded as 0."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.",
      "docstring": "Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters\n----------\n\nS : array-like of shape (n_samples, n_samples)\n    Matrix of similarities between points.\n\npreference : array-like of shape (n_samples,) or float, default=None\n    Preferences for each point - points with larger values of\n    preferences are more likely to be chosen as exemplars. The number of\n    exemplars, i.e. of clusters, is influenced by the input preferences\n    value. If the preferences are not passed as arguments, they will be\n    set to the median of the input similarities (resulting in a moderate\n    number of clusters). For a smaller amount of clusters, this can be set\n    to the minimum value of the similarities.\n\nconvergence_iter : int, default=15\n    Number of iterations with no change in the number\n    of estimated clusters that stops the convergence.\n\nmax_iter : int, default=200\n    Maximum number of iterations\n\ndamping : float, default=0.5\n    Damping factor between 0.5 and 1.\n\ncopy : bool, default=True\n    If copy is False, the affinity matrix is modified inplace by the\n    algorithm, for memory efficiency.\n\nverbose : bool, default=False\n    The verbosity level.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nrandom_state : int, RandomState instance or None, default=0\n    Pseudo-random number generator to control the starting state.\n    Use an int for reproducible results across function calls.\n    See the :term:`Glossary <random_state>`.\n\n    .. versionadded:: 0.23\n        this parameter was previously hardcoded as 0.\n\nReturns\n-------\n\ncluster_centers_indices : ndarray of shape (n_clusters,)\n    Index of clusters centers.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to True.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n<sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\nWhen the algorithm does not converge, it returns an empty array as\n``cluster_center_indices`` and ``-1`` as label for each training sample.\n\nWhen all training samples have equal similarities and equal preferences,\nthe assignment of cluster centers and labels depends on the preference.\nIf the preference is smaller than the similarities, a single cluster center\nand label ``0`` for every sample will be returned. Otherwise, every\ntraining sample becomes its own cluster center and is assigned a unique\nlabel.\n\nReferences\n----------\nBrendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\nBetween Data Points\", Science Feb. 2007",
      "code": "@_deprecate_positional_args\ndef affinity_propagation(S, *, preference=None, convergence_iter=15,\n                         max_iter=200, damping=0.5, copy=True, verbose=False,\n                         return_n_iter=False, random_state='warn'):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like of shape (n_samples, n_samples)\n        Matrix of similarities between points.\n\n    preference : array-like of shape (n_samples,) or float, default=None\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, default=15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, default=200\n        Maximum number of iterations\n\n    damping : float, default=0.5\n        Damping factor between 0.5 and 1.\n\n    copy : bool, default=True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency.\n\n    verbose : bool, default=False\n        The verbosity level.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    random_state : int, RandomState instance or None, default=0\n        Pseudo-random number generator to control the starting state.\n        Use an int for reproducible results across function calls.\n        See the :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.23\n            this parameter was previously hardcoded as 0.\n\n    Returns\n    -------\n\n    cluster_centers_indices : ndarray of shape (n_clusters,)\n        Index of clusters centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    When the algorithm does not converge, it returns an empty array as\n    ``cluster_center_indices`` and ``-1`` as label for each training sample.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    \"\"\"\n    S = as_float_array(S, copy=copy)\n    n_samples = S.shape[0]\n\n    if S.shape[0] != S.shape[1]:\n        raise ValueError(\"S must be a square array (shape=%s)\" % repr(S.shape))\n\n    if preference is None:\n        preference = np.median(S)\n    if damping < 0.5 or damping >= 1:\n        raise ValueError('damping must be >= 0.5 and < 1')\n\n    preference = np.array(preference)\n\n    if (n_samples == 1 or\n            _equal_similarities_and_preferences(S, preference)):\n        # It makes no sense to run the algorithm in this case, so return 1 or\n        # n_samples clusters, depending on preferences\n        warnings.warn(\"All samples have mutually equal similarities. \"\n                      \"Returning arbitrary cluster center(s).\")\n        if preference.flat[0] >= S.flat[n_samples - 1]:\n            return ((np.arange(n_samples), np.arange(n_samples), 0)\n                    if return_n_iter\n                    else (np.arange(n_samples), np.arange(n_samples)))\n        else:\n            return ((np.array([0]), np.array([0] * n_samples), 0)\n                    if return_n_iter\n                    else (np.array([0]), np.array([0] * n_samples)))\n\n    if random_state == 'warn':\n        warnings.warn(\n            \"'random_state' has been introduced in 0.23. It will be set to \"\n            \"None starting from 1.0 (renaming of 0.25) which means that \"\n            \"results will differ at every function call. Set 'random_state' \"\n            \"to None to silence this warning, or to 0 to keep the behavior of \"\n            \"versions <0.23.\",\n            FutureWarning\n        )\n        random_state = 0\n    random_state = check_random_state(random_state)\n\n    # Place preference on the diagonal of S\n    S.flat[::(n_samples + 1)] = preference\n\n    A = np.zeros((n_samples, n_samples))\n    R = np.zeros((n_samples, n_samples))  # Initialize messages\n    # Intermediate results\n    tmp = np.zeros((n_samples, n_samples))\n\n    # Remove degeneracies\n    S += ((np.finfo(S.dtype).eps * S + np.finfo(S.dtype).tiny * 100) *\n          random_state.randn(n_samples, n_samples))\n\n    # Execute parallel affinity propagation updates\n    e = np.zeros((n_samples, convergence_iter))\n\n    ind = np.arange(n_samples)\n\n    for it in range(max_iter):\n        # tmp = A + S; compute responsibilities\n        np.add(A, S, tmp)\n        I = np.argmax(tmp, axis=1)\n        Y = tmp[ind, I]  # np.max(A + S, axis=1)\n        tmp[ind, I] = -np.inf\n        Y2 = np.max(tmp, axis=1)\n\n        # tmp = Rnew\n        np.subtract(S, Y[:, None], tmp)\n        tmp[ind, I] = S[ind, I] - Y2\n\n        # Damping\n        tmp *= 1 - damping\n        R *= damping\n        R += tmp\n\n        # tmp = Rp; compute availabilities\n        np.maximum(R, 0, tmp)\n        tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]\n\n        # tmp = -Anew\n        tmp -= np.sum(tmp, axis=0)\n        dA = np.diag(tmp).copy()\n        tmp.clip(0, np.inf, tmp)\n        tmp.flat[::n_samples + 1] = dA\n\n        # Damping\n        tmp *= 1 - damping\n        A *= damping\n        A -= tmp\n\n        # Check for convergence\n        E = (np.diag(A) + np.diag(R)) > 0\n        e[:, it % convergence_iter] = E\n        K = np.sum(E, axis=0)\n\n        if it >= convergence_iter:\n            se = np.sum(e, axis=1)\n            unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                           != n_samples)\n            if (not unconverged and (K > 0)) or (it == max_iter):\n                never_converged = False\n                if verbose:\n                    print(\"Converged after %d iterations.\" % it)\n                break\n    else:\n        never_converged = True\n        if verbose:\n            print(\"Did not converge\")\n\n    I = np.flatnonzero(E)\n    K = I.size  # Identify exemplars\n\n    if K > 0 and not never_converged:\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)  # Identify clusters\n        # Refine the final set of exemplars and clusters and return results\n        for k in range(K):\n            ii = np.where(c == k)[0]\n            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))\n            I[k] = ii[j]\n\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)\n        labels = I[c]\n        # Reduce labels to a sorted, gapless, list\n        cluster_centers_indices = np.unique(labels)\n        labels = np.searchsorted(cluster_centers_indices, labels)\n    else:\n        warnings.warn(\"Affinity propagation did not converge, this model \"\n                      \"will not have any cluster centers.\", ConvergenceWarning)\n        labels = np.array([-1] * n_samples)\n        cluster_centers_indices = []\n\n    if return_n_iter:\n        return cluster_centers_indices, labels, it + 1\n    else:\n        return cluster_centers_indices, labels"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.n_clusters",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "2",
            "description": "The number of clusters to find. It must be ``None`` if\n``distance_threshold`` is not ``None``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'euclidean'",
            "description": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n\"manhattan\", \"cosine\", or \"precomputed\".\nIf linkage is \"ward\", only \"euclidean\" is accepted.\nIf \"precomputed\", a distance matrix (instead of a similarity matrix)\nis needed as input for the fit method."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/memory",
          "name": "memory",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.memory",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or object with the joblib.Memory interface",
            "default_value": "None",
            "description": "Used to cache the output of the computation of the tree.\nBy default, no caching is done. If a string is given, it is the\npath to the caching directory."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "object with the joblib.Memory interface"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.connectivity",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like or callable",
            "default_value": "None",
            "description": "Connectivity matrix. Defines for each sample the neighboring\nsamples following a given structure of the data.\nThis can be a connectivity matrix itself or a callable that transforms\nthe data into a connectivity matrix, such as derived from\nkneighbors_graph. Default is ``None``, i.e, the\nhierarchical clustering algorithm is unstructured."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/compute_full_tree",
          "name": "compute_full_tree",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.compute_full_tree",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or bool",
            "default_value": "'auto'",
            "description": "Stop early the construction of the tree at ``n_clusters``. This is\nuseful to decrease computation time if the number of clusters is not\nsmall compared to the number of samples. This option is useful only\nwhen specifying a connectivity matrix. Note also that when varying the\nnumber of clusters and using caching, it may be advantageous to compute\nthe full tree. It must be ``True`` if ``distance_threshold`` is not\n``None``. By default `compute_full_tree` is \"auto\", which is equivalent\nto `True` when `distance_threshold` is not `None` or that `n_clusters`\nis inferior to the maximum between 100 or `0.02 * n_samples`.\nOtherwise, \"auto\" is equivalent to `False`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/linkage",
          "name": "linkage",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.linkage",
          "default_value": "'ward'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ward', 'complete', 'average', 'single'}",
            "default_value": "'ward'",
            "description": "Which linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of observation. The algorithm will merge\nthe pairs of cluster that minimize this criterion.\n\n- 'ward' minimizes the variance of the clusters being merged.\n- 'average' uses the average of the distances of each observation of\n  the two sets.\n- 'complete' or 'maximum' linkage uses the maximum distances between\n  all observations of the two sets.\n- 'single' uses the minimum of the distances between all observations\n  of the two sets.\n\n.. versionadded:: 0.20\n    Added the 'single' option"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "complete",
              "average",
              "single",
              "ward"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/distance_threshold",
          "name": "distance_threshold",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.distance_threshold",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The linkage distance threshold above which, clusters will not be\nmerged. If not ``None``, ``n_clusters`` must be ``None`` and\n``compute_full_tree`` must be ``True``.\n\n.. versionadded:: 0.21"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/__init__/compute_distances",
          "name": "compute_distances",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.__init__.compute_distances",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Computes distances between clusters even if `distance_threshold` is not\nused. This can be used to make dendrogram visualization, but introduces\na computational and memory overhead.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Agglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', distance_threshold=None,\n                 compute_distances=False):\n        self.n_clusters = n_clusters\n        self.distance_threshold = distance_threshold\n        self.memory = memory\n        self.connectivity = connectivity\n        self.compute_full_tree = compute_full_tree\n        self.linkage = linkage\n        self.affinity = affinity\n        self.compute_distances = compute_distances"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit",
      "name": "fit",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, shape (n_samples, n_features) or (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``affinity='precomputed'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the hierarchical clustering from features, or distance matrix.",
      "docstring": "Fit the hierarchical clustering from features, or distance matrix.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``affinity='precomputed'``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator=self)\n        memory = check_memory(self.memory)\n\n        if self.n_clusters is not None and self.n_clusters <= 0:\n            raise ValueError(\"n_clusters should be an integer greater than 0.\"\n                             \" %s was provided.\" % str(self.n_clusters))\n\n        if not ((self.n_clusters is None) ^ (self.distance_threshold is None)):\n            raise ValueError(\"Exactly one of n_clusters and \"\n                             \"distance_threshold has to be set, and the other \"\n                             \"needs to be None.\")\n\n        if (self.distance_threshold is not None\n                and not self.compute_full_tree):\n            raise ValueError(\"compute_full_tree must be True if \"\n                             \"distance_threshold is set.\")\n\n        if self.linkage == \"ward\" and self.affinity != \"euclidean\":\n            raise ValueError(\"%s was provided as affinity. Ward can only \"\n                             \"work with euclidean distances.\" %\n                             (self.affinity, ))\n\n        if self.linkage not in _TREE_BUILDERS:\n            raise ValueError(\"Unknown linkage type %s. \"\n                             \"Valid options are %s\" % (self.linkage,\n                                                       _TREE_BUILDERS.keys()))\n        tree_builder = _TREE_BUILDERS[self.linkage]\n\n        connectivity = self.connectivity\n        if self.connectivity is not None:\n            if callable(self.connectivity):\n                connectivity = self.connectivity(X)\n            connectivity = check_array(\n                connectivity, accept_sparse=['csr', 'coo', 'lil'])\n\n        n_samples = len(X)\n        compute_full_tree = self.compute_full_tree\n        if self.connectivity is None:\n            compute_full_tree = True\n        if compute_full_tree == 'auto':\n            if self.distance_threshold is not None:\n                compute_full_tree = True\n            else:\n                # Early stopping is likely to give a speed up only for\n                # a large number of clusters. The actual threshold\n                # implemented here is heuristic\n                compute_full_tree = self.n_clusters < max(100, .02 * n_samples)\n        n_clusters = self.n_clusters\n        if compute_full_tree:\n            n_clusters = None\n\n        # Construct the tree\n        kwargs = {}\n        if self.linkage != 'ward':\n            kwargs['linkage'] = self.linkage\n            kwargs['affinity'] = self.affinity\n\n        distance_threshold = self.distance_threshold\n\n        return_distance = (\n            (distance_threshold is not None) or self.compute_distances\n        )\n\n        out = memory.cache(tree_builder)(X, connectivity=connectivity,\n                                         n_clusters=n_clusters,\n                                         return_distance=return_distance,\n                                         **kwargs)\n        (self.children_,\n         self.n_connected_components_,\n         self.n_leaves_,\n         parents) = out[:4]\n\n        if return_distance:\n            self.distances_ = out[-1]\n\n        if self.distance_threshold is not None:  # distance_threshold is used\n            self.n_clusters_ = np.count_nonzero(\n                self.distances_ >= distance_threshold) + 1\n        else:  # n_clusters is used\n            self.n_clusters_ = self.n_clusters\n\n        # Cut the tree\n        if compute_full_tree:\n            self.labels_ = _hc_cut(self.n_clusters_, self.children_,\n                                   self.n_leaves_)\n        else:\n            labels = _hierarchical.hc_get_heads(parents, copy=False)\n            # copy to avoid holding a reference on the original array\n            labels = np.copy(labels[:n_samples])\n            # Reassign cluster numbers\n            self.labels_ = np.searchsorted(np.unique(labels), labels)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``affinity='precomputed'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features) or (n_samples, n_samples)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/AgglomerativeClustering/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the hierarchical clustering from features or distance matrix,\nand return cluster labels.",
      "docstring": "Fit the hierarchical clustering from features or distance matrix,\nand return cluster labels.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``affinity='precomputed'``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def fit_predict(self, X, y=None):\n        \"\"\"Fit the hierarchical clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``affinity='precomputed'``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.n_clusters",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of clusters to find. It must be ``None`` if\n``distance_threshold`` is not ``None``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'euclidean'",
            "description": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n\"manhattan\", \"cosine\", or 'precomputed'.\nIf linkage is \"ward\", only \"euclidean\" is accepted."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/memory",
          "name": "memory",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.memory",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or object with the joblib.Memory interface",
            "default_value": "None",
            "description": "Used to cache the output of the computation of the tree.\nBy default, no caching is done. If a string is given, it is the\npath to the caching directory."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "object with the joblib.Memory interface"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.connectivity",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like or callable",
            "default_value": "None",
            "description": "Connectivity matrix. Defines for each feature the neighboring\nfeatures following a given structure of the data.\nThis can be a connectivity matrix itself or a callable that transforms\nthe data into a connectivity matrix, such as derived from\nkneighbors_graph. Default is None, i.e, the\nhierarchical clustering algorithm is unstructured."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/compute_full_tree",
          "name": "compute_full_tree",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.compute_full_tree",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or bool",
            "default_value": "'auto'",
            "description": "Stop early the construction of the tree at n_clusters. This is useful\nto decrease computation time if the number of clusters is not small\ncompared to the number of features. This option is useful only when\nspecifying a connectivity matrix. Note also that when varying the\nnumber of clusters and using caching, it may be advantageous to compute\nthe full tree. It must be ``True`` if ``distance_threshold`` is not\n``None``. By default `compute_full_tree` is \"auto\", which is equivalent\nto `True` when `distance_threshold` is not `None` or that `n_clusters`\nis inferior to the maximum between 100 or `0.02 * n_samples`.\nOtherwise, \"auto\" is equivalent to `False`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/linkage",
          "name": "linkage",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.linkage",
          "default_value": "'ward'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ward', 'complete', 'average', 'single'}",
            "default_value": "'ward'",
            "description": "Which linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of features. The algorithm will merge\nthe pairs of cluster that minimize this criterion.\n\n- ward minimizes the variance of the clusters being merged.\n- average uses the average of the distances of each feature of\n  the two sets.\n- complete or maximum linkage uses the maximum distances between\n  all features of the two sets.\n- single uses the minimum of the distances between all features\n  of the two sets."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "complete",
              "average",
              "single",
              "ward"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/pooling_func",
          "name": "pooling_func",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.pooling_func",
          "default_value": "np.mean",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "np.mean",
            "description": "This combines the values of agglomerated features into a single\nvalue, and should accept an array of shape [M, N] and the keyword\nargument `axis=1`, and reduce it to an array of size [M]."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/distance_threshold",
          "name": "distance_threshold",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.distance_threshold",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The linkage distance threshold above which, clusters will not be\nmerged. If not ``None``, ``n_clusters`` must be ``None`` and\n``compute_full_tree`` must be ``True``.\n\n.. versionadded:: 0.21"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/__init__/compute_distances",
          "name": "compute_distances",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.__init__.compute_distances",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Computes distances between clusters even if `distance_threshold` is not\nused. This can be used to make dendrogram visualization, but introduces\na computational and memory overhead.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=2, *, affinity=\"euclidean\",\n                 memory=None,\n                 connectivity=None, compute_full_tree='auto',\n                 linkage='ward', pooling_func=np.mean,\n                 distance_threshold=None, compute_distances=False):\n        super().__init__(\n            n_clusters=n_clusters, memory=memory, connectivity=connectivity,\n            compute_full_tree=compute_full_tree, linkage=linkage,\n            affinity=affinity, distance_threshold=distance_threshold,\n            compute_distances=compute_distances)\n        self.pooling_func = pooling_func"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit",
      "name": "fit",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit/params",
          "name": "params",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit.params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the hierarchical clustering on the data",
      "docstring": "Fit the hierarchical clustering on the data\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, **params):\n        \"\"\"Fit the hierarchical clustering on the data\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                ensure_min_features=2, estimator=self)\n        # save n_features_in_ attribute here to reset it after, because it will\n        # be overridden in AgglomerativeClustering since we passed it X.T.\n        n_features_in_ = self.n_features_in_\n        AgglomerativeClustering.fit(self, X.T, **params)\n        self.n_features_in_ = n_features_in_\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit_predict@getter",
      "name": "fit_predict",
      "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit_predict",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/FeatureAgglomeration/fit_predict@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._agglomerative.FeatureAgglomeration.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def fit_predict(self):\n        raise AttributeError"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree",
      "name": "linkage_tree",
      "qname": "sklearn.cluster._agglomerative.linkage_tree",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "feature matrix representing n_samples samples to be clustered"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.connectivity",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "sparse matrix",
            "default_value": "None",
            "description": "connectivity matrix. Defines for each sample the neighboring samples\nfollowing a given structure of the data. The matrix is assumed to\nbe symmetric and only the upper triangular half is used.\nDefault is None, i.e, the Ward algorithm is unstructured."
          },
          "type": {
            "kind": "NamedType",
            "name": "sparse matrix"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.n_clusters",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Stop early the construction of the tree at n_clusters. This is\nuseful to decrease computation time if the number of clusters is\nnot small compared to the number of samples. In this case, the\ncomplete tree is not computed, thus the 'children' output is of\nlimited use, and the 'parents' output should rather be used.\nThis option is valid only when specifying a connectivity matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/linkage",
          "name": "linkage",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.linkage",
          "default_value": "'complete'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{\"average\", \"complete\", \"single\"}",
            "default_value": "\"complete\"",
            "description": "Which linkage criteria to use. The linkage criterion determines which\ndistance to use between sets of observation.\n    - average uses the average of the distances of each observation of\n      the two sets\n    - complete or maximum linkage uses the maximum distances between\n      all observations of the two sets.\n    - single uses the minimum of the distances between all observations\n      of the two sets."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "complete",
              "single",
              "average"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.affinity",
          "default_value": "'euclidean'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "\"euclidean\".",
            "description": "which metric to use. Can be \"euclidean\", \"manhattan\", or any\ndistance know to paired distance (see metric.pairwise)"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/linkage_tree/return_distance",
          "name": "return_distance",
          "qname": "sklearn.cluster._agglomerative.linkage_tree.return_distance",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "whether or not to return the distances between the clusters."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Linkage agglomerative clustering based on a Feature matrix.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Linkage agglomerative clustering based on a Feature matrix.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    feature matrix representing n_samples samples to be clustered\n\nconnectivity : sparse matrix, default=None\n    connectivity matrix. Defines for each sample the neighboring samples\n    following a given structure of the data. The matrix is assumed to\n    be symmetric and only the upper triangular half is used.\n    Default is None, i.e, the Ward algorithm is unstructured.\n\nn_clusters : int, default=None\n    Stop early the construction of the tree at n_clusters. This is\n    useful to decrease computation time if the number of clusters is\n    not small compared to the number of samples. In this case, the\n    complete tree is not computed, thus the 'children' output is of\n    limited use, and the 'parents' output should rather be used.\n    This option is valid only when specifying a connectivity matrix.\n\nlinkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n    Which linkage criteria to use. The linkage criterion determines which\n    distance to use between sets of observation.\n        - average uses the average of the distances of each observation of\n          the two sets\n        - complete or maximum linkage uses the maximum distances between\n          all observations of the two sets.\n        - single uses the minimum of the distances between all observations\n          of the two sets.\n\naffinity : str or callable, default=\"euclidean\".\n    which metric to use. Can be \"euclidean\", \"manhattan\", or any\n    distance know to paired distance (see metric.pairwise)\n\nreturn_distance : bool, default=False\n    whether or not to return the distances between the clusters.\n\nReturns\n-------\nchildren : ndarray of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\nn_connected_components : int\n    The number of connected components in the graph.\n\nn_leaves : int\n    The number of leaves in the tree.\n\nparents : ndarray of shape (n_nodes, ) or None\n    The parent of each node. Only returned when a connectivity matrix\n    is specified, elsewhere 'None' is returned.\n\ndistances : ndarray of shape (n_nodes-1,)\n    Returned when return_distance is set to True.\n\n    distances[i] refers to the distance between children[i][0] and\n    children[i][1] when they are merged.\n\nSee Also\n--------\nward_tree : Hierarchical clustering with ward linkage.",
      "code": "def linkage_tree(X, connectivity=None, n_clusters=None, linkage='complete',\n                 affinity=\"euclidean\", return_distance=False):\n    \"\"\"Linkage agglomerative clustering based on a Feature matrix.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        feature matrix representing n_samples samples to be clustered\n\n    connectivity : sparse matrix, default=None\n        connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int, default=None\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the 'children' output is of\n        limited use, and the 'parents' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    linkage : {\"average\", \"complete\", \"single\"}, default=\"complete\"\n        Which linkage criteria to use. The linkage criterion determines which\n        distance to use between sets of observation.\n            - average uses the average of the distances of each observation of\n              the two sets\n            - complete or maximum linkage uses the maximum distances between\n              all observations of the two sets.\n            - single uses the minimum of the distances between all observations\n              of the two sets.\n\n    affinity : str or callable, default=\"euclidean\".\n        which metric to use. Can be \"euclidean\", \"manhattan\", or any\n        distance know to paired distance (see metric.pairwise)\n\n    return_distance : bool, default=False\n        whether or not to return the distances between the clusters.\n\n    Returns\n    -------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : ndarray of shape (n_nodes, ) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Returned when return_distance is set to True.\n\n        distances[i] refers to the distance between children[i][0] and\n        children[i][1] when they are merged.\n\n    See Also\n    --------\n    ward_tree : Hierarchical clustering with ward linkage.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = np.reshape(X, (-1, 1))\n    n_samples, n_features = X.shape\n\n    linkage_choices = {'complete': _hierarchical.max_merge,\n                       'average': _hierarchical.average_merge,\n                       'single': None}  # Single linkage is handled differently\n    try:\n        join_func = linkage_choices[linkage]\n    except KeyError as e:\n        raise ValueError(\n            'Unknown linkage option, linkage should be one '\n            'of %s, but %s was given' % (linkage_choices.keys(), linkage)\n        ) from e\n\n    if affinity == 'cosine' and np.any(~np.any(X, axis=1)):\n        raise ValueError(\n            'Cosine affinity cannot be used when X contains zero vectors')\n\n    if connectivity is None:\n        from scipy.cluster import hierarchy  # imports PIL\n\n        if n_clusters is not None:\n            warnings.warn('Partial build of the tree is implemented '\n                          'only for structured clustering (i.e. with '\n                          'explicit connectivity). The algorithm '\n                          'will build the full tree and only '\n                          'retain the lower branches required '\n                          'for the specified number of clusters',\n                          stacklevel=2)\n\n        if affinity == 'precomputed':\n            # for the linkage function of hierarchy to work on precomputed\n            # data, provide as first argument an ndarray of the shape returned\n            # by sklearn.metrics.pairwise_distances.\n            if X.shape[0] != X.shape[1]:\n                raise ValueError(\n                    'Distance matrix should be square, '\n                    'Got matrix of shape {X.shape}'\n                )\n            i, j = np.triu_indices(X.shape[0], k=1)\n            X = X[i, j]\n        elif affinity == 'l2':\n            # Translate to something understood by scipy\n            affinity = 'euclidean'\n        elif affinity in ('l1', 'manhattan'):\n            affinity = 'cityblock'\n        elif callable(affinity):\n            X = affinity(X)\n            i, j = np.triu_indices(X.shape[0], k=1)\n            X = X[i, j]\n        if (linkage == 'single'\n                and affinity != 'precomputed'\n                and not callable(affinity)\n                and affinity in METRIC_MAPPING):\n\n            # We need the fast cythonized metric from neighbors\n            dist_metric = DistanceMetric.get_metric(affinity)\n\n            # The Cython routines used require contiguous arrays\n            X = np.ascontiguousarray(X, dtype=np.double)\n\n            mst = _hierarchical.mst_linkage_core(X, dist_metric)\n            # Sort edges of the min_spanning_tree by weight\n            mst = mst[np.argsort(mst.T[2], kind='mergesort'), :]\n\n            # Convert edge list into standard hierarchical clustering format\n            out = _hierarchical.single_linkage_label(mst)\n        else:\n            out = hierarchy.linkage(X, method=linkage, metric=affinity)\n        children_ = out[:, :2].astype(int, copy=False)\n\n        if return_distance:\n            distances = out[:, 2]\n            return children_, 1, n_samples, None, distances\n        return children_, 1, n_samples, None\n\n    connectivity, n_connected_components = _fix_connectivity(\n                                                X, connectivity,\n                                                affinity=affinity)\n    connectivity = connectivity.tocoo()\n    # Put the diagonal to zero\n    diag_mask = (connectivity.row != connectivity.col)\n    connectivity.row = connectivity.row[diag_mask]\n    connectivity.col = connectivity.col[diag_mask]\n    connectivity.data = connectivity.data[diag_mask]\n    del diag_mask\n\n    if affinity == 'precomputed':\n        distances = X[connectivity.row, connectivity.col].astype(\n            'float64', **_astype_copy_false(X))\n    else:\n        # FIXME We compute all the distances, while we could have only computed\n        # the \"interesting\" distances\n        distances = paired_distances(X[connectivity.row],\n                                     X[connectivity.col],\n                                     metric=affinity)\n    connectivity.data = distances\n\n    if n_clusters is None:\n        n_nodes = 2 * n_samples - 1\n    else:\n        assert n_clusters <= n_samples\n        n_nodes = 2 * n_samples - n_clusters\n\n    if linkage == 'single':\n        return _single_linkage_tree(connectivity, n_samples, n_nodes,\n                                    n_clusters, n_connected_components,\n                                    return_distance)\n\n    if return_distance:\n        distances = np.empty(n_nodes - n_samples)\n    # create inertia heap and connection matrix\n    A = np.empty(n_nodes, dtype=object)\n    inertia = list()\n\n    # LIL seems to the best format to access the rows quickly,\n    # without the numpy overhead of slicing CSR indices and data.\n    connectivity = connectivity.tolil()\n    # We are storing the graph in a list of IntFloatDict\n    for ind, (data, row) in enumerate(zip(connectivity.data,\n                                          connectivity.rows)):\n        A[ind] = IntFloatDict(np.asarray(row, dtype=np.intp),\n                              np.asarray(data, dtype=np.float64))\n        # We keep only the upper triangular for the heap\n        # Generator expressions are faster than arrays on the following\n        inertia.extend(_hierarchical.WeightedEdge(d, ind, r)\n                       for r, d in zip(row, data) if r < ind)\n    del connectivity\n\n    heapify(inertia)\n\n    # prepare the main fields\n    parent = np.arange(n_nodes, dtype=np.intp)\n    used_node = np.ones(n_nodes, dtype=np.intp)\n    children = []\n\n    # recursive merge loop\n    for k in range(n_samples, n_nodes):\n        # identify the merge\n        while True:\n            edge = heappop(inertia)\n            if used_node[edge.a] and used_node[edge.b]:\n                break\n        i = edge.a\n        j = edge.b\n\n        if return_distance:\n            # store distances\n            distances[k - n_samples] = edge.weight\n\n        parent[i] = parent[j] = k\n        children.append((i, j))\n        # Keep track of the number of elements per cluster\n        n_i = used_node[i]\n        n_j = used_node[j]\n        used_node[k] = n_i + n_j\n        used_node[i] = used_node[j] = False\n\n        # update the structure matrix A and the inertia matrix\n        # a clever 'min', or 'max' operation between A[i] and A[j]\n        coord_col = join_func(A[i], A[j], used_node, n_i, n_j)\n        for col, d in coord_col:\n            A[col].append(k, d)\n            # Here we use the information from coord_col (containing the\n            # distances) to update the heap\n            heappush(inertia, _hierarchical.WeightedEdge(d, k, col))\n        A[k] = coord_col\n        # Clear A[i] and A[j] to save memory\n        A[i] = A[j] = 0\n\n    # Separate leaves in children (empty lists up to now)\n    n_leaves = n_samples\n\n    # # return numpy array for efficient caching\n    children = np.array(children)[:, ::-1]\n\n    if return_distance:\n        return children, n_connected_components, n_leaves, parent, distances\n    return children, n_connected_components, n_leaves, parent"
    },
    {
      "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree",
      "name": "ward_tree",
      "qname": "sklearn.cluster._agglomerative.ward_tree",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/X",
          "name": "X",
          "qname": "sklearn.cluster._agglomerative.ward_tree.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "feature matrix representing n_samples samples to be clustered"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/connectivity",
          "name": "connectivity",
          "qname": "sklearn.cluster._agglomerative.ward_tree.connectivity",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "sparse matrix",
            "default_value": "None",
            "description": "connectivity matrix. Defines for each sample the neighboring samples\nfollowing a given structure of the data. The matrix is assumed to\nbe symmetric and only the upper triangular half is used.\nDefault is None, i.e, the Ward algorithm is unstructured."
          },
          "type": {
            "kind": "NamedType",
            "name": "sparse matrix"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._agglomerative.ward_tree.n_clusters",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Stop early the construction of the tree at n_clusters. This is\nuseful to decrease computation time if the number of clusters is\nnot small compared to the number of samples. In this case, the\ncomplete tree is not computed, thus the 'children' output is of\nlimited use, and the 'parents' output should rather be used.\nThis option is valid only when specifying a connectivity matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._agglomerative/ward_tree/return_distance",
          "name": "return_distance",
          "qname": "sklearn.cluster._agglomerative.ward_tree.return_distance",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "None",
            "description": "If True, return the distance between the clusters."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Ward clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases\nwithin-cluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.",
      "docstring": "Ward clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases\nwithin-cluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    feature matrix representing n_samples samples to be clustered\n\nconnectivity : sparse matrix, default=None\n    connectivity matrix. Defines for each sample the neighboring samples\n    following a given structure of the data. The matrix is assumed to\n    be symmetric and only the upper triangular half is used.\n    Default is None, i.e, the Ward algorithm is unstructured.\n\nn_clusters : int, default=None\n    Stop early the construction of the tree at n_clusters. This is\n    useful to decrease computation time if the number of clusters is\n    not small compared to the number of samples. In this case, the\n    complete tree is not computed, thus the 'children' output is of\n    limited use, and the 'parents' output should rather be used.\n    This option is valid only when specifying a connectivity matrix.\n\nreturn_distance : bool, default=None\n    If True, return the distance between the clusters.\n\nReturns\n-------\nchildren : ndarray of shape (n_nodes-1, 2)\n    The children of each non-leaf node. Values less than `n_samples`\n    correspond to leaves of the tree which are the original samples.\n    A node `i` greater than or equal to `n_samples` is a non-leaf\n    node and has children `children_[i - n_samples]`. Alternatively\n    at the i-th iteration, children[i][0] and children[i][1]\n    are merged to form node `n_samples + i`\n\nn_connected_components : int\n    The number of connected components in the graph.\n\nn_leaves : int\n    The number of leaves in the tree\n\nparents : ndarray of shape (n_nodes,) or None\n    The parent of each node. Only returned when a connectivity matrix\n    is specified, elsewhere 'None' is returned.\n\ndistances : ndarray of shape (n_nodes-1,)\n    Only returned if return_distance is set to True (for compatibility).\n    The distances between the centers of the nodes. `distances[i]`\n    corresponds to a weighted euclidean distance between\n    the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n    leaves of the tree, then `distances[i]` is their unweighted euclidean\n    distance. Distances are updated in the following way\n    (from scipy.hierarchy.linkage):\n\n    The new entry :math:`d(u,v)` is computed as follows,\n\n    .. math::\n\n       d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                           {T}d(v,s)^2\n                    + \\frac{|v|+|t|}\n                           {T}d(v,t)^2\n                    - \\frac{|v|}\n                           {T}d(s,t)^2}\n\n    where :math:`u` is the newly joined cluster consisting of\n    clusters :math:`s` and :math:`t`, :math:`v` is an unused\n    cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n    :math:`|*|` is the cardinality of its argument. This is also\n    known as the incremental algorithm.",
      "code": "@_deprecate_positional_args\ndef ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False):\n    \"\"\"Ward clustering based on a Feature matrix.\n\n    Recursively merges the pair of clusters that minimally increases\n    within-cluster variance.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        feature matrix representing n_samples samples to be clustered\n\n    connectivity : sparse matrix, default=None\n        connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int, default=None\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the 'children' output is of\n        limited use, and the 'parents' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    return_distance : bool, default=None\n        If True, return the distance between the clusters.\n\n    Returns\n    -------\n    children : ndarray of shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    n_connected_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree\n\n    parents : ndarray of shape (n_nodes,) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : ndarray of shape (n_nodes-1,)\n        Only returned if return_distance is set to True (for compatibility).\n        The distances between the centers of the nodes. `distances[i]`\n        corresponds to a weighted euclidean distance between\n        the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n        leaves of the tree, then `distances[i]` is their unweighted euclidean\n        distance. Distances are updated in the following way\n        (from scipy.hierarchy.linkage):\n\n        The new entry :math:`d(u,v)` is computed as follows,\n\n        .. math::\n\n           d(u,v) = \\\\sqrt{\\\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\\\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\\\frac{|v|}\n                               {T}d(s,t)^2}\n\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n    \"\"\"\n    X = np.asarray(X)\n    if X.ndim == 1:\n        X = np.reshape(X, (-1, 1))\n    n_samples, n_features = X.shape\n\n    if connectivity is None:\n        from scipy.cluster import hierarchy  # imports PIL\n\n        if n_clusters is not None:\n            warnings.warn('Partial build of the tree is implemented '\n                          'only for structured clustering (i.e. with '\n                          'explicit connectivity). The algorithm '\n                          'will build the full tree and only '\n                          'retain the lower branches required '\n                          'for the specified number of clusters',\n                          stacklevel=2)\n        X = np.require(X, requirements=\"W\")\n        out = hierarchy.ward(X)\n        children_ = out[:, :2].astype(np.intp)\n\n        if return_distance:\n            distances = out[:, 2]\n            return children_, 1, n_samples, None, distances\n        else:\n            return children_, 1, n_samples, None\n\n    connectivity, n_connected_components = _fix_connectivity(\n                                                X, connectivity,\n                                                affinity='euclidean')\n    if n_clusters is None:\n        n_nodes = 2 * n_samples - 1\n    else:\n        if n_clusters > n_samples:\n            raise ValueError('Cannot provide more clusters than samples. '\n                             '%i n_clusters was asked, and there are %i '\n                             'samples.' % (n_clusters, n_samples))\n        n_nodes = 2 * n_samples - n_clusters\n\n    # create inertia matrix\n    coord_row = []\n    coord_col = []\n    A = []\n    for ind, row in enumerate(connectivity.rows):\n        A.append(row)\n        # We keep only the upper triangular for the moments\n        # Generator expressions are faster than arrays on the following\n        row = [i for i in row if i < ind]\n        coord_row.extend(len(row) * [ind, ])\n        coord_col.extend(row)\n\n    coord_row = np.array(coord_row, dtype=np.intp, order='C')\n    coord_col = np.array(coord_col, dtype=np.intp, order='C')\n\n    # build moments as a list\n    moments_1 = np.zeros(n_nodes, order='C')\n    moments_1[:n_samples] = 1\n    moments_2 = np.zeros((n_nodes, n_features), order='C')\n    moments_2[:n_samples] = X\n    inertia = np.empty(len(coord_row), dtype=np.float64, order='C')\n    _hierarchical.compute_ward_dist(moments_1, moments_2, coord_row, coord_col,\n                                    inertia)\n    inertia = list(zip(inertia, coord_row, coord_col))\n    heapify(inertia)\n\n    # prepare the main fields\n    parent = np.arange(n_nodes, dtype=np.intp)\n    used_node = np.ones(n_nodes, dtype=bool)\n    children = []\n    if return_distance:\n        distances = np.empty(n_nodes - n_samples)\n\n    not_visited = np.empty(n_nodes, dtype=np.int8, order='C')\n\n    # recursive merge loop\n    for k in range(n_samples, n_nodes):\n        # identify the merge\n        while True:\n            inert, i, j = heappop(inertia)\n            if used_node[i] and used_node[j]:\n                break\n        parent[i], parent[j] = k, k\n        children.append((i, j))\n        used_node[i] = used_node[j] = False\n        if return_distance:  # store inertia value\n            distances[k - n_samples] = inert\n\n        # update the moments\n        moments_1[k] = moments_1[i] + moments_1[j]\n        moments_2[k] = moments_2[i] + moments_2[j]\n\n        # update the structure matrix A and the inertia matrix\n        coord_col = []\n        not_visited.fill(1)\n        not_visited[k] = 0\n        _hierarchical._get_parents(A[i], coord_col, parent, not_visited)\n        _hierarchical._get_parents(A[j], coord_col, parent, not_visited)\n        # List comprehension is faster than a for loop\n        [A[col].append(k) for col in coord_col]\n        A.append(coord_col)\n        coord_col = np.array(coord_col, dtype=np.intp, order='C')\n        coord_row = np.empty(coord_col.shape, dtype=np.intp, order='C')\n        coord_row.fill(k)\n        n_additions = len(coord_row)\n        ini = np.empty(n_additions, dtype=np.float64, order='C')\n\n        _hierarchical.compute_ward_dist(moments_1, moments_2,\n                                        coord_row, coord_col, ini)\n\n        # List comprehension is faster than a for loop\n        [heappush(inertia, (ini[idx], k, coord_col[idx]))\n            for idx in range(n_additions)]\n\n    # Separate leaves in children (empty lists up to now)\n    n_leaves = n_samples\n    # sort children to get consistent output with unstructured version\n    children = [c[::-1] for c in children]\n    children = np.array(children)  # return numpy array for efficient caching\n\n    if return_distance:\n        # 2 is scaling factor to compare w/ unstructured version\n        distances = np.sqrt(2. * distances)\n        return children, n_connected_components, n_leaves, parent, distances\n    else:\n        return children, n_connected_components, n_leaves, parent"
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_clusters",
          "default_value": "3",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or tuple (n_row_clusters, n_column_clusters)",
            "default_value": "3",
            "description": "The number of row and column clusters in the checkerboard\nstructure."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "tuple (n_row_clusters, n_column_clusters)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/method",
          "name": "method",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.method",
          "default_value": "'bistochastic'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'bistochastic', 'scale', 'log'}",
            "default_value": "'bistochastic'",
            "description": "Method of normalizing and converting singular vectors into\nbiclusters. May be one of 'scale', 'bistochastic', or 'log'.\nThe authors recommend using 'log'. If the data is sparse,\nhowever, log normalization will not work, which is why the\ndefault is 'bistochastic'.\n\n.. warning::\n   if `method='log'`, the data must be sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "log",
              "scale",
              "bistochastic"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_components",
          "default_value": "6",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "6",
            "description": "Number of singular vectors to check."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_best",
          "name": "n_best",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_best",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of best singular vectors to which to project the data\nfor clustering."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/svd_method",
          "name": "svd_method",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.svd_method",
          "default_value": "'randomized'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'randomized', 'arpack'}",
            "default_value": "'randomized'",
            "description": "Selects the algorithm for finding singular vectors. May be\n'randomized' or 'arpack'. If 'randomized', uses\n:func:`~sklearn.utils.extmath.randomized_svd`, which may be faster\nfor large matrices. If 'arpack', uses\n`scipy.sparse.linalg.svds`, which is more accurate, but\npossibly slower in some cases."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "arpack",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_svd_vecs",
          "name": "n_svd_vecs",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_svd_vecs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of vectors to use in calculating the SVD. Corresponds\nto `ncv` when `svd_method=arpack` and `n_oversamples` when\n`svd_method` is 'randomized`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/mini_batch",
          "name": "mini_batch",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.mini_batch",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use mini-batch k-means, which is faster but may get\ndifferent results."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'} or ndarray of (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization of k-means algorithm; defaults to\n'k-means++'."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "ndarray of (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of random initializations that are tried with the\nk-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is\nchosen and the algorithm runs once. Otherwise, the algorithm\nis run for each initialization and the best solution chosen."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by breaking\ndown the pairwise matrix into n_jobs even slices and computing them in\nparallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralBiclustering/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._bicluster.SpectralBiclustering.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "Used for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, method='bistochastic',\n                 n_components=6, n_best=3, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)\n        self.method = method\n        self.n_components = n_components\n        self.n_best = n_best"
    },
    {
      "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_clusters",
          "default_value": "3",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "The number of biclusters to find."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/svd_method",
          "name": "svd_method",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.svd_method",
          "default_value": "'randomized'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'randomized', 'arpack'}",
            "default_value": "'randomized'",
            "description": "Selects the algorithm for finding singular vectors. May be\n'randomized' or 'arpack'. If 'randomized', use\n:func:`sklearn.utils.extmath.randomized_svd`, which may be faster\nfor large matrices. If 'arpack', use\n:func:`scipy.sparse.linalg.svds`, which is more accurate, but\npossibly slower in some cases."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "arpack",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_svd_vecs",
          "name": "n_svd_vecs",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_svd_vecs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of vectors to use in calculating the SVD. Corresponds\nto `ncv` when `svd_method=arpack` and `n_oversamples` when\n`svd_method` is 'randomized`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/mini_batch",
          "name": "mini_batch",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.mini_batch",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use mini-batch k-means, which is faster but may get\ndifferent results."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random', or ndarray of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization of k-means algorithm; defaults to\n'k-means++'."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "{'k-means++'"
              },
              {
                "kind": "NamedType",
                "name": "'random'"
              },
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of random initializations that are tried with the\nk-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is\nchosen and the algorithm runs once. Otherwise, the algorithm\nis run for each initialization and the best solution chosen."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by breaking\ndown the pairwise matrix into n_jobs even slices and computing them in\nparallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._bicluster/SpectralCoclustering/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._bicluster.SpectralCoclustering.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "Used for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=3, *, svd_method='randomized',\n                 n_svd_vecs=None, mini_batch=False, init='k-means++',\n                 n_init=10, n_jobs='deprecated', random_state=None):\n        super().__init__(n_clusters,\n                         svd_method,\n                         n_svd_vecs,\n                         mini_batch,\n                         init,\n                         n_init,\n                         n_jobs,\n                         random_state)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._birch.Birch.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/threshold",
          "name": "threshold",
          "qname": "sklearn.cluster._birch.Birch.__init__.threshold",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The radius of the subcluster obtained by merging a new sample and the\nclosest subcluster should be lesser than the threshold. Otherwise a new\nsubcluster is started. Setting this value to be very low promotes\nsplitting and vice-versa."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/branching_factor",
          "name": "branching_factor",
          "qname": "sklearn.cluster._birch.Birch.__init__.branching_factor",
          "default_value": "50",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "50",
            "description": "Maximum number of CF subclusters in each node. If a new samples enters\nsuch that the number of subclusters exceed the branching_factor then\nthat node is split into two nodes with the subclusters redistributed\nin each. The parent subcluster of that node is removed and two new\nsubclusters are added as parents of the 2 split nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._birch.Birch.__init__.n_clusters",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, instance of sklearn.cluster model",
            "default_value": "3",
            "description": "Number of clusters after the final clustering step, which treats the\nsubclusters from the leaves as new samples.\n\n- `None` : the final clustering step is not performed and the\n  subclusters are returned as they are.\n\n- :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n  is fit treating the subclusters as new samples and the initial data\n  is mapped to the label of the closest subcluster.\n\n- `int` : the model fit is :class:`AgglomerativeClustering` with\n  `n_clusters` set to be equal to the int."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "instance of sklearn.cluster model"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/compute_labels",
          "name": "compute_labels",
          "qname": "sklearn.cluster._birch.Birch.__init__.compute_labels",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to compute labels for each fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cluster._birch.Birch.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to make a copy of the given data. If set to False,\nthe initial data will be overwritten."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Implements the BIRCH clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an\nalternative to :class:`MiniBatchKMeans`. It constructs a tree\ndata structure with the cluster centroids being read off the leaf.\nThese can be either the final cluster centroids or can be provided as input\nto another clustering algorithm such as :class:`AgglomerativeClustering`.\n\nRead more in the :ref:`User Guide <birch>`.\n\n.. versionadded:: 0.16",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, threshold=0.5, branching_factor=50, n_clusters=3,\n                 compute_labels=True, copy=True):\n        self.threshold = threshold\n        self.branching_factor = branching_factor\n        self.n_clusters = n_clusters\n        self.compute_labels = compute_labels\n        self.copy = copy"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/fit",
      "name": "fit",
      "qname": "sklearn.cluster._birch.Birch.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._birch.Birch.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Build a CF Tree for the input data.",
      "docstring": "Build a CF Tree for the input data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself\n    Fitted estimator.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"\n        Build a CF Tree for the input data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.fit_, self.partial_fit_ = True, False\n        return self._fit(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.cluster._birch.Birch.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.partial_fit.X",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "None",
            "description": "Input data. If X is not provided, only the global clustering\nstep is done."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/partial_fit/y",
          "name": "y",
          "qname": "sklearn.cluster._birch.Birch.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Online learning. Prevents rebuilding of CFTree from scratch.",
      "docstring": "Online learning. Prevents rebuilding of CFTree from scratch.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features),             default=None\n    Input data. If X is not provided, only the global clustering\n    step is done.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself\n    Fitted estimator.",
      "code": "    def partial_fit(self, X=None, y=None):\n        \"\"\"\n        Online learning. Prevents rebuilding of CFTree from scratch.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), \\\n            default=None\n            Input data. If X is not provided, only the global clustering\n            step is done.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        self.partial_fit_, self.fit_ = True, False\n        if X is None:\n            # Perform just the final global clustering step.\n            self._global_clustering()\n            return self\n        else:\n            return self._fit(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/predict",
      "name": "predict",
      "qname": "sklearn.cluster._birch.Birch.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict data using the ``centroids_`` of subclusters.\n\nAvoid computation of the row norms of X.",
      "docstring": "Predict data using the ``centroids_`` of subclusters.\n\nAvoid computation of the row norms of X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nlabels : ndarray of shape(n_samples,)\n    Labelled data.",
      "code": "    def predict(self, X):\n        \"\"\"\n        Predict data using the ``centroids_`` of subclusters.\n\n        Avoid computation of the row norms of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        labels : ndarray of shape(n_samples,)\n            Labelled data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n        kwargs = {'Y_norm_squared': self._subcluster_norms}\n\n        with config_context(assume_finite=True):\n            argmin = pairwise_distances_argmin(X, self.subcluster_centers_,\n                                               metric_kwargs=kwargs)\n        return self.subcluster_labels_[argmin]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._birch/Birch/transform",
      "name": "transform",
      "qname": "sklearn.cluster._birch.Birch.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/transform/self",
          "name": "self",
          "qname": "sklearn.cluster._birch.Birch.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._birch/Birch/transform/X",
          "name": "X",
          "qname": "sklearn.cluster._birch.Birch.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each\ncluster centroid.",
      "docstring": "Transform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each\ncluster centroid.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data.\n\nReturns\n-------\nX_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n    Transformed data.",
      "code": "    def transform(self, X):\n        \"\"\"\n        Transform X into subcluster centroids dimension.\n\n        Each dimension represents the distance from the sample point to each\n        cluster centroid.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        self._validate_data(X, accept_sparse='csr', reset=False)\n        with config_context(assume_finite=True):\n            return euclidean_distances(X, self.subcluster_centers_)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._dbscan.DBSCAN.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/eps",
          "name": "eps",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.eps",
          "default_value": "0.5",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The maximum distance between two samples for one to be considered\nas in the neighborhood of the other. This is not a maximum bound\non the distances of points within a cluster. This is the most\nimportant DBSCAN parameter to choose appropriately for your data set\nand distance function."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.min_samples",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "The number of samples (or total weight) in a neighborhood for a point\nto be considered as a core point. This includes the point itself."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/metric",
          "name": "metric",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.metric",
          "default_value": "'euclidean'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string, or callable",
            "default_value": "'euclidean'",
            "description": "The metric to use when calculating distance between instances in a\nfeature array. If metric is a string or callable, it must be one of\nthe options allowed by :func:`sklearn.metrics.pairwise_distances` for\nits metric parameter.\nIf metric is \"precomputed\", X is assumed to be a distance matrix and\nmust be square. X may be a :term:`Glossary <sparse graph>`, in which\ncase only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n.. versionadded:: 0.17\n   metric *precomputed* to accept precomputed sparse matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "string"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.metric_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "The algorithm to be used by the NearestNeighbors module\nto compute pointwise distances and find nearest neighbors.\nSee NearestNeighbors module documentation for details."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "brute",
              "auto",
              "ball_tree",
              "kd_tree"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.leaf_size",
          "default_value": "30",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to BallTree or cKDTree. This can affect the speed\nof the construction and query, as well as the memory required\nto store the tree. The optimal value depends\non the nature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/p",
          "name": "p",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.p",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The power of the Minkowski metric to be used to calculate distance\nbetween points. If None, then ``p=2`` (equivalent to the Euclidean\ndistance)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._dbscan.DBSCAN.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, eps=0.5, *, min_samples=5, metric='euclidean',\n                 metric_params=None, algorithm='auto', leaf_size=30, p=None,\n                 n_jobs=None):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.metric = metric\n        self.metric_params = metric_params\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.p = p\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit",
      "name": "fit",
      "qname": "sklearn.cluster._dbscan.DBSCAN.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``metric='precomputed'``. If a sparse matrix is provided, it will\nbe converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with a\nnegative weight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform DBSCAN clustering from features, or distance matrix.",
      "docstring": "Perform DBSCAN clustering from features, or distance matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``metric='precomputed'``. If a sparse matrix is provided, it will\n    be converted into a sparse ``csr_matrix``.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with a\n    negative weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr')\n\n        if not self.eps > 0.0:\n            raise ValueError(\"eps must be positive.\")\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        # Calculate neighborhood for all samples. This leaves the original\n        # point in, which needs to be considered later (i.e. point i is in the\n        # neighborhood of point i. While True, its useless information)\n        if self.metric == 'precomputed' and sparse.issparse(X):\n            # set the diagonal to explicit values, as a point is its own\n            # neighbor\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n                X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n\n        neighbors_model = NearestNeighbors(\n            radius=self.eps, algorithm=self.algorithm,\n            leaf_size=self.leaf_size, metric=self.metric,\n            metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n        neighbors_model.fit(X)\n        # This has worst case O(n^2) memory complexity\n        neighborhoods = neighbors_model.radius_neighbors(X,\n                                                         return_distance=False)\n\n        if sample_weight is None:\n            n_neighbors = np.array([len(neighbors)\n                                    for neighbors in neighborhoods])\n        else:\n            n_neighbors = np.array([np.sum(sample_weight[neighbors])\n                                    for neighbors in neighborhoods])\n\n        # Initially, all samples are noise.\n        labels = np.full(X.shape[0], -1, dtype=np.intp)\n\n        # A list of all core samples found.\n        core_samples = np.asarray(n_neighbors >= self.min_samples,\n                                  dtype=np.uint8)\n        dbscan_inner(core_samples, neighborhoods, labels)\n\n        self.core_sample_indices_ = np.where(core_samples)[0]\n        self.labels_ = labels\n\n        if len(self.core_sample_indices_):\n            # fix for scipy sparse indexing issue\n            self.components_ = X[self.core_sample_indices_].copy()\n        else:\n            # no core samples\n            self.components_ = np.empty((0, X.shape[1]))\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, or distances between instances if\n``metric='precomputed'``. If a sparse matrix is provided, it will\nbe converted into a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/DBSCAN/fit_predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._dbscan.DBSCAN.fit_predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with a\nnegative weight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform DBSCAN clustering from features or distance matrix,\nand return cluster labels.",
      "docstring": "Perform DBSCAN clustering from features or distance matrix,\nand return cluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features), or             (n_samples, n_samples)\n    Training instances to cluster, or distances between instances if\n    ``metric='precomputed'``. If a sparse matrix is provided, it will\n    be converted into a sparse ``csr_matrix``.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with a\n    negative weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels. Noisy samples are given the label -1.",
      "code": "    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features or distance matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.\n        \"\"\"\n        self.fit(X, sample_weight=sample_weight)\n        return self.labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._dbscan/dbscan",
      "name": "dbscan",
      "qname": "sklearn.cluster._dbscan.dbscan",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/X",
          "name": "X",
          "qname": "sklearn.cluster._dbscan.dbscan.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)",
            "default_value": "",
            "description": "A feature array, or array of distances between samples if\n``metric='precomputed'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/eps",
          "name": "eps",
          "qname": "sklearn.cluster._dbscan.dbscan.eps",
          "default_value": "0.5",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The maximum distance between two samples for one to be considered\nas in the neighborhood of the other. This is not a maximum bound\non the distances of points within a cluster. This is the most\nimportant DBSCAN parameter to choose appropriately for your data set\nand distance function."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._dbscan.dbscan.min_samples",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "The number of samples (or total weight) in a neighborhood for a point\nto be considered as a core point. This includes the point itself."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/metric",
          "name": "metric",
          "qname": "sklearn.cluster._dbscan.dbscan.metric",
          "default_value": "'minkowski'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'minkowski'",
            "description": "The metric to use when calculating distance between instances in a\nfeature array. If metric is a string or callable, it must be one of\nthe options allowed by :func:`sklearn.metrics.pairwise_distances` for\nits metric parameter.\nIf metric is \"precomputed\", X is assumed to be a distance matrix and\nmust be square during fit.\nX may be a :term:`sparse graph <sparse graph>`,\nin which case only \"nonzero\" elements may be considered neighbors."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._dbscan.dbscan.metric_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._dbscan.dbscan.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "The algorithm to be used by the NearestNeighbors module\nto compute pointwise distances and find nearest neighbors.\nSee NearestNeighbors module documentation for details."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "brute",
              "auto",
              "ball_tree",
              "kd_tree"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._dbscan.dbscan.leaf_size",
          "default_value": "30",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to BallTree or cKDTree. This can affect the speed\nof the construction and query, as well as the memory required\nto store the tree. The optimal value depends\non the nature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/p",
          "name": "p",
          "qname": "sklearn.cluster._dbscan.dbscan.p",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "2",
            "description": "The power of the Minkowski metric to be used to calculate distance\nbetween points."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._dbscan.dbscan.sample_weight",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Weight of each sample, such that a sample with a weight of at least\n``min_samples`` is by itself a core sample; a sample with negative\nweight may inhibit its eps-neighbor from being core.\nNote that weights are absolute, and default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._dbscan/dbscan/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._dbscan.dbscan.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search. ``None`` means\n1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\nusing all processors. See :term:`Glossary <n_jobs>` for more details.\nIf precomputed distance are used, parallel execution is not available\nand thus n_jobs will have no effect."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.",
      "docstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters\n----------\nX : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or             (n_samples, n_samples)\n    A feature array, or array of distances between samples if\n    ``metric='precomputed'``.\n\neps : float, default=0.5\n    The maximum distance between two samples for one to be considered\n    as in the neighborhood of the other. This is not a maximum bound\n    on the distances of points within a cluster. This is the most\n    important DBSCAN parameter to choose appropriately for your data set\n    and distance function.\n\nmin_samples : int, default=5\n    The number of samples (or total weight) in a neighborhood for a point\n    to be considered as a core point. This includes the point itself.\n\nmetric : str or callable, default='minkowski'\n    The metric to use when calculating distance between instances in a\n    feature array. If metric is a string or callable, it must be one of\n    the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n    its metric parameter.\n    If metric is \"precomputed\", X is assumed to be a distance matrix and\n    must be square during fit.\n    X may be a :term:`sparse graph <sparse graph>`,\n    in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\n    .. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    The algorithm to be used by the NearestNeighbors module\n    to compute pointwise distances and find nearest neighbors.\n    See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30\n    Leaf size passed to BallTree or cKDTree. This can affect the speed\n    of the construction and query, as well as the memory required\n    to store the tree. The optimal value depends\n    on the nature of the problem.\n\np : float, default=2\n    The power of the Minkowski metric to be used to calculate distance\n    between points.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Weight of each sample, such that a sample with a weight of at least\n    ``min_samples`` is by itself a core sample; a sample with negative\n    weight may inhibit its eps-neighbor from being core.\n    Note that weights are absolute, and default to 1.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search. ``None`` means\n    1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n    using all processors. See :term:`Glossary <n_jobs>` for more details.\n    If precomputed distance are used, parallel execution is not available\n    and thus n_jobs will have no effect.\n\nReturns\n-------\ncore_samples : ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.  Noisy samples are given the label -1.\n\nSee Also\n--------\nDBSCAN : An estimator interface for this clustering algorithm.\nOPTICS : A similar estimator interface clustering at multiple values of\n    eps. Our implementation is optimized for memory usage.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_dbscan.py\n<sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending\non the ``algorithm``.\n\nOne way to avoid the query complexity is to pre-compute sparse\nneighborhoods in chunks using\n:func:`NearestNeighbors.radius_neighbors_graph\n<sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n``mode='distance'``, then using ``metric='precomputed'`` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use ``sample_weight`` instead.\n\n:func:`cluster.optics <sklearn.cluster.optics>` provides a similar\nclustering with lower memory usage.\n\nReferences\n----------\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\nAlgorithm for Discovering Clusters in Large Spatial Databases with Noise\".\nIn: Proceedings of the 2nd International Conference on Knowledge Discovery\nand Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.",
      "code": "@_deprecate_positional_args\ndef dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski',\n           metric_params=None, algorithm='auto', leaf_size=30, p=2,\n           sample_weight=None, n_jobs=None):\n    \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or \\\n            (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric='precomputed'``.\n\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : str or callable, default='minkowski'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit.\n        X may be a :term:`sparse graph <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=2\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weight of each sample, such that a sample with a weight of at least\n        ``min_samples`` is by itself a core sample; a sample with negative\n        weight may inhibit its eps-neighbor from being core.\n        Note that weights are absolute, and default to 1.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. ``None`` means\n        1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n        using all processors. See :term:`Glossary <n_jobs>` for more details.\n        If precomputed distance are used, parallel execution is not available\n        and thus n_jobs will have no effect.\n\n    Returns\n    -------\n    core_samples : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    See Also\n    --------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :func:`cluster.optics <sklearn.cluster.optics>` provides a similar\n    clustering with lower memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    \"\"\"\n\n    est = DBSCAN(eps=eps, min_samples=min_samples, metric=metric,\n                 metric_params=metric_params, algorithm=algorithm,\n                 leaf_size=leaf_size, p=p, n_jobs=n_jobs)\n    est.fit(X, sample_weight=sample_weight)\n    return est.core_sample_indices_, est.labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._kmeans.KMeans.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.n_clusters",
          "default_value": "8",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "8",
            "description": "The number of clusters to form as well as the number of\ncentroids to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization:\n\n'k-means++' : selects initial cluster centers for k-mean\nclustering in a smart way to speed up convergence. See section\nNotes in k_init for more details.\n\n'random': choose `n_clusters` observations (rows) at random from data\nfor the initial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features)\nand gives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a\nrandom state and return an initialization."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of\nn_init consecutive runs in terms of inertia."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations of the k-means algorithm for a\nsingle run."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Relative tolerance with regards to Frobenius norm of the difference\nin the cluster centers of two consecutive iterations to declare\nconvergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/precompute_distances",
          "name": "precompute_distances",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.precompute_distances",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', True, False}",
            "default_value": "'auto'",
            "description": "Precompute distances (faster but takes more memory).\n\n'auto' : do not precompute distances if n_samples * n_clusters > 12\nmillion. This corresponds to about 100MB overhead per job using\ndouble precision.\n\nTrue : always precompute distances.\n\nFalse : never precompute distances.\n\n.. deprecated:: 0.23\n    'precompute_distances' was deprecated in version 0.22 and will be\n    removed in 1.0 (renaming of 0.25). It has no effect."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity mode."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization. Use\nan int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/copy_x",
          "name": "copy_x",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.copy_x",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "When pre-computing distances it is more numerically accurate to center\nthe data first. If copy_x is True (default), then the original data is\nnot modified. If False, the original data is modified, and put back\nbefore the function returns, but small numerical differences may be\nintroduced by subtracting and then adding the data mean. Note that if\nthe original data is not C-contiguous, a copy will be made even if\ncopy_x is False. If the original data is sparse, but not in CSR format,\na copy will be made even if copy_x is False."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its\nclosest center.\n\n``None`` or ``-1`` means using all processors.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._kmeans.KMeans.__init__.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"full\", \"elkan\"}",
            "default_value": "\"auto\"",
            "description": "K-means algorithm to use. The classical EM-style algorithm is \"full\".\nThe \"elkan\" variation is more efficient on data with well-defined\nclusters, by using the triangle inequality. However it's more memory\nintensive due to the allocation of an extra array of shape\n(n_samples, n_clusters).\n\nFor now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\nmight change in the future for a better heuristic.\n\n.. versionchanged:: 0.18\n    Added Elkan algorithm"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto",
              "elkan",
              "full"
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "K-Means clustering.\n\nRead more in the :ref:`User Guide <k_means>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', n_init=10,\n                 max_iter=300, tol=1e-4, precompute_distances='deprecated',\n                 verbose=0, random_state=None, copy_x=True,\n                 n_jobs='deprecated', algorithm='auto'):\n\n        self.n_clusters = n_clusters\n        self.init = init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.precompute_distances = precompute_distances\n        self.n_init = n_init\n        self.verbose = verbose\n        self.random_state = random_state\n        self.copy_x = copy_x\n        self.n_jobs = n_jobs\n        self.algorithm = algorithm"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit",
      "name": "fit",
      "qname": "sklearn.cluster._kmeans.KMeans.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training instances to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory\ncopy if the given data is not C-contiguous.\nIf a sparse matrix is passed, a copy will be made if it's not in\nCSR format."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute k-means clustering.",
      "docstring": "Compute k-means clustering.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory\n    copy if the given data is not C-contiguous.\n    If a sparse matrix is passed, a copy will be made if it's not in\n    CSR format.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself\n    Fitted estimator.",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute k-means clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory\n            copy if the given data is not C-contiguous.\n            If a sparse matrix is passed, a copy will be made if it's not in\n            CSR format.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', copy=self.copy_x,\n                                accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        # subtract of mean of x for more accurate distance computations\n        if not sp.issparse(X):\n            X_mean = X.mean(axis=0)\n            # The copy was already done above\n            X -= X_mean\n\n            if hasattr(init, '__array__'):\n                init -= X_mean\n\n        # precompute squared norms of data points\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self._algorithm == \"full\":\n            kmeans_single = _kmeans_single_lloyd\n            self._check_mkl_vcomp(X, X.shape[0])\n        else:\n            kmeans_single = _kmeans_single_elkan\n\n        best_inertia = None\n\n        for i in range(self._n_init):\n            # Initialize centers\n            centers_init = self._init_centroids(\n                X, x_squared_norms=x_squared_norms, init=init,\n                random_state=random_state)\n            if self.verbose:\n                print(\"Initialization complete\")\n\n            # run a k-means once\n            labels, inertia, centers, n_iter_ = kmeans_single(\n                X, sample_weight, centers_init, max_iter=self.max_iter,\n                verbose=self.verbose, tol=self._tol,\n                x_squared_norms=x_squared_norms, n_threads=self._n_threads)\n\n            # determine if these results are the best so far\n            if best_inertia is None or inertia < best_inertia:\n                best_labels = labels\n                best_centers = centers\n                best_inertia = inertia\n                best_n_iter = n_iter_\n\n        if not sp.issparse(X):\n            if not self.copy_x:\n                X += X_mean\n            best_centers += X_mean\n\n        distinct_clusters = len(set(best_labels))\n        if distinct_clusters < self.n_clusters:\n            warnings.warn(\n                \"Number of distinct clusters ({}) found smaller than \"\n                \"n_clusters ({}). Possibly due to duplicate points \"\n                \"in X.\".format(distinct_clusters, self.n_clusters),\n                ConvergenceWarning, stacklevel=2)\n\n        self.cluster_centers_ = best_centers\n        self.labels_ = best_labels\n        self.inertia_ = best_inertia\n        self.n_iter_ = best_n_iter\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._kmeans.KMeans.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to transform."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).",
      "docstring": "Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute cluster centers and predict cluster index for each sample.\n\n        Convenience method; equivalent to calling fit(X) followed by\n        predict(X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        return self.fit(X, sample_weight=sample_weight).labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.cluster._kmeans.KMeans.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to transform."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/fit_transform/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.fit_transform.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.",
      "docstring": "Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_clusters)\n    X transformed in the new space.",
      "code": "    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clustering and transform X to cluster-distance space.\n\n        Equivalent to fit(X).transform(X), but more efficiently implemented.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        # Currently, this just skips a copy of the data if it is not in\n        # np.array or CSR format already.\n        # XXX This skips _check_test_data, which may change the dtype;\n        # we should refactor the input validation.\n        return self.fit(X, sample_weight=sample_weight)._transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict",
      "name": "predict",
      "qname": "sklearn.cluster._kmeans.KMeans.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return _labels_inertia(X, sample_weight, x_squared_norms,\n                               self.cluster_centers_, self._n_threads)[0]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score",
      "name": "score",
      "qname": "sklearn.cluster._kmeans.KMeans.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.KMeans.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/score/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.KMeans.score.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Opposite of the value of X on the K-means objective.",
      "docstring": "Opposite of the value of X on the K-means objective.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\nReturns\n-------\nscore : float\n    Opposite of the value of X on the K-means objective.",
      "code": "    def score(self, X, y=None, sample_weight=None):\n        \"\"\"Opposite of the value of X on the K-means objective.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight.\n\n        Returns\n        -------\n        score : float\n            Opposite of the value of X on the K-means objective.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        x_squared_norms = row_norms(X, squared=True)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        return -_labels_inertia(X, sample_weight, x_squared_norms,\n                                self.cluster_centers_)[1]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/transform",
      "name": "transform",
      "qname": "sklearn.cluster._kmeans.KMeans.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/transform/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.KMeans.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/KMeans/transform/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.KMeans.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to transform."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters. Note that even if X is sparse, the array returned by\n`transform` will typically be dense.",
      "docstring": "Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters. Note that even if X is sparse, the array returned by\n`transform` will typically be dense.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to transform.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_clusters)\n    X transformed in the new space.",
      "code": "    def transform(self, X):\n        \"\"\"Transform X to a cluster-distance space.\n\n        In the new space, each dimension is the distance to the cluster\n        centers. Note that even if X is sparse, the array returned by\n        `transform` will typically be dense.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to transform.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_clusters)\n            X transformed in the new space.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.n_clusters",
          "default_value": "8",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "8",
            "description": "The number of clusters to form as well as the number of\ncentroids to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/init",
          "name": "init",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization:\n\n'k-means++' : selects initial cluster centers for k-mean\nclustering in a smart way to speed up convergence. See section\nNotes in k_init for more details.\n\n'random': choose `n_clusters` observations (rows) at random from data\nfor the initial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features)\nand gives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a\nrandom state and return an initialization."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum number of iterations over the complete dataset before\nstopping independently of any early stopping criterion heuristics."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/batch_size",
          "name": "batch_size",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.batch_size",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Size of the mini batches."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity mode."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/compute_labels",
          "name": "compute_labels",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.compute_labels",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Compute label assignment and inertia for the complete dataset\nonce the minibatch optimization has converged in fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization and\nrandom reassignment. Use an int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Control early stopping based on the relative center changes as\nmeasured by a smoothed, variance-normalized of the mean center\nsquared position changes. This early stopping heuristics is\ncloser to the one used for the batch variant of the algorithms\nbut induces a slight computational and memory overhead over the\ninertia heuristic.\n\nTo disable convergence detection based on normalized center\nchange, set tol to 0.0 (default)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/max_no_improvement",
          "name": "max_no_improvement",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.max_no_improvement",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Control early stopping based on the consecutive number of mini\nbatches that does not yield an improvement on the smoothed inertia.\n\nTo disable convergence detection based on inertia, set\nmax_no_improvement to None."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/init_size",
          "name": "init_size",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.init_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of samples to randomly sample for speeding up the\ninitialization (sometimes at the expense of accuracy): the\nonly algorithm is initialized by running a batch KMeans on a\nrandom subset of the data. This needs to be larger than n_clusters.\n\nIf `None`, `init_size= 3 * batch_size`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.n_init",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of random initializations that are tried.\nIn contrast to KMeans, the algorithm is only run once, using the\nbest of the ``n_init`` initializations as measured by inertia."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/__init__/reassignment_ratio",
          "name": "reassignment_ratio",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.__init__.reassignment_ratio",
          "default_value": "0.01",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "Control the fraction of the maximum number of counts for a\ncenter to be reassigned. A higher value means that low count\ncenters are more easily reassigned, which means that the\nmodel will take longer to converge, but should converge in a\nbetter clustering."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Mini-Batch K-Means clustering.\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, init='k-means++', max_iter=100,\n                 batch_size=100, verbose=0, compute_labels=True,\n                 random_state=None, tol=0.0, max_no_improvement=10,\n                 init_size=None, n_init=3, reassignment_ratio=0.01):\n\n        super().__init__(\n            n_clusters=n_clusters, init=init, max_iter=max_iter,\n            verbose=verbose, random_state=random_state, tol=tol, n_init=n_init)\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.compute_labels = compute_labels\n        self.init_size = init_size\n        self.reassignment_ratio = reassignment_ratio"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/counts_@getter",
      "name": "counts_",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.counts_",
      "decorators": [
        "deprecated(\"The attribute 'counts_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/counts_@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.counts_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'counts_' is deprecated in 0.24\"  # type: ignore\n                \" and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def counts_(self):\n        return self._counts"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit",
      "name": "fit",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training instances to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory copy\nif the given data is not C-contiguous."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None).\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the centroids on X by chunking it into mini-batches.",
      "docstring": "Compute the centroids on X by chunking it into mini-batches.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training instances to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory copy\n    if the given data is not C-contiguous.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\n    .. versionadded:: 0.20\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training instances to cluster. It must be noted that the data\n            will be converted to C ordering, which will cause a memory copy\n            if the given data is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n            .. versionadded:: 0.20\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False)\n\n        self._check_params(X)\n        random_state = check_random_state(self.random_state)\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Validate init array\n        init = self.init\n        if hasattr(init, '__array__'):\n            init = check_array(init, dtype=X.dtype, copy=True, order='C')\n            self._validate_center_shape(X, init)\n\n        n_samples, n_features = X.shape\n        x_squared_norms = row_norms(X, squared=True)\n\n        if self.tol > 0.0:\n            tol = _tolerance(X, self.tol)\n\n            # using tol-based early stopping needs the allocation of a\n            # dedicated before which can be expensive for high dim data:\n            # hence we allocate it outside of the main loop\n            old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n        else:\n            tol = 0.0\n            # no need for the center buffer if tol-based early stopping is\n            # disabled\n            old_center_buffer = np.zeros(0, dtype=X.dtype)\n\n        distances = np.zeros(self.batch_size, dtype=X.dtype)\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n        n_iter = int(self.max_iter * n_batches)\n\n        self._check_mkl_vcomp(X, self.batch_size)\n\n        validation_indices = random_state.randint(0, n_samples,\n                                                  self._init_size)\n        X_valid = X[validation_indices]\n        sample_weight_valid = sample_weight[validation_indices]\n        x_squared_norms_valid = x_squared_norms[validation_indices]\n\n        # perform several inits with random sub-sets\n        best_inertia = None\n        for init_idx in range(self._n_init):\n            if self.verbose:\n                print(\"Init %d/%d with method: %s\"\n                      % (init_idx + 1, self._n_init, init))\n            weight_sums = np.zeros(self.n_clusters, dtype=sample_weight.dtype)\n\n            # TODO: once the `k_means` function works with sparse input we\n            # should refactor the following init to use it instead.\n\n            # Initialize the centers using only a fraction of the data as we\n            # expect n_samples to be very large when using MiniBatchKMeans\n            cluster_centers = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=random_state,\n                init_size=self._init_size)\n\n            # Compute the label assignment on the init dataset\n            _mini_batch_step(\n                X_valid, sample_weight_valid,\n                x_squared_norms[validation_indices], cluster_centers,\n                weight_sums, old_center_buffer, False, distances=None,\n                verbose=self.verbose)\n\n            # Keep only the best cluster centers across independent inits on\n            # the common validation set\n            _, inertia = _labels_inertia(X_valid, sample_weight_valid,\n                                         x_squared_norms_valid,\n                                         cluster_centers)\n            if self.verbose:\n                print(\"Inertia for init %d/%d: %f\"\n                      % (init_idx + 1, self._n_init, inertia))\n            if best_inertia is None or inertia < best_inertia:\n                self.cluster_centers_ = cluster_centers\n                self._counts = weight_sums\n                best_inertia = inertia\n\n        # Empty context to be used inplace by the convergence check routine\n        convergence_context = {}\n\n        # Perform the iterative optimization until the final convergence\n        # criterion\n        for iteration_idx in range(n_iter):\n            # Sample a minibatch from the full dataset\n            minibatch_indices = random_state.randint(\n                0, n_samples, self.batch_size)\n\n            # Perform the actual update step on the minibatch data\n            batch_inertia, centers_squared_diff = _mini_batch_step(\n                X[minibatch_indices], sample_weight[minibatch_indices],\n                x_squared_norms[minibatch_indices],\n                self.cluster_centers_, self._counts,\n                old_center_buffer, tol > 0.0, distances=distances,\n                # Here we randomly choose whether to perform\n                # random reassignment: the choice is done as a function\n                # of the iteration index, and the minimum number of\n                # counts, in order to force this reassignment to happen\n                # every once in a while\n                random_reassign=((iteration_idx + 1)\n                                 % (10 + int(self._counts.min())) == 0),\n                random_state=random_state,\n                reassignment_ratio=self.reassignment_ratio,\n                verbose=self.verbose)\n\n            # Monitor convergence and do early stopping if necessary\n            if _mini_batch_convergence(\n                    self, iteration_idx, n_iter, tol, n_samples,\n                    centers_squared_diff, batch_inertia, convergence_context,\n                    verbose=self.verbose):\n                break\n\n        self.n_iter_ = iteration_idx + 1\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = \\\n                    self._labels_inertia_minibatch(X, sample_weight)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/init_size_@getter",
      "name": "init_size_",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.init_size_",
      "decorators": [
        "deprecated(\"The attribute 'init_size_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/init_size_@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.init_size_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'init_size_' is deprecated in \"  # type: ignore\n                \"0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def init_size_(self):\n        return self._init_size"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Coordinates of the data points to cluster. It must be noted that\nX will be copied if it is not C-contiguous."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/y",
          "name": "y",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/partial_fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Update k means estimate on a single mini-batch X.",
      "docstring": "Update k means estimate on a single mini-batch X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Coordinates of the data points to cluster. It must be noted that\n    X will be copied if it is not C-contiguous.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\nReturns\n-------\nself",
      "code": "    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Update k means estimate on a single mini-batch X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Coordinates of the data points to cluster. It must be noted that\n            X will be copied if it is not C-contiguous.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        self\n        \"\"\"\n        is_first_call_to_partial_fit = not hasattr(self, 'cluster_centers_')\n\n        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                order='C', accept_large_sparse=False,\n                                reset=is_first_call_to_partial_fit)\n\n        self._random_state = getattr(self, \"_random_state\",\n                                     check_random_state(self.random_state))\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        x_squared_norms = row_norms(X, squared=True)\n\n        if is_first_call_to_partial_fit:\n            # this is the first call to partial_fit on this object\n            self._check_params(X)\n\n            # Validate init array\n            init = self.init\n            if hasattr(init, '__array__'):\n                init = check_array(init, dtype=X.dtype, copy=True, order='C')\n                self._validate_center_shape(X, init)\n\n            self._check_mkl_vcomp(X, X.shape[0])\n\n            # initialize the cluster centers\n            self.cluster_centers_ = self._init_centroids(\n                X, x_squared_norms=x_squared_norms,\n                init=init,\n                random_state=self._random_state,\n                init_size=self._init_size)\n\n            self._counts = np.zeros(self.n_clusters,\n                                    dtype=sample_weight.dtype)\n            random_reassign = False\n            distances = None\n        else:\n            # The lower the minimum count is, the more we do random\n            # reassignment, however, we don't want to do random\n            # reassignment too often, to allow for building up counts\n            random_reassign = self._random_state.randint(\n                10 * (1 + self._counts.min())) == 0\n            distances = np.zeros(X.shape[0], dtype=X.dtype)\n\n        _mini_batch_step(X, sample_weight, x_squared_norms,\n                         self.cluster_centers_, self._counts,\n                         np.zeros(0, dtype=X.dtype), 0,\n                         random_reassign=random_reassign, distances=distances,\n                         random_state=self._random_state,\n                         reassignment_ratio=self.reassignment_ratio,\n                         verbose=self.verbose)\n\n        if self.compute_labels:\n            self.labels_, self.inertia_ = _labels_inertia(\n                X, sample_weight, x_squared_norms, self.cluster_centers_)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict",
      "name": "predict",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/predict/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.predict.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight (default: None)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight (default: None).\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def predict(self, X, sample_weight=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            The weights for each observation in X. If None, all observations\n            are assigned equal weight (default: None).\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._check_test_data(X)\n        return self._labels_inertia_minibatch(X, sample_weight)[0]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/random_state_@getter",
      "name": "random_state_",
      "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.random_state_",
      "decorators": [
        "deprecated(\"The attribute 'random_state_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/MiniBatchKMeans/random_state_@getter/self",
          "name": "self",
          "qname": "sklearn.cluster._kmeans.MiniBatchKMeans.random_state_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'random_state_' is deprecated \"  # type: ignore\n                \"in 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def random_state_(self):\n        return getattr(self, \"_random_state\", None)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/k_means",
      "name": "k_means",
      "qname": "sklearn.cluster._kmeans.k_means",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.k_means.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The observations to cluster. It must be noted that the data\nwill be converted to C ordering, which will cause a memory copy\nif the given data is not C-contiguous."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.k_means.n_clusters",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "The number of clusters to form as well as the number of\ncentroids to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.cluster._kmeans.k_means.sample_weight",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The weights for each observation in X. If None, all observations\nare assigned equal weight."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/init",
          "name": "init",
          "qname": "sklearn.cluster._kmeans.k_means.init",
          "default_value": "'k-means++'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features)",
            "default_value": "'k-means++'",
            "description": "Method for initialization:\n\n'k-means++' : selects initial cluster centers for k-mean\nclustering in a smart way to speed up convergence. See section\nNotes in k_init for more details.\n\n'random': choose `n_clusters` observations (rows) at random from data\nfor the initial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features)\nand gives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a\nrandom state and return an initialization."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "random",
                  "k-means++"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_clusters, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/precompute_distances",
          "name": "precompute_distances",
          "qname": "sklearn.cluster._kmeans.k_means.precompute_distances",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', True, False}",
            "default_value": "",
            "description": "Precompute distances (faster but takes more memory).\n\n'auto' : do not precompute distances if n_samples * n_clusters > 12\nmillion. This corresponds to about 100MB overhead per job using\ndouble precision.\n\nTrue : always precompute distances\n\nFalse : never precompute distances\n\n.. deprecated:: 0.23\n    'precompute_distances' was deprecated in version 0.23 and will be\n    removed in 1.0 (renaming of 0.25). It has no effect."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._kmeans.k_means.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of\nn_init consecutive runs in terms of inertia."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._kmeans.k_means.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations of the k-means algorithm to run."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._kmeans.k_means.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Verbosity mode."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/tol",
          "name": "tol",
          "qname": "sklearn.cluster._kmeans.k_means.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Relative tolerance with regards to Frobenius norm of the difference\nin the cluster centers of two consecutive iterations to declare\nconvergence."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.k_means.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization. Use\nan int to make the randomness deterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/copy_x",
          "name": "copy_x",
          "qname": "sklearn.cluster._kmeans.k_means.copy_x",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "When pre-computing distances it is more numerically accurate to center\nthe data first. If copy_x is True (default), then the original data is\nnot modified. If False, the original data is modified, and put back\nbefore the function returns, but small numerical differences may be\nintroduced by subtracting and then adding the data mean. Note that if\nthe original data is not C-contiguous, a copy will be made even if\ncopy_x is False. If the original data is sparse, but not in CSR format,\na copy will be made even if copy_x is False."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._kmeans.k_means.n_jobs",
          "default_value": "'deprecated'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its\nclosest center.\n\n``None`` or ``-1`` means using all processors.\n\n.. deprecated:: 0.23\n    ``n_jobs`` was deprecated in version 0.23 and will be removed in\n    1.0 (renaming of 0.25)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._kmeans.k_means.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"full\", \"elkan\"}",
            "default_value": "\"auto\"",
            "description": "K-means algorithm to use. The classical EM-style algorithm is \"full\".\nThe \"elkan\" variation is more efficient on data with well-defined\nclusters, by using the triangle inequality. However it's more memory\nintensive due to the allocation of an extra array of shape\n(n_samples, n_clusters).\n\nFor now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\nmight change in the future for a better heuristic."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto",
              "elkan",
              "full"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/k_means/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.cluster._kmeans.k_means.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "K-means clustering algorithm.\n\nRead more in the :ref:`User Guide <k_means>`.",
      "docstring": "K-means clustering algorithm.\n\nRead more in the :ref:`User Guide <k_means>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The observations to cluster. It must be noted that the data\n    will be converted to C ordering, which will cause a memory copy\n    if the given data is not C-contiguous.\n\nn_clusters : int\n    The number of clusters to form as well as the number of\n    centroids to generate.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    The weights for each observation in X. If None, all observations\n    are assigned equal weight.\n\ninit : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n    Method for initialization:\n\n    'k-means++' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n\n    'random': choose `n_clusters` observations (rows) at random from data\n    for the initial centroids.\n\n    If an array is passed, it should be of shape (n_clusters, n_features)\n    and gives the initial centers.\n\n    If a callable is passed, it should take arguments X, n_clusters and a\n    random state and return an initialization.\n\nprecompute_distances : {'auto', True, False}\n    Precompute distances (faster but takes more memory).\n\n    'auto' : do not precompute distances if n_samples * n_clusters > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n\n    True : always precompute distances\n\n    False : never precompute distances\n\n    .. deprecated:: 0.23\n        'precompute_distances' was deprecated in version 0.23 and will be\n        removed in 1.0 (renaming of 0.25). It has no effect.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\n\nmax_iter : int, default=300\n    Maximum number of iterations of the k-means algorithm to run.\n\nverbose : bool, default=False\n    Verbosity mode.\n\ntol : float, default=1e-4\n    Relative tolerance with regards to Frobenius norm of the difference\n    in the cluster centers of two consecutive iterations to declare\n    convergence.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for centroid initialization. Use\n    an int to make the randomness deterministic.\n    See :term:`Glossary <random_state>`.\n\ncopy_x : bool, default=True\n    When pre-computing distances it is more numerically accurate to center\n    the data first. If copy_x is True (default), then the original data is\n    not modified. If False, the original data is modified, and put back\n    before the function returns, but small numerical differences may be\n    introduced by subtracting and then adding the data mean. Note that if\n    the original data is not C-contiguous, a copy will be made even if\n    copy_x is False. If the original data is sparse, but not in CSR format,\n    a copy will be made even if copy_x is False.\n\nn_jobs : int, default=None\n    The number of OpenMP threads to use for the computation. Parallelism is\n    sample-wise on the main cython loop which assigns each sample to its\n    closest center.\n\n    ``None`` or ``-1`` means using all processors.\n\n    .. deprecated:: 0.23\n        ``n_jobs`` was deprecated in version 0.23 and will be removed in\n        1.0 (renaming of 0.25).\n\nalgorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n    K-means algorithm to use. The classical EM-style algorithm is \"full\".\n    The \"elkan\" variation is more efficient on data with well-defined\n    clusters, by using the triangle inequality. However it's more memory\n    intensive due to the allocation of an extra array of shape\n    (n_samples, n_clusters).\n\n    For now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\n    might change in the future for a better heuristic.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncentroid : ndarray of shape (n_clusters, n_features)\n    Centroids found at the last iteration of k-means.\n\nlabel : ndarray of shape (n_samples,)\n    label[i] is the code or index of the centroid the\n    i'th observation is closest to.\n\ninertia : float\n    The final value of the inertia criterion (sum of squared distances to\n    the closest centroid for all observations in the training set).\n\nbest_n_iter : int\n    Number of iterations corresponding to the best results.\n    Returned only if `return_n_iter` is set to True.",
      "code": "@_deprecate_positional_args\ndef k_means(X, n_clusters, *, sample_weight=None, init='k-means++',\n            precompute_distances='deprecated', n_init=10, max_iter=300,\n            verbose=False, tol=1e-4, random_state=None, copy_x=True,\n            n_jobs='deprecated', algorithm=\"auto\", return_n_iter=False):\n    \"\"\"K-means clustering algorithm.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The observations to cluster. It must be noted that the data\n        will be converted to C ordering, which will cause a memory copy\n        if the given data is not C-contiguous.\n\n    n_clusters : int\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        The weights for each observation in X. If None, all observations\n        are assigned equal weight.\n\n    init : {'k-means++', 'random'}, callable or array-like of shape \\\n            (n_clusters, n_features), default='k-means++'\n        Method for initialization:\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose `n_clusters` observations (rows) at random from data\n        for the initial centroids.\n\n        If an array is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n        If a callable is passed, it should take arguments X, n_clusters and a\n        random state and return an initialization.\n\n    precompute_distances : {'auto', True, False}\n        Precompute distances (faster but takes more memory).\n\n        'auto' : do not precompute distances if n_samples * n_clusters > 12\n        million. This corresponds to about 100MB overhead per job using\n        double precision.\n\n        True : always precompute distances\n\n        False : never precompute distances\n\n        .. deprecated:: 0.23\n            'precompute_distances' was deprecated in version 0.23 and will be\n            removed in 1.0 (renaming of 0.25). It has no effect.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the k-means algorithm to run.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n    tol : float, default=1e-4\n        Relative tolerance with regards to Frobenius norm of the difference\n        in the cluster centers of two consecutive iterations to declare\n        convergence.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, default=True\n        When pre-computing distances it is more numerically accurate to center\n        the data first. If copy_x is True (default), then the original data is\n        not modified. If False, the original data is modified, and put back\n        before the function returns, but small numerical differences may be\n        introduced by subtracting and then adding the data mean. Note that if\n        the original data is not C-contiguous, a copy will be made even if\n        copy_x is False. If the original data is sparse, but not in CSR format,\n        a copy will be made even if copy_x is False.\n\n    n_jobs : int, default=None\n        The number of OpenMP threads to use for the computation. Parallelism is\n        sample-wise on the main cython loop which assigns each sample to its\n        closest center.\n\n        ``None`` or ``-1`` means using all processors.\n\n        .. deprecated:: 0.23\n            ``n_jobs`` was deprecated in version 0.23 and will be removed in\n            1.0 (renaming of 0.25).\n\n    algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n        The \"elkan\" variation is more efficient on data with well-defined\n        clusters, by using the triangle inequality. However it's more memory\n        intensive due to the allocation of an extra array of shape\n        (n_samples, n_clusters).\n\n        For now \"auto\" (kept for backward compatibility) chooses \"elkan\" but it\n        might change in the future for a better heuristic.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    centroid : ndarray of shape (n_clusters, n_features)\n        Centroids found at the last iteration of k-means.\n\n    label : ndarray of shape (n_samples,)\n        label[i] is the code or index of the centroid the\n        i'th observation is closest to.\n\n    inertia : float\n        The final value of the inertia criterion (sum of squared distances to\n        the closest centroid for all observations in the training set).\n\n    best_n_iter : int\n        Number of iterations corresponding to the best results.\n        Returned only if `return_n_iter` is set to True.\n    \"\"\"\n    est = KMeans(\n        n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter,\n        verbose=verbose, precompute_distances=precompute_distances, tol=tol,\n        random_state=random_state, copy_x=copy_x, n_jobs=n_jobs,\n        algorithm=algorithm\n    ).fit(X, sample_weight=sample_weight)\n    if return_n_iter:\n        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n    else:\n        return est.cluster_centers_, est.labels_, est.inertia_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus",
      "name": "kmeans_plusplus",
      "qname": "sklearn.cluster._kmeans.kmeans_plusplus",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/X",
          "name": "X",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data to pick seeds from."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.n_clusters",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "The number of centroids to initialize"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/x_squared_norms",
          "name": "x_squared_norms",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.x_squared_norms",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Squared Euclidean norm of each data point."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or RandomState instance",
            "default_value": "None",
            "description": "Determines random number generation for centroid initialization. Pass\nan int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._kmeans/kmeans_plusplus/n_local_trials",
          "name": "n_local_trials",
          "qname": "sklearn.cluster._kmeans.kmeans_plusplus.n_local_trials",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of seeding trials for each center (except the first),\nof which the one reducing inertia the most is greedily chosen.\nSet to None to make the number of trials depend logarithmically\non the number of seeds (2+log(k))."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Init n_clusters seeds according to k-means++\n\n.. versionadded:: 0.24",
      "docstring": "Init n_clusters seeds according to k-means++\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The data to pick seeds from.\n\nn_clusters : int\n    The number of centroids to initialize\n\nx_squared_norms : array-like of shape (n_samples,), default=None\n    Squared Euclidean norm of each data point.\n\nrandom_state : int or RandomState instance, default=None\n    Determines random number generation for centroid initialization. Pass\n    an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nn_local_trials : int, default=None\n    The number of seeding trials for each center (except the first),\n    of which the one reducing inertia the most is greedily chosen.\n    Set to None to make the number of trials depend logarithmically\n    on the number of seeds (2+log(k)).\n\nReturns\n-------\ncenters : ndarray of shape (n_clusters, n_features)\n    The inital centers for k-means.\n\nindices : ndarray of shape (n_clusters,)\n    The index location of the chosen centers in the data array X. For a\n    given index and center, X[index] = center.\n\nNotes\n-----\nSelects initial cluster centers for k-mean clustering in a smart way\nto speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n\"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\non Discrete algorithms. 2007\n\nExamples\n--------\n\n>>> from sklearn.cluster import kmeans_plusplus\n>>> import numpy as np\n>>> X = np.array([[1, 2], [1, 4], [1, 0],\n...               [10, 2], [10, 4], [10, 0]])\n>>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n>>> centers\narray([[10,  4],\n       [ 1,  0]])\n>>> indices\narray([4, 2])",
      "code": "def kmeans_plusplus(X, n_clusters, *, x_squared_norms=None,\n                    random_state=None, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to pick seeds from.\n\n    n_clusters : int\n        The number of centroids to initialize\n\n    x_squared_norms : array-like of shape (n_samples,), default=None\n        Squared Euclidean norm of each data point.\n\n    random_state : int or RandomState instance, default=None\n        Determines random number generation for centroid initialization. Pass\n        an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : int, default=None\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)).\n\n    Returns\n    -------\n    centers : ndarray of shape (n_clusters, n_features)\n        The inital centers for k-means.\n\n    indices : ndarray of shape (n_clusters,)\n        The index location of the chosen centers in the data array X. For a\n        given index and center, X[index] = center.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import kmeans_plusplus\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n    >>> centers\n    array([[10,  4],\n           [ 1,  0]])\n    >>> indices\n    array([4, 2])\n    \"\"\"\n\n    # Check data\n    check_array(X, accept_sparse='csr',\n                dtype=[np.float64, np.float32])\n\n    if X.shape[0] < n_clusters:\n        raise ValueError(f\"n_samples={X.shape[0]} should be >= \"\n                         f\"n_clusters={n_clusters}.\")\n\n    # Check parameters\n    if x_squared_norms is None:\n        x_squared_norms = row_norms(X, squared=True)\n    else:\n        x_squared_norms = check_array(x_squared_norms,\n                                      dtype=X.dtype,\n                                      ensure_2d=False)\n\n    if x_squared_norms.shape[0] != X.shape[0]:\n        raise ValueError(\n            f\"The length of x_squared_norms {x_squared_norms.shape[0]} should \"\n            f\"be equal to the length of n_samples {X.shape[0]}.\")\n\n    if n_local_trials is not None and n_local_trials < 1:\n        raise ValueError(\n            f\"n_local_trials is set to {n_local_trials} but should be an \"\n            f\"integer value greater than zero.\")\n\n    random_state = check_random_state(random_state)\n\n    # Call private k-means++\n    centers, indices = _kmeans_plusplus(X, n_clusters, x_squared_norms,\n                                        random_state, n_local_trials)\n\n    return centers, indices"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._mean_shift.MeanShift.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/bandwidth",
          "name": "bandwidth",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.bandwidth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Bandwidth used in the RBF kernel.\n\nIf not given, the bandwidth is estimated using\nsklearn.cluster.estimate_bandwidth; see the documentation for that\nfunction for hints on scalability (see also the Notes, below)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/seeds",
          "name": "seeds",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.seeds",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "None",
            "description": "Seeds used to initialize kernels. If not set,\nthe seeds are calculated by clustering.get_bin_seeds\nwith bandwidth as the grid size and default values for\nother parameters."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/bin_seeding",
          "name": "bin_seeding",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.bin_seeding",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If true, initial kernel locations are not locations of all\npoints, but rather the location of the discretized version of\npoints, where points are binned onto a grid whose coarseness\ncorresponds to the bandwidth. Setting this option to True will speed\nup the algorithm because fewer seeds will be initialized.\nThe default value is False.\nIgnored if seeds argument is not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/min_bin_freq",
          "name": "min_bin_freq",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.min_bin_freq",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "To speed up the algorithm, accept only those bins with at least\nmin_bin_freq points as seeds."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/cluster_all",
          "name": "cluster_all",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.cluster_all",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If true, then all points are clustered, even those orphans that are\nnot within any kernel. Orphans are assigned to the nearest kernel.\nIf false, then orphans are given cluster label -1."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by computing\neach of the n_init runs in parallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._mean_shift.MeanShift.__init__.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations, per seed point before the clustering\noperation terminates (for that seed point), if has not converged yet.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, bandwidth=None, seeds=None, bin_seeding=False,\n                 min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300):\n        self.bandwidth = bandwidth\n        self.seeds = seeds\n        self.bin_seeding = bin_seeding\n        self.cluster_all = cluster_all\n        self.min_bin_freq = min_bin_freq\n        self.n_jobs = n_jobs\n        self.max_iter = max_iter"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit",
      "name": "fit",
      "qname": "sklearn.cluster._mean_shift.MeanShift.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._mean_shift.MeanShift.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.MeanShift.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Samples to cluster."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._mean_shift.MeanShift.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform clustering.",
      "docstring": "Perform clustering.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Samples to cluster.\n\ny : Ignored",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Perform clustering.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to cluster.\n\n        y : Ignored\n\n        \"\"\"\n        X = self._validate_data(X)\n        bandwidth = self.bandwidth\n        if bandwidth is None:\n            bandwidth = estimate_bandwidth(X, n_jobs=self.n_jobs)\n        elif bandwidth <= 0:\n            raise ValueError(\"bandwidth needs to be greater than zero or None,\"\n                             \" got %f\" % bandwidth)\n\n        seeds = self.seeds\n        if seeds is None:\n            if self.bin_seeding:\n                seeds = get_bin_seeds(X, bandwidth, self.min_bin_freq)\n            else:\n                seeds = X\n        n_samples, n_features = X.shape\n        center_intensity_dict = {}\n\n        # We use n_jobs=1 because this will be used in nested calls under\n        # parallel calls to _mean_shift_single_seed so there is no need for\n        # for further parallelism.\n        nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)\n\n        # execute iterations on all seeds in parallel\n        all_res = Parallel(n_jobs=self.n_jobs)(\n            delayed(_mean_shift_single_seed)\n            (seed, X, nbrs, self.max_iter) for seed in seeds)\n        # copy results in a dictionary\n        for i in range(len(seeds)):\n            if all_res[i][1]:  # i.e. len(points_within) > 0\n                center_intensity_dict[all_res[i][0]] = all_res[i][1]\n\n        self.n_iter_ = max([x[2] for x in all_res])\n\n        if not center_intensity_dict:\n            # nothing near seeds\n            raise ValueError(\"No point was within bandwidth=%f of any seed.\"\n                             \" Try a different seeding strategy \\\n                             or increase the bandwidth.\"\n                             % bandwidth)\n\n        # POST PROCESSING: remove near duplicate points\n        # If the distance between two kernels is less than the bandwidth,\n        # then we have to remove one because it is a duplicate. Remove the\n        # one with fewer points.\n\n        sorted_by_intensity = sorted(center_intensity_dict.items(),\n                                     key=lambda tup: (tup[1], tup[0]),\n                                     reverse=True)\n        sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])\n        unique = np.ones(len(sorted_centers), dtype=bool)\n        nbrs = NearestNeighbors(radius=bandwidth,\n                                n_jobs=self.n_jobs).fit(sorted_centers)\n        for i, center in enumerate(sorted_centers):\n            if unique[i]:\n                neighbor_idxs = nbrs.radius_neighbors([center],\n                                                      return_distance=False)[0]\n                unique[neighbor_idxs] = 0\n                unique[i] = 1  # leave the current point as unique\n        cluster_centers = sorted_centers[unique]\n\n        # ASSIGN LABELS: a point belongs to the cluster that it is closest to\n        nbrs = NearestNeighbors(n_neighbors=1,\n                                n_jobs=self.n_jobs).fit(cluster_centers)\n        labels = np.zeros(n_samples, dtype=int)\n        distances, idxs = nbrs.kneighbors(X)\n        if self.cluster_all:\n            labels = idxs.flatten()\n        else:\n            labels.fill(-1)\n            bool_selector = distances.flatten() <= bandwidth\n            labels[bool_selector] = idxs.flatten()[bool_selector]\n\n        self.cluster_centers_, self.labels_ = cluster_centers, labels\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict",
      "name": "predict",
      "qname": "sklearn.cluster._mean_shift.MeanShift.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict/self",
          "name": "self",
          "qname": "sklearn.cluster._mean_shift.MeanShift.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/MeanShift/predict/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.MeanShift.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data to predict."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the closest cluster each sample in X belongs to.",
      "docstring": "Predict the closest cluster each sample in X belongs to.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data to predict.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Index of the cluster each sample belongs to.",
      "code": "    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data to predict.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Index of the cluster each sample belongs to.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        with config_context(assume_finite=True):\n            return pairwise_distances_argmin(X, self.cluster_centers_)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth",
      "name": "estimate_bandwidth",
      "qname": "sklearn.cluster._mean_shift.estimate_bandwidth",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input points."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/quantile",
          "name": "quantile",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.quantile",
          "default_value": "0.3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.3",
            "description": "should be between [0, 1]\n0.5 means that the median of all pairwise distances is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/n_samples",
          "name": "n_samples",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.n_samples",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of samples to use. If not given, all samples are used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.random_state",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "The generator used to randomly select the samples from input points\nfor bandwidth estimation. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/estimate_bandwidth/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._mean_shift.estimate_bandwidth.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Estimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it's wise to set that parameter to a small value.",
      "docstring": "Estimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it's wise to set that parameter to a small value.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input points.\n\nquantile : float, default=0.3\n    should be between [0, 1]\n    0.5 means that the median of all pairwise distances is used.\n\nn_samples : int, default=None\n    The number of samples to use. If not given, all samples are used.\n\nrandom_state : int, RandomState instance, default=None\n    The generator used to randomly select the samples from input points\n    for bandwidth estimation. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nbandwidth : float\n    The bandwidth parameter.",
      "code": "@_deprecate_positional_args\ndef estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0,\n                       n_jobs=None):\n    \"\"\"Estimate the bandwidth to use with the mean-shift algorithm.\n\n    That this function takes time at least quadratic in n_samples. For large\n    datasets, it's wise to set that parameter to a small value.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input points.\n\n    quantile : float, default=0.3\n        should be between [0, 1]\n        0.5 means that the median of all pairwise distances is used.\n\n    n_samples : int, default=None\n        The number of samples to use. If not given, all samples are used.\n\n    random_state : int, RandomState instance, default=None\n        The generator used to randomly select the samples from input points\n        for bandwidth estimation. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    bandwidth : float\n        The bandwidth parameter.\n    \"\"\"\n    X = check_array(X)\n\n    random_state = check_random_state(random_state)\n    if n_samples is not None:\n        idx = random_state.permutation(X.shape[0])[:n_samples]\n        X = X[idx]\n    n_neighbors = int(X.shape[0] * quantile)\n    if n_neighbors < 1:  # cannot fit NearestNeighbors with n_neighbors = 0\n        n_neighbors = 1\n    nbrs = NearestNeighbors(n_neighbors=n_neighbors,\n                            n_jobs=n_jobs)\n    nbrs.fit(X)\n\n    bandwidth = 0.\n    for batch in gen_batches(len(X), 500):\n        d, _ = nbrs.kneighbors(X[batch, :], return_distance=True)\n        bandwidth += np.max(d, axis=1).sum()\n\n    return bandwidth / X.shape[0]"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds",
      "name": "get_bin_seeds",
      "qname": "sklearn.cluster._mean_shift.get_bin_seeds",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.get_bin_seeds.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input points, the same points that will be used in mean_shift."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds/bin_size",
          "name": "bin_size",
          "qname": "sklearn.cluster._mean_shift.get_bin_seeds.bin_size",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "",
            "description": "Controls the coarseness of the binning. Smaller values lead\nto more seeding (which is computationally more expensive). If you're\nnot sure how to set this, set it to the value of the bandwidth used\nin clustering.mean_shift."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/get_bin_seeds/min_bin_freq",
          "name": "min_bin_freq",
          "qname": "sklearn.cluster._mean_shift.get_bin_seeds.min_bin_freq",
          "default_value": "1",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "Only bins with at least min_bin_freq will be selected as seeds.\nRaising this value decreases the number of seeds found, which\nmakes mean_shift computationally cheaper."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are\nspaced bin_size apart, and then choosing those bins with at least\nmin_bin_freq points.",
      "docstring": "Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are\nspaced bin_size apart, and then choosing those bins with at least\nmin_bin_freq points.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Input points, the same points that will be used in mean_shift.\n\nbin_size : float\n    Controls the coarseness of the binning. Smaller values lead\n    to more seeding (which is computationally more expensive). If you're\n    not sure how to set this, set it to the value of the bandwidth used\n    in clustering.mean_shift.\n\nmin_bin_freq : int, default=1\n    Only bins with at least min_bin_freq will be selected as seeds.\n    Raising this value decreases the number of seeds found, which\n    makes mean_shift computationally cheaper.\n\nReturns\n-------\nbin_seeds : array-like of shape (n_samples, n_features)\n    Points used as initial kernel positions in clustering.mean_shift.",
      "code": "def get_bin_seeds(X, bin_size, min_bin_freq=1):\n    \"\"\"Finds seeds for mean_shift.\n\n    Finds seeds by first binning data onto a grid whose lines are\n    spaced bin_size apart, and then choosing those bins with at least\n    min_bin_freq points.\n\n    Parameters\n    ----------\n\n    X : array-like of shape (n_samples, n_features)\n        Input points, the same points that will be used in mean_shift.\n\n    bin_size : float\n        Controls the coarseness of the binning. Smaller values lead\n        to more seeding (which is computationally more expensive). If you're\n        not sure how to set this, set it to the value of the bandwidth used\n        in clustering.mean_shift.\n\n    min_bin_freq : int, default=1\n        Only bins with at least min_bin_freq will be selected as seeds.\n        Raising this value decreases the number of seeds found, which\n        makes mean_shift computationally cheaper.\n\n    Returns\n    -------\n    bin_seeds : array-like of shape (n_samples, n_features)\n        Points used as initial kernel positions in clustering.mean_shift.\n    \"\"\"\n    if bin_size == 0:\n        return X\n\n    # Bin points\n    bin_sizes = defaultdict(int)\n    for point in X:\n        binned_point = np.round(point / bin_size)\n        bin_sizes[tuple(binned_point)] += 1\n\n    # Select only those bins as seeds which have enough members\n    bin_seeds = np.array([point for point, freq in bin_sizes.items() if\n                          freq >= min_bin_freq], dtype=np.float32)\n    if len(bin_seeds) == len(X):\n        warnings.warn(\"Binning data failed with provided bin_size=%f,\"\n                      \" using data points as seeds.\" % bin_size)\n        return X\n    bin_seeds = bin_seeds * bin_size\n    return bin_seeds"
    },
    {
      "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift",
      "name": "mean_shift",
      "qname": "sklearn.cluster._mean_shift.mean_shift",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/X",
          "name": "X",
          "qname": "sklearn.cluster._mean_shift.mean_shift.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/bandwidth",
          "name": "bandwidth",
          "qname": "sklearn.cluster._mean_shift.mean_shift.bandwidth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Kernel bandwidth.\n\nIf bandwidth is not given, it is determined using a heuristic based on\nthe median of all pairwise distances. This will take quadratic time in\nthe number of samples. The sklearn.cluster.estimate_bandwidth function\ncan be used to do this more efficiently."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/seeds",
          "name": "seeds",
          "qname": "sklearn.cluster._mean_shift.mean_shift.seeds",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_seeds, n_features) or None",
            "default_value": "",
            "description": "Point used as initial kernel locations. If None and bin_seeding=False,\neach data point is used as a seed. If None and bin_seeding=True,\nsee bin_seeding."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_seeds, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/bin_seeding",
          "name": "bin_seeding",
          "qname": "sklearn.cluster._mean_shift.mean_shift.bin_seeding",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If true, initial kernel locations are not locations of all\npoints, but rather the location of the discretized version of\npoints, where points are binned onto a grid whose coarseness\ncorresponds to the bandwidth. Setting this option to True will speed\nup the algorithm because fewer seeds will be initialized.\nIgnored if seeds argument is not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/min_bin_freq",
          "name": "min_bin_freq",
          "qname": "sklearn.cluster._mean_shift.mean_shift.min_bin_freq",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "To speed up the algorithm, accept only those bins with at least\nmin_bin_freq points as seeds."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/cluster_all",
          "name": "cluster_all",
          "qname": "sklearn.cluster._mean_shift.mean_shift.cluster_all",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If true, then all points are clustered, even those orphans that are\nnot within any kernel. Orphans are assigned to the nearest kernel.\nIf false, then orphans are given cluster label -1."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cluster._mean_shift.mean_shift.max_iter",
          "default_value": "300",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "300",
            "description": "Maximum number of iterations, per seed point before the clustering\noperation terminates (for that seed point), if has not converged yet."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._mean_shift/mean_shift/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._mean_shift.mean_shift.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use for the computation. This works by computing\neach of the n_init runs in parallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionadded:: 0.17\n   Parallel Execution using *n_jobs*."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Perform mean shift clustering of data using a flat kernel.\n\nRead more in the :ref:`User Guide <mean_shift>`.",
      "docstring": "Perform mean shift clustering of data using a flat kernel.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n\nParameters\n----------\n\nX : array-like of shape (n_samples, n_features)\n    Input data.\n\nbandwidth : float, default=None\n    Kernel bandwidth.\n\n    If bandwidth is not given, it is determined using a heuristic based on\n    the median of all pairwise distances. This will take quadratic time in\n    the number of samples. The sklearn.cluster.estimate_bandwidth function\n    can be used to do this more efficiently.\n\nseeds : array-like of shape (n_seeds, n_features) or None\n    Point used as initial kernel locations. If None and bin_seeding=False,\n    each data point is used as a seed. If None and bin_seeding=True,\n    see bin_seeding.\n\nbin_seeding : bool, default=False\n    If true, initial kernel locations are not locations of all\n    points, but rather the location of the discretized version of\n    points, where points are binned onto a grid whose coarseness\n    corresponds to the bandwidth. Setting this option to True will speed\n    up the algorithm because fewer seeds will be initialized.\n    Ignored if seeds argument is not None.\n\nmin_bin_freq : int, default=1\n   To speed up the algorithm, accept only those bins with at least\n   min_bin_freq points as seeds.\n\ncluster_all : bool, default=True\n    If true, then all points are clustered, even those orphans that are\n    not within any kernel. Orphans are assigned to the nearest kernel.\n    If false, then orphans are given cluster label -1.\n\nmax_iter : int, default=300\n    Maximum number of iterations, per seed point before the clustering\n    operation terminates (for that seed point), if has not converged yet.\n\nn_jobs : int, default=None\n    The number of jobs to use for the computation. This works by computing\n    each of the n_init runs in parallel.\n\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\n    .. versionadded:: 0.17\n       Parallel Execution using *n_jobs*.\n\nReturns\n-------\n\ncluster_centers : ndarray of shape (n_clusters, n_features)\n    Coordinates of cluster centers.\n\nlabels : ndarray of shape (n_samples,)\n    Cluster labels for each point.\n\nNotes\n-----\nFor an example, see :ref:`examples/cluster/plot_mean_shift.py\n<sphx_glr_auto_examples_cluster_plot_mean_shift.py>`.",
      "code": "@_deprecate_positional_args\ndef mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False,\n               min_bin_freq=1, cluster_all=True, max_iter=300,\n               n_jobs=None):\n    \"\"\"Perform mean shift clustering of data using a flat kernel.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n\n    X : array-like of shape (n_samples, n_features)\n        Input data.\n\n    bandwidth : float, default=None\n        Kernel bandwidth.\n\n        If bandwidth is not given, it is determined using a heuristic based on\n        the median of all pairwise distances. This will take quadratic time in\n        the number of samples. The sklearn.cluster.estimate_bandwidth function\n        can be used to do this more efficiently.\n\n    seeds : array-like of shape (n_seeds, n_features) or None\n        Point used as initial kernel locations. If None and bin_seeding=False,\n        each data point is used as a seed. If None and bin_seeding=True,\n        see bin_seeding.\n\n    bin_seeding : bool, default=False\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, default=1\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds.\n\n    cluster_all : bool, default=True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by computing\n        each of the n_init runs in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.17\n           Parallel Execution using *n_jobs*.\n\n    Returns\n    -------\n\n    cluster_centers : ndarray of shape (n_clusters, n_features)\n        Coordinates of cluster centers.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_mean_shift.py\n    <sphx_glr_auto_examples_cluster_plot_mean_shift.py>`.\n\n    \"\"\"\n    model = MeanShift(bandwidth=bandwidth, seeds=seeds,\n                      min_bin_freq=min_bin_freq,\n                      bin_seeding=bin_seeding,\n                      cluster_all=cluster_all, n_jobs=n_jobs,\n                      max_iter=max_iter).fit(X)\n    return model.cluster_centers_, model.labels_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._optics.OPTICS.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.min_samples",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "5",
            "description": "The number of samples in a neighborhood for a point to be considered as\na core point. Also, up and down steep regions can't have more than\n``min_samples`` consecutive non-steep points. Expressed as an absolute\nnumber or a fraction of the number of samples (rounded to be at least\n2)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/max_eps",
          "name": "max_eps",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.max_eps",
          "default_value": "np.inf",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "np.inf",
            "description": "The maximum distance between two samples for one to be considered as\nin the neighborhood of the other. Default value of ``np.inf`` will\nidentify clusters across all scales; reducing ``max_eps`` will result\nin shorter run times."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/metric",
          "name": "metric",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.metric",
          "default_value": "'minkowski'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'minkowski'",
            "description": "Metric to use for distance computation. Any metric from scikit-learn\nor scipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each\npair of instances (rows) and the resulting value recorded. The callable\nshould take two arrays as input and return one value indicating the\ndistance between them. This works for Scipy's metrics, but is less\nefficient than passing the metric name as a string. If metric is\n\"precomputed\", X is assumed to be a distance matrix and must be square.\n\nValid values for metric are:\n\n- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n  'manhattan']\n\n- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n  'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n  'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n  'yule']\n\nSee the documentation for scipy.spatial.distance for details on these\nmetrics."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/p",
          "name": "p",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.p",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Parameter for the Minkowski metric from\n:class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.metric_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/cluster_method",
          "name": "cluster_method",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.cluster_method",
          "default_value": "'xi'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "'xi'",
            "description": "The extraction method used to extract clusters using the calculated\nreachability and ordering. Possible values are \"xi\" and \"dbscan\"."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/eps",
          "name": "eps",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.eps",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The maximum distance between two samples for one to be considered as\nin the neighborhood of the other. By default it assumes the same value\nas ``max_eps``.\nUsed only when ``cluster_method='dbscan'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/xi",
          "name": "xi",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.xi",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float between 0 and 1",
            "default_value": "0.05",
            "description": "Determines the minimum steepness on the reachability plot that\nconstitutes a cluster boundary. For example, an upwards point in the\nreachability plot is defined by the ratio from one point to its\nsuccessor being at most 1-xi.\nUsed only when ``cluster_method='xi'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float between 0 and 1"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/predecessor_correction",
          "name": "predecessor_correction",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.predecessor_correction",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Correct clusters according to the predecessors calculated by OPTICS\n[2]_. This parameter has minimal effect on most datasets.\nUsed only when ``cluster_method='xi'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/min_cluster_size",
          "name": "min_cluster_size",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.min_cluster_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "None",
            "description": "Minimum number of samples in an OPTICS cluster, expressed as an\nabsolute number or a fraction of the number of samples (rounded to be\nat least 2). If ``None``, the value of ``min_samples`` is used instead.\nUsed only when ``cluster_method='xi'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.algorithm",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm\n  based on the values passed to :meth:`fit` method. (default)\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "brute",
              "auto",
              "ball_tree",
              "kd_tree"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.leaf_size",
          "default_value": "30",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\naffect the speed of the construction and query, as well as the memory\nrequired to store the tree. The optimal value depends on the\nnature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._optics.OPTICS.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Estimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely\nrelated to DBSCAN, finds core sample of high density and expands clusters\nfrom them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable\nneighborhood radius. Better suited for usage on large datasets than the\ncurrent sklearn implementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method\n(cluster_method = 'dbscan') or an automatic\ntechnique proposed in [1]_ (cluster_method = 'xi').\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the :ref:`User Guide <optics>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, min_samples=5, max_eps=np.inf, metric='minkowski',\n                 p=2, metric_params=None, cluster_method='xi', eps=None,\n                 xi=0.05, predecessor_correction=True, min_cluster_size=None,\n                 algorithm='auto', leaf_size=30, n_jobs=None):\n        self.max_eps = max_eps\n        self.min_samples = min_samples\n        self.min_cluster_size = min_cluster_size\n        self.algorithm = algorithm\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.leaf_size = leaf_size\n        self.cluster_method = cluster_method\n        self.eps = eps\n        self.xi = xi\n        self.predecessor_correction = predecessor_correction\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit",
      "name": "fit",
      "qname": "sklearn.cluster._optics.OPTICS.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._optics.OPTICS.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._optics.OPTICS.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric=\u2019precomputed\u2019",
            "default_value": "",
            "description": "A feature array, or array of distances between samples if\nmetric='precomputed'."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples) if metric=\u2019precomputed\u2019"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/OPTICS/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._optics.OPTICS.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ignored",
            "default_value": "",
            "description": "Ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and\nperforms initial clustering using ``max_eps`` distance specified at\nOPTICS object instantiation.",
      "docstring": "Perform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and\nperforms initial clustering using ``max_eps`` distance specified at\nOPTICS object instantiation.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features), or                 (n_samples, n_samples) if metric=\u2019precomputed\u2019\n    A feature array, or array of distances between samples if\n    metric='precomputed'.\n\ny : ignored\n    Ignored.\n\nReturns\n-------\nself : instance of OPTICS\n    The instance.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Perform OPTICS clustering.\n\n        Extracts an ordered list of points and reachability distances, and\n        performs initial clustering using ``max_eps`` distance specified at\n        OPTICS object instantiation.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features), or \\\n                (n_samples, n_samples) if metric=\u2019precomputed\u2019\n            A feature array, or array of distances between samples if\n            metric='precomputed'.\n\n        y : ignored\n            Ignored.\n\n        Returns\n        -------\n        self : instance of OPTICS\n            The instance.\n        \"\"\"\n        X = self._validate_data(X, dtype=float)\n\n        if self.cluster_method not in ['dbscan', 'xi']:\n            raise ValueError(\"cluster_method should be one of\"\n                             \" 'dbscan' or 'xi' but is %s\" %\n                             self.cluster_method)\n\n        (self.ordering_, self.core_distances_, self.reachability_,\n         self.predecessor_) = compute_optics_graph(\n             X=X, min_samples=self.min_samples, algorithm=self.algorithm,\n             leaf_size=self.leaf_size, metric=self.metric,\n             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs,\n             max_eps=self.max_eps)\n\n        # Extract clusters from the calculated orders and reachability\n        if self.cluster_method == 'xi':\n            labels_, clusters_ = cluster_optics_xi(\n                reachability=self.reachability_,\n                predecessor=self.predecessor_,\n                ordering=self.ordering_,\n                min_samples=self.min_samples,\n                min_cluster_size=self.min_cluster_size,\n                xi=self.xi,\n                predecessor_correction=self.predecessor_correction)\n            self.cluster_hierarchy_ = clusters_\n        elif self.cluster_method == 'dbscan':\n            if self.eps is None:\n                eps = self.max_eps\n            else:\n                eps = self.eps\n\n            if eps > self.max_eps:\n                raise ValueError('Specify an epsilon smaller than %s. Got %s.'\n                                 % (self.max_eps, eps))\n\n            labels_ = cluster_optics_dbscan(\n                reachability=self.reachability_,\n                core_distances=self.core_distances_,\n                ordering=self.ordering_, eps=eps)\n\n        self.labels_ = labels_\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan",
      "name": "cluster_optics_dbscan",
      "qname": "sklearn.cluster._optics.cluster_optics_dbscan",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/reachability",
          "name": "reachability",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.reachability",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_samples,)",
            "default_value": "",
            "description": "Reachability distances calculated by OPTICS (``reachability_``)"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/core_distances",
          "name": "core_distances",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.core_distances",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_samples,)",
            "default_value": "",
            "description": "Distances at which points become core (``core_distances_``)"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/ordering",
          "name": "ordering",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.ordering",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_samples,)",
            "default_value": "",
            "description": "OPTICS ordered point indices (``ordering_``)"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_dbscan/eps",
          "name": "eps",
          "qname": "sklearn.cluster._optics.cluster_optics_dbscan.eps",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "",
            "description": "DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\nwill be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\nto one another."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Performs DBSCAN extraction for an arbitrary epsilon.\n\nExtracting the clusters runs in linear time. Note that this results in\n``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\nsimilar settings and ``eps``, only if ``eps`` is close to ``max_eps``.",
      "docstring": "Performs DBSCAN extraction for an arbitrary epsilon.\n\nExtracting the clusters runs in linear time. Note that this results in\n``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\nsimilar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n\nParameters\n----------\nreachability : array of shape (n_samples,)\n    Reachability distances calculated by OPTICS (``reachability_``)\n\ncore_distances : array of shape (n_samples,)\n    Distances at which points become core (``core_distances_``)\n\nordering : array of shape (n_samples,)\n    OPTICS ordered point indices (``ordering_``)\n\neps : float\n    DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n    will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n    to one another.\n\nReturns\n-------\nlabels_ : array of shape (n_samples,)\n    The estimated labels.",
      "code": "@_deprecate_positional_args\ndef cluster_optics_dbscan(*, reachability, core_distances, ordering, eps):\n    \"\"\"Performs DBSCAN extraction for an arbitrary epsilon.\n\n    Extracting the clusters runs in linear time. Note that this results in\n    ``labels_`` which are close to a :class:`~sklearn.cluster.DBSCAN` with\n    similar settings and ``eps``, only if ``eps`` is close to ``max_eps``.\n\n    Parameters\n    ----------\n    reachability : array of shape (n_samples,)\n        Reachability distances calculated by OPTICS (``reachability_``)\n\n    core_distances : array of shape (n_samples,)\n        Distances at which points become core (``core_distances_``)\n\n    ordering : array of shape (n_samples,)\n        OPTICS ordered point indices (``ordering_``)\n\n    eps : float\n        DBSCAN ``eps`` parameter. Must be set to < ``max_eps``. Results\n        will be close to DBSCAN algorithm if ``eps`` and ``max_eps`` are close\n        to one another.\n\n    Returns\n    -------\n    labels_ : array of shape (n_samples,)\n        The estimated labels.\n\n    \"\"\"\n    n_samples = len(core_distances)\n    labels = np.zeros(n_samples, dtype=int)\n\n    far_reach = reachability > eps\n    near_core = core_distances <= eps\n    labels[ordering] = np.cumsum(far_reach[ordering] & near_core[ordering]) - 1\n    labels[far_reach & ~near_core] = -1\n    return labels"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi",
      "name": "cluster_optics_xi",
      "qname": "sklearn.cluster._optics.cluster_optics_xi",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/reachability",
          "name": "reachability",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.reachability",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "Reachability distances calculated by OPTICS (`reachability_`)"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/predecessor",
          "name": "predecessor",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.predecessor",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "Predecessors calculated by OPTICS."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/ordering",
          "name": "ordering",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.ordering",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "OPTICS ordered point indices (`ordering_`)"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.min_samples",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "",
            "description": "The same as the min_samples given to OPTICS. Up and down steep regions\ncan't have more then ``min_samples`` consecutive non-steep points.\nExpressed as an absolute number or a fraction of the number of samples\n(rounded to be at least 2)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/min_cluster_size",
          "name": "min_cluster_size",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.min_cluster_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "None",
            "description": "Minimum number of samples in an OPTICS cluster, expressed as an\nabsolute number or a fraction of the number of samples (rounded to be\nat least 2). If ``None``, the value of ``min_samples`` is used instead."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/xi",
          "name": "xi",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.xi",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float between 0 and 1",
            "default_value": "0.05",
            "description": "Determines the minimum steepness on the reachability plot that\nconstitutes a cluster boundary. For example, an upwards point in the\nreachability plot is defined by the ratio from one point to its\nsuccessor being at most 1-xi."
          },
          "type": {
            "kind": "NamedType",
            "name": "float between 0 and 1"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/cluster_optics_xi/predecessor_correction",
          "name": "predecessor_correction",
          "qname": "sklearn.cluster._optics.cluster_optics_xi.predecessor_correction",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Correct clusters based on the calculated predecessors."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Automatically extract clusters according to the Xi-steep method.",
      "docstring": "Automatically extract clusters according to the Xi-steep method.\n\nParameters\n----------\nreachability : ndarray of shape (n_samples,)\n    Reachability distances calculated by OPTICS (`reachability_`)\n\npredecessor : ndarray of shape (n_samples,)\n    Predecessors calculated by OPTICS.\n\nordering : ndarray of shape (n_samples,)\n    OPTICS ordered point indices (`ordering_`)\n\nmin_samples : int > 1 or float between 0 and 1\n    The same as the min_samples given to OPTICS. Up and down steep regions\n    can't have more then ``min_samples`` consecutive non-steep points.\n    Expressed as an absolute number or a fraction of the number of samples\n    (rounded to be at least 2).\n\nmin_cluster_size : int > 1 or float between 0 and 1, default=None\n    Minimum number of samples in an OPTICS cluster, expressed as an\n    absolute number or a fraction of the number of samples (rounded to be\n    at least 2). If ``None``, the value of ``min_samples`` is used instead.\n\nxi : float between 0 and 1, default=0.05\n    Determines the minimum steepness on the reachability plot that\n    constitutes a cluster boundary. For example, an upwards point in the\n    reachability plot is defined by the ratio from one point to its\n    successor being at most 1-xi.\n\npredecessor_correction : bool, default=True\n    Correct clusters based on the calculated predecessors.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    The labels assigned to samples. Points which are not included\n    in any cluster are labeled as -1.\n\nclusters : ndarray of shape (n_clusters, 2)\n    The list of clusters in the form of ``[start, end]`` in each row, with\n    all indices inclusive. The clusters are ordered according to ``(end,\n    -start)`` (ascending) so that larger clusters encompassing smaller\n    clusters come after such nested smaller clusters. Since ``labels`` does\n    not reflect the hierarchy, usually ``len(clusters) >\n    np.unique(labels)``.",
      "code": "def cluster_optics_xi(*, reachability, predecessor, ordering, min_samples,\n                      min_cluster_size=None, xi=0.05,\n                      predecessor_correction=True):\n    \"\"\"Automatically extract clusters according to the Xi-steep method.\n\n    Parameters\n    ----------\n    reachability : ndarray of shape (n_samples,)\n        Reachability distances calculated by OPTICS (`reachability_`)\n\n    predecessor : ndarray of shape (n_samples,)\n        Predecessors calculated by OPTICS.\n\n    ordering : ndarray of shape (n_samples,)\n        OPTICS ordered point indices (`ordering_`)\n\n    min_samples : int > 1 or float between 0 and 1\n        The same as the min_samples given to OPTICS. Up and down steep regions\n        can't have more then ``min_samples`` consecutive non-steep points.\n        Expressed as an absolute number or a fraction of the number of samples\n        (rounded to be at least 2).\n\n    min_cluster_size : int > 1 or float between 0 and 1, default=None\n        Minimum number of samples in an OPTICS cluster, expressed as an\n        absolute number or a fraction of the number of samples (rounded to be\n        at least 2). If ``None``, the value of ``min_samples`` is used instead.\n\n    xi : float between 0 and 1, default=0.05\n        Determines the minimum steepness on the reachability plot that\n        constitutes a cluster boundary. For example, an upwards point in the\n        reachability plot is defined by the ratio from one point to its\n        successor being at most 1-xi.\n\n    predecessor_correction : bool, default=True\n        Correct clusters based on the calculated predecessors.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        The labels assigned to samples. Points which are not included\n        in any cluster are labeled as -1.\n\n    clusters : ndarray of shape (n_clusters, 2)\n        The list of clusters in the form of ``[start, end]`` in each row, with\n        all indices inclusive. The clusters are ordered according to ``(end,\n        -start)`` (ascending) so that larger clusters encompassing smaller\n        clusters come after such nested smaller clusters. Since ``labels`` does\n        not reflect the hierarchy, usually ``len(clusters) >\n        np.unique(labels)``.\n    \"\"\"\n    n_samples = len(reachability)\n    _validate_size(min_samples, n_samples, 'min_samples')\n    if min_samples <= 1:\n        min_samples = max(2, int(min_samples * n_samples))\n    if min_cluster_size is None:\n        min_cluster_size = min_samples\n    _validate_size(min_cluster_size, n_samples, 'min_cluster_size')\n    if min_cluster_size <= 1:\n        min_cluster_size = max(2, int(min_cluster_size * n_samples))\n\n    clusters = _xi_cluster(reachability[ordering], predecessor[ordering],\n                           ordering, xi,\n                           min_samples, min_cluster_size,\n                           predecessor_correction)\n    labels = _extract_xi_labels(ordering, clusters)\n    return labels, clusters"
    },
    {
      "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph",
      "name": "compute_optics_graph",
      "qname": "sklearn.cluster._optics.compute_optics_graph",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/X",
          "name": "X",
          "qname": "sklearn.cluster._optics.compute_optics_graph.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=\u2019precomputed\u2019.",
            "default_value": "",
            "description": "A feature array, or array of distances between samples if\nmetric='precomputed'"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_samples, n_features)"
              },
              {
                "kind": "NamedType",
                "name": "(n_samples, n_samples) if metric=\u2019precomputed\u2019."
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/min_samples",
          "name": "min_samples",
          "qname": "sklearn.cluster._optics.compute_optics_graph.min_samples",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int > 1 or float between 0 and 1",
            "default_value": "",
            "description": "The number of samples in a neighborhood for a point to be considered\nas a core point. Expressed as an absolute number or a fraction of the\nnumber of samples (rounded to be at least 2)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int > 1"
              },
              {
                "kind": "NamedType",
                "name": "float between 0 and 1"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/max_eps",
          "name": "max_eps",
          "qname": "sklearn.cluster._optics.compute_optics_graph.max_eps",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "np.inf",
            "description": "The maximum distance between two samples for one to be considered as\nin the neighborhood of the other. Default value of ``np.inf`` will\nidentify clusters across all scales; reducing ``max_eps`` will result\nin shorter run times."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/metric",
          "name": "metric",
          "qname": "sklearn.cluster._optics.compute_optics_graph.metric",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'minkowski'",
            "description": "Metric to use for distance computation. Any metric from scikit-learn\nor scipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each\npair of instances (rows) and the resulting value recorded. The callable\nshould take two arrays as input and return one value indicating the\ndistance between them. This works for Scipy's metrics, but is less\nefficient than passing the metric name as a string. If metric is\n\"precomputed\", X is assumed to be a distance matrix and must be square.\n\nValid values for metric are:\n\n- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n  'manhattan']\n\n- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n  'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n  'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n  'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n  'yule']\n\nSee the documentation for scipy.spatial.distance for details on these\nmetrics."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/p",
          "name": "p",
          "qname": "sklearn.cluster._optics.compute_optics_graph.p",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Parameter for the Minkowski metric from\n:class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/metric_params",
          "name": "metric_params",
          "qname": "sklearn.cluster._optics.compute_optics_graph.metric_params",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Additional keyword arguments for the metric function."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cluster._optics.compute_optics_graph.algorithm",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'ball_tree', 'kd_tree', 'brute'}",
            "default_value": "'auto'",
            "description": "Algorithm used to compute the nearest neighbors:\n\n- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm\n  based on the values passed to :meth:`fit` method. (default)\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "brute",
              "auto",
              "ball_tree",
              "kd_tree"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/leaf_size",
          "name": "leaf_size",
          "qname": "sklearn.cluster._optics.compute_optics_graph.leaf_size",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "30",
            "description": "Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\naffect the speed of the construction and query, as well as the memory\nrequired to store the tree. The optimal value depends on the\nnature of the problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._optics/compute_optics_graph/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._optics.compute_optics_graph.n_jobs",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Computes the OPTICS reachability graph.\n\nRead more in the :ref:`User Guide <optics>`.",
      "docstring": "Computes the OPTICS reachability graph.\n\nRead more in the :ref:`User Guide <optics>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features), or             (n_samples, n_samples) if metric=\u2019precomputed\u2019.\n    A feature array, or array of distances between samples if\n    metric='precomputed'\n\nmin_samples : int > 1 or float between 0 and 1\n    The number of samples in a neighborhood for a point to be considered\n    as a core point. Expressed as an absolute number or a fraction of the\n    number of samples (rounded to be at least 2).\n\nmax_eps : float, default=np.inf\n    The maximum distance between two samples for one to be considered as\n    in the neighborhood of the other. Default value of ``np.inf`` will\n    identify clusters across all scales; reducing ``max_eps`` will result\n    in shorter run times.\n\nmetric : str or callable, default='minkowski'\n    Metric to use for distance computation. Any metric from scikit-learn\n    or scipy.spatial.distance can be used.\n\n    If metric is a callable function, it is called on each\n    pair of instances (rows) and the resulting value recorded. The callable\n    should take two arrays as input and return one value indicating the\n    distance between them. This works for Scipy's metrics, but is less\n    efficient than passing the metric name as a string. If metric is\n    \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n    Valid values for metric are:\n\n    - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']\n\n    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n      'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n      'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n      'yule']\n\n    See the documentation for scipy.spatial.distance for details on these\n    metrics.\n\np : int, default=2\n    Parameter for the Minkowski metric from\n    :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n    equivalent to using manhattan_distance (l1), and euclidean_distance\n    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None\n    Additional keyword arguments for the metric function.\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n    Algorithm used to compute the nearest neighbors:\n\n    - 'ball_tree' will use :class:`BallTree`\n    - 'kd_tree' will use :class:`KDTree`\n    - 'brute' will use a brute-force search.\n    - 'auto' will attempt to decide the most appropriate algorithm\n      based on the values passed to :meth:`fit` method. (default)\n\n    Note: fitting on sparse input will override the setting of\n    this parameter, using brute force.\n\nleaf_size : int, default=30\n    Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n    affect the speed of the construction and query, as well as the memory\n    required to store the tree. The optimal value depends on the\n    nature of the problem.\n\nn_jobs : int, default=None\n    The number of parallel jobs to run for neighbors search.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nReturns\n-------\nordering_ : array of shape (n_samples,)\n    The cluster ordered list of sample indices.\n\ncore_distances_ : array of shape (n_samples,)\n    Distance at which each sample becomes a core point, indexed by object\n    order. Points which will never be core have a distance of inf. Use\n    ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\nreachability_ : array of shape (n_samples,)\n    Reachability distances per sample, indexed by object order. Use\n    ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\npredecessor_ : array of shape (n_samples,)\n    Point that a sample was reached from, indexed by object order.\n    Seed points have a predecessor of -1.\n\nReferences\n----------\n.. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n   and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n   structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.",
      "code": "@_deprecate_positional_args\ndef compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params,\n                         algorithm, leaf_size, n_jobs):\n    \"\"\"Computes the OPTICS reachability graph.\n\n    Read more in the :ref:`User Guide <optics>`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features), or \\\n            (n_samples, n_samples) if metric=\u2019precomputed\u2019.\n        A feature array, or array of distances between samples if\n        metric='precomputed'\n\n    min_samples : int > 1 or float between 0 and 1\n        The number of samples in a neighborhood for a point to be considered\n        as a core point. Expressed as an absolute number or a fraction of the\n        number of samples (rounded to be at least 2).\n\n    max_eps : float, default=np.inf\n        The maximum distance between two samples for one to be considered as\n        in the neighborhood of the other. Default value of ``np.inf`` will\n        identify clusters across all scales; reducing ``max_eps`` will result\n        in shorter run times.\n\n    metric : str or callable, default='minkowski'\n        Metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string. If metric is\n        \"precomputed\", X is assumed to be a distance matrix and must be square.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n    p : int, default=2\n        Parameter for the Minkowski metric from\n        :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method. (default)\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n        affect the speed of the construction and query, as well as the memory\n        required to store the tree. The optimal value depends on the\n        nature of the problem.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    ordering_ : array of shape (n_samples,)\n        The cluster ordered list of sample indices.\n\n    core_distances_ : array of shape (n_samples,)\n        Distance at which each sample becomes a core point, indexed by object\n        order. Points which will never be core have a distance of inf. Use\n        ``clust.core_distances_[clust.ordering_]`` to access in cluster order.\n\n    reachability_ : array of shape (n_samples,)\n        Reachability distances per sample, indexed by object order. Use\n        ``clust.reachability_[clust.ordering_]`` to access in cluster order.\n\n    predecessor_ : array of shape (n_samples,)\n        Point that a sample was reached from, indexed by object order.\n        Seed points have a predecessor of -1.\n\n    References\n    ----------\n    .. [1] Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel,\n       and J\u00f6rg Sander. \"OPTICS: ordering points to identify the clustering\n       structure.\" ACM SIGMOD Record 28, no. 2 (1999): 49-60.\n    \"\"\"\n    n_samples = X.shape[0]\n    _validate_size(min_samples, n_samples, 'min_samples')\n    if min_samples <= 1:\n        min_samples = max(2, int(min_samples * n_samples))\n\n    # Start all points as 'unprocessed' ##\n    reachability_ = np.empty(n_samples)\n    reachability_.fill(np.inf)\n    predecessor_ = np.empty(n_samples, dtype=int)\n    predecessor_.fill(-1)\n\n    nbrs = NearestNeighbors(n_neighbors=min_samples,\n                            algorithm=algorithm,\n                            leaf_size=leaf_size,\n                            metric=metric,\n                            metric_params=metric_params,\n                            p=p,\n                            n_jobs=n_jobs)\n\n    nbrs.fit(X)\n    # Here we first do a kNN query for each point, this differs from\n    # the original OPTICS that only used epsilon range queries.\n    # TODO: handle working_memory somehow?\n    core_distances_ = _compute_core_distances_(X=X, neighbors=nbrs,\n                                               min_samples=min_samples,\n                                               working_memory=None)\n    # OPTICS puts an upper limit on these, use inf for undefined.\n    core_distances_[core_distances_ > max_eps] = np.inf\n    np.around(core_distances_,\n              decimals=np.finfo(core_distances_.dtype).precision,\n              out=core_distances_)\n\n    # Main OPTICS loop. Not parallelizable. The order that entries are\n    # written to the 'ordering_' list is important!\n    # Note that this implementation is O(n^2) theoretically, but\n    # supposedly with very low constant factors.\n    processed = np.zeros(X.shape[0], dtype=bool)\n    ordering = np.zeros(X.shape[0], dtype=int)\n    for ordering_idx in range(X.shape[0]):\n        # Choose next based on smallest reachability distance\n        # (And prefer smaller ids on ties, possibly np.inf!)\n        index = np.where(processed == 0)[0]\n        point = index[np.argmin(reachability_[index])]\n\n        processed[point] = True\n        ordering[ordering_idx] = point\n        if core_distances_[point] != np.inf:\n            _set_reach_dist(core_distances_=core_distances_,\n                            reachability_=reachability_,\n                            predecessor_=predecessor_,\n                            point_index=point,\n                            processed=processed, X=X, nbrs=nbrs,\n                            metric=metric, metric_params=metric_params,\n                            p=p, max_eps=max_eps)\n    if np.all(np.isinf(reachability_)):\n        warnings.warn(\"All reachability values are inf. Set a larger\"\n                      \" max_eps or all data will be considered outliers.\",\n                      UserWarning)\n    return ordering, core_distances_, reachability_, predecessor_"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__",
      "name": "__init__",
      "qname": "sklearn.cluster._spectral.SpectralClustering.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/self",
          "name": "self",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_clusters",
          "default_value": "8",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "8",
            "description": "The dimension of the projection subspace."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/eigen_solver",
          "name": "eigen_solver",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.eigen_solver",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'arpack', 'lobpcg', 'amg'}",
            "default_value": "None",
            "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg\nto be installed. It can be faster on very large, sparse problems,\nbut may also lead to instabilities. If None, then ``'arpack'`` is\nused."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "amg",
              "lobpcg",
              "arpack"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_components",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "n_clusters",
            "description": "Number of eigenvectors to use for the spectral embedding"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "A pseudo random number generator used for the initialization of the\nlobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and by\nthe K-Means initialization. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of n_init\nconsecutive runs in terms of inertia. Only used if\n``assign_labels='kmeans'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/gamma",
          "name": "gamma",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.gamma",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\nIgnored for ``affinity='nearest_neighbors'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.affinity",
          "default_value": "'rbf'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'rbf'",
            "description": "How to construct the affinity matrix.\n - 'nearest_neighbors': construct the affinity matrix by computing a\n   graph of nearest neighbors.\n - 'rbf': construct the affinity matrix using a radial basis function\n   (RBF) kernel.\n - 'precomputed': interpret ``X`` as a precomputed affinity matrix,\n   where larger values indicate greater similarity between instances.\n - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph\n   of precomputed distances, and construct a binary affinity matrix\n   from the ``n_neighbors`` nearest neighbors of each instance.\n - one of the kernels supported by\n   :func:`~sklearn.metrics.pairwise_kernels`.\n\nOnly kernels that produce similarity scores (non-negative values that\nincrease with similarity) should be used. This property is not checked\nby the clustering algorithm."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_neighbors",
          "name": "n_neighbors",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_neighbors",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of neighbors to use when constructing the affinity matrix using\nthe nearest neighbors method. Ignored for ``affinity='rbf'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/eigen_tol",
          "name": "eigen_tol",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.eigen_tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Stopping criterion for eigendecomposition of the Laplacian matrix\nwhen ``eigen_solver='arpack'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/assign_labels",
          "name": "assign_labels",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.assign_labels",
          "default_value": "'kmeans'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'kmeans', 'discretize'}",
            "default_value": "'kmeans'",
            "description": "The strategy for assigning labels in the embedding space. There are two\nways to assign labels after the Laplacian embedding. k-means is a\npopular choice, but it can be sensitive to initialization.\nDiscretization is another approach which is less sensitive to random\ninitialization."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "discretize",
              "kmeans"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/degree",
          "name": "degree",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.degree",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "3",
            "description": "Degree of the polynomial kernel. Ignored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/coef0",
          "name": "coef0",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.coef0",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1",
            "description": "Zero coefficient for polynomial and sigmoid kernels.\nIgnored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/kernel_params",
          "name": "kernel_params",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.kernel_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict of str to any",
            "default_value": "None",
            "description": "Parameters (keyword arguments) and values for kernel passed as\ncallable object. Ignored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict of str to any"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run when `affinity='nearest_neighbors'`\nor `affinity='precomputed_nearest_neighbors'`. The neighbors search\nwill be done in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._spectral.SpectralClustering.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Verbosity mode.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex, or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster, such as when clusters are\nnested circles on the 2D plane.\n\nIf the affinity matrix is the adjacency matrix of a graph, this method\ncan be used to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\na kernel function such the Gaussian (aka RBF) kernel with Euclidean\ndistance ``d(X, X)``::\n\n        np.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, a user-provided affinity matrix can be specified by\nsetting ``affinity='precomputed'``.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_clusters=8, *, eigen_solver=None, n_components=None,\n                 random_state=None, n_init=10, gamma=1., affinity='rbf',\n                 n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans',\n                 degree=3, coef0=1, kernel_params=None, n_jobs=None,\n                 verbose=False):\n        self.n_clusters = n_clusters\n        self.eigen_solver = eigen_solver\n        self.n_components = n_components\n        self.random_state = random_state\n        self.n_init = n_init\n        self.gamma = gamma\n        self.affinity = affinity\n        self.n_neighbors = n_neighbors\n        self.eigen_tol = eigen_tol\n        self.assign_labels = assign_labels\n        self.degree = degree\n        self.coef0 = coef0\n        self.kernel_params = kernel_params\n        self.n_jobs = n_jobs\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit",
      "name": "fit",
      "qname": "sklearn.cluster._spectral.SpectralClustering.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit/self",
          "name": "self",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit/X",
          "name": "X",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, similarities / affinities between\ninstances if ``affinity='precomputed'``, or distances between\ninstances if ``affinity='precomputed_nearest_neighbors``. If a\nsparse matrix is provided in a format other than ``csr_matrix``,\n``csc_matrix``, or ``coo_matrix``, it will be converted into a\nsparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit/y",
          "name": "y",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform spectral clustering from features, or affinity matrix.",
      "docstring": "Perform spectral clustering from features, or affinity matrix.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, similarities / affinities between\n    instances if ``affinity='precomputed'``, or distances between\n    instances if ``affinity='precomputed_nearest_neighbors``. If a\n    sparse matrix is provided in a format other than ``csr_matrix``,\n    ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n    sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n                                dtype=np.float64, ensure_min_samples=2)\n        allow_squared = self.affinity in [\"precomputed\",\n                                          \"precomputed_nearest_neighbors\"]\n        if X.shape[0] == X.shape[1] and not allow_squared:\n            warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n                          \"now constructs an affinity matrix from data. To use\"\n                          \" a custom affinity matrix, \"\n                          \"set ``affinity=precomputed``.\")\n\n        if self.affinity == 'nearest_neighbors':\n            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors,\n                                            include_self=True,\n                                            n_jobs=self.n_jobs)\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed_nearest_neighbors':\n            estimator = NearestNeighbors(n_neighbors=self.n_neighbors,\n                                         n_jobs=self.n_jobs,\n                                         metric=\"precomputed\").fit(X)\n            connectivity = estimator.kneighbors_graph(X=X, mode='connectivity')\n            self.affinity_matrix_ = 0.5 * (connectivity + connectivity.T)\n        elif self.affinity == 'precomputed':\n            self.affinity_matrix_ = X\n        else:\n            params = self.kernel_params\n            if params is None:\n                params = {}\n            if not callable(self.affinity):\n                params['gamma'] = self.gamma\n                params['degree'] = self.degree\n                params['coef0'] = self.coef0\n            self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n                                                     filter_params=True,\n                                                     **params)\n\n        random_state = check_random_state(self.random_state)\n        self.labels_ = spectral_clustering(self.affinity_matrix_,\n                                           n_clusters=self.n_clusters,\n                                           n_components=self.n_components,\n                                           eigen_solver=self.eigen_solver,\n                                           random_state=random_state,\n                                           n_init=self.n_init,\n                                           eigen_tol=self.eigen_tol,\n                                           assign_labels=self.assign_labels,\n                                           verbose=self.verbose)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict",
      "name": "fit_predict",
      "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict/self",
          "name": "self",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict/X",
          "name": "X",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)",
            "default_value": "",
            "description": "Training instances to cluster, similarities / affinities between\ninstances if ``affinity='precomputed'``, or distances between\ninstances if ``affinity='precomputed_nearest_neighbors``. If a\nsparse matrix is provided in a format other than ``csr_matrix``,\n``csc_matrix``, or ``coo_matrix``, it will be converted into a\nsparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features) or (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/SpectralClustering/fit_predict/y",
          "name": "y",
          "qname": "sklearn.cluster._spectral.SpectralClustering.fit_predict.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present here for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform spectral clustering from features, or affinity matrix,\nand return cluster labels.",
      "docstring": "Perform spectral clustering from features, or affinity matrix,\nand return cluster labels.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples)\n    Training instances to cluster, similarities / affinities between\n    instances if ``affinity='precomputed'``, or distances between\n    instances if ``affinity='precomputed_nearest_neighbors``. If a\n    sparse matrix is provided in a format other than ``csr_matrix``,\n    ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n    sparse ``csr_matrix``.\n\ny : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nlabels : ndarray of shape (n_samples,)\n    Cluster labels.",
      "code": "    def fit_predict(self, X, y=None):\n        \"\"\"Perform spectral clustering from features, or affinity matrix,\n        and return cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples)\n            Training instances to cluster, similarities / affinities between\n            instances if ``affinity='precomputed'``, or distances between\n            instances if ``affinity='precomputed_nearest_neighbors``. If a\n            sparse matrix is provided in a format other than ``csr_matrix``,\n            ``csc_matrix``, or ``coo_matrix``, it will be converted into a\n            sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        return super().fit_predict(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering",
      "name": "spectral_clustering",
      "qname": "sklearn.cluster._spectral.spectral_clustering",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/affinity",
          "name": "affinity",
          "qname": "sklearn.cluster._spectral.spectral_clustering.affinity",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_samples)",
            "default_value": "",
            "description": "The affinity matrix describing the relationship of the samples to\nembed. **Must be symmetric**.\n\nPossible examples:\n  - adjacency matrix of a graph,\n  - heat kernel of the pairwise distance matrix of the samples,\n  - symmetric k-nearest neighbours connectivity matrix of the samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_samples)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.cluster._spectral.spectral_clustering.n_clusters",
          "default_value": "8",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of clusters to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/n_components",
          "name": "n_components",
          "qname": "sklearn.cluster._spectral.spectral_clustering.n_components",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "n_clusters",
            "description": "Number of eigenvectors to use for the spectral embedding"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/eigen_solver",
          "name": "eigen_solver",
          "qname": "sklearn.cluster._spectral.spectral_clustering.eigen_solver",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{None, 'arpack', 'lobpcg', or 'amg'}",
            "default_value": "",
            "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg\nto be installed. It can be faster on very large, sparse problems,\nbut may also lead to instabilities. If None, then ``'arpack'`` is\nused."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "amg",
              "lobpcg",
              "arpack"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/random_state",
          "name": "random_state",
          "qname": "sklearn.cluster._spectral.spectral_clustering.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "A pseudo random number generator used for the initialization of the\nlobpcg eigenvectors decomposition when eigen_solver == 'amg' and by\nthe K-Means initialization. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/n_init",
          "name": "n_init",
          "qname": "sklearn.cluster._spectral.spectral_clustering.n_init",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of time the k-means algorithm will be run with different\ncentroid seeds. The final results will be the best output of n_init\nconsecutive runs in terms of inertia. Only used if\n``assign_labels='kmeans'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/eigen_tol",
          "name": "eigen_tol",
          "qname": "sklearn.cluster._spectral.spectral_clustering.eigen_tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Stopping criterion for eigendecomposition of the Laplacian matrix\nwhen using arpack eigen_solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/assign_labels",
          "name": "assign_labels",
          "qname": "sklearn.cluster._spectral.spectral_clustering.assign_labels",
          "default_value": "'kmeans'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'kmeans', 'discretize'}",
            "default_value": "'kmeans'",
            "description": "The strategy to use to assign labels in the embedding\nspace.  There are two ways to assign labels after the Laplacian\nembedding.  k-means can be applied and is a popular choice. But it can\nalso be sensitive to initialization. Discretization is another\napproach which is less sensitive to random initialization. See\nthe 'Multiclass spectral clustering' paper referenced below for\nmore details on the discretization approach."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "discretize",
              "kmeans"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cluster._spectral/spectral_clustering/verbose",
          "name": "verbose",
          "qname": "sklearn.cluster._spectral.spectral_clustering.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Verbosity mode.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.cluster"
      ],
      "description": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance, when clusters are\nnested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.",
      "docstring": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance, when clusters are\nnested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters\n----------\naffinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n    The affinity matrix describing the relationship of the samples to\n    embed. **Must be symmetric**.\n\n    Possible examples:\n      - adjacency matrix of a graph,\n      - heat kernel of the pairwise distance matrix of the samples,\n      - symmetric k-nearest neighbours connectivity matrix of the samples.\n\nn_clusters : int, default=None\n    Number of clusters to extract.\n\nn_components : int, default=n_clusters\n    Number of eigenvectors to use for the spectral embedding\n\neigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n    The eigenvalue decomposition strategy to use. AMG requires pyamg\n    to be installed. It can be faster on very large, sparse problems,\n    but may also lead to instabilities. If None, then ``'arpack'`` is\n    used.\n\nrandom_state : int, RandomState instance, default=None\n    A pseudo random number generator used for the initialization of the\n    lobpcg eigenvectors decomposition when eigen_solver == 'amg' and by\n    the K-Means initialization. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nn_init : int, default=10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of n_init\n    consecutive runs in terms of inertia. Only used if\n    ``assign_labels='kmeans'``.\n\neigen_tol : float, default=0.0\n    Stopping criterion for eigendecomposition of the Laplacian matrix\n    when using arpack eigen_solver.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans'\n    The strategy to use to assign labels in the embedding\n    space.  There are two ways to assign labels after the Laplacian\n    embedding.  k-means can be applied and is a popular choice. But it can\n    also be sensitive to initialization. Discretization is another\n    approach which is less sensitive to random initialization. See\n    the 'Multiclass spectral clustering' paper referenced below for\n    more details on the discretization approach.\n\nverbose : bool, default=False\n    Verbosity mode.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nlabels : array of integers, shape: n_samples\n    The labels of the clusters.\n\nReferences\n----------\n\n- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\nNotes\n-----\nThe graph should contain only one connect component, elsewhere\nthe results make little sense.\n\nThis algorithm solves the normalized cut for k=2: it is a\nnormalized spectral clustering.",
      "code": "@_deprecate_positional_args\ndef spectral_clustering(affinity, *, n_clusters=8, n_components=None,\n                        eigen_solver=None, random_state=None, n_init=10,\n                        eigen_tol=0.0, assign_labels='kmeans',\n                        verbose=False):\n    \"\"\"Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster. For instance, when clusters are\n    nested circles on the 2D plane.\n\n    If affinity is the adjacency matrix of a graph, this method can be\n    used to find normalized graph cuts.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    affinity : {array-like, sparse matrix} of shape (n_samples, n_samples)\n        The affinity matrix describing the relationship of the samples to\n        embed. **Must be symmetric**.\n\n        Possible examples:\n          - adjacency matrix of a graph,\n          - heat kernel of the pairwise distance matrix of the samples,\n          - symmetric k-nearest neighbours connectivity matrix of the samples.\n\n    n_clusters : int, default=None\n        Number of clusters to extract.\n\n    n_components : int, default=n_clusters\n        Number of eigenvectors to use for the spectral embedding\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities. If None, then ``'arpack'`` is\n        used.\n\n    random_state : int, RandomState instance, default=None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors decomposition when eigen_solver == 'amg' and by\n        the K-Means initialization. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_init : int, default=10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of n_init\n        consecutive runs in terms of inertia. Only used if\n        ``assign_labels='kmeans'``.\n\n    eigen_tol : float, default=0.0\n        Stopping criterion for eigendecomposition of the Laplacian matrix\n        when using arpack eigen_solver.\n\n    assign_labels : {'kmeans', 'discretize'}, default='kmeans'\n        The strategy to use to assign labels in the embedding\n        space.  There are two ways to assign labels after the Laplacian\n        embedding.  k-means can be applied and is a popular choice. But it can\n        also be sensitive to initialization. Discretization is another\n        approach which is less sensitive to random initialization. See\n        the 'Multiclass spectral clustering' paper referenced below for\n        more details on the discretization approach.\n\n    verbose : bool, default=False\n        Verbosity mode.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    labels : array of integers, shape: n_samples\n        The labels of the clusters.\n\n    References\n    ----------\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - Multiclass spectral clustering, 2003\n      Stella X. Yu, Jianbo Shi\n      https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\n    Notes\n    -----\n    The graph should contain only one connect component, elsewhere\n    the results make little sense.\n\n    This algorithm solves the normalized cut for k=2: it is a\n    normalized spectral clustering.\n    \"\"\"\n    if assign_labels not in ('kmeans', 'discretize'):\n        raise ValueError(\"The 'assign_labels' parameter should be \"\n                         \"'kmeans' or 'discretize', but '%s' was given\"\n                         % assign_labels)\n\n    random_state = check_random_state(random_state)\n    n_components = n_clusters if n_components is None else n_components\n\n    # The first eigenvector is constant only for fully connected graphs\n    # and should be kept for spectral clustering (drop_first = False)\n    # See spectral_embedding documentation.\n    maps = spectral_embedding(affinity, n_components=n_components,\n                              eigen_solver=eigen_solver,\n                              random_state=random_state,\n                              eigen_tol=eigen_tol, drop_first=False)\n    if verbose:\n        print(f'Computing label assignment using {assign_labels}')\n\n    if assign_labels == 'kmeans':\n        _, labels, _ = k_means(maps, n_clusters, random_state=random_state,\n                               n_init=n_init, verbose=verbose)\n    else:\n        labels = discretize(maps, random_state=random_state)\n\n    return labels"
    },
    {
      "id": "scikit-learn/sklearn.cluster.setup/configuration",
      "name": "configuration",
      "qname": "sklearn.cluster.setup.configuration",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cluster.setup/configuration/parent_package",
          "name": "parent_package",
          "qname": "sklearn.cluster.setup.configuration.parent_package",
          "default_value": "''",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cluster.setup/configuration/top_path",
          "name": "top_path",
          "qname": "sklearn.cluster.setup.configuration.top_path",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def configuration(parent_package='', top_path=None):\n    from numpy.distutils.misc_util import Configuration\n\n    libraries = []\n    if os.name == 'posix':\n        libraries.append('m')\n\n    config = Configuration('cluster', parent_package, top_path)\n\n    config.add_extension('_dbscan_inner',\n                         sources=['_dbscan_inner.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         language=\"c++\")\n\n    config.add_extension('_hierarchical_fast',\n                         sources=['_hierarchical_fast.pyx'],\n                         language=\"c++\",\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_k_means_fast',\n                         sources=['_k_means_fast.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_k_means_lloyd',\n                         sources=['_k_means_lloyd.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_k_means_elkan',\n                         sources=['_k_means_elkan.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_subpackage('tests')\n\n    return config"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__",
      "name": "__init__",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/transformers",
          "name": "transformers",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.transformers",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "list of tuples",
            "default_value": "",
            "description": "List of (name, transformer, columns) tuples specifying the\ntransformer objects to be applied to subsets of the data.\n\nname : str\n    Like in Pipeline and FeatureUnion, this allows the transformer and\n    its parameters to be set using ``set_params`` and searched in grid\n    search.\ntransformer : {'drop', 'passthrough'} or estimator\n    Estimator must support :term:`fit` and :term:`transform`.\n    Special-cased strings 'drop' and 'passthrough' are accepted as\n    well, to indicate to drop the columns or to pass them through\n    untransformed, respectively.\ncolumns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable\n    Indexes the data on its second axis. Integers are interpreted as\n    positional columns, while strings can reference DataFrame columns\n    by name.  A scalar string or int should be used where\n    ``transformer`` expects X to be a 1d array-like (vector),\n    otherwise a 2d array will be passed to the transformer.\n    A callable is passed the input data `X` and can return any of the\n    above. To select multiple columns by name or dtype, you can use\n    :obj:`make_column_selector`."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of tuples"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/remainder",
          "name": "remainder",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.remainder",
          "default_value": "'drop'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'drop', 'passthrough'} or estimator",
            "default_value": "'drop'",
            "description": "By default, only the specified columns in `transformers` are\ntransformed and combined in the output, and the non-specified\ncolumns are dropped. (default of ``'drop'``).\nBy specifying ``remainder='passthrough'``, all remaining columns that\nwere not specified in `transformers` will be automatically passed\nthrough. This subset of columns is concatenated with the output of\nthe transformers.\nBy setting ``remainder`` to be an estimator, the remaining\nnon-specified columns will use the ``remainder`` estimator. The\nestimator must support :term:`fit` and :term:`transform`.\nNote that using this feature requires that the DataFrame columns\ninput at :term:`fit` and :term:`transform` have identical order."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "drop",
                  "passthrough"
                ]
              },
              {
                "kind": "NamedType",
                "name": "estimator"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/sparse_threshold",
          "name": "sparse_threshold",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.sparse_threshold",
          "default_value": "0.3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.3",
            "description": "If the output of the different transformers contains sparse matrices,\nthese will be stacked as a sparse matrix if the overall density is\nlower than this value. Use ``sparse_threshold=0`` to always return\ndense.  When the transformed output consists of all dense data, the\nstacked result will be dense, and this keyword will be ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/transformer_weights",
          "name": "transformer_weights",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.transformer_weights",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Multiplicative weights for features per transformer. The output of the\ntransformer is multiplied by these weights. Keys are transformer names,\nvalues the weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the time elapsed while fitting each transformer will be\nprinted as it is completed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Applies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input\nto be transformed separately and the features generated by each transformer\nwill be concatenated to form a single feature space.\nThis is useful for heterogeneous or columnar data, to combine several\nfeature extraction mechanisms or transformations into a single transformer.\n\nRead more in the :ref:`User Guide <column_transformer>`.\n\n.. versionadded:: 0.20",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 transformers, *,\n                 remainder='drop',\n                 sparse_threshold=0.3,\n                 n_jobs=None,\n                 transformer_weights=None,\n                 verbose=False):\n        self.transformers = transformers\n        self.remainder = remainder\n        self.sparse_threshold = sparse_threshold\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit",
      "name": "fit",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit/X",
          "name": "X",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, dataframe} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data, of which specified subsets are used to fit the\ntransformers."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit/y",
          "name": "y",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,...)",
            "default_value": "None",
            "description": "Targets for supervised learning."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,...)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit all transformers using X.",
      "docstring": "Fit all transformers using X.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    Input data, of which specified subsets are used to fit the\n    transformers.\n\ny : array-like of shape (n_samples,...), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nself : ColumnTransformer\n    This estimator",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,...), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        self : ColumnTransformer\n            This estimator\n\n        \"\"\"\n        # we use fit_transform to make sure to set sparse_output_ (for which we\n        # need the transformed data) to have consistent output type in predict\n        self.fit_transform(X, y=y)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform/X",
          "name": "X",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, dataframe} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data, of which specified subsets are used to fit the\ntransformers."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/fit_transform/y",
          "name": "y",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Targets for supervised learning."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit all transformers, transform the data and concatenate results.",
      "docstring": "Fit all transformers, transform the data and concatenate results.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    Input data, of which specified subsets are used to fit the\n    transformers.\n\ny : array-like of shape (n_samples,), default=None\n    Targets for supervised learning.\n\nReturns\n-------\nX_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers. If\n    any result is a sparse matrix, everything will be converted to\n    sparse matrices.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like of shape (n_samples,), default=None\n            Targets for supervised learning.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        # TODO: this should be `feature_names_in_` when we start having it\n        if hasattr(X, \"columns\"):\n            self._feature_names_in = np.asarray(X.columns)\n        else:\n            self._feature_names_in = None\n        X = _check_X(X)\n        # set n_features_in_ attribute\n        self._check_n_features(X, reset=True)\n        self._validate_transformers()\n        self._validate_column_callables(X)\n        self._validate_remainder(X)\n\n        result = self._fit_transform(X, y, _fit_transform_one)\n\n        if not result:\n            self._update_fitted_transformers([])\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*result)\n\n        # determine if concatenated output will be sparse or not\n        if any(sparse.issparse(X) for X in Xs):\n            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)\n            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)\n                        else X.size for X in Xs)\n            density = nnz / total\n            self.sparse_output_ = density < self.sparse_threshold\n        else:\n            self.sparse_output_ = False\n\n        self._update_fitted_transformers(transformers)\n        self._validate_output(Xs)\n\n        return self._hstack(list(Xs))"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_feature_names",
      "name": "get_feature_names",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_feature_names",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_feature_names/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_feature_names.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Get feature names from all transformers.",
      "docstring": "Get feature names from all transformers.\n\nReturns\n-------\nfeature_names : list of strings\n    Names of the features produced by transform.",
      "code": "    def get_feature_names(self):\n        \"\"\"Get feature names from all transformers.\n\n        Returns\n        -------\n        feature_names : list of strings\n            Names of the features produced by transform.\n        \"\"\"\n        check_is_fitted(self)\n        feature_names = []\n        for name, trans, column, _ in self._iter(fitted=True):\n            if trans == 'drop' or _is_empty_column_selection(column):\n                continue\n            if trans == 'passthrough':\n                if hasattr(self, '_df_columns'):\n                    if ((not isinstance(column, slice))\n                            and all(isinstance(col, str) for col in column)):\n                        feature_names.extend(column)\n                    else:\n                        feature_names.extend(self._df_columns[column])\n                else:\n                    indices = np.arange(self._n_features)\n                    feature_names.extend(['x%d' % i for i in indices[column]])\n                continue\n            if not hasattr(trans, 'get_feature_names'):\n                raise AttributeError(\"Transformer %s (type %s) does not \"\n                                     \"provide get_feature_names.\"\n                                     % (str(name), type(trans).__name__))\n            feature_names.extend([name + \"__\" + f for f in\n                                  trans.get_feature_names()])\n        return feature_names"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params",
      "name": "get_params",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_params",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_params.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/get_params/deep",
          "name": "deep",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.get_params.deep",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True, will return the parameters for this estimator and\ncontained subobjects that are estimators."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformers` of the\n`ColumnTransformer`.",
      "docstring": "Get parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the\nestimators contained within the `transformers` of the\n`ColumnTransformer`.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, will return the parameters for this estimator and\n    contained subobjects that are estimators.\n\nReturns\n-------\nparams : dict\n    Parameter names mapped to their values.",
      "code": "    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `transformers` of the\n        `ColumnTransformer`.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        return self._get_params('_transformers', deep=deep)"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/named_transformers_@getter",
      "name": "named_transformers_",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.named_transformers_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/named_transformers_@getter/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.named_transformers_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Access the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name.\nKeys are transformer names and values are the fitted transformer\nobjects.",
      "docstring": "Access the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name.\nKeys are transformer names and values are the fitted transformer\nobjects.",
      "code": "    @property\n    def named_transformers_(self):\n        \"\"\"Access the fitted transformer by name.\n\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n        \"\"\"\n        # Use Bunch object to improve autocomplete\n        return Bunch(**{name: trans for name, trans, _\n                        in self.transformers_})"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params",
      "name": "set_params",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.set_params",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.set_params.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/set_params/kwargs",
          "name": "kwargs",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.set_params.kwargs",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that you\ncan directly set the parameters of the estimators contained in\n`transformers` of `ColumnTransformer`.",
      "docstring": "Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``. Note that you\ncan directly set the parameters of the estimators contained in\n`transformers` of `ColumnTransformer`.\n\nReturns\n-------\nself",
      "code": "    def set_params(self, **kwargs):\n        \"\"\"Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``. Note that you\n        can directly set the parameters of the estimators contained in\n        `transformers` of `ColumnTransformer`.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._set_params('_transformers', **kwargs)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform",
      "name": "transform",
      "qname": "sklearn.compose._column_transformer.ColumnTransformer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/ColumnTransformer/transform/X",
          "name": "X",
          "qname": "sklearn.compose._column_transformer.ColumnTransformer.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, dataframe} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data to be transformed by subset."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X separately by each transformer, concatenate results.",
      "docstring": "Transform X separately by each transformer, concatenate results.\n\nParameters\n----------\nX : {array-like, dataframe} of shape (n_samples, n_features)\n    The data to be transformed by subset.\n\nReturns\n-------\nX_t : {array-like, sparse matrix} of                 shape (n_samples, sum_n_components)\n    hstack of results of transformers. sum_n_components is the\n    sum of n_components (output dimension) over transformers. If\n    any result is a sparse matrix, everything will be converted to\n    sparse matrices.",
      "code": "    def transform(self, X):\n        \"\"\"Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : {array-like, dataframe} of shape (n_samples, n_features)\n            The data to be transformed by subset.\n\n        Returns\n        -------\n        X_t : {array-like, sparse matrix} of \\\n                shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        \"\"\"\n        check_is_fitted(self)\n        X = _check_X(X)\n        if hasattr(X, \"columns\"):\n            X_feature_names = np.asarray(X.columns)\n        else:\n            X_feature_names = None\n\n        self._check_n_features(X, reset=False)\n        if (self._feature_names_in is not None and\n            X_feature_names is not None and\n                np.any(self._feature_names_in != X_feature_names)):\n            raise RuntimeError(\n                \"Given feature/column names do not match the ones for the \"\n                \"data given during fit.\"\n            )\n        Xs = self._fit_transform(X, None, _transform_one, fitted=True)\n        self._validate_output(Xs)\n\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        return self._hstack(list(Xs))"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__",
      "name": "__call__",
      "qname": "sklearn.compose._column_transformer.make_column_selector.__call__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__call__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__call__/df",
          "name": "df",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__call__.df",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "dataframe of shape (n_features, n_samples)",
            "default_value": "",
            "description": "DataFrame to select columns from."
          },
          "type": {
            "kind": "NamedType",
            "name": "dataframe of shape (n_features, n_samples)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Callable for column selection to be used by a\n:class:`ColumnTransformer`.",
      "docstring": "Callable for column selection to be used by a\n:class:`ColumnTransformer`.\n\nParameters\n----------\ndf : dataframe of shape (n_features, n_samples)\n    DataFrame to select columns from.",
      "code": "    def __call__(self, df):\n        \"\"\"Callable for column selection to be used by a\n        :class:`ColumnTransformer`.\n\n        Parameters\n        ----------\n        df : dataframe of shape (n_features, n_samples)\n            DataFrame to select columns from.\n        \"\"\"\n        if not hasattr(df, 'iloc'):\n            raise ValueError(\"make_column_selector can only be applied to \"\n                             \"pandas dataframes\")\n        df_row = df.iloc[:1]\n        if self.dtype_include is not None or self.dtype_exclude is not None:\n            df_row = df_row.select_dtypes(include=self.dtype_include,\n                                          exclude=self.dtype_exclude)\n        cols = df_row.columns\n        if self.pattern is not None:\n            cols = cols[cols.str.contains(self.pattern, regex=True)]\n        return cols.tolist()"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__",
      "name": "__init__",
      "qname": "sklearn.compose._column_transformer.make_column_selector.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/self",
          "name": "self",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/pattern",
          "name": "pattern",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.pattern",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Name of columns containing this regex pattern will be included. If\nNone, column selection will not be selected based on pattern."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/dtype_include",
          "name": "dtype_include",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.dtype_include",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "column dtype or list of column dtypes",
            "default_value": "None",
            "description": "A selection of dtypes to include. For more details, see\n:meth:`pandas.DataFrame.select_dtypes`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "column dtype"
              },
              {
                "kind": "NamedType",
                "name": "list of column dtypes"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_selector/__init__/dtype_exclude",
          "name": "dtype_exclude",
          "qname": "sklearn.compose._column_transformer.make_column_selector.__init__.dtype_exclude",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "column dtype or list of column dtypes",
            "default_value": "None",
            "description": "A selection of dtypes to exclude. For more details, see\n:meth:`pandas.DataFrame.select_dtypes`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "column dtype"
              },
              {
                "kind": "NamedType",
                "name": "list of column dtypes"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Create a callable to select columns to be used with\n:class:`ColumnTransformer`.\n\n:func:`make_column_selector` can select columns based on datatype or the\ncolumns name with a regex. When using multiple selection criteria, **all**\ncriteria must match for a column to be selected.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, pattern=None, *, dtype_include=None,\n                 dtype_exclude=None):\n        self.pattern = pattern\n        self.dtype_include = dtype_include\n        self.dtype_exclude = dtype_exclude"
    },
    {
      "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer",
      "name": "make_column_transformer",
      "qname": "sklearn.compose._column_transformer.make_column_transformer",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/transformers",
          "name": "transformers",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.transformers",
          "default_value": null,
          "assigned_by": "POSITIONAL_VARARG",
          "is_public": true,
          "docstring": {
            "type": "tuples",
            "default_value": "",
            "description": "Tuples of the form (transformer, columns) specifying the\ntransformer objects to be applied to subsets of the data.\n\ntransformer : {'drop', 'passthrough'} or estimator\n    Estimator must support :term:`fit` and :term:`transform`.\n    Special-cased strings 'drop' and 'passthrough' are accepted as\n    well, to indicate to drop the columns or to pass them through\n    untransformed, respectively.\ncolumns : str,  array-like of str, int, array-like of int, slice,                 array-like of bool or callable\n    Indexes the data on its second axis. Integers are interpreted as\n    positional columns, while strings can reference DataFrame columns\n    by name. A scalar string or int should be used where\n    ``transformer`` expects X to be a 1d array-like (vector),\n    otherwise a 2d array will be passed to the transformer.\n    A callable is passed the input data `X` and can return any of the\n    above. To select multiple columns by name or dtype, you can use\n    :obj:`make_column_selector`."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuples"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/remainder",
          "name": "remainder",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.remainder",
          "default_value": "'drop'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'drop', 'passthrough'} or estimator",
            "default_value": "'drop'",
            "description": "By default, only the specified columns in `transformers` are\ntransformed and combined in the output, and the non-specified\ncolumns are dropped. (default of ``'drop'``).\nBy specifying ``remainder='passthrough'``, all remaining columns that\nwere not specified in `transformers` will be automatically passed\nthrough. This subset of columns is concatenated with the output of\nthe transformers.\nBy setting ``remainder`` to be an estimator, the remaining\nnon-specified columns will use the ``remainder`` estimator. The\nestimator must support :term:`fit` and :term:`transform`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "drop",
                  "passthrough"
                ]
              },
              {
                "kind": "NamedType",
                "name": "estimator"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/sparse_threshold",
          "name": "sparse_threshold",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.sparse_threshold",
          "default_value": "0.3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.3",
            "description": "If the transformed output consists of a mix of sparse and dense data,\nit will be stacked as a sparse matrix if the density is lower than this\nvalue. Use ``sparse_threshold=0`` to always return dense.\nWhen the transformed output consists of all sparse or all dense data,\nthe stacked result will be sparse or dense, respectively, and this\nkeyword will be ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._column_transformer/make_column_transformer/verbose",
          "name": "verbose",
          "qname": "sklearn.compose._column_transformer.make_column_transformer.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the time elapsed while fitting each transformer will be\nprinted as it is completed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.compose"
      ],
      "description": "Construct a ColumnTransformer from the given transformers.\n\nThis is a shorthand for the ColumnTransformer constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will\nbe given names automatically based on their types. It also does not allow\nweighting with ``transformer_weights``.\n\nRead more in the :ref:`User Guide <make_column_transformer>`.",
      "docstring": "Construct a ColumnTransformer from the given transformers.\n\nThis is a shorthand for the ColumnTransformer constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will\nbe given names automatically based on their types. It also does not allow\nweighting with ``transformer_weights``.\n\nRead more in the :ref:`User Guide <make_column_transformer>`.\n\nParameters\n----------\n*transformers : tuples\n    Tuples of the form (transformer, columns) specifying the\n    transformer objects to be applied to subsets of the data.\n\n    transformer : {'drop', 'passthrough'} or estimator\n        Estimator must support :term:`fit` and :term:`transform`.\n        Special-cased strings 'drop' and 'passthrough' are accepted as\n        well, to indicate to drop the columns or to pass them through\n        untransformed, respectively.\n    columns : str,  array-like of str, int, array-like of int, slice,                 array-like of bool or callable\n        Indexes the data on its second axis. Integers are interpreted as\n        positional columns, while strings can reference DataFrame columns\n        by name. A scalar string or int should be used where\n        ``transformer`` expects X to be a 1d array-like (vector),\n        otherwise a 2d array will be passed to the transformer.\n        A callable is passed the input data `X` and can return any of the\n        above. To select multiple columns by name or dtype, you can use\n        :obj:`make_column_selector`.\n\nremainder : {'drop', 'passthrough'} or estimator, default='drop'\n    By default, only the specified columns in `transformers` are\n    transformed and combined in the output, and the non-specified\n    columns are dropped. (default of ``'drop'``).\n    By specifying ``remainder='passthrough'``, all remaining columns that\n    were not specified in `transformers` will be automatically passed\n    through. This subset of columns is concatenated with the output of\n    the transformers.\n    By setting ``remainder`` to be an estimator, the remaining\n    non-specified columns will use the ``remainder`` estimator. The\n    estimator must support :term:`fit` and :term:`transform`.\n\nsparse_threshold : float, default=0.3\n    If the transformed output consists of a mix of sparse and dense data,\n    it will be stacked as a sparse matrix if the density is lower than this\n    value. Use ``sparse_threshold=0`` to always return dense.\n    When the transformed output consists of all sparse or all dense data,\n    the stacked result will be sparse or dense, respectively, and this\n    keyword will be ignored.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nverbose : bool, default=False\n    If True, the time elapsed while fitting each transformer will be\n    printed as it is completed.\n\nReturns\n-------\nct : ColumnTransformer\n\nSee Also\n--------\nColumnTransformer : Class that allows combining the\n    outputs of multiple transformer objects used on column subsets\n    of the data into a single feature space.\n\nExamples\n--------\n>>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n>>> from sklearn.compose import make_column_transformer\n>>> make_column_transformer(\n...     (StandardScaler(), ['numerical_column']),\n...     (OneHotEncoder(), ['categorical_column']))\nColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                 ['numerical_column']),\n                                ('onehotencoder', OneHotEncoder(...),\n                                 ['categorical_column'])])",
      "code": "def make_column_transformer(*transformers,\n                            remainder='drop',\n                            sparse_threshold=0.3,\n                            n_jobs=None,\n                            verbose=False):\n    \"\"\"Construct a ColumnTransformer from the given transformers.\n\n    This is a shorthand for the ColumnTransformer constructor; it does not\n    require, and does not permit, naming the transformers. Instead, they will\n    be given names automatically based on their types. It also does not allow\n    weighting with ``transformer_weights``.\n\n    Read more in the :ref:`User Guide <make_column_transformer>`.\n\n    Parameters\n    ----------\n    *transformers : tuples\n        Tuples of the form (transformer, columns) specifying the\n        transformer objects to be applied to subsets of the data.\n\n        transformer : {'drop', 'passthrough'} or estimator\n            Estimator must support :term:`fit` and :term:`transform`.\n            Special-cased strings 'drop' and 'passthrough' are accepted as\n            well, to indicate to drop the columns or to pass them through\n            untransformed, respectively.\n        columns : str,  array-like of str, int, array-like of int, slice, \\\n                array-like of bool or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name. A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above. To select multiple columns by name or dtype, you can use\n            :obj:`make_column_selector`.\n\n    remainder : {'drop', 'passthrough'} or estimator, default='drop'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``'drop'``).\n        By specifying ``remainder='passthrough'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support :term:`fit` and :term:`transform`.\n\n    sparse_threshold : float, default=0.3\n        If the transformed output consists of a mix of sparse and dense data,\n        it will be stacked as a sparse matrix if the density is lower than this\n        value. Use ``sparse_threshold=0`` to always return dense.\n        When the transformed output consists of all sparse or all dense data,\n        the stacked result will be sparse or dense, respectively, and this\n        keyword will be ignored.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    Returns\n    -------\n    ct : ColumnTransformer\n\n    See Also\n    --------\n    ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> make_column_transformer(\n    ...     (StandardScaler(), ['numerical_column']),\n    ...     (OneHotEncoder(), ['categorical_column']))\n    ColumnTransformer(transformers=[('standardscaler', StandardScaler(...),\n                                     ['numerical_column']),\n                                    ('onehotencoder', OneHotEncoder(...),\n                                     ['categorical_column'])])\n\n    \"\"\"\n    # transformer_weights keyword is not passed through because the user\n    # would need to know the automatically generated names of the transformers\n    transformer_list = _get_transformer_list(transformers)\n    return ColumnTransformer(transformer_list, n_jobs=n_jobs,\n                             remainder=remainder,\n                             sparse_threshold=sparse_threshold,\n                             verbose=verbose)"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/regressor",
          "name": "regressor",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.regressor",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "Regressor object such as derived from ``RegressorMixin``. This\nregressor will automatically be cloned each time prior to fitting.\nIf regressor is ``None``, ``LinearRegression()`` is created and used."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/transformer",
          "name": "transformer",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.transformer",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "Estimator object such as derived from ``TransformerMixin``. Cannot be\nset at the same time as ``func`` and ``inverse_func``. If\n``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,\nthe transformer will be an identity transformer. Note that the\ntransformer will be cloned during fitting. Also, the transformer is\nrestricting ``y`` to be a numpy array."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/func",
          "name": "func",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.func",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "function",
            "default_value": "None",
            "description": "Function to apply to ``y`` before passing to ``fit``. Cannot be set at\nthe same time as ``transformer``. The function needs to return a\n2-dimensional array. If ``func`` is ``None``, the function used will be\nthe identity function."
          },
          "type": {
            "kind": "NamedType",
            "name": "function"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/inverse_func",
          "name": "inverse_func",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.inverse_func",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "function",
            "default_value": "None",
            "description": "Function to apply to the prediction of the regressor. Cannot be set at\nthe same time as ``transformer`` as well. The function needs to return\na 2-dimensional array. The inverse function is used to return\npredictions to the same space of the original training labels."
          },
          "type": {
            "kind": "NamedType",
            "name": "function"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/__init__/check_inverse",
          "name": "check_inverse",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.__init__.check_inverse",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to check that ``transform`` followed by ``inverse_transform``\nor ``func`` followed by ``inverse_func`` leads to the original targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Meta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target ``y`` in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a function and its inverse such as\n``log`` and ``exp``.\n\nThe computation during ``fit`` is::\n\n    regressor.fit(X, func(y))\n\nor::\n\n    regressor.fit(X, transformer.transform(y))\n\nThe computation during ``predict`` is::\n\n    inverse_func(regressor.predict(X))\n\nor::\n\n    transformer.inverse_transform(regressor.predict(X))\n\nRead more in the :ref:`User Guide <transformed_target_regressor>`.\n\n.. versionadded:: 0.20",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, regressor=None, *, transformer=None,\n                 func=None, inverse_func=None, check_inverse=True):\n        self.regressor = regressor\n        self.transformer = transformer\n        self.func = func\n        self.inverse_func = inverse_func\n        self.check_inverse = check_inverse"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit",
      "name": "fit",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/X",
          "name": "X",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/y",
          "name": "y",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/fit/fit_params",
          "name": "fit_params",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.fit.fit_params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "",
            "description": "Parameters passed to the ``fit`` method of the underlying\nregressor."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model according to the given training data.",
      "docstring": "Fit the model according to the given training data.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\n**fit_params : dict\n    Parameters passed to the ``fit`` method of the underlying\n    regressor.\n\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **fit_params : dict\n            Parameters passed to the ``fit`` method of the underlying\n            regressor.\n\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        y = check_array(y, accept_sparse=False, force_all_finite=True,\n                        ensure_2d=False, dtype='numeric')\n\n        # store the number of dimension of the target to predict an array of\n        # similar shape at predict\n        self._training_dim = y.ndim\n\n        # transformers are designed to modify X which is 2d dimensional, we\n        # need to modify y accordingly.\n        if y.ndim == 1:\n            y_2d = y.reshape(-1, 1)\n        else:\n            y_2d = y\n        self._fit_transformer(y_2d)\n\n        # transform y and convert back to 1d array if needed\n        y_trans = self.transformer_.transform(y_2d)\n        # FIXME: a FunctionTransformer can return a 1D array even when validate\n        # is set to True. Therefore, we need to check the number of dimension\n        # first.\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\n            y_trans = y_trans.squeeze(axis=1)\n\n        if self.regressor is None:\n            from ..linear_model import LinearRegression\n            self.regressor_ = LinearRegression()\n        else:\n            self.regressor_ = clone(self.regressor)\n\n        self.regressor_.fit(X, y_trans, **fit_params)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/n_features_in_@getter",
      "name": "n_features_in_",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.n_features_in_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/n_features_in_@getter/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.n_features_in_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def n_features_in_(self):\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() returns False the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n\n        return self.regressor_.n_features_in_"
    },
    {
      "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict",
      "name": "predict",
      "qname": "sklearn.compose._target.TransformedTargetRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.compose._target/TransformedTargetRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.compose._target.TransformedTargetRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the ``inverse_func`` or\n``inverse_transform`` is applied before returning the prediction.",
      "docstring": "Predict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the ``inverse_func`` or\n``inverse_transform`` is applied before returning the prediction.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Samples.\n\nReturns\n-------\ny_hat : ndarray of shape (n_samples,)\n    Predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict using the base regressor, applying inverse.\n\n        The regressor is used to predict and the ``inverse_func`` or\n        ``inverse_transform`` is applied before returning the prediction.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        y_hat : ndarray of shape (n_samples,)\n            Predicted values.\n\n        \"\"\"\n        check_is_fitted(self)\n        pred = self.regressor_.predict(X)\n        if pred.ndim == 1:\n            pred_trans = self.transformer_.inverse_transform(\n                pred.reshape(-1, 1))\n        else:\n            pred_trans = self.transformer_.inverse_transform(pred)\n        if (self._training_dim == 1 and\n                pred_trans.ndim == 2 and pred_trans.shape[1] == 1):\n            pred_trans = pred_trans.squeeze(axis=1)\n\n        return pred_trans"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the support of robust location and covariance estimates\nis computed, and a covariance estimate is recomputed from it,\nwithout centering the data.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, the robust location and covariance are directly computed\nwith the FastMCD algorithm without additional treatment."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/support_fraction",
          "name": "support_fraction",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.support_fraction",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The proportion of points to be included in the support of the raw\nMCD estimate. If None, the minimum value of support_fraction will\nbe used within the algorithm: `[n_sample + n_features + 1] / 2`.\nRange is (0, 1)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/contamination",
          "name": "contamination",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.contamination",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The amount of contamination of the data set, i.e. the proportion\nof outliers in the data set. Range is (0, 0.5)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines the pseudo random number generator for shuffling\nthe data. Pass an int for reproducible results across multiple function\ncalls. See :term: `Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, contamination=0.1,\n                 random_state=None):\n        super().__init__(\n            store_precision=store_precision,\n            assume_centered=assume_centered,\n            support_fraction=support_fraction,\n            random_state=random_state)\n        self.contamination = contamination"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function",
      "name": "decision_function",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/decision_function/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the decision function of the given observations.",
      "docstring": "Compute the decision function of the given observations.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\ndecision : ndarray of shape (n_samples,)\n    Decision function of the samples.\n    It is equal to the shifted Mahalanobis distances.\n    The threshold for being an outlier is 0, which ensures a\n    compatibility with other outlier detection algorithms.",
      "code": "    def decision_function(self, X):\n        \"\"\"Compute the decision function of the given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,)\n            Decision function of the samples.\n            It is equal to the shifted Mahalanobis distances.\n            The threshold for being an outlier is 0, which ensures a\n            compatibility with other outlier detection algorithms.\n        \"\"\"\n        check_is_fitted(self)\n        negative_mahal_dist = self.score_samples(X)\n        return negative_mahal_dist - self.offset_"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit",
      "name": "fit",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the EllipticEnvelope model.",
      "docstring": "Fit the EllipticEnvelope model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n    Not used, present for API consistency by convention.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the EllipticEnvelope model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n        \"\"\"\n        super().fit(X)\n        self.offset_ = np.percentile(-self.dist_, 100. * self.contamination)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict",
      "name": "predict",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/predict/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict the labels (1 inlier, -1 outlier) of X according to the\nfitted model.",
      "docstring": "Predict the labels (1 inlier, -1 outlier) of X according to the\nfitted model.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    Returns -1 for anomalies/outliers and +1 for inliers.",
      "code": "    def predict(self, X):\n        \"\"\"\n        Predict the labels (1 inlier, -1 outlier) of X according to the\n        fitted model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            Returns -1 for anomalies/outliers and +1 for inliers.\n        \"\"\"\n        X = check_array(X)\n        is_inlier = np.full(X.shape[0], -1, dtype=int)\n        values = self.decision_function(X)\n        is_inlier[values >= 0] = 1\n\n        return is_inlier"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score",
      "name": "score",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Test samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/y",
          "name": "y",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_outputs)",
            "default_value": "",
            "description": "True labels for X."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_outputs)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.",
      "docstring": "Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Test samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    True labels for X.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights.\n\nReturns\n-------\nscore : float\n    Mean accuracy of self.predict(X) w.r.t. y.",
      "code": "    def score(self, X, y, sample_weight=None):\n        \"\"\"Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples",
      "name": "score_samples",
      "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples/self",
          "name": "self",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._elliptic_envelope/EllipticEnvelope/score_samples/X",
          "name": "X",
          "qname": "sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the negative Mahalanobis distances.",
      "docstring": "Compute the negative Mahalanobis distances.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix.\n\nReturns\n-------\nnegative_mahal_distances : array-like of shape (n_samples,)\n    Opposite of the Mahalanobis distances.",
      "code": "    def score_samples(self, X):\n        \"\"\"Compute the negative Mahalanobis distances.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data matrix.\n\n        Returns\n        -------\n        negative_mahal_distances : array-like of shape (n_samples,)\n            Opposite of the Mahalanobis distances.\n        \"\"\"\n        check_is_fitted(self)\n        return -self.mahalanobis(X)"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specifies if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data are not centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False (default), data are centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm",
      "name": "error_norm",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/comp_cov",
          "name": "comp_cov",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.comp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_features, n_features)",
            "default_value": "",
            "description": "The covariance to compare with."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/norm",
          "name": "norm",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.norm",
          "default_value": "'frobenius'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{\"frobenius\", \"spectral\"}",
            "default_value": "\"frobenius\"",
            "description": "The type of norm used to compute the error. Available error types:\n- 'frobenius' (default): sqrt(tr(A^t.A))\n- 'spectral': sqrt(max(eigenvalues(A^t.A))\nwhere A is the error ``(comp_cov - self.covariance_)``."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "spectral",
              "frobenius"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/scaling",
          "name": "scaling",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.scaling",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True (default), the squared error norm is divided by n_features.\nIf False, the squared error norm is not rescaled."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/error_norm/squared",
          "name": "squared",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm.squared",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to compute the squared error norm or the error norm.\nIf True (default), the squared error norm is returned.\nIf False, the error norm is returned."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).",
      "docstring": "Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n\nParameters\n----------\ncomp_cov : array-like of shape (n_features, n_features)\n    The covariance to compare with.\n\nnorm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n    The type of norm used to compute the error. Available error types:\n    - 'frobenius' (default): sqrt(tr(A^t.A))\n    - 'spectral': sqrt(max(eigenvalues(A^t.A))\n    where A is the error ``(comp_cov - self.covariance_)``.\n\nscaling : bool, default=True\n    If True (default), the squared error norm is divided by n_features.\n    If False, the squared error norm is not rescaled.\n\nsquared : bool, default=True\n    Whether to compute the squared error norm or the error norm.\n    If True (default), the squared error norm is returned.\n    If False, the error norm is returned.\n\nReturns\n-------\nresult : float\n    The Mean Squared Error (in the sense of the Frobenius norm) between\n    `self` and `comp_cov` covariance estimators.",
      "code": "    def error_norm(self, comp_cov, norm='frobenius', scaling=True,\n                   squared=True):\n        \"\"\"Computes the Mean Squared Error between two covariance estimators.\n        (In the sense of the Frobenius norm).\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.\n\n        Returns\n        -------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.\n        \"\"\"\n        # compute the error\n        error = comp_cov - self.covariance_\n        # compute the error norm\n        if norm == \"frobenius\":\n            squared_norm = np.sum(error ** 2)\n        elif norm == \"spectral\":\n            squared_norm = np.amax(linalg.svdvals(np.dot(error.T, error)))\n        else:\n            raise NotImplementedError(\n                \"Only spectral and frobenius norms are implemented\")\n        # optionally scale the error norm\n        if scaling:\n            squared_norm = squared_norm / error.shape[0]\n        # finally get either the squared norm or the norm\n        if squared:\n            result = squared_norm\n        else:\n            result = np.sqrt(squared_norm)\n\n        return result"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit",
      "name": "fit",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits the Maximum Likelihood Estimator covariance model\naccording to the given training data and parameters.",
      "docstring": "Fits the Maximum Likelihood Estimator covariance model\naccording to the given training data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n  Training data, where n_samples is the number of samples and\n  n_features is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits the Maximum Likelihood Estimator covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n          Training data, where n_samples is the number of samples and\n          n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/get_precision",
      "name": "get_precision",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.get_precision",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/get_precision/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.get_precision.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Getter for the precision matrix.",
      "docstring": "Getter for the precision matrix.\n\nReturns\n-------\nprecision_ : array-like of shape (n_features, n_features)\n    The precision matrix associated to the current covariance object.",
      "code": "    def get_precision(self):\n        \"\"\"Getter for the precision matrix.\n\n        Returns\n        -------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.\n        \"\"\"\n        if self.store_precision:\n            precision = self.precision_\n        else:\n            precision = linalg.pinvh(self.covariance_, check_finite=False)\n        return precision"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis",
      "name": "mahalanobis",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/mahalanobis/X",
          "name": "X",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The observations, the Mahalanobis distances of the which we\ncompute. Observations are assumed to be drawn from the same\ndistribution than the data used in fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Computes the squared Mahalanobis distances of given observations.",
      "docstring": "Computes the squared Mahalanobis distances of given observations.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The observations, the Mahalanobis distances of the which we\n    compute. Observations are assumed to be drawn from the same\n    distribution than the data used in fit.\n\nReturns\n-------\ndist : ndarray of shape (n_samples,)\n    Squared Mahalanobis distances of the observations.",
      "code": "    def mahalanobis(self, X):\n        \"\"\"Computes the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.\n\n        Returns\n        -------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.\n        \"\"\"\n        precision = self.get_precision()\n        # compute mahalanobis distances\n        dist = pairwise_distances(X, self.location_[np.newaxis, :],\n                                  metric='mahalanobis', VI=precision)\n\n        return np.reshape(dist, (len(X),)) ** 2"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score",
      "name": "score",
      "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score/self",
          "name": "self",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score/X_test",
          "name": "X_test",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score.X_test",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Test data of which we compute the likelihood, where n_samples is\nthe number of samples and n_features is the number of features.\nX_test is assumed to be drawn from the same distribution than\nthe data used in fit (including centering)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/EmpiricalCovariance/score/y",
          "name": "y",
          "qname": "sklearn.covariance._empirical_covariance.EmpiricalCovariance.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.",
      "docstring": "Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n\nParameters\n----------\nX_test : array-like of shape (n_samples, n_features)\n    Test data of which we compute the likelihood, where n_samples is\n    the number of samples and n_features is the number of features.\n    X_test is assumed to be drawn from the same distribution than\n    the data used in fit (including centering).\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nres : float\n    The likelihood of the data set with `self.covariance_` as an\n    estimator of its covariance matrix.",
      "code": "    def score(self, X_test, y=None):\n        \"\"\"Computes the log-likelihood of a Gaussian data set with\n        `self.covariance_` as an estimator of its covariance matrix.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where n_samples is\n            the number of samples and n_features is the number of features.\n            X_test is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        res : float\n            The likelihood of the data set with `self.covariance_` as an\n            estimator of its covariance matrix.\n        \"\"\"\n        # compute empirical covariance of the test set\n        test_cov = empirical_covariance(\n            X_test - self.location_, assume_centered=True)\n        # compute log likelihood\n        res = log_likelihood(test_cov, self.get_precision())\n\n        return res"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/empirical_covariance",
      "name": "empirical_covariance",
      "qname": "sklearn.covariance._empirical_covariance.empirical_covariance",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/empirical_covariance/X",
          "name": "X",
          "qname": "sklearn.covariance._empirical_covariance.empirical_covariance.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/empirical_covariance/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._empirical_covariance.empirical_covariance.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Computes the Maximum likelihood covariance estimator",
      "docstring": "Computes the Maximum likelihood covariance estimator\n\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful when working with data whose mean is almost, but not exactly\n    zero.\n    If False, data will be centered before computation.\n\nReturns\n-------\ncovariance : ndarray of shape (n_features, n_features)\n    Empirical covariance (Maximum Likelihood Estimator).\n\nExamples\n--------\n>>> from sklearn.covariance import empirical_covariance\n>>> X = [[1,1,1],[1,1,1],[1,1,1],\n...      [0,0,0],[0,0,0],[0,0,0]]\n>>> empirical_covariance(X)\narray([[0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25]])",
      "code": "@_deprecate_positional_args\ndef empirical_covariance(X, *, assume_centered=False):\n    \"\"\"Computes the Maximum likelihood covariance estimator\n\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False, data will be centered before computation.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        Empirical covariance (Maximum Likelihood Estimator).\n\n    Examples\n    --------\n    >>> from sklearn.covariance import empirical_covariance\n    >>> X = [[1,1,1],[1,1,1],[1,1,1],\n    ...      [0,0,0],[0,0,0],[0,0,0]]\n    >>> empirical_covariance(X)\n    array([[0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25],\n           [0.25, 0.25, 0.25]])\n    \"\"\"\n    X = np.asarray(X)\n\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n\n    if X.shape[0] == 1:\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n\n    if assume_centered:\n        covariance = np.dot(X.T, X) / X.shape[0]\n    else:\n        covariance = np.cov(X.T, bias=1)\n\n    if covariance.ndim == 0:\n        covariance = np.array([[covariance]])\n    return covariance"
    },
    {
      "id": "scikit-learn/sklearn.covariance._empirical_covariance/log_likelihood",
      "name": "log_likelihood",
      "qname": "sklearn.covariance._empirical_covariance.log_likelihood",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/log_likelihood/emp_cov",
          "name": "emp_cov",
          "qname": "sklearn.covariance._empirical_covariance.log_likelihood.emp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features, n_features)",
            "default_value": "",
            "description": "Maximum Likelihood Estimator of covariance."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._empirical_covariance/log_likelihood/precision",
          "name": "precision",
          "qname": "sklearn.covariance._empirical_covariance.log_likelihood.precision",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features, n_features)",
            "default_value": "",
            "description": "The precision matrix of the covariance model to be tested."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Computes the sample mean of the log_likelihood under a covariance model\n\ncomputes the empirical expected log-likelihood (accounting for the\nnormalization terms and scaling), allowing for universal comparison (beyond\nthis software package)",
      "docstring": "Computes the sample mean of the log_likelihood under a covariance model\n\ncomputes the empirical expected log-likelihood (accounting for the\nnormalization terms and scaling), allowing for universal comparison (beyond\nthis software package)\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    Maximum Likelihood Estimator of covariance.\n\nprecision : ndarray of shape (n_features, n_features)\n    The precision matrix of the covariance model to be tested.\n\nReturns\n-------\nlog_likelihood_ : float\n    Sample mean of the log-likelihood.",
      "code": "def log_likelihood(emp_cov, precision):\n    \"\"\"Computes the sample mean of the log_likelihood under a covariance model\n\n    computes the empirical expected log-likelihood (accounting for the\n    normalization terms and scaling), allowing for universal comparison (beyond\n    this software package)\n\n    Parameters\n    ----------\n    emp_cov : ndarray of shape (n_features, n_features)\n        Maximum Likelihood Estimator of covariance.\n\n    precision : ndarray of shape (n_features, n_features)\n        The precision matrix of the covariance model to be tested.\n\n    Returns\n    -------\n    log_likelihood_ : float\n        Sample mean of the log-likelihood.\n    \"\"\"\n    p = precision.shape[0]\n    log_likelihood_ = - np.sum(emp_cov * precision) + fast_logdet(precision)\n    log_likelihood_ -= p * np.log(2 * np.pi)\n    log_likelihood_ /= 2.\n    return log_likelihood_"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.alpha",
          "default_value": "0.01",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "The regularization parameter: the higher alpha, the more\nregularization, the sparser the inverse covariance.\nRange is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/mode",
          "name": "mode",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.mode",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'lars'}",
            "default_value": "'cd'",
            "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\nwhich is more numerically stable."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/tol",
          "name": "tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/enet_tol",
          "name": "enet_tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.enet_tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If verbose is True, the objective function and dual gap are\nplotted at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data are not centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data are centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLasso has been renamed to GraphicalLasso",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, alpha=.01, *, mode='cd', tol=1e-4, enet_tol=1e-4,\n                 max_iter=100, verbose=False, assume_centered=False):\n        super().__init__(assume_centered=assume_centered)\n        self.alpha = alpha\n        self.mode = mode\n        self.tol = tol\n        self.enet_tol = enet_tol\n        self.max_iter = max_iter\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit",
      "name": "fit",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLasso/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLasso.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits the GraphicalLasso model to X.",
      "docstring": "Fits the GraphicalLasso model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, ensure_min_samples=2,\n                                estimator=self)\n\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=self.alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=self.verbose, return_n_iter=True)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/alphas",
          "name": "alphas",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.alphas",
          "default_value": "4",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or array-like of shape (n_alphas,), dtype=float",
            "default_value": "4",
            "description": "If an integer is given, it fixes the number of points on the\ngrids of alpha to be used. If a list is given, it gives the\ngrid to be used. See the notes in the class docstring for\nmore details. Range is (0, inf] when floats given."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_alphas,)"
              },
              {
                "kind": "NamedType",
                "name": "dtype=float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/n_refinements",
          "name": "n_refinements",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.n_refinements",
          "default_value": "4",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "4",
            "description": "The number of times the grid is refined. Not used if explicit\nvalues of alphas are passed. Range is [1, inf)."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/cv",
          "name": "cv",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.cv",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, cross-validation generator or iterable",
            "default_value": "None",
            "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\n- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs :class:`KFold` is used.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here.\n\n.. versionchanged:: 0.20\n    ``cv`` default value if None changed from 3-fold to 5-fold."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "cross-validation generator"
              },
              {
                "kind": "NamedType",
                "name": "iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/tol",
          "name": "tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/enet_tol",
          "name": "enet_tol",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.enet_tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/mode",
          "name": "mode",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.mode",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'lars'}",
            "default_value": "'cd'",
            "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where number of features is greater\nthan number of samples. Elsewhere prefer cd which is more numerically\nstable."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "number of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionchanged:: v0.20\n   `n_jobs` default changed from 1 to None"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If verbose is True, the objective function and duality gap are\nprinted at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data are not centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data are centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    GraphLassoCV has been renamed to GraphicalLassoCV",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, alphas=4, n_refinements=4, cv=None, tol=1e-4,\n                 enet_tol=1e-4, max_iter=100, mode='cd', n_jobs=None,\n                 verbose=False, assume_centered=False):\n        super().__init__(\n            mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,\n            max_iter=max_iter, assume_centered=assume_centered)\n        self.alphas = alphas\n        self.n_refinements = n_refinements\n        self.cv = cv\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/cv_alphas_@getter",
      "name": "cv_alphas_",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.cv_alphas_",
      "decorators": [
        "deprecated(\"The cv_alphas_ attribute is deprecated in version 0.24 in favor of cv_results_['alpha'] and will be removed in version 1.1 (renaming of 0.26).\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/cv_alphas_@getter/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.cv_alphas_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"The cv_alphas_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_['alpha'] and will be removed in version 1.1 \"\n        \"(renaming of 0.26).\"\n    )\n    @property\n    def cv_alphas_(self):\n        return self.cv_results_['alphas'].tolist()"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit",
      "name": "fit",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits the GraphicalLasso covariance model to X.",
      "docstring": "Fits the GraphicalLasso covariance model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits the GraphicalLasso covariance model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data from which to compute the covariance estimate\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Covariance does not make sense for a single feature\n        X = self._validate_data(X, ensure_min_features=2, estimator=self)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        emp_cov = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n\n        cv = check_cv(self.cv, y, classifier=False)\n\n        # List of (alpha, scores, covs)\n        path = list()\n        n_alphas = self.alphas\n        inner_verbose = max(0, self.verbose - 1)\n\n        if isinstance(n_alphas, Sequence):\n            alphas = self.alphas\n            n_refinements = 1\n        else:\n            n_refinements = self.n_refinements\n            alpha_1 = alpha_max(emp_cov)\n            alpha_0 = 1e-2 * alpha_1\n            alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1),\n                                 n_alphas)[::-1]\n\n        t0 = time.time()\n        for i in range(n_refinements):\n            with warnings.catch_warnings():\n                # No need to see the convergence warnings on this grid:\n                # they will always be points that will not converge\n                # during the cross-validation\n                warnings.simplefilter('ignore', ConvergenceWarning)\n                # Compute the cross-validated loss on the current grid\n\n                # NOTE: Warm-restarting graphical_lasso_path has been tried,\n                # and this did not allow to gain anything\n                # (same execution time with or without).\n                this_path = Parallel(\n                    n_jobs=self.n_jobs,\n                    verbose=self.verbose\n                )(delayed(graphical_lasso_path)(X[train], alphas=alphas,\n                                                X_test=X[test], mode=self.mode,\n                                                tol=self.tol,\n                                                enet_tol=self.enet_tol,\n                                                max_iter=int(.1 *\n                                                             self.max_iter),\n                                                verbose=inner_verbose)\n                  for train, test in cv.split(X, y))\n\n            # Little danse to transform the list in what we need\n            covs, _, scores = zip(*this_path)\n            covs = zip(*covs)\n            scores = zip(*scores)\n            path.extend(zip(alphas, scores, covs))\n            path = sorted(path, key=operator.itemgetter(0), reverse=True)\n\n            # Find the maximum (avoid using built in 'max' function to\n            # have a fully-reproducible selection of the smallest alpha\n            # in case of equality)\n            best_score = -np.inf\n            last_finite_idx = 0\n            for index, (alpha, scores, _) in enumerate(path):\n                this_score = np.mean(scores)\n                if this_score >= .1 / np.finfo(np.float64).eps:\n                    this_score = np.nan\n                if np.isfinite(this_score):\n                    last_finite_idx = index\n                if this_score >= best_score:\n                    best_score = this_score\n                    best_index = index\n\n            # Refine the grid\n            if best_index == 0:\n                # We do not need to go back: we have chosen\n                # the highest value of alpha for which there are\n                # non-zero coefficients\n                alpha_1 = path[0][0]\n                alpha_0 = path[1][0]\n            elif (best_index == last_finite_idx\n                    and not best_index == len(path) - 1):\n                # We have non-converged models on the upper bound of the\n                # grid, we need to refine the grid there\n                alpha_1 = path[best_index][0]\n                alpha_0 = path[best_index + 1][0]\n            elif best_index == len(path) - 1:\n                alpha_1 = path[best_index][0]\n                alpha_0 = 0.01 * path[best_index][0]\n            else:\n                alpha_1 = path[best_index - 1][0]\n                alpha_0 = path[best_index + 1][0]\n\n            if not isinstance(n_alphas, Sequence):\n                alphas = np.logspace(np.log10(alpha_1), np.log10(alpha_0),\n                                     n_alphas + 2)\n                alphas = alphas[1:-1]\n\n            if self.verbose and n_refinements > 1:\n                print('[GraphicalLassoCV] Done refinement % 2i out of'\n                      ' %i: % 3is' % (i + 1, n_refinements, time.time() - t0))\n\n        path = list(zip(*path))\n        grid_scores = list(path[1])\n        alphas = list(path[0])\n        # Finally, compute the score with alpha = 0\n        alphas.append(0)\n        grid_scores.append(cross_val_score(EmpiricalCovariance(), X,\n                                           cv=cv, n_jobs=self.n_jobs,\n                                           verbose=inner_verbose))\n        grid_scores = np.array(grid_scores)\n        self.cv_results_ = {'alphas': np.array(alphas)}\n        for i in range(grid_scores.shape[1]):\n            key = \"split{}_score\".format(i)\n            self.cv_results_[key] = grid_scores[:, i]\n\n        self.cv_results_[\"mean_score\"] = np.mean(grid_scores, axis=1)\n        self.cv_results_[\"std_score\"] = np.std(grid_scores, axis=1)\n\n        best_alpha = alphas[best_index]\n        self.alpha_ = best_alpha\n\n        # Finally fit the model with the selected alpha\n        self.covariance_, self.precision_, self.n_iter_ = graphical_lasso(\n            emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol,\n            enet_tol=self.enet_tol, max_iter=self.max_iter,\n            verbose=inner_verbose, return_n_iter=True)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/grid_scores_@getter",
      "name": "grid_scores_",
      "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.grid_scores_",
      "decorators": [
        "deprecated('The grid_scores_ attribute is deprecated in version 0.24 in favor of cv_results_ and will be removed in version 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/GraphicalLassoCV/grid_scores_@getter/self",
          "name": "self",
          "qname": "sklearn.covariance._graph_lasso.GraphicalLassoCV.grid_scores_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"The grid_scores_ attribute is deprecated in version 0.24 in favor \"\n        \"of cv_results_ and will be removed in version 1.1 (renaming of 0.26).\"\n    )\n    @property\n    def grid_scores_(self):\n        # remove 3 for mean_score, std_score, and alphas\n        n_alphas = len(self.cv_results_) - 3\n        return np.asarray(\n            [self.cv_results_[\"split{}_score\".format(i)]\n             for i in range(n_alphas)]).T"
    },
    {
      "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso",
      "name": "graphical_lasso",
      "qname": "sklearn.covariance._graph_lasso.graphical_lasso",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/emp_cov",
          "name": "emp_cov",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.emp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features, n_features)",
            "default_value": "",
            "description": "Empirical covariance from which to compute the covariance estimate."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/alpha",
          "name": "alpha",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.alpha",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "",
            "description": "The regularization parameter: the higher alpha, the more\nregularization, the sparser the inverse covariance.\nRange is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/cov_init",
          "name": "cov_init",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.cov_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array of shape (n_features, n_features)",
            "default_value": "None",
            "description": "The initial guess for the covariance. If None, then the empirical\ncovariance is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/mode",
          "name": "mode",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.mode",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'lars'}",
            "default_value": "'cd'",
            "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for\nvery sparse underlying graphs, where p > n. Elsewhere prefer cd\nwhich is more numerically stable."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/tol",
          "name": "tol",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance to declare convergence: if the dual gap goes below\nthis value, iterations are stopped. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/enet_tol",
          "name": "enet_tol",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.enet_tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "The tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction\nfor a given column update, not of the overall parameter estimate. Only\nused for mode='cd'. Range is (0, inf]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/max_iter",
          "name": "max_iter",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/verbose",
          "name": "verbose",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If verbose is True, the objective function and dual gap are\nprinted at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/return_costs",
          "name": "return_costs",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.return_costs",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "Flase",
            "description": "If return_costs is True, the objective function and dual gap\nat each iteration are returned."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/eps",
          "name": "eps",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.eps",
          "default_value": "np.finfo(np.float64).eps",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "eps",
            "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Default is `np.finfo(np.float64).eps`."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._graph_lasso/graphical_lasso/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.covariance._graph_lasso.graphical_lasso.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "l1-penalized covariance estimator\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    graph_lasso has been renamed to graphical_lasso",
      "docstring": "l1-penalized covariance estimator\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n.. versionchanged:: v0.20\n    graph_lasso has been renamed to graphical_lasso\n\nParameters\n----------\nemp_cov : ndarray of shape (n_features, n_features)\n    Empirical covariance from which to compute the covariance estimate.\n\nalpha : float\n    The regularization parameter: the higher alpha, the more\n    regularization, the sparser the inverse covariance.\n    Range is (0, inf].\n\ncov_init : array of shape (n_features, n_features), default=None\n    The initial guess for the covariance. If None, then the empirical\n    covariance is used.\n\nmode : {'cd', 'lars'}, default='cd'\n    The Lasso solver to use: coordinate descent or LARS. Use LARS for\n    very sparse underlying graphs, where p > n. Elsewhere prefer cd\n    which is more numerically stable.\n\ntol : float, default=1e-4\n    The tolerance to declare convergence: if the dual gap goes below\n    this value, iterations are stopped. Range is (0, inf].\n\nenet_tol : float, default=1e-4\n    The tolerance for the elastic net solver used to calculate the descent\n    direction. This parameter controls the accuracy of the search direction\n    for a given column update, not of the overall parameter estimate. Only\n    used for mode='cd'. Range is (0, inf].\n\nmax_iter : int, default=100\n    The maximum number of iterations.\n\nverbose : bool, default=False\n    If verbose is True, the objective function and dual gap are\n    printed at each iteration.\n\nreturn_costs : bool, default=Flase\n    If return_costs is True, the objective function and dual gap\n    at each iteration are returned.\n\neps : float, default=eps\n    The machine-precision regularization in the computation of the\n    Cholesky diagonal factors. Increase this for very ill-conditioned\n    systems. Default is `np.finfo(np.float64).eps`.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\ncovariance : ndarray of shape (n_features, n_features)\n    The estimated covariance matrix.\n\nprecision : ndarray of shape (n_features, n_features)\n    The estimated (sparse) precision matrix.\n\ncosts : list of (objective, dual_gap) pairs\n    The list of values of the objective function and the dual gap at\n    each iteration. Returned only if return_costs is True.\n\nn_iter : int\n    Number of iterations. Returned only if `return_n_iter` is set to True.\n\nSee Also\n--------\nGraphicalLasso, GraphicalLassoCV\n\nNotes\n-----\nThe algorithm employed to solve this problem is the GLasso algorithm,\nfrom the Friedman 2008 Biostatistics paper. It is the same algorithm\nas in the R `glasso` package.\n\nOne possible difference with the `glasso` R package is that the\ndiagonal coefficients are not penalized.",
      "code": "@_deprecate_positional_args\ndef graphical_lasso(emp_cov, alpha, *, cov_init=None, mode='cd', tol=1e-4,\n                    enet_tol=1e-4, max_iter=100, verbose=False,\n                    return_costs=False, eps=np.finfo(np.float64).eps,\n                    return_n_iter=False):\n    \"\"\"l1-penalized covariance estimator\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    .. versionchanged:: v0.20\n        graph_lasso has been renamed to graphical_lasso\n\n    Parameters\n    ----------\n    emp_cov : ndarray of shape (n_features, n_features)\n        Empirical covariance from which to compute the covariance estimate.\n\n    alpha : float\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n        Range is (0, inf].\n\n    cov_init : array of shape (n_features, n_features), default=None\n        The initial guess for the covariance. If None, then the empirical\n        covariance is used.\n\n    mode : {'cd', 'lars'}, default='cd'\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : float, default=1e-4\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped. Range is (0, inf].\n\n    enet_tol : float, default=1e-4\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'. Range is (0, inf].\n\n    max_iter : int, default=100\n        The maximum number of iterations.\n\n    verbose : bool, default=False\n        If verbose is True, the objective function and dual gap are\n        printed at each iteration.\n\n    return_costs : bool, default=Flase\n        If return_costs is True, the objective function and dual gap\n        at each iteration are returned.\n\n    eps : float, default=eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Default is `np.finfo(np.float64).eps`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    covariance : ndarray of shape (n_features, n_features)\n        The estimated covariance matrix.\n\n    precision : ndarray of shape (n_features, n_features)\n        The estimated (sparse) precision matrix.\n\n    costs : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n    n_iter : int\n        Number of iterations. Returned only if `return_n_iter` is set to True.\n\n    See Also\n    --------\n    GraphicalLasso, GraphicalLassoCV\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n    \"\"\"\n    _, n_features = emp_cov.shape\n    if alpha == 0:\n        if return_costs:\n            precision_ = linalg.inv(emp_cov)\n            cost = - 2. * log_likelihood(emp_cov, precision_)\n            cost += n_features * np.log(2 * np.pi)\n            d_gap = np.sum(emp_cov * precision_) - n_features\n            if return_n_iter:\n                return emp_cov, precision_, (cost, d_gap), 0\n            else:\n                return emp_cov, precision_, (cost, d_gap)\n        else:\n            if return_n_iter:\n                return emp_cov, linalg.inv(emp_cov), 0\n            else:\n                return emp_cov, linalg.inv(emp_cov)\n    if cov_init is None:\n        covariance_ = emp_cov.copy()\n    else:\n        covariance_ = cov_init.copy()\n    # As a trivial regularization (Tikhonov like), we scale down the\n    # off-diagonal coefficients of our starting point: This is needed, as\n    # in the cross-validation the cov_init can easily be\n    # ill-conditioned, and the CV loop blows. Beside, this takes\n    # conservative stand-point on the initial conditions, and it tends to\n    # make the convergence go faster.\n    covariance_ *= 0.95\n    diagonal = emp_cov.flat[::n_features + 1]\n    covariance_.flat[::n_features + 1] = diagonal\n    precision_ = linalg.pinvh(covariance_)\n\n    indices = np.arange(n_features)\n    costs = list()\n    # The different l1 regression solver have different numerical errors\n    if mode == 'cd':\n        errors = dict(over='raise', invalid='ignore')\n    else:\n        errors = dict(invalid='raise')\n    try:\n        # be robust to the max_iter=0 edge case, see:\n        # https://github.com/scikit-learn/scikit-learn/issues/4134\n        d_gap = np.inf\n        # set a sub_covariance buffer\n        sub_covariance = np.copy(covariance_[1:, 1:], order='C')\n        for i in range(max_iter):\n            for idx in range(n_features):\n                # To keep the contiguous matrix `sub_covariance` equal to\n                # covariance_[indices != idx].T[indices != idx]\n                # we only need to update 1 column and 1 line when idx changes\n                if idx > 0:\n                    di = idx - 1\n                    sub_covariance[di] = covariance_[di][indices != idx]\n                    sub_covariance[:, di] = covariance_[:, di][indices != idx]\n                else:\n                    sub_covariance[:] = covariance_[1:, 1:]\n                row = emp_cov[idx, indices != idx]\n                with np.errstate(**errors):\n                    if mode == 'cd':\n                        # Use coordinate descent\n                        coefs = -(precision_[indices != idx, idx]\n                                  / (precision_[idx, idx] + 1000 * eps))\n                        coefs, _, _, _ = cd_fast.enet_coordinate_descent_gram(\n                            coefs, alpha, 0, sub_covariance,\n                            row, row, max_iter, enet_tol,\n                            check_random_state(None), False)\n                    else:\n                        # Use LARS\n                        _, _, coefs = lars_path_gram(\n                            Xy=row, Gram=sub_covariance, n_samples=row.size,\n                            alpha_min=alpha / (n_features - 1), copy_Gram=True,\n                            eps=eps, method='lars', return_path=False)\n                # Update the precision matrix\n                precision_[idx, idx] = (\n                    1. / (covariance_[idx, idx]\n                          - np.dot(covariance_[indices != idx, idx], coefs)))\n                precision_[indices != idx, idx] = (- precision_[idx, idx]\n                                                   * coefs)\n                precision_[idx, indices != idx] = (- precision_[idx, idx]\n                                                   * coefs)\n                coefs = np.dot(sub_covariance, coefs)\n                covariance_[idx, indices != idx] = coefs\n                covariance_[indices != idx, idx] = coefs\n            if not np.isfinite(precision_.sum()):\n                raise FloatingPointError('The system is too ill-conditioned '\n                                         'for this solver')\n            d_gap = _dual_gap(emp_cov, precision_, alpha)\n            cost = _objective(emp_cov, precision_, alpha)\n            if verbose:\n                print('[graphical_lasso] Iteration '\n                      '% 3i, cost % 3.2e, dual gap %.3e'\n                      % (i, cost, d_gap))\n            if return_costs:\n                costs.append((cost, d_gap))\n            if np.abs(d_gap) < tol:\n                break\n            if not np.isfinite(cost) and i > 0:\n                raise FloatingPointError('Non SPD result: the system is '\n                                         'too ill-conditioned for this solver')\n        else:\n            warnings.warn('graphical_lasso: did not converge after '\n                          '%i iteration: dual gap: %.3e'\n                          % (max_iter, d_gap), ConvergenceWarning)\n    except FloatingPointError as e:\n        e.args = (e.args[0]\n                  + '. The system is too ill-conditioned for this solver',)\n        raise e\n\n    if return_costs:\n        if return_n_iter:\n            return covariance_, precision_, costs, i + 1\n        else:\n            return covariance_, precision_, costs\n    else:\n        if return_n_iter:\n            return covariance_, precision_, i + 1\n        else:\n            return covariance_, precision_"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the support of the robust location and the covariance\nestimates is computed, and a covariance estimate is recomputed from\nit, without centering the data.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, the robust location and covariance are directly computed\nwith the FastMCD algorithm without additional treatment."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/support_fraction",
          "name": "support_fraction",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.support_fraction",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The proportion of points to be included in the support of the raw\nMCD estimate. Default is None, which implies that the minimum\nvalue of support_fraction will be used within the algorithm:\n`(n_sample + n_features + 1) / 2`. The parameter must be in the range\n(0, 1)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 support_fraction=None, random_state=None):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n        self.support_fraction = support_fraction\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance",
      "name": "correct_covariance",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.correct_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.correct_covariance.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/correct_covariance/data",
          "name": "data",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.correct_covariance.data",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix, with p features and n samples.\nThe data set must be the one which was used to compute\nthe raw estimates."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [RVD]_.",
      "docstring": "Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [RVD]_.\n\nParameters\n----------\ndata : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n    The data set must be the one which was used to compute\n    the raw estimates.\n\nReturns\n-------\ncovariance_corrected : ndarray of shape (n_features, n_features)\n    Corrected robust covariance estimate.\n\nReferences\n----------\n\n.. [RVD] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS",
      "code": "    def correct_covariance(self, data):\n        \"\"\"Apply a correction to raw Minimum Covariance Determinant estimates.\n\n        Correction using the empirical correction factor suggested\n        by Rousseeuw and Van Driessen in [RVD]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        covariance_corrected : ndarray of shape (n_features, n_features)\n            Corrected robust covariance estimate.\n\n        References\n        ----------\n\n        .. [RVD] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n\n        # Check that the covariance of the support data is not equal to 0.\n        # Otherwise self.dist_ = 0 and thus correction = 0.\n        n_samples = len(self.dist_)\n        n_support = np.sum(self.support_)\n        if n_support < n_samples and np.allclose(self.raw_covariance_, 0):\n            raise ValueError('The covariance matrix of the support data '\n                             'is equal to 0, try to increase support_fraction')\n        correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5)\n        covariance_corrected = self.raw_covariance_ * correction\n        self.dist_ /= correction\n        return covariance_corrected"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit",
      "name": "fit",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fits a Minimum Covariance Determinant with the FastMCD algorithm.",
      "docstring": "Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X, ensure_min_samples=2, estimator='MinCovDet')\n        random_state = check_random_state(self.random_state)\n        n_samples, n_features = X.shape\n        # check that the empirical covariance is full rank\n        if (linalg.svdvals(np.dot(X.T, X)) > 1e-8).sum() != n_features:\n            warnings.warn(\"The covariance matrix associated to your dataset \"\n                          \"is not full rank\")\n        # compute and store raw estimates\n        raw_location, raw_covariance, raw_support, raw_dist = fast_mcd(\n            X, support_fraction=self.support_fraction,\n            cov_computation_method=self._nonrobust_covariance,\n            random_state=random_state)\n        if self.assume_centered:\n            raw_location = np.zeros(n_features)\n            raw_covariance = self._nonrobust_covariance(X[raw_support],\n                                                        assume_centered=True)\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(raw_covariance)\n            raw_dist = np.sum(np.dot(X, precision) * X, 1)\n        self.raw_location_ = raw_location\n        self.raw_covariance_ = raw_covariance\n        self.raw_support_ = raw_support\n        self.location_ = raw_location\n        self.support_ = raw_support\n        self.dist_ = raw_dist\n        # obtain consistency at normal models\n        self.correct_covariance(X)\n        # re-weight estimator\n        self.reweight_covariance(X)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance",
      "name": "reweight_covariance",
      "qname": "sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance/self",
          "name": "self",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/MinCovDet/reweight_covariance/data",
          "name": "data",
          "qname": "sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance.data",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix, with p features and n samples.\nThe data set must be the one which was used to compute\nthe raw estimates."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates) described\nin [RVDriessen]_.",
      "docstring": "Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates) described\nin [RVDriessen]_.\n\nParameters\n----------\ndata : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n    The data set must be the one which was used to compute\n    the raw estimates.\n\nReturns\n-------\nlocation_reweighted : ndarray of shape (n_features,)\n    Re-weighted robust location estimate.\n\ncovariance_reweighted : ndarray of shape (n_features, n_features)\n    Re-weighted robust covariance estimate.\n\nsupport_reweighted : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the re-weighted robust location and covariance estimates.\n\nReferences\n----------\n\n.. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS",
      "code": "    def reweight_covariance(self, data):\n        \"\"\"Re-weight raw Minimum Covariance Determinant estimates.\n\n        Re-weight observations using Rousseeuw's method (equivalent to\n        deleting outlying observations from the data set before\n        computing location and covariance estimates) described\n        in [RVDriessen]_.\n\n        Parameters\n        ----------\n        data : array-like of shape (n_samples, n_features)\n            The data matrix, with p features and n samples.\n            The data set must be the one which was used to compute\n            the raw estimates.\n\n        Returns\n        -------\n        location_reweighted : ndarray of shape (n_features,)\n            Re-weighted robust location estimate.\n\n        covariance_reweighted : ndarray of shape (n_features, n_features)\n            Re-weighted robust covariance estimate.\n\n        support_reweighted : ndarray of shape (n_samples,), dtype=bool\n            A mask of the observations that have been used to compute\n            the re-weighted robust location and covariance estimates.\n\n        References\n        ----------\n\n        .. [RVDriessen] A Fast Algorithm for the Minimum Covariance\n            Determinant Estimator, 1999, American Statistical Association\n            and the American Society for Quality, TECHNOMETRICS\n        \"\"\"\n        n_samples, n_features = data.shape\n        mask = self.dist_ < chi2(n_features).isf(0.025)\n        if self.assume_centered:\n            location_reweighted = np.zeros(n_features)\n        else:\n            location_reweighted = data[mask].mean(0)\n        covariance_reweighted = self._nonrobust_covariance(\n            data[mask], assume_centered=self.assume_centered)\n        support_reweighted = np.zeros(n_samples, dtype=bool)\n        support_reweighted[mask] = True\n        self._set_covariance(covariance_reweighted)\n        self.location_ = location_reweighted\n        self.support_ = support_reweighted\n        X_centered = data - self.location_\n        self.dist_ = np.sum(\n            np.dot(X_centered, self.get_precision()) * X_centered, 1)\n        return location_reweighted, covariance_reweighted, support_reweighted"
    },
    {
      "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd",
      "name": "fast_mcd",
      "qname": "sklearn.covariance._robust_covariance.fast_mcd",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/X",
          "name": "X",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data matrix, with p features and n samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/support_fraction",
          "name": "support_fraction",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.support_fraction",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "The proportion of points to be included in the support of the raw\nMCD estimate. Default is `None`, which implies that the minimum\nvalue of `support_fraction` will be used within the algorithm:\n`(n_sample + n_features + 1) / 2`. This parameter must be in the\nrange (0, 1)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/cov_computation_method",
          "name": "cov_computation_method",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.cov_computation_method",
          "default_value": "empirical_covariance",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": ":func:`sklearn.covariance.empirical_covariance`",
            "description": "The function which will be used to compute the covariance.\nMust return an array of shape (n_features, n_features)."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._robust_covariance/fast_mcd/random_state",
          "name": "random_state",
          "qname": "sklearn.covariance._robust_covariance.fast_mcd.random_state",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines the pseudo random number generator for shuffling the data.\nPass an int for reproducible results across multiple function calls.\nSee :term: `Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.",
      "docstring": "Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is `None`, which implies that the minimum\n    value of `support_fraction` will be used within the algorithm:\n    `(n_sample + n_features + 1) / 2`. This parameter must be in the\n    range (0, 1).\n\ncov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n    The function which will be used to compute the covariance.\n    Must return an array of shape (n_features, n_features).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term: `Glossary <random_state>`.\n\nReturns\n-------\nlocation : ndarray of shape (n_features,)\n    Robust location of the data.\n\ncovariance : ndarray of shape (n_features, n_features)\n    Robust covariance of the features.\n\nsupport : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the robust location and covariance estimates of the data set.\n\nNotes\n-----\nThe FastMCD algorithm has been introduced by Rousseuw and Van Driessen\nin \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n1999, American Statistical Association and the American Society\nfor Quality, TECHNOMETRICS\".\nThe principle is to compute robust estimates and random subsets before\npooling them into a larger subsets, and finally into the full data set.\nDepending on the size of the initial sample, we have one, two or three\nsuch computation levels.\n\nNote that only raw estimates are returned. If one is interested in\nthe correction and reweighting steps described in [RouseeuwVan]_,\nsee the MinCovDet object.\n\nReferences\n----------\n\n.. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS\n\n.. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
      "code": "def fast_mcd(X, support_fraction=None,\n             cov_computation_method=empirical_covariance,\n             random_state=None):\n    \"\"\"Estimates the Minimum Covariance Determinant matrix.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data matrix, with p features and n samples.\n\n    support_fraction : float, default=None\n        The proportion of points to be included in the support of the raw\n        MCD estimate. Default is `None`, which implies that the minimum\n        value of `support_fraction` will be used within the algorithm:\n        `(n_sample + n_features + 1) / 2`. This parameter must be in the\n        range (0, 1).\n\n    cov_computation_method : callable, \\\n            default=:func:`sklearn.covariance.empirical_covariance`\n        The function which will be used to compute the covariance.\n        Must return an array of shape (n_features, n_features).\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the pseudo random number generator for shuffling the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term: `Glossary <random_state>`.\n\n    Returns\n    -------\n    location : ndarray of shape (n_features,)\n        Robust location of the data.\n\n    covariance : ndarray of shape (n_features, n_features)\n        Robust covariance of the features.\n\n    support : ndarray of shape (n_samples,), dtype=bool\n        A mask of the observations that have been used to compute\n        the robust location and covariance estimates of the data set.\n\n    Notes\n    -----\n    The FastMCD algorithm has been introduced by Rousseuw and Van Driessen\n    in \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n    1999, American Statistical Association and the American Society\n    for Quality, TECHNOMETRICS\".\n    The principle is to compute robust estimates and random subsets before\n    pooling them into a larger subsets, and finally into the full data set.\n    Depending on the size of the initial sample, we have one, two or three\n    such computation levels.\n\n    Note that only raw estimates are returned. If one is interested in\n    the correction and reweighting steps described in [RouseeuwVan]_,\n    see the MinCovDet object.\n\n    References\n    ----------\n\n    .. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n        Determinant Estimator, 1999, American Statistical Association\n        and the American Society for Quality, TECHNOMETRICS\n\n    .. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400\n    \"\"\"\n    random_state = check_random_state(random_state)\n\n    X = check_array(X, ensure_min_samples=2, estimator='fast_mcd')\n    n_samples, n_features = X.shape\n\n    # minimum breakdown value\n    if support_fraction is None:\n        n_support = int(np.ceil(0.5 * (n_samples + n_features + 1)))\n    else:\n        n_support = int(support_fraction * n_samples)\n\n    # 1-dimensional case quick computation\n    # (Rousseeuw, P. J. and Leroy, A. M. (2005) References, in Robust\n    #  Regression and Outlier Detection, John Wiley & Sons, chapter 4)\n    if n_features == 1:\n        if n_support < n_samples:\n            # find the sample shortest halves\n            X_sorted = np.sort(np.ravel(X))\n            diff = X_sorted[n_support:] - X_sorted[:(n_samples - n_support)]\n            halves_start = np.where(diff == np.min(diff))[0]\n            # take the middle points' mean to get the robust location estimate\n            location = 0.5 * (X_sorted[n_support + halves_start] +\n                              X_sorted[halves_start]).mean()\n            support = np.zeros(n_samples, dtype=bool)\n            X_centered = X - location\n            support[np.argsort(np.abs(X_centered), 0)[:n_support]] = True\n            covariance = np.asarray([[np.var(X[support])]])\n            location = np.array([location])\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(covariance)\n            dist = (np.dot(X_centered, precision) * (X_centered)).sum(axis=1)\n        else:\n            support = np.ones(n_samples, dtype=bool)\n            covariance = np.asarray([[np.var(X)]])\n            location = np.asarray([np.mean(X)])\n            X_centered = X - location\n            # get precision matrix in an optimized way\n            precision = linalg.pinvh(covariance)\n            dist = (np.dot(X_centered, precision) * (X_centered)).sum(axis=1)\n    # Starting FastMCD algorithm for p-dimensional case\n    if (n_samples > 500) and (n_features > 1):\n        # 1. Find candidate supports on subsets\n        # a. split the set in subsets of size ~ 300\n        n_subsets = n_samples // 300\n        n_samples_subsets = n_samples // n_subsets\n        samples_shuffle = random_state.permutation(n_samples)\n        h_subset = int(np.ceil(n_samples_subsets *\n                       (n_support / float(n_samples))))\n        # b. perform a total of 500 trials\n        n_trials_tot = 500\n        # c. select 10 best (location, covariance) for each subset\n        n_best_sub = 10\n        n_trials = max(10, n_trials_tot // n_subsets)\n        n_best_tot = n_subsets * n_best_sub\n        all_best_locations = np.zeros((n_best_tot, n_features))\n        try:\n            all_best_covariances = np.zeros((n_best_tot, n_features,\n                                             n_features))\n        except MemoryError:\n            # The above is too big. Let's try with something much small\n            # (and less optimal)\n            n_best_tot = 10\n            all_best_covariances = np.zeros((n_best_tot, n_features,\n                                             n_features))\n            n_best_sub = 2\n        for i in range(n_subsets):\n            low_bound = i * n_samples_subsets\n            high_bound = low_bound + n_samples_subsets\n            current_subset = X[samples_shuffle[low_bound:high_bound]]\n            best_locations_sub, best_covariances_sub, _, _ = select_candidates(\n                current_subset, h_subset, n_trials,\n                select=n_best_sub, n_iter=2,\n                cov_computation_method=cov_computation_method,\n                random_state=random_state)\n            subset_slice = np.arange(i * n_best_sub, (i + 1) * n_best_sub)\n            all_best_locations[subset_slice] = best_locations_sub\n            all_best_covariances[subset_slice] = best_covariances_sub\n        # 2. Pool the candidate supports into a merged set\n        # (possibly the full dataset)\n        n_samples_merged = min(1500, n_samples)\n        h_merged = int(np.ceil(n_samples_merged *\n                       (n_support / float(n_samples))))\n        if n_samples > 1500:\n            n_best_merged = 10\n        else:\n            n_best_merged = 1\n        # find the best couples (location, covariance) on the merged set\n        selection = random_state.permutation(n_samples)[:n_samples_merged]\n        locations_merged, covariances_merged, supports_merged, d = \\\n            select_candidates(\n                X[selection], h_merged,\n                n_trials=(all_best_locations, all_best_covariances),\n                select=n_best_merged,\n                cov_computation_method=cov_computation_method,\n                random_state=random_state)\n        # 3. Finally get the overall best (locations, covariance) couple\n        if n_samples < 1500:\n            # directly get the best couple (location, covariance)\n            location = locations_merged[0]\n            covariance = covariances_merged[0]\n            support = np.zeros(n_samples, dtype=bool)\n            dist = np.zeros(n_samples)\n            support[selection] = supports_merged[0]\n            dist[selection] = d[0]\n        else:\n            # select the best couple on the full dataset\n            locations_full, covariances_full, supports_full, d = \\\n                select_candidates(\n                    X, n_support,\n                    n_trials=(locations_merged, covariances_merged),\n                    select=1,\n                    cov_computation_method=cov_computation_method,\n                    random_state=random_state)\n            location = locations_full[0]\n            covariance = covariances_full[0]\n            support = supports_full[0]\n            dist = d[0]\n    elif n_features > 1:\n        # 1. Find the 10 best couples (location, covariance)\n        # considering two iterations\n        n_trials = 30\n        n_best = 10\n        locations_best, covariances_best, _, _ = select_candidates(\n            X, n_support, n_trials=n_trials, select=n_best, n_iter=2,\n            cov_computation_method=cov_computation_method,\n            random_state=random_state)\n        # 2. Select the best couple on the full dataset amongst the 10\n        locations_full, covariances_full, supports_full, d = select_candidates(\n            X, n_support, n_trials=(locations_best, covariances_best),\n            select=1, cov_computation_method=cov_computation_method,\n            random_state=random_state)\n        location = locations_full[0]\n        covariance = covariances_full[0]\n        support = supports_full[0]\n        dist = d[0]\n\n    return location, covariance, support, dist"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False (default), data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/__init__/block_size",
          "name": "block_size",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.__init__.block_size",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Size of blocks into which the covariance matrix will be split\nduring its Ledoit-Wolf estimation. This is purely a memory\noptimization and does not affect results."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf's formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 block_size=1000):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.block_size = block_size"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit",
      "name": "fit",
      "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/LedoitWolf/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._shrunk_covariance.LedoitWolf.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the Ledoit-Wolf shrunk covariance model according to the given\ntraining data and parameters.",
      "docstring": "Fit the Ledoit-Wolf shrunk covariance model according to the given\ntraining data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the Ledoit-Wolf shrunk covariance model according to the given\n        training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        X = self._validate_data(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance, shrinkage = ledoit_wolf(X - self.location_,\n                                            assume_centered=True,\n                                            block_size=self.block_size)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit",
      "name": "fit",
      "qname": "sklearn.covariance._shrunk_covariance.OAS.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.OAS.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.OAS.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where `n_samples` is the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/OAS/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._shrunk_covariance.OAS.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the Oracle Approximating Shrinkage covariance model\naccording to the given training data and parameters.",
      "docstring": "Fit the Oracle Approximating Shrinkage covariance model\naccording to the given training data and parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where `n_samples` is the number of samples\n    and `n_features` is the number of features.\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the Oracle Approximating Shrinkage covariance model\n        according to the given training data and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid computing the\n        # covariance matrix (and potentially the precision)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n\n        covariance, shrinkage = oas(X - self.location_, assume_centered=True)\n        self.shrinkage_ = shrinkage\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__",
      "name": "__init__",
      "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/store_precision",
          "name": "store_precision",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.store_precision",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Specify if the estimated precision is stored"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful when working with data whose mean is almost, but not exactly\nzero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/__init__/shrinkage",
          "name": "shrinkage",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__.shrinkage",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate. Range is [0, 1]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, store_precision=True, assume_centered=False,\n                 shrinkage=0.1):\n        super().__init__(store_precision=store_precision,\n                         assume_centered=assume_centered)\n        self.shrinkage = shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit",
      "name": "fit",
      "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit/self",
          "name": "self",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ShrunkCovariance/fit/y",
          "name": "y",
          "qname": "sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the shrunk covariance model according to the given training data\nand parameters.",
      "docstring": "Fit the shrunk covariance model according to the given training data\nand parameters.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the shrunk covariance model according to the given training data\n        and parameters.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X = self._validate_data(X)\n        # Not calling the parent object to fit, to avoid a potential\n        # matrix inversion when setting the precision\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = empirical_covariance(\n            X, assume_centered=self.assume_centered)\n        covariance = shrunk_covariance(covariance, self.shrinkage)\n        self._set_covariance(covariance)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf",
      "name": "ledoit_wolf",
      "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf/block_size",
          "name": "block_size",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf.block_size",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Size of blocks into which the covariance matrix will be split.\nThis is purely a memory optimization and does not affect results."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split.\n    This is purely a memory optimization and does not affect results.\n\nReturns\n-------\nshrunk_cov : ndarray of shape (n_features, n_features)\n    Shrunk covariance.\n\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "@_deprecate_positional_args\ndef ledoit_wolf(X, *, assume_centered=False, block_size=1000):\n    \"\"\"Estimates the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n        This is purely a memory optimization and does not affect results.\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    X = np.asarray(X)\n    # for only one feature, the result is the same whatever the shrinkage\n    if len(X.shape) == 2 and X.shape[1] == 1:\n        if not assume_centered:\n            X = X - X.mean()\n        return np.atleast_2d((X ** 2).mean()), 0.\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n        n_features = X.size\n    else:\n        _, n_features = X.shape\n\n    # get Ledoit-Wolf shrinkage\n    shrinkage = ledoit_wolf_shrinkage(\n        X, assume_centered=assume_centered, block_size=block_size)\n    emp_cov = empirical_covariance(X, assume_centered=assume_centered)\n    mu = np.sum(np.trace(emp_cov)) / n_features\n    shrunk_cov = (1. - shrinkage) * emp_cov\n    shrunk_cov.flat[::n_features + 1] += shrinkage * mu\n\n    return shrunk_cov, shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage",
      "name": "ledoit_wolf_shrinkage",
      "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage.assume_centered",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/ledoit_wolf_shrinkage/block_size",
          "name": "block_size",
          "qname": "sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage.block_size",
          "default_value": "1000",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Size of blocks into which the covariance matrix will be split."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\n\nassume_centered : bool, default=False\n    If True, data will not be centered before computation.\n    Useful to work with data whose mean is significantly equal to\n    zero but is not exactly zero.\n    If False, data will be centered before computation.\n\nblock_size : int, default=1000\n    Size of blocks into which the covariance matrix will be split.\n\nReturns\n-------\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "def ledoit_wolf_shrinkage(X, assume_centered=False, block_size=1000):\n    \"\"\"Estimates the shrunk Ledoit-Wolf covariance matrix.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of blocks into which the covariance matrix will be split.\n\n    Returns\n    -------\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    X = np.asarray(X)\n    # for only one feature, the result is the same whatever the shrinkage\n    if len(X.shape) == 2 and X.shape[1] == 1:\n        return 0.\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n\n    if X.shape[0] == 1:\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n    n_samples, n_features = X.shape\n\n    # optionally center data\n    if not assume_centered:\n        X = X - X.mean(0)\n\n    # A non-blocked version of the computation is present in the tests\n    # in tests/test_covariance.py\n\n    # number of blocks to split the covariance matrix into\n    n_splits = int(n_features / block_size)\n    X2 = X ** 2\n    emp_cov_trace = np.sum(X2, axis=0) / n_samples\n    mu = np.sum(emp_cov_trace) / n_features\n    beta_ = 0.  # sum of the coefficients of <X2.T, X2>\n    delta_ = 0.  # sum of the *squared* coefficients of <X.T, X>\n    # starting block computation\n    for i in range(n_splits):\n        for j in range(n_splits):\n            rows = slice(block_size * i, block_size * (i + 1))\n            cols = slice(block_size * j, block_size * (j + 1))\n            beta_ += np.sum(np.dot(X2.T[rows], X2[:, cols]))\n            delta_ += np.sum(np.dot(X.T[rows], X[:, cols]) ** 2)\n        rows = slice(block_size * i, block_size * (i + 1))\n        beta_ += np.sum(np.dot(X2.T[rows], X2[:, block_size * n_splits:]))\n        delta_ += np.sum(\n            np.dot(X.T[rows], X[:, block_size * n_splits:]) ** 2)\n    for j in range(n_splits):\n        cols = slice(block_size * j, block_size * (j + 1))\n        beta_ += np.sum(np.dot(X2.T[block_size * n_splits:], X2[:, cols]))\n        delta_ += np.sum(\n            np.dot(X.T[block_size * n_splits:], X[:, cols]) ** 2)\n    delta_ += np.sum(np.dot(X.T[block_size * n_splits:],\n                            X[:, block_size * n_splits:]) ** 2)\n    delta_ /= n_samples ** 2\n    beta_ += np.sum(np.dot(X2.T[block_size * n_splits:],\n                           X2[:, block_size * n_splits:]))\n    # use delta_ to compute beta\n    beta = 1. / (n_features * n_samples) * (beta_ / n_samples - delta_)\n    # delta is the sum of the squared coefficients of (<X.T,X> - mu*Id) / p\n    delta = delta_ - 2. * mu * emp_cov_trace.sum() + n_features * mu ** 2\n    delta /= n_features\n    # get final beta as the min between beta and delta\n    # We do this to prevent shrinking more than \"1\", which whould invert\n    # the value of covariances\n    beta = min(beta, delta)\n    # finally get shrinkage\n    shrinkage = 0 if beta == 0 else beta / delta\n    return shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/oas",
      "name": "oas",
      "qname": "sklearn.covariance._shrunk_covariance.oas",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/oas/X",
          "name": "X",
          "qname": "sklearn.covariance._shrunk_covariance.oas.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data from which to compute the covariance estimate."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/oas/assume_centered",
          "name": "assume_centered",
          "qname": "sklearn.covariance._shrunk_covariance.oas.assume_centered",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, data will not be centered before computation.\nUseful to work with data whose mean is significantly equal to\nzero but is not exactly zero.\nIf False, data will be centered before computation."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.",
      "docstring": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data from which to compute the covariance estimate.\n\nassume_centered : bool, default=False\n  If True, data will not be centered before computation.\n  Useful to work with data whose mean is significantly equal to\n  zero but is not exactly zero.\n  If False, data will be centered before computation.\n\nReturns\n-------\nshrunk_cov : array-like of shape (n_features, n_features)\n    Shrunk covariance.\n\nshrinkage : float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\nNotes\n-----\nThe regularised (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\n\nThe formula we used to implement the OAS is slightly modified compared\nto the one given in the article. See :class:`OAS` for more details.",
      "code": "@_deprecate_positional_args\ndef oas(X, *, assume_centered=False):\n    \"\"\"Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data from which to compute the covariance estimate.\n\n    assume_centered : bool, default=False\n      If True, data will not be centered before computation.\n      Useful to work with data whose mean is significantly equal to\n      zero but is not exactly zero.\n      If False, data will be centered before computation.\n\n    Returns\n    -------\n    shrunk_cov : array-like of shape (n_features, n_features)\n        Shrunk covariance.\n\n    shrinkage : float\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Notes\n    -----\n    The regularised (shrunk) covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n\n    The formula we used to implement the OAS is slightly modified compared\n    to the one given in the article. See :class:`OAS` for more details.\n    \"\"\"\n    X = np.asarray(X)\n    # for only one feature, the result is the same whatever the shrinkage\n    if len(X.shape) == 2 and X.shape[1] == 1:\n        if not assume_centered:\n            X = X - X.mean()\n        return np.atleast_2d((X ** 2).mean()), 0.\n    if X.ndim == 1:\n        X = np.reshape(X, (1, -1))\n        warnings.warn(\"Only one sample available. \"\n                      \"You may want to reshape your data array\")\n        n_samples = 1\n        n_features = X.size\n    else:\n        n_samples, n_features = X.shape\n\n    emp_cov = empirical_covariance(X, assume_centered=assume_centered)\n    mu = np.trace(emp_cov) / n_features\n\n    # formula from Chen et al.'s **implementation**\n    alpha = np.mean(emp_cov ** 2)\n    num = alpha + mu ** 2\n    den = (n_samples + 1.) * (alpha - (mu ** 2) / n_features)\n\n    shrinkage = 1. if den == 0 else min(num / den, 1.)\n    shrunk_cov = (1. - shrinkage) * emp_cov\n    shrunk_cov.flat[::n_features + 1] += shrinkage * mu\n\n    return shrunk_cov, shrinkage"
    },
    {
      "id": "scikit-learn/sklearn.covariance._shrunk_covariance/shrunk_covariance",
      "name": "shrunk_covariance",
      "qname": "sklearn.covariance._shrunk_covariance.shrunk_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/shrunk_covariance/emp_cov",
          "name": "emp_cov",
          "qname": "sklearn.covariance._shrunk_covariance.shrunk_covariance.emp_cov",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_features, n_features)",
            "default_value": "",
            "description": "Covariance matrix to be shrunk"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_features, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.covariance._shrunk_covariance/shrunk_covariance/shrinkage",
          "name": "shrinkage",
          "qname": "sklearn.covariance._shrunk_covariance.shrunk_covariance.shrinkage",
          "default_value": "0.1",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "Coefficient in the convex combination used for the computation\nof the shrunk estimate. Range is [0, 1]."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.covariance"
      ],
      "description": "Calculates a covariance matrix shrunk on the diagonal\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.",
      "docstring": "Calculates a covariance matrix shrunk on the diagonal\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nParameters\n----------\nemp_cov : array-like of shape (n_features, n_features)\n    Covariance matrix to be shrunk\n\nshrinkage : float, default=0.1\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate. Range is [0, 1].\n\nReturns\n-------\nshrunk_cov : ndarray of shape (n_features, n_features)\n    Shrunk covariance.\n\nNotes\n-----\nThe regularized (shrunk) covariance is given by:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features",
      "code": "def shrunk_covariance(emp_cov, shrinkage=0.1):\n    \"\"\"Calculates a covariance matrix shrunk on the diagonal\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    emp_cov : array-like of shape (n_features, n_features)\n        Covariance matrix to be shrunk\n\n    shrinkage : float, default=0.1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate. Range is [0, 1].\n\n    Returns\n    -------\n    shrunk_cov : ndarray of shape (n_features, n_features)\n        Shrunk covariance.\n\n    Notes\n    -----\n    The regularized (shrunk) covariance is given by:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    \"\"\"\n    emp_cov = check_array(emp_cov)\n    n_features = emp_cov.shape[0]\n\n    mu = np.trace(emp_cov) / n_features\n    shrunk_cov = (1. - shrinkage) * emp_cov\n    shrunk_cov.flat[::n_features + 1] += shrinkage * mu\n\n    return shrunk_cov"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.CCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of components to keep. Should be in `[1, min(n_samples,\nn_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.max_iter",
          "default_value": "500",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "500",
            "description": "the maximum number of iterations of the power method."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.tol",
          "default_value": "1e-06",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-06",
            "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\nthan `tol`, where `u` corresponds to the left singular vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/CCA/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.CCA.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(n_components=n_components, scale=scale,\n                         deflation_mode=\"canonical\", mode=\"B\",\n                         algorithm=\"nipals\", max_iter=max_iter, tol=tol,\n                         copy=copy)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of components to keep. Should be in `[1, min(n_samples,\nn_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.algorithm",
          "default_value": "'nipals'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'nipals', 'svd'}",
            "default_value": "'nipals'",
            "description": "The algorithm used to estimate the first singular vectors of the\ncross-covariance matrix. 'nipals' uses the power method while 'svd'\nwill compute the whole SVD."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "nipals",
              "svd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.max_iter",
          "default_value": "500",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "500",
            "description": "the maximum number of iterations of the power method when\n`algorithm='nipals'`. Ignored otherwise."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.tol",
          "default_value": "1e-06",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-06",
            "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\nthan `tol`, where `u` corresponds to the left singular vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSCanonical/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.PLSCanonical.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Partial Least Squares transformer and regressor.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, algorithm=\"nipals\",\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"canonical\", mode=\"A\",\n            algorithm=algorithm,\n            max_iter=max_iter, tol=tol, copy=copy)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of components to keep. Should be in `[1, min(n_samples,\nn_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.max_iter",
          "default_value": "500",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "500",
            "description": "The maximum number of iterations of the power method when\n`algorithm='nipals'`. Ignored otherwise."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/tol",
          "name": "tol",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.tol",
          "default_value": "1e-06",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-06",
            "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of `u_i - u_{i-1}` is less\nthan `tol`, where `u` corresponds to the left singular vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSRegression/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.PLSRegression.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "PLS regression\n\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True,\n                 max_iter=500, tol=1e-06, copy=True):\n        super().__init__(\n            n_components=n_components, scale=scale,\n            deflation_mode=\"regression\", mode=\"A\",\n            algorithm='nipals', max_iter=max_iter,\n            tol=tol, copy=copy)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__",
      "name": "__init__",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of components to keep. Should be in `[1,\nmin(n_samples, n_features, n_targets)]`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/scale",
          "name": "scale",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.scale",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to scale `X` and `Y`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/__init__/copy",
          "name": "copy",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy `X` and `Y` in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Partial Least Square SVD.\n\nThis transformer simply performs a SVD on the crosscovariance matrix X'Y.\nIt is able to project both the training data `X` and the targets `Y`. The\ntraining data X is projected on the left singular vectors, while the\ntargets are projected on the right singular vectors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n\n.. versionadded:: 0.8",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, scale=True, copy=True):\n        self.n_components = n_components\n        self.scale = scale\n        self.copy = copy"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit",
      "name": "fit",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit/X",
          "name": "X",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit/Y",
          "name": "Y",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit.Y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
            "default_value": "",
            "description": "Targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_targets)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit model to data.",
      "docstring": "Fit model to data.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training samples.\n\nY : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Targets.",
      "code": "    def fit(self, X, Y):\n        \"\"\"Fit model to data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Targets.\n        \"\"\"\n        check_consistent_length(X, Y)\n        X = self._validate_data(X, dtype=np.float64, copy=self.copy,\n                                ensure_min_samples=2)\n        Y = check_array(Y, dtype=np.float64, copy=self.copy, ensure_2d=False)\n        if Y.ndim == 1:\n            Y = Y.reshape(-1, 1)\n\n        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)\n        # This matrix rank is at most min(n_samples, n_features, n_targets) so\n        # n_components cannot be bigger than that.\n        n_components = self.n_components\n        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])\n        if not 1 <= n_components <= rank_upper_bound:\n            # TODO: raise an error in 1.1\n            warnings.warn(\n                f\"As of version 0.24, n_components({n_components}) should be \"\n                f\"in [1, min(n_features, n_samples, n_targets)] = \"\n                f\"[1, {rank_upper_bound}]. \"\n                f\"n_components={rank_upper_bound} will be used instead. \"\n                f\"In version 1.1 (renaming of 0.26), an error will be raised.\",\n                FutureWarning\n            )\n            n_components = rank_upper_bound\n\n        X, Y, self._x_mean, self._y_mean, self._x_std, self._y_std = (\n            _center_scale_xy(X, Y, self.scale))\n\n        # Compute SVD of cross-covariance matrix\n        C = np.dot(X.T, Y)\n        U, s, Vt = svd(C, full_matrices=False)\n        U = U[:, :n_components]\n        Vt = Vt[:n_components]\n        U, Vt = svd_flip(U, Vt)\n        V = Vt.T\n\n        self._x_scores = np.dot(X, U)  # TODO: remove in 1.1\n        self._y_scores = np.dot(Y, V)  # TODO: remove in 1.1\n        self.x_weights_ = U\n        self.y_weights_ = V\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform/X",
          "name": "X",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/fit_transform/y",
          "name": "y",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
            "default_value": "None",
            "description": "Targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_targets)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn and apply the dimensionality reduction.",
      "docstring": "Learn and apply the dimensionality reduction.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training samples.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Targets.\n\nReturns\n-------\nout : array-like or tuple of array-like\n    The transformed data `X_tranformed` if `Y` is not None,\n    `(X_transformed, Y_transformed)` otherwise.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Learn and apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        return self.fit(X, y).transform(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform",
      "name": "transform",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform/X",
          "name": "X",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Samples to be transformed."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/transform/Y",
          "name": "Y",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.transform.Y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or (n_samples, n_targets)",
            "default_value": "None",
            "description": "Targets."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,) or (n_samples, n_targets)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply the dimensionality reduction.",
      "docstring": "Apply the dimensionality reduction.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Samples to be transformed.\n\nY : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n    Targets.\n\nReturns\n-------\nout : array-like or tuple of array-like\n    The transformed data `X_tranformed` if `Y` is not None,\n    `(X_transformed, Y_transformed)` otherwise.",
      "code": "    def transform(self, X, Y=None):\n        \"\"\"\n        Apply the dimensionality reduction.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Samples to be transformed.\n\n        Y : array-like of shape (n_samples,) or (n_samples, n_targets), \\\n                default=None\n            Targets.\n\n        Returns\n        -------\n        out : array-like or tuple of array-like\n            The transformed data `X_tranformed` if `Y` is not None,\n            `(X_transformed, Y_transformed)` otherwise.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, dtype=np.float64)\n        Xr = (X - self._x_mean) / self._x_std\n        x_scores = np.dot(Xr, self.x_weights_)\n        if Y is not None:\n            Y = check_array(Y, ensure_2d=False, dtype=np.float64)\n            if Y.ndim == 1:\n                Y = Y.reshape(-1, 1)\n            Yr = (Y - self._y_mean) / self._y_std\n            y_scores = np.dot(Yr, self.y_weights_)\n            return x_scores, y_scores\n        return x_scores"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_mean_@getter",
      "name": "x_mean_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_mean_",
      "decorators": [
        "deprecated('Attribute x_mean_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_mean_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_mean_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute x_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_mean_(self):\n        return self._x_mean"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_scores_@getter",
      "name": "x_scores_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_scores_",
      "decorators": [
        "deprecated('Attribute x_scores_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). Use est.transform(X) on the training data instead.')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_scores_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_scores_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute x_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X) on \"\n        \"the training data instead.\"\n    )\n    @property\n    def x_scores_(self):\n        return self._x_scores"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_std_@getter",
      "name": "x_std_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_std_",
      "decorators": [
        "deprecated('Attribute x_std_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/x_std_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.x_std_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute x_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def x_std_(self):\n        return self._x_std"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_mean_@getter",
      "name": "y_mean_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_mean_",
      "decorators": [
        "deprecated('Attribute y_mean_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_mean_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_mean_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute y_mean_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_mean_(self):\n        return self._y_mean"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_scores_@getter",
      "name": "y_scores_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_scores_",
      "decorators": [
        "deprecated('Attribute y_scores_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). Use est.transform(X, Y) on the training data instead.')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_scores_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_scores_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute y_scores_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26). Use est.transform(X, Y) \"\n        \"on the training data instead.\"\n    )\n    @property\n    def y_scores_(self):\n        return self._y_scores"
    },
    {
      "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_std_@getter",
      "name": "y_std_",
      "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_std_",
      "decorators": [
        "deprecated('Attribute y_std_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.cross_decomposition._pls/PLSSVD/y_std_@getter/self",
          "name": "self",
          "qname": "sklearn.cross_decomposition._pls.PLSSVD.y_std_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(  # type: ignore\n        \"Attribute y_std_ was deprecated in version 0.24 and \"\n        \"will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def y_std_(self):\n        return self._y_std"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/clear_data_home",
      "name": "clear_data_home",
      "qname": "sklearn.datasets._base.clear_data_home",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/clear_data_home/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._base.clear_data_home.data_home",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "The path to scikit-learn data directory. If `None`, the default path\nis `~/sklearn_learn_data`."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Delete all the content of the data home cache.",
      "docstring": "Delete all the content of the data home cache.\n\nParameters\n----------\ndata_home : str, default=None\n    The path to scikit-learn data directory. If `None`, the default path\n    is `~/sklearn_learn_data`.",
      "code": "def clear_data_home(data_home=None):\n    \"\"\"Delete all the content of the data home cache.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        The path to scikit-learn data directory. If `None`, the default path\n        is `~/sklearn_learn_data`.\n    \"\"\"\n    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/get_data_home",
      "name": "get_data_home",
      "qname": "sklearn.datasets._base.get_data_home",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/get_data_home/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._base.get_data_home.data_home",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "The path to scikit-learn data directory. If `None`, the default path\nis `~/sklearn_learn_data`."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Return the path of the scikit-learn data dir.\n\nThis folder is used by some large dataset loaders to avoid downloading the\ndata several times.\n\nBy default the data dir is set to a folder named 'scikit_learn_data' in the\nuser home folder.\n\nAlternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\nvariable or programmatically by giving an explicit folder path. The '~'\nsymbol is expanded to the user home folder.\n\nIf the folder does not already exist, it is automatically created.",
      "docstring": "Return the path of the scikit-learn data dir.\n\nThis folder is used by some large dataset loaders to avoid downloading the\ndata several times.\n\nBy default the data dir is set to a folder named 'scikit_learn_data' in the\nuser home folder.\n\nAlternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\nvariable or programmatically by giving an explicit folder path. The '~'\nsymbol is expanded to the user home folder.\n\nIf the folder does not already exist, it is automatically created.\n\nParameters\n----------\ndata_home : str, default=None\n    The path to scikit-learn data directory. If `None`, the default path\n    is `~/sklearn_learn_data`.",
      "code": "def get_data_home(data_home=None) -> str:\n    \"\"\"Return the path of the scikit-learn data dir.\n\n    This folder is used by some large dataset loaders to avoid downloading the\n    data several times.\n\n    By default the data dir is set to a folder named 'scikit_learn_data' in the\n    user home folder.\n\n    Alternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n\n    If the folder does not already exist, it is automatically created.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        The path to scikit-learn data directory. If `None`, the default path\n        is `~/sklearn_learn_data`.\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get('SCIKIT_LEARN_DATA',\n                                join('~', 'scikit_learn_data'))\n    data_home = expanduser(data_home)\n    makedirs(data_home, exist_ok=True)\n    return data_home"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_breast_cancer",
      "name": "load_breast_cancer",
      "qname": "sklearn.datasets._base.load_breast_cancer",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_breast_cancer/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._base.load_breast_cancer.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object.\nSee below for more information about the `data` and `target` object.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_breast_cancer/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._base.load_breast_cancer.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric). The target is\na pandas DataFrame or Series depending on the number of target columns.\nIf `return_X_y` is True, then (`data`, `target`) will be pandas\nDataFrames or Series as described below.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and return the breast cancer wisconsin dataset (classification).\n\nThe breast cancer dataset is a classic and very easy binary classification\ndataset.\n\n=================   ==============\nClasses                          2\nSamples per class    212(M),357(B)\nSamples total                  569\nDimensionality                  30\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <breast_cancer_dataset>`.",
      "docstring": "Load and return the breast cancer wisconsin dataset (classification).\n\nThe breast cancer dataset is a classic and very easy binary classification\ndataset.\n\n=================   ==============\nClasses                          2\nSamples per class    212(M),357(B)\nSamples total                  569\nDimensionality                  30\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <breast_cancer_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (569, 30)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (569,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (569, 31)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nThe copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\ndownloaded from:\nhttps://goo.gl/U2Uwz2\n\nExamples\n--------\nLet's say you are interested in the samples 10, 50, and 85, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_breast_cancer\n>>> data = load_breast_cancer()\n>>> data.target[[10, 50, 85]]\narray([0, 1, 0])\n>>> list(data.target_names)\n['malignant', 'benign']",
      "code": "@_deprecate_positional_args\ndef load_breast_cancer(*, return_X_y=False, as_frame=False):\n    \"\"\"Load and return the breast cancer wisconsin dataset (classification).\n\n    The breast cancer dataset is a classic and very easy binary classification\n    dataset.\n\n    =================   ==============\n    Classes                          2\n    Samples per class    212(M),357(B)\n    Samples total                  569\n    Dimensionality                  30\n    Features            real, positive\n    =================   ==============\n\n    Read more in the :ref:`User Guide <breast_cancer_dataset>`.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric). The target is\n        a pandas DataFrame or Series depending on the number of target columns.\n        If `return_X_y` is True, then (`data`, `target`) will be pandas\n        DataFrames or Series as described below.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (569, 30)\n            The data matrix. If `as_frame=True`, `data` will be a pandas\n            DataFrame.\n        target: {ndarray, Series} of shape (569,)\n            The classification target. If `as_frame=True`, `target` will be\n            a pandas Series.\n        feature_names: list\n            The names of the dataset columns.\n        target_names: list\n            The names of target classes.\n        frame: DataFrame of shape (569, 31)\n            Only present when `as_frame=True`. DataFrame with `data` and\n            `target`.\n\n            .. versionadded:: 0.23\n        DESCR: str\n            The full description of the dataset.\n        filename: str\n            The path to the location of the data.\n\n            .. versionadded:: 0.20\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n\n    The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n    downloaded from:\n    https://goo.gl/U2Uwz2\n\n    Examples\n    --------\n    Let's say you are interested in the samples 10, 50, and 85, and want to\n    know their class name.\n\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> data = load_breast_cancer()\n    >>> data.target[[10, 50, 85]]\n    array([0, 1, 0])\n    >>> list(data.target_names)\n    ['malignant', 'benign']\n    \"\"\"\n    module_path = dirname(__file__)\n    data, target, target_names = load_data(module_path, 'breast_cancer.csv')\n    csv_filename = join(module_path, 'data', 'breast_cancer.csv')\n\n    with open(join(module_path, 'descr', 'breast_cancer.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    feature_names = np.array(['mean radius', 'mean texture',\n                              'mean perimeter', 'mean area',\n                              'mean smoothness', 'mean compactness',\n                              'mean concavity', 'mean concave points',\n                              'mean symmetry', 'mean fractal dimension',\n                              'radius error', 'texture error',\n                              'perimeter error', 'area error',\n                              'smoothness error', 'compactness error',\n                              'concavity error', 'concave points error',\n                              'symmetry error', 'fractal dimension error',\n                              'worst radius', 'worst texture',\n                              'worst perimeter', 'worst area',\n                              'worst smoothness', 'worst compactness',\n                              'worst concavity', 'worst concave points',\n                              'worst symmetry', 'worst fractal dimension'])\n\n    frame = None\n    target_columns = ['target', ]\n    if as_frame:\n        frame, data, target = _convert_data_dataframe(\"load_breast_cancer\",\n                                                      data,\n                                                      target,\n                                                      feature_names,\n                                                      target_columns)\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data,\n                 target=target,\n                 frame=frame,\n                 target_names=target_names,\n                 DESCR=fdescr,\n                 feature_names=feature_names,\n                 filename=csv_filename)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_diabetes",
      "name": "load_diabetes",
      "qname": "sklearn.datasets._base.load_diabetes",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_diabetes/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._base.load_diabetes.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False.",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object.\nSee below for more information about the `data` and `target` object.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_diabetes/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._base.load_diabetes.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric). The target is\na pandas DataFrame or Series depending on the number of target columns.\nIf `return_X_y` is True, then (`data`, `target`) will be pandas\nDataFrames or Series as described below.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and return the diabetes dataset (regression).\n\n==============   ==================\nSamples total    442\nDimensionality   10\nFeatures         real, -.2 < x < .2\nTargets          integer 25 - 346\n==============   ==================\n\n.. note::\n   The meaning of each feature (i.e. `feature_names`) might be unclear\n   (especially for `ltg`) as the documentation of the original dataset is\n   not explicit. We provide information that seems correct in regard with\n   the scientific literature in this field of research.\n\nRead more in the :ref:`User Guide <diabetes_dataset>`.",
      "docstring": "Load and return the diabetes dataset (regression).\n\n==============   ==================\nSamples total    442\nDimensionality   10\nFeatures         real, -.2 < x < .2\nTargets          integer 25 - 346\n==============   ==================\n\n.. note::\n   The meaning of each feature (i.e. `feature_names`) might be unclear\n   (especially for `ltg`) as the documentation of the original dataset is\n   not explicit. We provide information that seems correct in regard with\n   the scientific literature in this field of research.\n\nRead more in the :ref:`User Guide <diabetes_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False.\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (442, 10)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (442,)\n        The regression target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    frame: DataFrame of shape (442, 11)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    data_filename: str\n        The path to the location of the data.\n    target_filename: str\n        The path to the location of the target.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18",
      "code": "@_deprecate_positional_args\ndef load_diabetes(*, return_X_y=False, as_frame=False):\n    \"\"\"Load and return the diabetes dataset (regression).\n\n    ==============   ==================\n    Samples total    442\n    Dimensionality   10\n    Features         real, -.2 < x < .2\n    Targets          integer 25 - 346\n    ==============   ==================\n\n    .. note::\n       The meaning of each feature (i.e. `feature_names`) might be unclear\n       (especially for `ltg`) as the documentation of the original dataset is\n       not explicit. We provide information that seems correct in regard with\n       the scientific literature in this field of research.\n\n    Read more in the :ref:`User Guide <diabetes_dataset>`.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False.\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric). The target is\n        a pandas DataFrame or Series depending on the number of target columns.\n        If `return_X_y` is True, then (`data`, `target`) will be pandas\n        DataFrames or Series as described below.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (442, 10)\n            The data matrix. If `as_frame=True`, `data` will be a pandas\n            DataFrame.\n        target: {ndarray, Series} of shape (442,)\n            The regression target. If `as_frame=True`, `target` will be\n            a pandas Series.\n        feature_names: list\n            The names of the dataset columns.\n        frame: DataFrame of shape (442, 11)\n            Only present when `as_frame=True`. DataFrame with `data` and\n            `target`.\n\n            .. versionadded:: 0.23\n        DESCR: str\n            The full description of the dataset.\n        data_filename: str\n            The path to the location of the data.\n        target_filename: str\n            The path to the location of the target.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n    \"\"\"\n    module_path = dirname(__file__)\n    base_dir = join(module_path, 'data')\n    data_filename = join(base_dir, 'diabetes_data.csv.gz')\n    data = np.loadtxt(data_filename)\n    target_filename = join(base_dir, 'diabetes_target.csv.gz')\n    target = np.loadtxt(target_filename)\n\n    with open(join(module_path, 'descr', 'diabetes.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    feature_names = ['age', 'sex', 'bmi', 'bp',\n                     's1', 's2', 's3', 's4', 's5', 's6']\n\n    frame = None\n    target_columns = ['target', ]\n    if as_frame:\n        frame, data, target = _convert_data_dataframe(\"load_diabetes\",\n                                                      data,\n                                                      target,\n                                                      feature_names,\n                                                      target_columns)\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data,\n                 target=target,\n                 frame=frame,\n                 DESCR=fdescr,\n                 feature_names=feature_names,\n                 data_filename=data_filename,\n                 target_filename=target_filename)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_digits",
      "name": "load_digits",
      "qname": "sklearn.datasets._base.load_digits",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_digits/n_class",
          "name": "n_class",
          "qname": "sklearn.datasets._base.load_digits.n_class",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of classes to return. Between 0 and 10."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_digits/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._base.load_digits.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object.\nSee below for more information about the `data` and `target` object.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_digits/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._base.load_digits.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric). The target is\na pandas DataFrame or Series depending on the number of target columns.\nIf `return_X_y` is True, then (`data`, `target`) will be pandas\nDataFrames or Series as described below.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and return the digits dataset (classification).\n\nEach datapoint is a 8x8 image of a digit.\n\n=================   ==============\nClasses                         10\nSamples per class             ~180\nSamples total                 1797\nDimensionality                  64\nFeatures             integers 0-16\n=================   ==============\n\nRead more in the :ref:`User Guide <digits_dataset>`.",
      "docstring": "Load and return the digits dataset (classification).\n\nEach datapoint is a 8x8 image of a digit.\n\n=================   ==============\nClasses                         10\nSamples per class             ~180\nSamples total                 1797\nDimensionality                  64\nFeatures             integers 0-16\n=================   ==============\n\nRead more in the :ref:`User Guide <digits_dataset>`.\n\nParameters\n----------\nn_class : int, default=10\n    The number of classes to return. Between 0 and 10.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (1797, 64)\n        The flattened data matrix. If `as_frame=True`, `data` will be\n        a pandas DataFrame.\n    target: {ndarray, Series} of shape (1797,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n\n        .. versionadded:: 0.20\n\n    frame: DataFrame of shape (1797, 65)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    images: {ndarray} of shape (1797, 8, 8)\n        The raw image data.\n    DESCR: str\n        The full description of the dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nExamples\n--------\nTo load the data and visualize the images::\n\n    >>> from sklearn.datasets import load_digits\n    >>> digits = load_digits()\n    >>> print(digits.data.shape)\n    (1797, 64)\n    >>> import matplotlib.pyplot as plt #doctest: +SKIP\n    >>> plt.gray() #doctest: +SKIP\n    >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n    >>> plt.show() #doctest: +SKIP",
      "code": "@_deprecate_positional_args\ndef load_digits(*, n_class=10, return_X_y=False, as_frame=False):\n    \"\"\"Load and return the digits dataset (classification).\n\n    Each datapoint is a 8x8 image of a digit.\n\n    =================   ==============\n    Classes                         10\n    Samples per class             ~180\n    Samples total                 1797\n    Dimensionality                  64\n    Features             integers 0-16\n    =================   ==============\n\n    Read more in the :ref:`User Guide <digits_dataset>`.\n\n    Parameters\n    ----------\n    n_class : int, default=10\n        The number of classes to return. Between 0 and 10.\n\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric). The target is\n        a pandas DataFrame or Series depending on the number of target columns.\n        If `return_X_y` is True, then (`data`, `target`) will be pandas\n        DataFrames or Series as described below.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (1797, 64)\n            The flattened data matrix. If `as_frame=True`, `data` will be\n            a pandas DataFrame.\n        target: {ndarray, Series} of shape (1797,)\n            The classification target. If `as_frame=True`, `target` will be\n            a pandas Series.\n        feature_names: list\n            The names of the dataset columns.\n        target_names: list\n            The names of target classes.\n\n            .. versionadded:: 0.20\n\n        frame: DataFrame of shape (1797, 65)\n            Only present when `as_frame=True`. DataFrame with `data` and\n            `target`.\n\n            .. versionadded:: 0.23\n        images: {ndarray} of shape (1797, 8, 8)\n            The raw image data.\n        DESCR: str\n            The full description of the dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n\n    This is a copy of the test set of the UCI ML hand-written digits datasets\n    https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\n    Examples\n    --------\n    To load the data and visualize the images::\n\n        >>> from sklearn.datasets import load_digits\n        >>> digits = load_digits()\n        >>> print(digits.data.shape)\n        (1797, 64)\n        >>> import matplotlib.pyplot as plt #doctest: +SKIP\n        >>> plt.gray() #doctest: +SKIP\n        >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n        >>> plt.show() #doctest: +SKIP\n    \"\"\"\n    module_path = dirname(__file__)\n    data = np.loadtxt(join(module_path, 'data', 'digits.csv.gz'),\n                      delimiter=',')\n    with open(join(module_path, 'descr', 'digits.rst')) as f:\n        descr = f.read()\n    target = data[:, -1].astype(int, copy=False)\n    flat_data = data[:, :-1]\n    images = flat_data.view()\n    images.shape = (-1, 8, 8)\n\n    if n_class < 10:\n        idx = target < n_class\n        flat_data, target = flat_data[idx], target[idx]\n        images = images[idx]\n\n    feature_names = ['pixel_{}_{}'.format(row_idx, col_idx)\n                     for row_idx in range(8)\n                     for col_idx in range(8)]\n\n    frame = None\n    target_columns = ['target', ]\n    if as_frame:\n        frame, flat_data, target = _convert_data_dataframe(\"load_digits\",\n                                                           flat_data,\n                                                           target,\n                                                           feature_names,\n                                                           target_columns)\n\n    if return_X_y:\n        return flat_data, target\n\n    return Bunch(data=flat_data,\n                 target=target,\n                 frame=frame,\n                 feature_names=feature_names,\n                 target_names=np.arange(10),\n                 images=images,\n                 DESCR=descr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_files",
      "name": "load_files",
      "qname": "sklearn.datasets._base.load_files",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/container_path",
          "name": "container_path",
          "qname": "sklearn.datasets._base.load_files.container_path",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str or unicode",
            "default_value": "",
            "description": "Path to the main folder holding one subfolder per category"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "unicode"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/description",
          "name": "description",
          "qname": "sklearn.datasets._base.load_files.description",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or unicode",
            "default_value": "None",
            "description": "A paragraph describing the characteristic of the dataset: its source,\nreference, etc."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "unicode"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/categories",
          "name": "categories",
          "qname": "sklearn.datasets._base.load_files.categories",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "list of str",
            "default_value": "None",
            "description": "If None (default), load all the categories. If not None, list of\ncategory names to load (other categories ignored)."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/load_content",
          "name": "load_content",
          "qname": "sklearn.datasets._base.load_files.load_content",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to load or not the content of the different files. If true a\n'data' attribute containing the text information is present in the data\nstructure returned. If not, a filenames attribute gives the path to the\nfiles."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._base.load_files.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to shuffle the data: might be important for models that\nmake the assumption that the samples are independent and identically\ndistributed (i.i.d.), such as stochastic gradient descent."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/encoding",
          "name": "encoding",
          "qname": "sklearn.datasets._base.load_files.encoding",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "If None, do not try to decode the content of the files (e.g. for images\nor other non-text content). If not None, encoding to use to decode text\nfiles to Unicode if load_content is True."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/decode_error",
          "name": "decode_error",
          "qname": "sklearn.datasets._base.load_files.decode_error",
          "default_value": "'strict'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'strict', 'ignore', 'replace'}",
            "default_value": "'strict'",
            "description": "Instruction on what to do if a byte sequence is given to analyze that\ncontains characters not of the given `encoding`. Passed as keyword\nargument 'errors' to bytes.decode."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ignore",
              "replace",
              "strict"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_files/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._base.load_files.random_state",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "0",
            "description": "Determines random number generation for dataset shuffling. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load text files with categories as subfolder names.\n\nIndividual samples are assumed to be files stored a two levels folder\nstructure such as the following:\n\n    container_folder/\n        category_1_folder/\n            file_1.txt\n            file_2.txt\n            ...\n            file_42.txt\n        category_2_folder/\n            file_43.txt\n            file_44.txt\n            ...\n\nThe folder names are used as supervised signal label names. The individual\nfile names are not important.\n\nThis function does not try to extract features into a numpy array or scipy\nsparse matrix. In addition, if load_content is false it does not try to\nload the files in memory.\n\nTo use text files in a scikit-learn classification or clustering algorithm,\nyou will need to use the :mod`~sklearn.feature_extraction.text` module to\nbuild a feature extraction transformer that suits your problem.\n\nIf you set load_content=True, you should also specify the encoding of the\ntext using the 'encoding' parameter. For many modern text files, 'utf-8'\nwill be the correct encoding. If you leave encoding equal to None, then the\ncontent will be made of bytes instead of Unicode, and you will not be able\nto use most functions in :mod:`~sklearn.feature_extraction.text`.\n\nSimilar feature extractors should be built for other kind of unstructured\ndata input such as images, audio, video, ...\n\nRead more in the :ref:`User Guide <datasets>`.",
      "docstring": "Load text files with categories as subfolder names.\n\nIndividual samples are assumed to be files stored a two levels folder\nstructure such as the following:\n\n    container_folder/\n        category_1_folder/\n            file_1.txt\n            file_2.txt\n            ...\n            file_42.txt\n        category_2_folder/\n            file_43.txt\n            file_44.txt\n            ...\n\nThe folder names are used as supervised signal label names. The individual\nfile names are not important.\n\nThis function does not try to extract features into a numpy array or scipy\nsparse matrix. In addition, if load_content is false it does not try to\nload the files in memory.\n\nTo use text files in a scikit-learn classification or clustering algorithm,\nyou will need to use the :mod`~sklearn.feature_extraction.text` module to\nbuild a feature extraction transformer that suits your problem.\n\nIf you set load_content=True, you should also specify the encoding of the\ntext using the 'encoding' parameter. For many modern text files, 'utf-8'\nwill be the correct encoding. If you leave encoding equal to None, then the\ncontent will be made of bytes instead of Unicode, and you will not be able\nto use most functions in :mod:`~sklearn.feature_extraction.text`.\n\nSimilar feature extractors should be built for other kind of unstructured\ndata input such as images, audio, video, ...\n\nRead more in the :ref:`User Guide <datasets>`.\n\nParameters\n----------\ncontainer_path : str or unicode\n    Path to the main folder holding one subfolder per category\n\ndescription : str or unicode, default=None\n    A paragraph describing the characteristic of the dataset: its source,\n    reference, etc.\n\ncategories : list of str, default=None\n    If None (default), load all the categories. If not None, list of\n    category names to load (other categories ignored).\n\nload_content : bool, default=True\n    Whether to load or not the content of the different files. If true a\n    'data' attribute containing the text information is present in the data\n    structure returned. If not, a filenames attribute gives the path to the\n    files.\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data: might be important for models that\n    make the assumption that the samples are independent and identically\n    distributed (i.i.d.), such as stochastic gradient descent.\n\nencoding : str, default=None\n    If None, do not try to decode the content of the files (e.g. for images\n    or other non-text content). If not None, encoding to use to decode text\n    files to Unicode if load_content is True.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. Passed as keyword\n    argument 'errors' to bytes.decode.\n\nrandom_state : int, RandomState instance or None, default=0\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : list of str\n        Only present when `load_content=True`.\n        The raw text data to learn.\n    target : ndarray\n        The target labels (integer index).\n    target_names : list\n        The names of target classes.\n    DESCR : str\n        The full description of the dataset.\n    filenames: ndarray\n        The filenames holding the dataset.",
      "code": "@_deprecate_positional_args\ndef load_files(container_path, *, description=None, categories=None,\n               load_content=True, shuffle=True, encoding=None,\n               decode_error='strict', random_state=0):\n    \"\"\"Load text files with categories as subfolder names.\n\n    Individual samples are assumed to be files stored a two levels folder\n    structure such as the following:\n\n        container_folder/\n            category_1_folder/\n                file_1.txt\n                file_2.txt\n                ...\n                file_42.txt\n            category_2_folder/\n                file_43.txt\n                file_44.txt\n                ...\n\n    The folder names are used as supervised signal label names. The individual\n    file names are not important.\n\n    This function does not try to extract features into a numpy array or scipy\n    sparse matrix. In addition, if load_content is false it does not try to\n    load the files in memory.\n\n    To use text files in a scikit-learn classification or clustering algorithm,\n    you will need to use the :mod`~sklearn.feature_extraction.text` module to\n    build a feature extraction transformer that suits your problem.\n\n    If you set load_content=True, you should also specify the encoding of the\n    text using the 'encoding' parameter. For many modern text files, 'utf-8'\n    will be the correct encoding. If you leave encoding equal to None, then the\n    content will be made of bytes instead of Unicode, and you will not be able\n    to use most functions in :mod:`~sklearn.feature_extraction.text`.\n\n    Similar feature extractors should be built for other kind of unstructured\n    data input such as images, audio, video, ...\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    container_path : str or unicode\n        Path to the main folder holding one subfolder per category\n\n    description : str or unicode, default=None\n        A paragraph describing the characteristic of the dataset: its source,\n        reference, etc.\n\n    categories : list of str, default=None\n        If None (default), load all the categories. If not None, list of\n        category names to load (other categories ignored).\n\n    load_content : bool, default=True\n        Whether to load or not the content of the different files. If true a\n        'data' attribute containing the text information is present in the data\n        structure returned. If not, a filenames attribute gives the path to the\n        files.\n\n    shuffle : bool, default=True\n        Whether or not to shuffle the data: might be important for models that\n        make the assumption that the samples are independent and identically\n        distributed (i.i.d.), such as stochastic gradient descent.\n\n    encoding : str, default=None\n        If None, do not try to decode the content of the files (e.g. for images\n        or other non-text content). If not None, encoding to use to decode text\n        files to Unicode if load_content is True.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. Passed as keyword\n        argument 'errors' to bytes.decode.\n\n    random_state : int, RandomState instance or None, default=0\n        Determines random number generation for dataset shuffling. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : list of str\n            Only present when `load_content=True`.\n            The raw text data to learn.\n        target : ndarray\n            The target labels (integer index).\n        target_names : list\n            The names of target classes.\n        DESCR : str\n            The full description of the dataset.\n        filenames: ndarray\n            The filenames holding the dataset.\n    \"\"\"\n    target = []\n    target_names = []\n    filenames = []\n\n    folders = [f for f in sorted(listdir(container_path))\n               if isdir(join(container_path, f))]\n\n    if categories is not None:\n        folders = [f for f in folders if f in categories]\n\n    for label, folder in enumerate(folders):\n        target_names.append(folder)\n        folder_path = join(container_path, folder)\n        documents = [join(folder_path, d)\n                     for d in sorted(listdir(folder_path))]\n        target.extend(len(documents) * [label])\n        filenames.extend(documents)\n\n    # convert to array for fancy indexing\n    filenames = np.array(filenames)\n    target = np.array(target)\n\n    if shuffle:\n        random_state = check_random_state(random_state)\n        indices = np.arange(filenames.shape[0])\n        random_state.shuffle(indices)\n        filenames = filenames[indices]\n        target = target[indices]\n\n    if load_content:\n        data = []\n        for filename in filenames:\n            with open(filename, 'rb') as f:\n                data.append(f.read())\n        if encoding is not None:\n            data = [d.decode(encoding, decode_error) for d in data]\n        return Bunch(data=data,\n                     filenames=filenames,\n                     target_names=target_names,\n                     target=target,\n                     DESCR=description)\n\n    return Bunch(filenames=filenames,\n                 target_names=target_names,\n                 target=target,\n                 DESCR=description)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_iris",
      "name": "load_iris",
      "qname": "sklearn.datasets._base.load_iris",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_iris/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._base.load_iris.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object. See\nbelow for more information about the `data` and `target` object.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_iris/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._base.load_iris.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric). The target is\na pandas DataFrame or Series depending on the number of target columns.\nIf `return_X_y` is True, then (`data`, `target`) will be pandas\nDataFrames or Series as described below.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.",
      "docstring": "Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <iris_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (150, 4)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (150,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (150, 5)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    filename: str\n        The path to the location of the data.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18\n\nNotes\n-----\n    .. versionchanged:: 0.20\n        Fixed two wrong data points according to Fisher's paper.\n        The new version is the same as in R, but not as in the UCI\n        Machine Learning Repository.\n\nExamples\n--------\nLet's say you are interested in the samples 10, 25, and 50, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_iris\n>>> data = load_iris()\n>>> data.target[[10, 25, 50]]\narray([0, 0, 1])\n>>> list(data.target_names)\n['setosa', 'versicolor', 'virginica']",
      "code": "@_deprecate_positional_args\ndef load_iris(*, return_X_y=False, as_frame=False):\n    \"\"\"Load and return the iris dataset (classification).\n\n    The iris dataset is a classic and very easy multi-class classification\n    dataset.\n\n    =================   ==============\n    Classes                          3\n    Samples per class               50\n    Samples total                  150\n    Dimensionality                   4\n    Features            real, positive\n    =================   ==============\n\n    Read more in the :ref:`User Guide <iris_dataset>`.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object. See\n        below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric). The target is\n        a pandas DataFrame or Series depending on the number of target columns.\n        If `return_X_y` is True, then (`data`, `target`) will be pandas\n        DataFrames or Series as described below.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (150, 4)\n            The data matrix. If `as_frame=True`, `data` will be a pandas\n            DataFrame.\n        target: {ndarray, Series} of shape (150,)\n            The classification target. If `as_frame=True`, `target` will be\n            a pandas Series.\n        feature_names: list\n            The names of the dataset columns.\n        target_names: list\n            The names of target classes.\n        frame: DataFrame of shape (150, 5)\n            Only present when `as_frame=True`. DataFrame with `data` and\n            `target`.\n\n            .. versionadded:: 0.23\n        DESCR: str\n            The full description of the dataset.\n        filename: str\n            The path to the location of the data.\n\n            .. versionadded:: 0.20\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n\n    Notes\n    -----\n        .. versionchanged:: 0.20\n            Fixed two wrong data points according to Fisher's paper.\n            The new version is the same as in R, but not as in the UCI\n            Machine Learning Repository.\n\n    Examples\n    --------\n    Let's say you are interested in the samples 10, 25, and 50, and want to\n    know their class name.\n\n    >>> from sklearn.datasets import load_iris\n    >>> data = load_iris()\n    >>> data.target[[10, 25, 50]]\n    array([0, 0, 1])\n    >>> list(data.target_names)\n    ['setosa', 'versicolor', 'virginica']\n    \"\"\"\n    module_path = dirname(__file__)\n    data, target, target_names = load_data(module_path, 'iris.csv')\n    iris_csv_filename = join(module_path, 'data', 'iris.csv')\n\n    with open(join(module_path, 'descr', 'iris.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    feature_names = ['sepal length (cm)', 'sepal width (cm)',\n                     'petal length (cm)', 'petal width (cm)']\n\n    frame = None\n    target_columns = ['target', ]\n    if as_frame:\n        frame, data, target = _convert_data_dataframe(\"load_iris\",\n                                                      data,\n                                                      target,\n                                                      feature_names,\n                                                      target_columns)\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data,\n                 target=target,\n                 frame=frame,\n                 target_names=target_names,\n                 DESCR=fdescr,\n                 feature_names=feature_names,\n                 filename=iris_csv_filename)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_linnerud",
      "name": "load_linnerud",
      "qname": "sklearn.datasets._base.load_linnerud",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_linnerud/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._base.load_linnerud.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object.\nSee below for more information about the `data` and `target` object.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_linnerud/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._base.load_linnerud.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric, string or categorical). The target is\na pandas DataFrame or Series depending on the number of target columns.\nIf `return_X_y` is True, then (`data`, `target`) will be pandas\nDataFrames or Series as described below.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and return the physical excercise linnerud dataset.\n\nThis dataset is suitable for multi-ouput regression tasks.\n\n==============   ============================\nSamples total    20\nDimensionality   3 (for both data and target)\nFeatures         integer\nTargets          integer\n==============   ============================\n\nRead more in the :ref:`User Guide <linnerrud_dataset>`.",
      "docstring": "Load and return the physical excercise linnerud dataset.\n\nThis dataset is suitable for multi-ouput regression tasks.\n\n==============   ============================\nSamples total    20\nDimensionality   3 (for both data and target)\nFeatures         integer\nTargets          integer\n==============   ============================\n\nRead more in the :ref:`User Guide <linnerrud_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.18\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string or categorical). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (20, 3)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, dataframe} of shape (20, 3)\n        The regression targets. If `as_frame=True`, `target` will be\n        a pandas DataFrame.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of the target columns.\n    frame: DataFrame of shape (20, 6)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n    data_filename: str\n        The path to the location of the data.\n    target_filename: str\n        The path to the location of the target.\n\n        .. versionadded:: 0.20\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.18",
      "code": "@_deprecate_positional_args\ndef load_linnerud(*, return_X_y=False, as_frame=False):\n    \"\"\"Load and return the physical excercise linnerud dataset.\n\n    This dataset is suitable for multi-ouput regression tasks.\n\n    ==============   ============================\n    Samples total    20\n    Dimensionality   3 (for both data and target)\n    Features         integer\n    Targets          integer\n    ==============   ============================\n\n    Read more in the :ref:`User Guide <linnerrud_dataset>`.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric, string or categorical). The target is\n        a pandas DataFrame or Series depending on the number of target columns.\n        If `return_X_y` is True, then (`data`, `target`) will be pandas\n        DataFrames or Series as described below.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (20, 3)\n            The data matrix. If `as_frame=True`, `data` will be a pandas\n            DataFrame.\n        target: {ndarray, dataframe} of shape (20, 3)\n            The regression targets. If `as_frame=True`, `target` will be\n            a pandas DataFrame.\n        feature_names: list\n            The names of the dataset columns.\n        target_names: list\n            The names of the target columns.\n        frame: DataFrame of shape (20, 6)\n            Only present when `as_frame=True`. DataFrame with `data` and\n            `target`.\n\n            .. versionadded:: 0.23\n        DESCR: str\n            The full description of the dataset.\n        data_filename: str\n            The path to the location of the data.\n        target_filename: str\n            The path to the location of the target.\n\n            .. versionadded:: 0.20\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n    \"\"\"\n    base_dir = join(dirname(__file__), 'data/')\n    data_filename = join(base_dir, 'linnerud_exercise.csv')\n    target_filename = join(base_dir, 'linnerud_physiological.csv')\n\n    # Read data\n    data_exercise = np.loadtxt(data_filename, skiprows=1)\n    data_physiological = np.loadtxt(target_filename, skiprows=1)\n\n    # Read header\n    with open(data_filename) as f:\n        header_exercise = f.readline().split()\n    with open(target_filename) as f:\n        header_physiological = f.readline().split()\n\n    with open(dirname(__file__) + '/descr/linnerud.rst') as f:\n        descr = f.read()\n\n    frame = None\n    if as_frame:\n        (frame,\n         data_exercise,\n         data_physiological) = _convert_data_dataframe(\"load_linnerud\",\n                                                       data_exercise,\n                                                       data_physiological,\n                                                       header_exercise,\n                                                       header_physiological)\n    if return_X_y:\n        return data_exercise, data_physiological\n\n    return Bunch(data=data_exercise,\n                 feature_names=header_exercise,\n                 target=data_physiological,\n                 target_names=header_physiological,\n                 frame=frame,\n                 DESCR=descr,\n                 data_filename=data_filename,\n                 target_filename=target_filename)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_sample_image",
      "name": "load_sample_image",
      "qname": "sklearn.datasets._base.load_sample_image",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_sample_image/image_name",
          "name": "image_name",
          "qname": "sklearn.datasets._base.load_sample_image.image_name",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{`china.jpg`, `flower.jpg`}",
            "default_value": "",
            "description": "The name of the sample image loaded"
          },
          "type": {
            "kind": "EnumType",
            "values": []
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the numpy array of a single sample image\n\nRead more in the :ref:`User Guide <sample_images>`.",
      "docstring": "Load the numpy array of a single sample image\n\nRead more in the :ref:`User Guide <sample_images>`.\n\nParameters\n----------\nimage_name : {`china.jpg`, `flower.jpg`}\n    The name of the sample image loaded\n\nReturns\n-------\nimg : 3D array\n    The image as a numpy array: height x width x color\n\nExamples\n--------\n\n>>> from sklearn.datasets import load_sample_image\n>>> china = load_sample_image('china.jpg')   # doctest: +SKIP\n>>> china.dtype                              # doctest: +SKIP\ndtype('uint8')\n>>> china.shape                              # doctest: +SKIP\n(427, 640, 3)\n>>> flower = load_sample_image('flower.jpg') # doctest: +SKIP\n>>> flower.dtype                             # doctest: +SKIP\ndtype('uint8')\n>>> flower.shape                             # doctest: +SKIP\n(427, 640, 3)",
      "code": "def load_sample_image(image_name):\n    \"\"\"Load the numpy array of a single sample image\n\n    Read more in the :ref:`User Guide <sample_images>`.\n\n    Parameters\n    ----------\n    image_name : {`china.jpg`, `flower.jpg`}\n        The name of the sample image loaded\n\n    Returns\n    -------\n    img : 3D array\n        The image as a numpy array: height x width x color\n\n    Examples\n    --------\n\n    >>> from sklearn.datasets import load_sample_image\n    >>> china = load_sample_image('china.jpg')   # doctest: +SKIP\n    >>> china.dtype                              # doctest: +SKIP\n    dtype('uint8')\n    >>> china.shape                              # doctest: +SKIP\n    (427, 640, 3)\n    >>> flower = load_sample_image('flower.jpg') # doctest: +SKIP\n    >>> flower.dtype                             # doctest: +SKIP\n    dtype('uint8')\n    >>> flower.shape                             # doctest: +SKIP\n    (427, 640, 3)\n    \"\"\"\n    images = load_sample_images()\n    index = None\n    for i, filename in enumerate(images.filenames):\n        if filename.endswith(image_name):\n            index = i\n            break\n    if index is None:\n        raise AttributeError(\"Cannot find sample image: %s\" % image_name)\n    return images.images[index]"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_sample_images",
      "name": "load_sample_images",
      "qname": "sklearn.datasets._base.load_sample_images",
      "decorators": [],
      "parameters": [],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load sample images for image manipulation.\n\nLoads both, ``china`` and ``flower``.\n\nRead more in the :ref:`User Guide <sample_images>`.",
      "docstring": "Load sample images for image manipulation.\n\nLoads both, ``china`` and ``flower``.\n\nRead more in the :ref:`User Guide <sample_images>`.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    images : list of ndarray of shape (427, 640, 3)\n        The two sample image.\n    filenames : list\n        The filenames for the images.\n    DESCR : str\n        The full description of the dataset.\n\nExamples\n--------\nTo load the data and visualize the images:\n\n>>> from sklearn.datasets import load_sample_images\n>>> dataset = load_sample_images()     #doctest: +SKIP\n>>> len(dataset.images)                #doctest: +SKIP\n2\n>>> first_img_data = dataset.images[0] #doctest: +SKIP\n>>> first_img_data.shape               #doctest: +SKIP\n(427, 640, 3)\n>>> first_img_data.dtype               #doctest: +SKIP\ndtype('uint8')",
      "code": "def load_sample_images():\n    \"\"\"Load sample images for image manipulation.\n\n    Loads both, ``china`` and ``flower``.\n\n    Read more in the :ref:`User Guide <sample_images>`.\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        images : list of ndarray of shape (427, 640, 3)\n            The two sample image.\n        filenames : list\n            The filenames for the images.\n        DESCR : str\n            The full description of the dataset.\n\n    Examples\n    --------\n    To load the data and visualize the images:\n\n    >>> from sklearn.datasets import load_sample_images\n    >>> dataset = load_sample_images()     #doctest: +SKIP\n    >>> len(dataset.images)                #doctest: +SKIP\n    2\n    >>> first_img_data = dataset.images[0] #doctest: +SKIP\n    >>> first_img_data.shape               #doctest: +SKIP\n    (427, 640, 3)\n    >>> first_img_data.dtype               #doctest: +SKIP\n    dtype('uint8')\n    \"\"\"\n    # import PIL only when needed\n    from ..externals._pilutil import imread\n\n    module_path = join(dirname(__file__), \"images\")\n    with open(join(module_path, 'README.txt')) as f:\n        descr = f.read()\n    filenames = [join(module_path, filename)\n                 for filename in sorted(os.listdir(module_path))\n                 if filename.endswith(\".jpg\")]\n    # Load image data for each image in the source folder.\n    images = [imread(filename) for filename in filenames]\n\n    return Bunch(images=images,\n                 filenames=filenames,\n                 DESCR=descr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._base/load_wine",
      "name": "load_wine",
      "qname": "sklearn.datasets._base.load_wine",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._base/load_wine/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._base.load_wine.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object.\nSee below for more information about the `data` and `target` object."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._base/load_wine/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._base.load_wine.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric). The target is\na pandas DataFrame or Series depending on the number of target columns.\nIf `return_X_y` is True, then (`data`, `target`) will be pandas\nDataFrames or Series as described below.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and return the wine dataset (classification).\n\n.. versionadded:: 0.18\n\nThe wine dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class        [59,71,48]\nSamples total                  178\nDimensionality                  13\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <wine_dataset>`.",
      "docstring": "Load and return the wine dataset (classification).\n\n.. versionadded:: 0.18\n\nThe wine dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class        [59,71,48]\nSamples total                  178\nDimensionality                  13\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <wine_dataset>`.\n\nParameters\n----------\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object.\n    See below for more information about the `data` and `target` object.\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is\n    a pandas DataFrame or Series depending on the number of target columns.\n    If `return_X_y` is True, then (`data`, `target`) will be pandas\n    DataFrames or Series as described below.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (178, 13)\n        The data matrix. If `as_frame=True`, `data` will be a pandas\n        DataFrame.\n    target: {ndarray, Series} of shape (178,)\n        The classification target. If `as_frame=True`, `target` will be\n        a pandas Series.\n    feature_names: list\n        The names of the dataset columns.\n    target_names: list\n        The names of target classes.\n    frame: DataFrame of shape (178, 14)\n        Only present when `as_frame=True`. DataFrame with `data` and\n        `target`.\n\n        .. versionadded:: 0.23\n    DESCR: str\n        The full description of the dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\nThe copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\nstandard format from:\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n\nExamples\n--------\nLet's say you are interested in the samples 10, 80, and 140, and want to\nknow their class name.\n\n>>> from sklearn.datasets import load_wine\n>>> data = load_wine()\n>>> data.target[[10, 80, 140]]\narray([0, 1, 2])\n>>> list(data.target_names)\n['class_0', 'class_1', 'class_2']",
      "code": "@_deprecate_positional_args\ndef load_wine(*, return_X_y=False, as_frame=False):\n    \"\"\"Load and return the wine dataset (classification).\n\n    .. versionadded:: 0.18\n\n    The wine dataset is a classic and very easy multi-class classification\n    dataset.\n\n    =================   ==============\n    Classes                          3\n    Samples per class        [59,71,48]\n    Samples total                  178\n    Dimensionality                  13\n    Features            real, positive\n    =================   ==============\n\n    Read more in the :ref:`User Guide <wine_dataset>`.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric). The target is\n        a pandas DataFrame or Series depending on the number of target columns.\n        If `return_X_y` is True, then (`data`, `target`) will be pandas\n        DataFrames or Series as described below.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (178, 13)\n            The data matrix. If `as_frame=True`, `data` will be a pandas\n            DataFrame.\n        target: {ndarray, Series} of shape (178,)\n            The classification target. If `as_frame=True`, `target` will be\n            a pandas Series.\n        feature_names: list\n            The names of the dataset columns.\n        target_names: list\n            The names of target classes.\n        frame: DataFrame of shape (178, 14)\n            Only present when `as_frame=True`. DataFrame with `data` and\n            `target`.\n\n            .. versionadded:: 0.23\n        DESCR: str\n            The full description of the dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n    The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\n    standard format from:\n    https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n\n    Examples\n    --------\n    Let's say you are interested in the samples 10, 80, and 140, and want to\n    know their class name.\n\n    >>> from sklearn.datasets import load_wine\n    >>> data = load_wine()\n    >>> data.target[[10, 80, 140]]\n    array([0, 1, 2])\n    >>> list(data.target_names)\n    ['class_0', 'class_1', 'class_2']\n    \"\"\"\n    module_path = dirname(__file__)\n    data, target, target_names = load_data(module_path, 'wine_data.csv')\n\n    with open(join(module_path, 'descr', 'wine_data.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    feature_names = ['alcohol',\n                     'malic_acid',\n                     'ash',\n                     'alcalinity_of_ash',\n                     'magnesium',\n                     'total_phenols',\n                     'flavanoids',\n                     'nonflavanoid_phenols',\n                     'proanthocyanins',\n                     'color_intensity',\n                     'hue',\n                     'od280/od315_of_diluted_wines',\n                     'proline']\n\n    frame = None\n    target_columns = ['target', ]\n    if as_frame:\n        frame, data, target = _convert_data_dataframe(\"load_wine\",\n                                                      data,\n                                                      target,\n                                                      feature_names,\n                                                      target_columns)\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data,\n                 target=target,\n                 frame=frame,\n                 target_names=target_names,\n                 DESCR=fdescr,\n                 feature_names=feature_names)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._california_housing/fetch_california_housing",
      "name": "fetch_california_housing",
      "qname": "sklearn.datasets._california_housing.fetch_california_housing",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._california_housing/fetch_california_housing/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._california_housing.fetch_california_housing.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._california_housing/fetch_california_housing/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._california_housing.fetch_california_housing.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._california_housing/fetch_california_housing/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._california_housing.fetch_california_housing.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False.",
            "description": "If True, returns ``(data.data, data.target)`` instead of a Bunch\nobject.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._california_housing/fetch_california_housing/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._california_housing.fetch_california_housing.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric, string or categorical). The target is\na pandas DataFrame or Series depending on the number of target_columns.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the California housing dataset (regression).\n\n==============   ==============\nSamples total             20640\nDimensionality                8\nFeatures                   real\nTarget           real 0.15 - 5.\n==============   ==============\n\nRead more in the :ref:`User Guide <california_housing_dataset>`.",
      "docstring": "Load the California housing dataset (regression).\n\n==============   ==============\nSamples total             20640\nDimensionality                8\nFeatures                   real\nTarget           real 0.15 - 5.\n==============   ==============\n\nRead more in the :ref:`User Guide <california_housing_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\n\nreturn_X_y : bool, default=False.\n    If True, returns ``(data.data, data.target)`` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.20\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string or categorical). The target is\n    a pandas DataFrame or Series depending on the number of target_columns.\n\n    .. versionadded:: 0.23\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray, shape (20640, 8)\n        Each row corresponding to the 8 feature values in order.\n        If ``as_frame`` is True, ``data`` is a pandas object.\n    target : numpy array of shape (20640,)\n        Each value corresponds to the average\n        house value in units of 100,000.\n        If ``as_frame`` is True, ``target`` is a pandas object.\n    feature_names : list of length 8\n        Array of ordered feature names used in the dataset.\n    DESCR : string\n        Description of the California housing dataset.\n    frame : pandas DataFrame\n        Only present when `as_frame=True`. DataFrame with ``data`` and\n        ``target``.\n\n        .. versionadded:: 0.23\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20\n\nNotes\n-----\n\nThis dataset consists of 20,640 samples and 9 features.",
      "code": "@_deprecate_positional_args\ndef fetch_california_housing(*, data_home=None, download_if_missing=True,\n                             return_X_y=False, as_frame=False):\n    \"\"\"Load the California housing dataset (regression).\n\n    ==============   ==============\n    Samples total             20640\n    Dimensionality                8\n    Features                   real\n    Target           real 0.15 - 5.\n    ==============   ==============\n\n    Read more in the :ref:`User Guide <california_housing_dataset>`.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n\n    return_X_y : bool, default=False.\n        If True, returns ``(data.data, data.target)`` instead of a Bunch\n        object.\n\n        .. versionadded:: 0.20\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric, string or categorical). The target is\n        a pandas DataFrame or Series depending on the number of target_columns.\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    dataset : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : ndarray, shape (20640, 8)\n            Each row corresponding to the 8 feature values in order.\n            If ``as_frame`` is True, ``data`` is a pandas object.\n        target : numpy array of shape (20640,)\n            Each value corresponds to the average\n            house value in units of 100,000.\n            If ``as_frame`` is True, ``target`` is a pandas object.\n        feature_names : list of length 8\n            Array of ordered feature names used in the dataset.\n        DESCR : string\n            Description of the California housing dataset.\n        frame : pandas DataFrame\n            Only present when `as_frame=True`. DataFrame with ``data`` and\n            ``target``.\n\n            .. versionadded:: 0.23\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    Notes\n    -----\n\n    This dataset consists of 20,640 samples and 9 features.\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    if not exists(data_home):\n        makedirs(data_home)\n\n    filepath = _pkl_filepath(data_home, 'cal_housing.pkz')\n    if not exists(filepath):\n        if not download_if_missing:\n            raise IOError(\"Data not found and `download_if_missing` is False\")\n\n        logger.info('Downloading Cal. housing from {} to {}'.format(\n            ARCHIVE.url, data_home))\n\n        archive_path = _fetch_remote(ARCHIVE, dirname=data_home)\n\n        with tarfile.open(mode=\"r:gz\", name=archive_path) as f:\n            cal_housing = np.loadtxt(\n                f.extractfile('CaliforniaHousing/cal_housing.data'),\n                delimiter=',')\n            # Columns are not in the same order compared to the previous\n            # URL resource on lib.stat.cmu.edu\n            columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0]\n            cal_housing = cal_housing[:, columns_index]\n\n            joblib.dump(cal_housing, filepath, compress=6)\n        remove(archive_path)\n\n    else:\n        cal_housing = joblib.load(filepath)\n\n    feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                     \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n\n    target, data = cal_housing[:, 0], cal_housing[:, 1:]\n\n    # avg rooms = total rooms / households\n    data[:, 2] /= data[:, 5]\n\n    # avg bed rooms = total bed rooms / households\n    data[:, 3] /= data[:, 5]\n\n    # avg occupancy = population / households\n    data[:, 5] = data[:, 4] / data[:, 5]\n\n    # target in units of 100,000\n    target = target / 100000.0\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'california_housing.rst')) as dfile:\n        descr = dfile.read()\n\n    X = data\n    y = target\n\n    frame = None\n    target_names = [\"MedHouseVal\", ]\n    if as_frame:\n        frame, X, y = _convert_data_dataframe(\"fetch_california_housing\",\n                                              data,\n                                              target,\n                                              feature_names,\n                                              target_names)\n\n    if return_X_y:\n        return X, y\n\n    return Bunch(data=X,\n                 target=y,\n                 frame=frame,\n                 target_names=target_names,\n                 feature_names=feature_names,\n                 DESCR=descr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype",
      "name": "fetch_covtype",
      "qname": "sklearn.datasets._covtype.fetch_covtype",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._covtype.fetch_covtype.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._covtype.fetch_covtype.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._covtype.fetch_covtype.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset shuffling. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._covtype.fetch_covtype.shuffle",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to shuffle dataset."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._covtype.fetch_covtype.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data.data, data.target)`` instead of a Bunch\nobject.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._covtype/fetch_covtype/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._covtype.fetch_covtype.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric). The target is a pandas DataFrame or\nSeries depending on the number of target columns. If `return_X_y` is\nTrue, then (`data`, `target`) will be pandas DataFrames or Series as\ndescribed below.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the covertype dataset (classification).\n\nDownload it if necessary.\n\n=================   ============\nClasses                        7\nSamples total             581012\nDimensionality                54\nFeatures                     int\n=================   ============\n\nRead more in the :ref:`User Guide <covtype_dataset>`.",
      "docstring": "Load the covertype dataset (classification).\n\nDownload it if necessary.\n\n=================   ============\nClasses                        7\nSamples total             581012\nDimensionality                54\nFeatures                     int\n=================   ============\n\nRead more in the :ref:`User Guide <covtype_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=False\n    Whether to shuffle dataset.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data.data, data.target)`` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.20\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric). The target is a pandas DataFrame or\n    Series depending on the number of target columns. If `return_X_y` is\n    True, then (`data`, `target`) will be pandas DataFrames or Series as\n    described below.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray of shape (581012, 54)\n        Each row corresponds to the 54 features in the dataset.\n    target : ndarray of shape (581012,)\n        Each value corresponds to one of\n        the 7 forest covertypes with values\n        ranging between 1 to 7.\n    frame : dataframe of shape (581012, 55)\n        Only present when `as_frame=True`. Contains `data` and `target`.\n    DESCR : str\n        Description of the forest covertype dataset.\n    feature_names : list\n        The names of the dataset columns.\n    target_names: list\n        The names of the target columns.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20",
      "code": "@_deprecate_positional_args\ndef fetch_covtype(*, data_home=None, download_if_missing=True,\n                  random_state=None, shuffle=False, return_X_y=False,\n                  as_frame=False):\n    \"\"\"Load the covertype dataset (classification).\n\n    Download it if necessary.\n\n    =================   ============\n    Classes                        7\n    Samples total             581012\n    Dimensionality                54\n    Features                     int\n    =================   ============\n\n    Read more in the :ref:`User Guide <covtype_dataset>`.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset shuffling. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    return_X_y : bool, default=False\n        If True, returns ``(data.data, data.target)`` instead of a Bunch\n        object.\n\n        .. versionadded:: 0.20\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric). The target is a pandas DataFrame or\n        Series depending on the number of target columns. If `return_X_y` is\n        True, then (`data`, `target`) will be pandas DataFrames or Series as\n        described below.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    dataset : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : ndarray of shape (581012, 54)\n            Each row corresponds to the 54 features in the dataset.\n        target : ndarray of shape (581012,)\n            Each value corresponds to one of\n            the 7 forest covertypes with values\n            ranging between 1 to 7.\n        frame : dataframe of shape (581012, 55)\n            Only present when `as_frame=True`. Contains `data` and `target`.\n        DESCR : str\n            Description of the forest covertype dataset.\n        feature_names : list\n            The names of the dataset columns.\n        target_names: list\n            The names of the target columns.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    \"\"\"\n\n    data_home = get_data_home(data_home=data_home)\n    covtype_dir = join(data_home, \"covertype\")\n    samples_path = _pkl_filepath(covtype_dir, \"samples\")\n    targets_path = _pkl_filepath(covtype_dir, \"targets\")\n    available = exists(samples_path)\n\n    if download_if_missing and not available:\n        if not exists(covtype_dir):\n            makedirs(covtype_dir)\n        logger.info(\"Downloading %s\" % ARCHIVE.url)\n\n        archive_path = _fetch_remote(ARCHIVE, dirname=covtype_dir)\n        Xy = np.genfromtxt(GzipFile(filename=archive_path), delimiter=',')\n        # delete archive\n        remove(archive_path)\n\n        X = Xy[:, :-1]\n        y = Xy[:, -1].astype(np.int32, copy=False)\n\n        joblib.dump(X, samples_path, compress=9)\n        joblib.dump(y, targets_path, compress=9)\n\n    elif not available and not download_if_missing:\n        raise IOError(\"Data not found and `download_if_missing` is False\")\n    try:\n        X, y\n    except NameError:\n        X = joblib.load(samples_path)\n        y = joblib.load(targets_path)\n\n    if shuffle:\n        ind = np.arange(X.shape[0])\n        rng = check_random_state(random_state)\n        rng.shuffle(ind)\n        X = X[ind]\n        y = y[ind]\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'covtype.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    frame = None\n    if as_frame:\n        frame, X, y = _convert_data_dataframe(caller_name=\"fetch_covtype\",\n                                              data=X,\n                                              target=y,\n                                              feature_names=FEATURE_NAMES,\n                                              target_names=TARGET_NAMES)\n    if return_X_y:\n        return X, y\n\n    return Bunch(data=X,\n                 target=y,\n                 frame=frame,\n                 target_names=TARGET_NAMES,\n                 feature_names=FEATURE_NAMES,\n                 DESCR=fdescr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99",
      "name": "fetch_kddcup99",
      "qname": "sklearn.datasets._kddcup99.fetch_kddcup99",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/subset",
          "name": "subset",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.subset",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'SA', 'SF', 'http', 'smtp'}",
            "default_value": "None",
            "description": "To return the corresponding classical subsets of kddcup 99.\nIf None, return the entire kddcup 99 dataset."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "http",
              "SA",
              "smtp",
              "SF"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.shuffle",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to shuffle dataset."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset shuffling and for\nselection of abnormal samples if `subset='SA'`. Pass an int for\nreproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/percent10",
          "name": "percent10",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.percent10",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to load only 10 percent of the data."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object. See\nbelow for more information about the `data` and `target` object.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._kddcup99/fetch_kddcup99/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._kddcup99.fetch_kddcup99.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If `True`, returns a pandas Dataframe for the ``data`` and ``target``\nobjects in the `Bunch` returned object; `Bunch` return object will also\nhave a ``frame`` member.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the kddcup99 dataset (classification).\n\nDownload it if necessary.\n\n=================   ====================================\nClasses                                               23\nSamples total                                    4898431\nDimensionality                                        41\nFeatures            discrete (int) or continuous (float)\n=================   ====================================\n\nRead more in the :ref:`User Guide <kddcup99_dataset>`.\n\n.. versionadded:: 0.18",
      "docstring": "Load the kddcup99 dataset (classification).\n\nDownload it if necessary.\n\n=================   ====================================\nClasses                                               23\nSamples total                                    4898431\nDimensionality                                        41\nFeatures            discrete (int) or continuous (float)\n=================   ====================================\n\nRead more in the :ref:`User Guide <kddcup99_dataset>`.\n\n.. versionadded:: 0.18\n\nParameters\n----------\nsubset : {'SA', 'SF', 'http', 'smtp'}, default=None\n    To return the corresponding classical subsets of kddcup 99.\n    If None, return the entire kddcup 99 dataset.\n\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n    .. versionadded:: 0.19\n\nshuffle : bool, default=False\n    Whether to shuffle dataset.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling and for\n    selection of abnormal samples if `subset='SA'`. Pass an int for\n    reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\npercent10 : bool, default=True\n    Whether to load only 10 percent of the data.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.20\n\nas_frame : bool, default=False\n    If `True`, returns a pandas Dataframe for the ``data`` and ``target``\n    objects in the `Bunch` returned object; `Bunch` return object will also\n    have a ``frame`` member.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : {ndarray, dataframe} of shape (494021, 41)\n        The data matrix to learn. If `as_frame=True`, `data` will be a\n        pandas DataFrame.\n    target : {ndarray, series} of shape (494021,)\n        The regression target for each sample. If `as_frame=True`, `target`\n        will be a pandas Series.\n    frame : dataframe of shape (494021, 42)\n        Only present when `as_frame=True`. Contains `data` and `target`.\n    DESCR : str\n        The full description of the dataset.\n    feature_names : list\n        The names of the dataset columns\n    target_names: list\n        The names of the target columns\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20",
      "code": "@_deprecate_positional_args\ndef fetch_kddcup99(*, subset=None, data_home=None, shuffle=False,\n                   random_state=None,\n                   percent10=True, download_if_missing=True, return_X_y=False,\n                   as_frame=False):\n    \"\"\"Load the kddcup99 dataset (classification).\n\n    Download it if necessary.\n\n    =================   ====================================\n    Classes                                               23\n    Samples total                                    4898431\n    Dimensionality                                        41\n    Features            discrete (int) or continuous (float)\n    =================   ====================================\n\n    Read more in the :ref:`User Guide <kddcup99_dataset>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    subset : {'SA', 'SF', 'http', 'smtp'}, default=None\n        To return the corresponding classical subsets of kddcup 99.\n        If None, return the entire kddcup 99 dataset.\n\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset shuffling and for\n        selection of abnormal samples if `subset='SA'`. Pass an int for\n        reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    percent10 : bool, default=True\n        Whether to load only 10 percent of the data.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object. See\n        below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.20\n\n    as_frame : bool, default=False\n        If `True`, returns a pandas Dataframe for the ``data`` and ``target``\n        objects in the `Bunch` returned object; `Bunch` return object will also\n        have a ``frame`` member.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : {ndarray, dataframe} of shape (494021, 41)\n            The data matrix to learn. If `as_frame=True`, `data` will be a\n            pandas DataFrame.\n        target : {ndarray, series} of shape (494021,)\n            The regression target for each sample. If `as_frame=True`, `target`\n            will be a pandas Series.\n        frame : dataframe of shape (494021, 42)\n            Only present when `as_frame=True`. Contains `data` and `target`.\n        DESCR : str\n            The full description of the dataset.\n        feature_names : list\n            The names of the dataset columns\n        target_names: list\n            The names of the target columns\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    kddcup99 = _fetch_brute_kddcup99(\n        data_home=data_home,\n        percent10=percent10,\n        download_if_missing=download_if_missing\n    )\n\n    data = kddcup99.data\n    target = kddcup99.target\n    feature_names = kddcup99.feature_names\n    target_names = kddcup99.target_names\n\n    if subset == 'SA':\n        s = target == b'normal.'\n        t = np.logical_not(s)\n        normal_samples = data[s, :]\n        normal_targets = target[s]\n        abnormal_samples = data[t, :]\n        abnormal_targets = target[t]\n\n        n_samples_abnormal = abnormal_samples.shape[0]\n        # selected abnormal samples:\n        random_state = check_random_state(random_state)\n        r = random_state.randint(0, n_samples_abnormal, 3377)\n        abnormal_samples = abnormal_samples[r]\n        abnormal_targets = abnormal_targets[r]\n\n        data = np.r_[normal_samples, abnormal_samples]\n        target = np.r_[normal_targets, abnormal_targets]\n\n    if subset == 'SF' or subset == 'http' or subset == 'smtp':\n        # select all samples with positive logged_in attribute:\n        s = data[:, 11] == 1\n        data = np.c_[data[s, :11], data[s, 12:]]\n        feature_names = feature_names[:11] + feature_names[12:]\n        target = target[s]\n\n        data[:, 0] = np.log((data[:, 0] + 0.1).astype(float, copy=False))\n        data[:, 4] = np.log((data[:, 4] + 0.1).astype(float, copy=False))\n        data[:, 5] = np.log((data[:, 5] + 0.1).astype(float, copy=False))\n\n        if subset == 'http':\n            s = data[:, 2] == b'http'\n            data = data[s]\n            target = target[s]\n            data = np.c_[data[:, 0], data[:, 4], data[:, 5]]\n            feature_names = [feature_names[0], feature_names[4],\n                             feature_names[5]]\n\n        if subset == 'smtp':\n            s = data[:, 2] == b'smtp'\n            data = data[s]\n            target = target[s]\n            data = np.c_[data[:, 0], data[:, 4], data[:, 5]]\n            feature_names = [feature_names[0], feature_names[4],\n                             feature_names[5]]\n\n        if subset == 'SF':\n            data = np.c_[data[:, 0], data[:, 2], data[:, 4], data[:, 5]]\n            feature_names = [feature_names[0], feature_names[2],\n                             feature_names[4], feature_names[5]]\n\n    if shuffle:\n        data, target = shuffle_method(data, target, random_state=random_state)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'kddcup99.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    frame = None\n    if as_frame:\n        frame, data, target = _convert_data_dataframe(\n            \"fetch_kddcup99\", data, target, feature_names, target_names\n        )\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(\n        data=data,\n        target=target,\n        frame=frame,\n        target_names=target_names,\n        feature_names=feature_names,\n        DESCR=fdescr,\n    )"
    },
    {
      "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs",
      "name": "fetch_lfw_pairs",
      "qname": "sklearn.datasets._lfw.fetch_lfw_pairs",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/subset",
          "name": "subset",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.subset",
          "default_value": "'train'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'train', 'test', '10_folds'}",
            "default_value": "'train'",
            "description": "Select the dataset to load: 'train' for the development training\nset, 'test' for the development test set, and '10_folds' for the\nofficial evaluation set that is meant to be used with a 10-folds\ncross validation."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "test",
              "train",
              "10_folds"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By\ndefault all scikit-learn data is stored in '~/scikit_learn_data'\nsubfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/funneled",
          "name": "funneled",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.funneled",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Download and use the funneled variant of the dataset."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/resize",
          "name": "resize",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.resize",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "Ratio used to resize the each face picture."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/color",
          "name": "color",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.color",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Keep the 3 RGB channels instead of averaging them to a single\ngray level channel. If color is True the shape of the data has\none more dimension than the shape with color = False."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/slice_",
          "name": "slice_",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.slice_",
          "default_value": "(slice(70, 195), slice(78, 172))",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple of slice",
            "default_value": "(slice(70, 195), slice(78, 172))",
            "description": "Provide a custom 2D slice (height, width) to extract the\n'interesting' part of the jpeg files and avoid use statistical\ncorrelation from the background"
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of slice"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_pairs/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._lfw.fetch_lfw_pairs.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\nDownload it if necessary.\n\n=================   =======================\nClasses                                   2\nSamples total                         13233\nDimensionality                         5828\nFeatures            real, between 0 and 255\n=================   =======================\n\nIn the official `README.txt`_ this task is described as the\n\"Restricted\" task.  As I am not sure as to implement the\n\"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n  .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\nThe original images are 250 x 250 pixels, but the default slice and resize\narguments reduce them to 62 x 47.\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.",
      "docstring": "Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\nDownload it if necessary.\n\n=================   =======================\nClasses                                   2\nSamples total                         13233\nDimensionality                         5828\nFeatures            real, between 0 and 255\n=================   =======================\n\nIn the official `README.txt`_ this task is described as the\n\"Restricted\" task.  As I am not sure as to implement the\n\"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n  .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\nThe original images are 250 x 250 pixels, but the default slice and resize\narguments reduce them to 62 x 47.\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\nParameters\n----------\nsubset : {'train', 'test', '10_folds'}, default='train'\n    Select the dataset to load: 'train' for the development training\n    set, 'test' for the development test set, and '10_folds' for the\n    official evaluation set that is meant to be used with a 10-folds\n    cross validation.\n\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By\n    default all scikit-learn data is stored in '~/scikit_learn_data'\n    subfolders.\n\nfunneled : bool, default=True\n    Download and use the funneled variant of the dataset.\n\nresize : float, default=0.5\n    Ratio used to resize the each face picture.\n\ncolor : bool, default=False\n    Keep the 3 RGB channels instead of averaging them to a single\n    gray level channel. If color is True the shape of the data has\n    one more dimension than the shape with color = False.\n\nslice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n    Provide a custom 2D slice (height, width) to extract the\n    'interesting' part of the jpeg files and avoid use statistical\n    correlation from the background\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : ndarray of shape (2200, 5828). Shape depends on ``subset``.\n        Each row corresponds to 2 ravel'd face images\n        of original size 62 x 47 pixels.\n        Changing the ``slice_``, ``resize`` or ``subset`` parameters\n        will change the shape of the output.\n    pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``\n        Each row has 2 face images corresponding\n        to same or different person from the dataset\n        containing 5749 people. Changing the ``slice_``,\n        ``resize`` or ``subset`` parameters will change the shape of the\n        output.\n    target : numpy array of shape (2200,). Shape depends on ``subset``.\n        Labels associated to each pair of images.\n        The two label values being different persons or the same person.\n    DESCR : string\n        Description of the Labeled Faces in the Wild (LFW) dataset.",
      "code": "@_deprecate_positional_args\ndef fetch_lfw_pairs(*, subset='train', data_home=None, funneled=True,\n                    resize=0.5,\n                    color=False, slice_=(slice(70, 195), slice(78, 172)),\n                    download_if_missing=True):\n    \"\"\"Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\n    Download it if necessary.\n\n    =================   =======================\n    Classes                                   2\n    Samples total                         13233\n    Dimensionality                         5828\n    Features            real, between 0 and 255\n    =================   =======================\n\n    In the official `README.txt`_ this task is described as the\n    \"Restricted\" task.  As I am not sure as to implement the\n    \"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n      .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\n    The original images are 250 x 250 pixels, but the default slice and resize\n    arguments reduce them to 62 x 47.\n\n    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\n    Parameters\n    ----------\n    subset : {'train', 'test', '10_folds'}, default='train'\n        Select the dataset to load: 'train' for the development training\n        set, 'test' for the development test set, and '10_folds' for the\n        official evaluation set that is meant to be used with a 10-folds\n        cross validation.\n\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By\n        default all scikit-learn data is stored in '~/scikit_learn_data'\n        subfolders.\n\n    funneled : bool, default=True\n        Download and use the funneled variant of the dataset.\n\n    resize : float, default=0.5\n        Ratio used to resize the each face picture.\n\n    color : bool, default=False\n        Keep the 3 RGB channels instead of averaging them to a single\n        gray level channel. If color is True the shape of the data has\n        one more dimension than the shape with color = False.\n\n    slice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n        Provide a custom 2D slice (height, width) to extract the\n        'interesting' part of the jpeg files and avoid use statistical\n        correlation from the background\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : ndarray of shape (2200, 5828). Shape depends on ``subset``.\n            Each row corresponds to 2 ravel'd face images\n            of original size 62 x 47 pixels.\n            Changing the ``slice_``, ``resize`` or ``subset`` parameters\n            will change the shape of the output.\n        pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``\n            Each row has 2 face images corresponding\n            to same or different person from the dataset\n            containing 5749 people. Changing the ``slice_``,\n            ``resize`` or ``subset`` parameters will change the shape of the\n            output.\n        target : numpy array of shape (2200,). Shape depends on ``subset``.\n            Labels associated to each pair of images.\n            The two label values being different persons or the same person.\n        DESCR : string\n            Description of the Labeled Faces in the Wild (LFW) dataset.\n\n    \"\"\"\n    lfw_home, data_folder_path = _check_fetch_lfw(\n        data_home=data_home, funneled=funneled,\n        download_if_missing=download_if_missing)\n    logger.debug('Loading %s LFW pairs from %s', subset, lfw_home)\n\n    # wrap the loader in a memoizing function that will return memmaped data\n    # arrays for optimal memory usage\n    if parse_version(joblib.__version__) < parse_version('0.12'):\n        # Deal with change of API in joblib\n        m = Memory(cachedir=lfw_home, compress=6, verbose=0)\n    else:\n        m = Memory(location=lfw_home, compress=6, verbose=0)\n    load_func = m.cache(_fetch_lfw_pairs)\n\n    # select the right metadata file according to the requested subset\n    label_filenames = {\n        'train': 'pairsDevTrain.txt',\n        'test': 'pairsDevTest.txt',\n        '10_folds': 'pairs.txt',\n    }\n    if subset not in label_filenames:\n        raise ValueError(\"subset='%s' is invalid: should be one of %r\" % (\n            subset, list(sorted(label_filenames.keys()))))\n    index_file_path = join(lfw_home, label_filenames[subset])\n\n    # load and memoize the pairs as np arrays\n    pairs, target, target_names = load_func(\n        index_file_path, data_folder_path, resize=resize, color=color,\n        slice_=slice_)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'lfw.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    # pack the results as a Bunch instance\n    return Bunch(data=pairs.reshape(len(pairs), -1), pairs=pairs,\n                 target=target, target_names=target_names,\n                 DESCR=fdescr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people",
      "name": "fetch_lfw_people",
      "qname": "sklearn.datasets._lfw.fetch_lfw_people",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/funneled",
          "name": "funneled",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.funneled",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Download and use the funneled variant of the dataset."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/resize",
          "name": "resize",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.resize",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "Ratio used to resize the each face picture."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/min_faces_per_person",
          "name": "min_faces_per_person",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.min_faces_per_person",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The extracted dataset will only retain pictures of people that have at\nleast `min_faces_per_person` different pictures."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/color",
          "name": "color",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.color",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Keep the 3 RGB channels instead of averaging them to a single\ngray level channel. If color is True the shape of the data has\none more dimension than the shape with color = False."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/slice_",
          "name": "slice_",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.slice_",
          "default_value": "(slice(70, 195), slice(78, 172))",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple of slice",
            "default_value": "(slice(70, 195), slice(78, 172))",
            "description": "Provide a custom 2D slice (height, width) to extract the\n'interesting' part of the jpeg files and avoid use statistical\ncorrelation from the background"
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of slice"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._lfw/fetch_lfw_people/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._lfw.fetch_lfw_people.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\nobject. See below for more information about the `dataset.data` and\n`dataset.target` object.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the Labeled Faces in the Wild (LFW) people dataset (classification).\n\nDownload it if necessary.\n\n=================   =======================\nClasses                                5749\nSamples total                         13233\nDimensionality                         5828\nFeatures            real, between 0 and 255\n=================   =======================\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.",
      "docstring": "Load the Labeled Faces in the Wild (LFW) people dataset (classification).\n\nDownload it if necessary.\n\n=================   =======================\nClasses                                5749\nSamples total                         13233\nDimensionality                         5828\nFeatures            real, between 0 and 255\n=================   =======================\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nfunneled : bool, default=True\n    Download and use the funneled variant of the dataset.\n\nresize : float, default=0.5\n    Ratio used to resize the each face picture.\n\nmin_faces_per_person : int, default=None\n    The extracted dataset will only retain pictures of people that have at\n    least `min_faces_per_person` different pictures.\n\ncolor : bool, default=False\n    Keep the 3 RGB channels instead of averaging them to a single\n    gray level channel. If color is True the shape of the data has\n    one more dimension than the shape with color = False.\n\nslice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n    Provide a custom 2D slice (height, width) to extract the\n    'interesting' part of the jpeg files and avoid use statistical\n    correlation from the background\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n    object. See below for more information about the `dataset.data` and\n    `dataset.target` object.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : numpy array of shape (13233, 2914)\n        Each row corresponds to a ravelled face image\n        of original size 62 x 47 pixels.\n        Changing the ``slice_`` or resize parameters will change the\n        shape of the output.\n    images : numpy array of shape (13233, 62, 47)\n        Each row is a face image corresponding to one of the 5749 people in\n        the dataset. Changing the ``slice_``\n        or resize parameters will change the shape of the output.\n    target : numpy array of shape (13233,)\n        Labels associated to each face image.\n        Those labels range from 0-5748 and correspond to the person IDs.\n    DESCR : string\n        Description of the Labeled Faces in the Wild (LFW) dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20",
      "code": "@_deprecate_positional_args\ndef fetch_lfw_people(*, data_home=None, funneled=True, resize=0.5,\n                     min_faces_per_person=0, color=False,\n                     slice_=(slice(70, 195), slice(78, 172)),\n                     download_if_missing=True, return_X_y=False):\n    \"\"\"Load the Labeled Faces in the Wild (LFW) people dataset \\\n(classification).\n\n    Download it if necessary.\n\n    =================   =======================\n    Classes                                5749\n    Samples total                         13233\n    Dimensionality                         5828\n    Features            real, between 0 and 255\n    =================   =======================\n\n    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    funneled : bool, default=True\n        Download and use the funneled variant of the dataset.\n\n    resize : float, default=0.5\n        Ratio used to resize the each face picture.\n\n    min_faces_per_person : int, default=None\n        The extracted dataset will only retain pictures of people that have at\n        least `min_faces_per_person` different pictures.\n\n    color : bool, default=False\n        Keep the 3 RGB channels instead of averaging them to a single\n        gray level channel. If color is True the shape of the data has\n        one more dimension than the shape with color = False.\n\n    slice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n        Provide a custom 2D slice (height, width) to extract the\n        'interesting' part of the jpeg files and avoid use statistical\n        correlation from the background\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : bool, default=False\n        If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n        object. See below for more information about the `dataset.data` and\n        `dataset.target` object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    dataset : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : numpy array of shape (13233, 2914)\n            Each row corresponds to a ravelled face image\n            of original size 62 x 47 pixels.\n            Changing the ``slice_`` or resize parameters will change the\n            shape of the output.\n        images : numpy array of shape (13233, 62, 47)\n            Each row is a face image corresponding to one of the 5749 people in\n            the dataset. Changing the ``slice_``\n            or resize parameters will change the shape of the output.\n        target : numpy array of shape (13233,)\n            Labels associated to each face image.\n            Those labels range from 0-5748 and correspond to the person IDs.\n        DESCR : string\n            Description of the Labeled Faces in the Wild (LFW) dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n\n    \"\"\"\n    lfw_home, data_folder_path = _check_fetch_lfw(\n        data_home=data_home, funneled=funneled,\n        download_if_missing=download_if_missing)\n    logger.debug('Loading LFW people faces from %s', lfw_home)\n\n    # wrap the loader in a memoizing function that will return memmaped data\n    # arrays for optimal memory usage\n    if parse_version(joblib.__version__) < parse_version('0.12'):\n        # Deal with change of API in joblib\n        m = Memory(cachedir=lfw_home, compress=6, verbose=0)\n    else:\n        m = Memory(location=lfw_home, compress=6, verbose=0)\n    load_func = m.cache(_fetch_lfw_people)\n\n    # load and memoize the pairs as np arrays\n    faces, target, target_names = load_func(\n        data_folder_path, resize=resize,\n        min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)\n\n    X = faces.reshape(len(faces), -1)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'lfw.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    if return_X_y:\n        return X, target\n\n    # pack the results as a Bunch instance\n    return Bunch(data=X, images=faces,\n                 target=target, target_names=target_names,\n                 DESCR=fdescr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._olivetti_faces/fetch_olivetti_faces",
      "name": "fetch_olivetti_faces",
      "qname": "sklearn.datasets._olivetti_faces.fetch_olivetti_faces",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._olivetti_faces/fetch_olivetti_faces/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._olivetti_faces.fetch_olivetti_faces.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._olivetti_faces/fetch_olivetti_faces/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._olivetti_faces.fetch_olivetti_faces.shuffle",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True the order of the dataset is shuffled to avoid having\nimages of the same person grouped."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._olivetti_faces/fetch_olivetti_faces/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._olivetti_faces.fetch_olivetti_faces.random_state",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "0",
            "description": "Determines random number generation for dataset shuffling. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._olivetti_faces/fetch_olivetti_faces/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._olivetti_faces.fetch_olivetti_faces.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._olivetti_faces/fetch_olivetti_faces/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._olivetti_faces.fetch_olivetti_faces.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns `(data, target)` instead of a `Bunch` object. See\nbelow for more information about the `data` and `target` object.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the Olivetti faces data-set from AT&T (classification).\n\nDownload it if necessary.\n\n=================   =====================\nClasses                                40\nSamples total                         400\nDimensionality                       4096\nFeatures            real, between 0 and 1\n=================   =====================\n\nRead more in the :ref:`User Guide <olivetti_faces_dataset>`.",
      "docstring": "Load the Olivetti faces data-set from AT&T (classification).\n\nDownload it if necessary.\n\n=================   =====================\nClasses                                40\nSamples total                         400\nDimensionality                       4096\nFeatures            real, between 0 and 1\n=================   =====================\n\nRead more in the :ref:`User Guide <olivetti_faces_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nshuffle : bool, default=False\n    If True the order of the dataset is shuffled to avoid having\n    images of the same person grouped.\n\nrandom_state : int, RandomState instance or None, default=0\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns `(data, target)` instead of a `Bunch` object. See\n    below for more information about the `data` and `target` object.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data: ndarray, shape (400, 4096)\n        Each row corresponds to a ravelled\n        face image of original size 64 x 64 pixels.\n    images : ndarray, shape (400, 64, 64)\n        Each row is a face image\n        corresponding to one of the 40 subjects of the dataset.\n    target : ndarray, shape (400,)\n        Labels associated to each face image.\n        Those labels are ranging from 0-39 and correspond to the\n        Subject IDs.\n    DESCR : str\n        Description of the modified Olivetti Faces Dataset.\n\n(data, target) : tuple if `return_X_y=True`\n    .. versionadded:: 0.22",
      "code": "@_deprecate_positional_args\ndef fetch_olivetti_faces(*, data_home=None, shuffle=False, random_state=0,\n                         download_if_missing=True, return_X_y=False):\n    \"\"\"Load the Olivetti faces data-set from AT&T (classification).\n\n    Download it if necessary.\n\n    =================   =====================\n    Classes                                40\n    Samples total                         400\n    Dimensionality                       4096\n    Features            real, between 0 and 1\n    =================   =====================\n\n    Read more in the :ref:`User Guide <olivetti_faces_dataset>`.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    shuffle : bool, default=False\n        If True the order of the dataset is shuffled to avoid having\n        images of the same person grouped.\n\n    random_state : int, RandomState instance or None, default=0\n        Determines random number generation for dataset shuffling. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : bool, default=False\n        If True, returns `(data, target)` instead of a `Bunch` object. See\n        below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data: ndarray, shape (400, 4096)\n            Each row corresponds to a ravelled\n            face image of original size 64 x 64 pixels.\n        images : ndarray, shape (400, 64, 64)\n            Each row is a face image\n            corresponding to one of the 40 subjects of the dataset.\n        target : ndarray, shape (400,)\n            Labels associated to each face image.\n            Those labels are ranging from 0-39 and correspond to the\n            Subject IDs.\n        DESCR : str\n            Description of the modified Olivetti Faces Dataset.\n\n    (data, target) : tuple if `return_X_y=True`\n        .. versionadded:: 0.22\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    if not exists(data_home):\n        makedirs(data_home)\n    filepath = _pkl_filepath(data_home, 'olivetti.pkz')\n    if not exists(filepath):\n        if not download_if_missing:\n            raise IOError(\"Data not found and `download_if_missing` is False\")\n\n        print('downloading Olivetti faces from %s to %s'\n              % (FACES.url, data_home))\n        mat_path = _fetch_remote(FACES, dirname=data_home)\n        mfile = loadmat(file_name=mat_path)\n        # delete raw .mat data\n        remove(mat_path)\n\n        faces = mfile['faces'].T.copy()\n        joblib.dump(faces, filepath, compress=6)\n        del mfile\n    else:\n        faces = joblib.load(filepath)\n\n    # We want floating point data, but float32 is enough (there is only\n    # one byte of precision in the original uint8s anyway)\n    faces = np.float32(faces)\n    faces = faces - faces.min()\n    faces /= faces.max()\n    faces = faces.reshape((400, 64, 64)).transpose(0, 2, 1)\n    # 10 images per class, 400 images total, each class is contiguous.\n    target = np.array([i // 10 for i in range(400)])\n    if shuffle:\n        random_state = check_random_state(random_state)\n        order = random_state.permutation(len(faces))\n        faces = faces[order]\n        target = target[order]\n    faces_vectorized = faces.reshape(len(faces), -1)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'olivetti_faces.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    if return_X_y:\n        return faces_vectorized, target\n\n    return Bunch(data=faces_vectorized,\n                 images=faces,\n                 target=target,\n                 DESCR=fdescr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._openml/fetch_openml",
      "name": "fetch_openml",
      "qname": "sklearn.datasets._openml.fetch_openml",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/name",
          "name": "name",
          "qname": "sklearn.datasets._openml.fetch_openml.name",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "String identifier of the dataset. Note that OpenML can have multiple\ndatasets with the same name."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/version",
          "name": "version",
          "qname": "sklearn.datasets._openml.fetch_openml.version",
          "default_value": "'active'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or 'active'",
            "default_value": "'active'",
            "description": "Version of the dataset. Can only be provided if also ``name`` is given.\nIf 'active' the oldest version that's still active is used. Since\nthere may be more than one active version of a dataset, and those\nversions may fundamentally be different from one another, setting an\nexact version is highly recommended."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "'active'"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/data_id",
          "name": "data_id",
          "qname": "sklearn.datasets._openml.fetch_openml.data_id",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "OpenML ID of the dataset. The most specific way of retrieving a\ndataset. If data_id is not given, name (and potential version) are\nused to obtain a dataset."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._openml.fetch_openml.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the data sets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/target_column",
          "name": "target_column",
          "qname": "sklearn.datasets._openml.fetch_openml.target_column",
          "default_value": "'default-target'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str, list or None",
            "default_value": "'default-target'",
            "description": "Specify the column name in the data to use as target. If\n'default-target', the standard target column a stored on the server\nis used. If ``None``, all columns are returned as data and the\ntarget is ``None``. If list (of strings), all columns with these names\nare returned as multi-target (Note: not all scikit-learn classifiers\ncan handle all types of multi-output combinations)"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "list"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/cache",
          "name": "cache",
          "qname": "sklearn.datasets._openml.fetch_openml.cache",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to cache downloaded datasets using joblib."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._openml.fetch_openml.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data, target)`` instead of a Bunch object. See\nbelow for more information about the `data` and `target` objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._openml/fetch_openml/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._openml.fetch_openml.as_frame",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool or 'auto'",
            "default_value": "'auto'",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric, string or categorical). The target is\na pandas DataFrame or Series depending on the number of target_columns.\nThe Bunch will contain a ``frame`` attribute with the target and the\ndata. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\nDataFrames or Series as describe above.\n\nIf as_frame is 'auto', the data and target will be converted to\nDataFrame or Series as if as_frame is set to True, unless the dataset\nis stored in sparse format.\n\n.. versionchanged:: 0.24\n   The default value of `as_frame` changed from `False` to `'auto'`\n   in 0.24."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "bool"
              },
              {
                "kind": "NamedType",
                "name": "'auto'"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Fetch dataset from openml by name or dataset id.\n\nDatasets are uniquely identified by either an integer ID or by a\ncombination of name and version (i.e. there might be multiple\nversions of the 'iris' dataset). Please give either name or data_id\n(not both). In case a name is given, a version can also be\nprovided.\n\nRead more in the :ref:`User Guide <openml>`.\n\n.. versionadded:: 0.20\n\n.. note:: EXPERIMENTAL\n\n    The API is experimental (particularly the return value structure),\n    and might have small backward-incompatible changes without notice\n    or warning in future releases.",
      "docstring": "Fetch dataset from openml by name or dataset id.\n\nDatasets are uniquely identified by either an integer ID or by a\ncombination of name and version (i.e. there might be multiple\nversions of the 'iris' dataset). Please give either name or data_id\n(not both). In case a name is given, a version can also be\nprovided.\n\nRead more in the :ref:`User Guide <openml>`.\n\n.. versionadded:: 0.20\n\n.. note:: EXPERIMENTAL\n\n    The API is experimental (particularly the return value structure),\n    and might have small backward-incompatible changes without notice\n    or warning in future releases.\n\nParameters\n----------\nname : str, default=None\n    String identifier of the dataset. Note that OpenML can have multiple\n    datasets with the same name.\n\nversion : int or 'active', default='active'\n    Version of the dataset. Can only be provided if also ``name`` is given.\n    If 'active' the oldest version that's still active is used. Since\n    there may be more than one active version of a dataset, and those\n    versions may fundamentally be different from one another, setting an\n    exact version is highly recommended.\n\ndata_id : int, default=None\n    OpenML ID of the dataset. The most specific way of retrieving a\n    dataset. If data_id is not given, name (and potential version) are\n    used to obtain a dataset.\n\ndata_home : str, default=None\n    Specify another download and cache folder for the data sets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ntarget_column : str, list or None, default='default-target'\n    Specify the column name in the data to use as target. If\n    'default-target', the standard target column a stored on the server\n    is used. If ``None``, all columns are returned as data and the\n    target is ``None``. If list (of strings), all columns with these names\n    are returned as multi-target (Note: not all scikit-learn classifiers\n    can handle all types of multi-output combinations)\n\ncache : bool, default=True\n    Whether to cache downloaded datasets using joblib.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data, target)`` instead of a Bunch object. See\n    below for more information about the `data` and `target` objects.\n\nas_frame : bool or 'auto', default='auto'\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string or categorical). The target is\n    a pandas DataFrame or Series depending on the number of target_columns.\n    The Bunch will contain a ``frame`` attribute with the target and the\n    data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\n    DataFrames or Series as describe above.\n\n    If as_frame is 'auto', the data and target will be converted to\n    DataFrame or Series as if as_frame is set to True, unless the dataset\n    is stored in sparse format.\n\n    .. versionchanged:: 0.24\n       The default value of `as_frame` changed from `False` to `'auto'`\n       in 0.24.\n\nReturns\n-------\n\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n        The feature matrix. Categorical features are encoded as ordinals.\n    target : np.array, pandas Series or DataFrame\n        The regression target or classification labels, if applicable.\n        Dtype is float if numeric, and object if categorical. If\n        ``as_frame`` is True, ``target`` is a pandas object.\n    DESCR : str\n        The full description of the dataset\n    feature_names : list\n        The names of the dataset columns\n    target_names: list\n        The names of the target columns\n\n    .. versionadded:: 0.22\n\n    categories : dict or None\n        Maps each categorical feature name to a list of values, such\n        that the value encoded as i is ith in the list. If ``as_frame``\n        is True, this is None.\n    details : dict\n        More metadata from OpenML\n    frame : pandas DataFrame\n        Only present when `as_frame=True`. DataFrame with ``data`` and\n        ``target``.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. note:: EXPERIMENTAL\n\n        This interface is **experimental** and subsequent releases may\n        change attributes without notice (although there should only be\n        minor changes to ``data`` and ``target``).\n\n    Missing values in the 'data' are represented as NaN's. Missing values\n    in 'target' are represented as NaN's (numerical target) or None\n    (categorical target)",
      "code": "@_deprecate_positional_args\ndef fetch_openml(\n    name: Optional[str] = None,\n    *,\n    version: Union[str, int] = 'active',\n    data_id: Optional[int] = None,\n    data_home: Optional[str] = None,\n    target_column: Optional[Union[str, List]] = 'default-target',\n    cache: bool = True,\n    return_X_y: bool = False,\n    as_frame: Union[str, bool] = 'auto'\n):\n    \"\"\"Fetch dataset from openml by name or dataset id.\n\n    Datasets are uniquely identified by either an integer ID or by a\n    combination of name and version (i.e. there might be multiple\n    versions of the 'iris' dataset). Please give either name or data_id\n    (not both). In case a name is given, a version can also be\n    provided.\n\n    Read more in the :ref:`User Guide <openml>`.\n\n    .. versionadded:: 0.20\n\n    .. note:: EXPERIMENTAL\n\n        The API is experimental (particularly the return value structure),\n        and might have small backward-incompatible changes without notice\n        or warning in future releases.\n\n    Parameters\n    ----------\n    name : str, default=None\n        String identifier of the dataset. Note that OpenML can have multiple\n        datasets with the same name.\n\n    version : int or 'active', default='active'\n        Version of the dataset. Can only be provided if also ``name`` is given.\n        If 'active' the oldest version that's still active is used. Since\n        there may be more than one active version of a dataset, and those\n        versions may fundamentally be different from one another, setting an\n        exact version is highly recommended.\n\n    data_id : int, default=None\n        OpenML ID of the dataset. The most specific way of retrieving a\n        dataset. If data_id is not given, name (and potential version) are\n        used to obtain a dataset.\n\n    data_home : str, default=None\n        Specify another download and cache folder for the data sets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    target_column : str, list or None, default='default-target'\n        Specify the column name in the data to use as target. If\n        'default-target', the standard target column a stored on the server\n        is used. If ``None``, all columns are returned as data and the\n        target is ``None``. If list (of strings), all columns with these names\n        are returned as multi-target (Note: not all scikit-learn classifiers\n        can handle all types of multi-output combinations)\n\n    cache : bool, default=True\n        Whether to cache downloaded datasets using joblib.\n\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object. See\n        below for more information about the `data` and `target` objects.\n\n    as_frame : bool or 'auto', default='auto'\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric, string or categorical). The target is\n        a pandas DataFrame or Series depending on the number of target_columns.\n        The Bunch will contain a ``frame`` attribute with the target and the\n        data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\n        DataFrames or Series as describe above.\n\n        If as_frame is 'auto', the data and target will be converted to\n        DataFrame or Series as if as_frame is set to True, unless the dataset\n        is stored in sparse format.\n\n        .. versionchanged:: 0.24\n           The default value of `as_frame` changed from `False` to `'auto'`\n           in 0.24.\n\n    Returns\n    -------\n\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n            The feature matrix. Categorical features are encoded as ordinals.\n        target : np.array, pandas Series or DataFrame\n            The regression target or classification labels, if applicable.\n            Dtype is float if numeric, and object if categorical. If\n            ``as_frame`` is True, ``target`` is a pandas object.\n        DESCR : str\n            The full description of the dataset\n        feature_names : list\n            The names of the dataset columns\n        target_names: list\n            The names of the target columns\n\n        .. versionadded:: 0.22\n\n        categories : dict or None\n            Maps each categorical feature name to a list of values, such\n            that the value encoded as i is ith in the list. If ``as_frame``\n            is True, this is None.\n        details : dict\n            More metadata from OpenML\n        frame : pandas DataFrame\n            Only present when `as_frame=True`. DataFrame with ``data`` and\n            ``target``.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. note:: EXPERIMENTAL\n\n            This interface is **experimental** and subsequent releases may\n            change attributes without notice (although there should only be\n            minor changes to ``data`` and ``target``).\n\n        Missing values in the 'data' are represented as NaN's. Missing values\n        in 'target' are represented as NaN's (numerical target) or None\n        (categorical target)\n    \"\"\"\n    if cache is False:\n        # no caching will be applied\n        data_home = None\n    else:\n        data_home = get_data_home(data_home=data_home)\n        data_home = join(data_home, 'openml')\n\n    # check valid function arguments. data_id XOR (name, version) should be\n    # provided\n    if name is not None:\n        # OpenML is case-insensitive, but the caching mechanism is not\n        # convert all data names (str) to lower case\n        name = name.lower()\n        if data_id is not None:\n            raise ValueError(\n                \"Dataset data_id={} and name={} passed, but you can only \"\n                \"specify a numeric data_id or a name, not \"\n                \"both.\".format(data_id, name))\n        data_info = _get_data_info_by_name(name, version, data_home)\n        data_id = data_info['did']\n    elif data_id is not None:\n        # from the previous if statement, it is given that name is None\n        if version != \"active\":\n            raise ValueError(\n                \"Dataset data_id={} and version={} passed, but you can only \"\n                \"specify a numeric data_id or a version, not \"\n                \"both.\".format(data_id, name))\n    else:\n        raise ValueError(\n            \"Neither name nor data_id are provided. Please provide name or \"\n            \"data_id.\")\n\n    data_description = _get_data_description_by_id(data_id, data_home)\n    if data_description['status'] != \"active\":\n        warn(\"Version {} of dataset {} is inactive, meaning that issues have \"\n             \"been found in the dataset. Try using a newer version from \"\n             \"this URL: {}\".format(\n                data_description['version'],\n                data_description['name'],\n                data_description['url']))\n    if 'error' in data_description:\n        warn(\"OpenML registered a problem with the dataset. It might be \"\n             \"unusable. Error: {}\".format(data_description['error']))\n    if 'warning' in data_description:\n        warn(\"OpenML raised a warning on the dataset. It might be \"\n             \"unusable. Warning: {}\".format(data_description['warning']))\n\n    return_sparse = False\n    if data_description['format'].lower() == 'sparse_arff':\n        return_sparse = True\n\n    if as_frame == 'auto':\n        as_frame = not return_sparse\n\n    if as_frame and return_sparse:\n        raise ValueError('Cannot return dataframe with sparse data')\n\n    # download data features, meta-info about column types\n    features_list = _get_data_features(data_id, data_home)\n\n    if not as_frame:\n        for feature in features_list:\n            if 'true' in (feature['is_ignore'], feature['is_row_identifier']):\n                continue\n            if feature['data_type'] == 'string':\n                raise ValueError('STRING attributes are not supported for '\n                                 'array representation. Try as_frame=True')\n\n    if target_column == \"default-target\":\n        # determines the default target based on the data feature results\n        # (which is currently more reliable than the data description;\n        # see issue: https://github.com/openml/OpenML/issues/768)\n        target_columns = [feature['name'] for feature in features_list\n                          if feature['is_target'] == 'true']\n    elif isinstance(target_column, str):\n        # for code-simplicity, make target_column by default a list\n        target_columns = [target_column]\n    elif target_column is None:\n        target_columns = []\n    elif isinstance(target_column, list):\n        target_columns = target_column\n    else:\n        raise TypeError(\"Did not recognize type of target_column\"\n                        \"Should be str, list or None. Got: \"\n                        \"{}\".format(type(target_column)))\n    data_columns = _valid_data_column_names(features_list,\n                                            target_columns)\n\n    shape: Optional[Tuple[int, int]]\n    # determine arff encoding to return\n    if not return_sparse:\n        # The shape must include the ignored features to keep the right indexes\n        # during the arff data conversion.\n        data_qualities = _get_data_qualities(data_id, data_home)\n        shape = _get_num_samples(data_qualities), len(features_list)\n    else:\n        shape = None\n\n    # obtain the data\n    url = _DATA_FILE.format(data_description['file_id'])\n    bunch = _download_data_to_bunch(url, return_sparse, data_home,\n                                    as_frame=bool(as_frame),\n                                    features_list=features_list, shape=shape,\n                                    target_columns=target_columns,\n                                    data_columns=data_columns,\n                                    md5_checksum=data_description[\n                                        \"md5_checksum\"])\n\n    if return_X_y:\n        return bunch.data, bunch.target\n\n    description = \"{}\\n\\nDownloaded from openml.org.\".format(\n        data_description.pop('description'))\n\n    bunch.update(\n        DESCR=description, details=data_description,\n        url=\"https://www.openml.org/d/{}\".format(data_id))\n\n    return bunch"
    },
    {
      "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1",
      "name": "fetch_rcv1",
      "qname": "sklearn.datasets._rcv1.fetch_rcv1",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._rcv1.fetch_rcv1.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1/subset",
          "name": "subset",
          "qname": "sklearn.datasets._rcv1.fetch_rcv1.subset",
          "default_value": "'all'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'train', 'test', 'all'}",
            "default_value": "'all'",
            "description": "Select the dataset to load: 'train' for the training set\n(23149 samples), 'test' for the test set (781265 samples),\n'all' for both, with the training samples first if shuffle is False.\nThis follows the official LYRL2004 chronological split."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "test",
              "train",
              "all"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._rcv1.fetch_rcv1.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._rcv1.fetch_rcv1.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset shuffling. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._rcv1.fetch_rcv1.shuffle",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to shuffle dataset."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._rcv1/fetch_rcv1/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._rcv1.fetch_rcv1.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\nobject. See below for more information about the `dataset.data` and\n`dataset.target` object.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the RCV1 multilabel dataset (classification).\n\nDownload it if necessary.\n\nVersion: RCV1-v2, vectors, full sets, topics multilabels.\n\n=================   =====================\nClasses                               103\nSamples total                      804414\nDimensionality                      47236\nFeatures            real, between 0 and 1\n=================   =====================\n\nRead more in the :ref:`User Guide <rcv1_dataset>`.\n\n.. versionadded:: 0.17",
      "docstring": "Load the RCV1 multilabel dataset (classification).\n\nDownload it if necessary.\n\nVersion: RCV1-v2, vectors, full sets, topics multilabels.\n\n=================   =====================\nClasses                               103\nSamples total                      804414\nDimensionality                      47236\nFeatures            real, between 0 and 1\n=================   =====================\n\nRead more in the :ref:`User Guide <rcv1_dataset>`.\n\n.. versionadded:: 0.17\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nsubset : {'train', 'test', 'all'}, default='all'\n    Select the dataset to load: 'train' for the training set\n    (23149 samples), 'test' for the test set (781265 samples),\n    'all' for both, with the training samples first if shuffle is False.\n    This follows the official LYRL2004 chronological split.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=False\n    Whether to shuffle dataset.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n    object. See below for more information about the `dataset.data` and\n    `dataset.target` object.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ndataset : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : sparse matrix of shape (804414, 47236), dtype=np.float64\n        The array has 0.16% of non zero values. Will be of CSR format.\n    target : sparse matrix of shape (804414, 103), dtype=np.uint8\n        Each sample has a value of 1 in its categories, and 0 in others.\n        The array has 3.15% of non zero values. Will be of CSR format.\n    sample_id : ndarray of shape (804414,), dtype=np.uint32,\n        Identification number of each sample, as ordered in dataset.data.\n    target_names : ndarray of shape (103,), dtype=object\n        Names of each target (RCV1 topics), as ordered in dataset.target.\n    DESCR : str\n        Description of the RCV1 dataset.\n\n(data, target) : tuple if ``return_X_y`` is True\n\n    .. versionadded:: 0.20",
      "code": "@_deprecate_positional_args\ndef fetch_rcv1(*, data_home=None, subset='all', download_if_missing=True,\n               random_state=None, shuffle=False, return_X_y=False):\n    \"\"\"Load the RCV1 multilabel dataset (classification).\n\n    Download it if necessary.\n\n    Version: RCV1-v2, vectors, full sets, topics multilabels.\n\n    =================   =====================\n    Classes                               103\n    Samples total                      804414\n    Dimensionality                      47236\n    Features            real, between 0 and 1\n    =================   =====================\n\n    Read more in the :ref:`User Guide <rcv1_dataset>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    subset : {'train', 'test', 'all'}, default='all'\n        Select the dataset to load: 'train' for the training set\n        (23149 samples), 'test' for the test set (781265 samples),\n        'all' for both, with the training samples first if shuffle is False.\n        This follows the official LYRL2004 chronological split.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset shuffling. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    return_X_y : bool, default=False\n        If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n        object. See below for more information about the `dataset.data` and\n        `dataset.target` object.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    dataset : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : sparse matrix of shape (804414, 47236), dtype=np.float64\n            The array has 0.16% of non zero values. Will be of CSR format.\n        target : sparse matrix of shape (804414, 103), dtype=np.uint8\n            Each sample has a value of 1 in its categories, and 0 in others.\n            The array has 3.15% of non zero values. Will be of CSR format.\n        sample_id : ndarray of shape (804414,), dtype=np.uint32,\n            Identification number of each sample, as ordered in dataset.data.\n        target_names : ndarray of shape (103,), dtype=object\n            Names of each target (RCV1 topics), as ordered in dataset.target.\n        DESCR : str\n            Description of the RCV1 dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.20\n    \"\"\"\n    N_SAMPLES = 804414\n    N_FEATURES = 47236\n    N_CATEGORIES = 103\n    N_TRAIN = 23149\n\n    data_home = get_data_home(data_home=data_home)\n    rcv1_dir = join(data_home, \"RCV1\")\n    if download_if_missing:\n        if not exists(rcv1_dir):\n            makedirs(rcv1_dir)\n\n    samples_path = _pkl_filepath(rcv1_dir, \"samples.pkl\")\n    sample_id_path = _pkl_filepath(rcv1_dir, \"sample_id.pkl\")\n    sample_topics_path = _pkl_filepath(rcv1_dir, \"sample_topics.pkl\")\n    topics_path = _pkl_filepath(rcv1_dir, \"topics_names.pkl\")\n\n    # load data (X) and sample_id\n    if download_if_missing and (not exists(samples_path) or\n                                not exists(sample_id_path)):\n        files = []\n        for each in XY_METADATA:\n            logger.info(\"Downloading %s\" % each.url)\n            file_path = _fetch_remote(each, dirname=rcv1_dir)\n            files.append(GzipFile(filename=file_path))\n\n        Xy = load_svmlight_files(files, n_features=N_FEATURES)\n\n        # Training data is before testing data\n        X = sp.vstack([Xy[8], Xy[0], Xy[2], Xy[4], Xy[6]]).tocsr()\n        sample_id = np.hstack((Xy[9], Xy[1], Xy[3], Xy[5], Xy[7]))\n        sample_id = sample_id.astype(np.uint32, copy=False)\n\n        joblib.dump(X, samples_path, compress=9)\n        joblib.dump(sample_id, sample_id_path, compress=9)\n\n        # delete archives\n        for f in files:\n            f.close()\n            remove(f.name)\n    else:\n        X = joblib.load(samples_path)\n        sample_id = joblib.load(sample_id_path)\n\n    # load target (y), categories, and sample_id_bis\n    if download_if_missing and (not exists(sample_topics_path) or\n                                not exists(topics_path)):\n        logger.info(\"Downloading %s\" % TOPICS_METADATA.url)\n        topics_archive_path = _fetch_remote(TOPICS_METADATA,\n                                            dirname=rcv1_dir)\n\n        # parse the target file\n        n_cat = -1\n        n_doc = -1\n        doc_previous = -1\n        y = np.zeros((N_SAMPLES, N_CATEGORIES), dtype=np.uint8)\n        sample_id_bis = np.zeros(N_SAMPLES, dtype=np.int32)\n        category_names = {}\n        with GzipFile(filename=topics_archive_path, mode='rb') as f:\n            for line in f:\n                line_components = line.decode(\"ascii\").split(\" \")\n                if len(line_components) == 3:\n                    cat, doc, _ = line_components\n                    if cat not in category_names:\n                        n_cat += 1\n                        category_names[cat] = n_cat\n\n                    doc = int(doc)\n                    if doc != doc_previous:\n                        doc_previous = doc\n                        n_doc += 1\n                        sample_id_bis[n_doc] = doc\n                    y[n_doc, category_names[cat]] = 1\n\n        # delete archive\n        remove(topics_archive_path)\n\n        # Samples in X are ordered with sample_id,\n        # whereas in y, they are ordered with sample_id_bis.\n        permutation = _find_permutation(sample_id_bis, sample_id)\n        y = y[permutation, :]\n\n        # save category names in a list, with same order than y\n        categories = np.empty(N_CATEGORIES, dtype=object)\n        for k in category_names.keys():\n            categories[category_names[k]] = k\n\n        # reorder categories in lexicographic order\n        order = np.argsort(categories)\n        categories = categories[order]\n        y = sp.csr_matrix(y[:, order])\n\n        joblib.dump(y, sample_topics_path, compress=9)\n        joblib.dump(categories, topics_path, compress=9)\n    else:\n        y = joblib.load(sample_topics_path)\n        categories = joblib.load(topics_path)\n\n    if subset == 'all':\n        pass\n    elif subset == 'train':\n        X = X[:N_TRAIN, :]\n        y = y[:N_TRAIN, :]\n        sample_id = sample_id[:N_TRAIN]\n    elif subset == 'test':\n        X = X[N_TRAIN:, :]\n        y = y[N_TRAIN:, :]\n        sample_id = sample_id[N_TRAIN:]\n    else:\n        raise ValueError(\"Unknown subset parameter. Got '%s' instead of one\"\n                         \" of ('all', 'train', test')\" % subset)\n\n    if shuffle:\n        X, y, sample_id = shuffle_(X, y, sample_id, random_state=random_state)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'rcv1.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    if return_X_y:\n        return X, y\n\n    return Bunch(data=X, target=y, sample_id=sample_id,\n                 target_names=categories, DESCR=fdescr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters",
      "name": "make_biclusters",
      "qname": "sklearn.datasets._samples_generator.make_biclusters",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/shape",
          "name": "shape",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.shape",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable of shape (n_rows, n_cols)",
            "default_value": "",
            "description": "The shape of the result."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable of shape (n_rows, n_cols)"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.n_clusters",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "The number of biclusters."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/minval",
          "name": "minval",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.minval",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Minimum value of a bicluster."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/maxval",
          "name": "maxval",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.maxval",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum value of a bicluster."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Shuffle the samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_biclusters/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_biclusters.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate an array with constant block diagonal structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate an array with constant block diagonal structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nshape : iterable of shape (n_rows, n_cols)\n    The shape of the result.\n\nn_clusters : int\n    The number of biclusters.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nminval : int, default=10\n    Minimum value of a bicluster.\n\nmaxval : int, default=100\n    Maximum value of a bicluster.\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape `shape`\n    The generated array.\n\nrows : ndarray of shape (n_clusters, X.shape[0])\n    The indicators for cluster membership of each row.\n\ncols : ndarray of shape (n_clusters, X.shape[1])\n    The indicators for cluster membership of each column.\n\nReferences\n----------\n\n.. [1] Dhillon, I. S. (2001, August). Co-clustering documents and\n    words using bipartite spectral graph partitioning. In Proceedings\n    of the seventh ACM SIGKDD international conference on Knowledge\n    discovery and data mining (pp. 269-274). ACM.\n\nSee Also\n--------\nmake_checkerboard",
      "code": "@_deprecate_positional_args\ndef make_biclusters(shape, n_clusters, *, noise=0.0, minval=10,\n                    maxval=100, shuffle=True, random_state=None):\n    \"\"\"Generate an array with constant block diagonal structure for\n    biclustering.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    shape : iterable of shape (n_rows, n_cols)\n        The shape of the result.\n\n    n_clusters : int\n        The number of biclusters.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise.\n\n    minval : int, default=10\n        Minimum value of a bicluster.\n\n    maxval : int, default=100\n        Maximum value of a bicluster.\n\n    shuffle : bool, default=True\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape `shape`\n        The generated array.\n\n    rows : ndarray of shape (n_clusters, X.shape[0])\n        The indicators for cluster membership of each row.\n\n    cols : ndarray of shape (n_clusters, X.shape[1])\n        The indicators for cluster membership of each column.\n\n    References\n    ----------\n\n    .. [1] Dhillon, I. S. (2001, August). Co-clustering documents and\n        words using bipartite spectral graph partitioning. In Proceedings\n        of the seventh ACM SIGKDD international conference on Knowledge\n        discovery and data mining (pp. 269-274). ACM.\n\n    See Also\n    --------\n    make_checkerboard\n    \"\"\"\n    generator = check_random_state(random_state)\n    n_rows, n_cols = shape\n    consts = generator.uniform(minval, maxval, n_clusters)\n\n    # row and column clusters of approximately equal sizes\n    row_sizes = generator.multinomial(n_rows,\n                                      np.repeat(1.0 / n_clusters,\n                                                n_clusters))\n    col_sizes = generator.multinomial(n_cols,\n                                      np.repeat(1.0 / n_clusters,\n                                                n_clusters))\n\n    row_labels = np.hstack(list(np.repeat(val, rep) for val, rep in\n                                zip(range(n_clusters), row_sizes)))\n    col_labels = np.hstack(list(np.repeat(val, rep) for val, rep in\n                                zip(range(n_clusters), col_sizes)))\n\n    result = np.zeros(shape, dtype=np.float64)\n    for i in range(n_clusters):\n        selector = np.outer(row_labels == i, col_labels == i)\n        result[selector] += consts[i]\n\n    if noise > 0:\n        result += generator.normal(scale=noise, size=result.shape)\n\n    if shuffle:\n        result, row_idx, col_idx = _shuffle(result, random_state)\n        row_labels = row_labels[row_idx]\n        col_labels = col_labels[col_idx]\n\n    rows = np.vstack([row_labels == c for c in range(n_clusters)])\n    cols = np.vstack([col_labels == c for c in range(n_clusters)])\n\n    return result, rows, cols"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs",
      "name": "make_blobs",
      "qname": "sklearn.datasets._samples_generator.make_blobs",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_blobs.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or array-like",
            "default_value": "100",
            "description": "If int, it is the total number of points equally divided among\nclusters.\nIf array-like, each element of the sequence indicates\nthe number of samples per cluster.\n\n.. versionchanged:: v0.20\n    one can now pass an array-like to the ``n_samples`` parameter"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "array-like"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_blobs.n_features",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of features for each sample."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/centers",
          "name": "centers",
          "qname": "sklearn.datasets._samples_generator.make_blobs.centers",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or ndarray of shape (n_centers, n_features)",
            "default_value": "None",
            "description": "The number of centers to generate, or the fixed center locations.\nIf n_samples is an int and centers is None, 3 centers are generated.\nIf n_samples is array-like, centers must be\neither None or an array of length equal to the length of n_samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_centers, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/cluster_std",
          "name": "cluster_std",
          "qname": "sklearn.datasets._samples_generator.make_blobs.cluster_std",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or array-like of float",
            "default_value": "1.0",
            "description": "The standard deviation of the clusters."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "array-like of float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/center_box",
          "name": "center_box",
          "qname": "sklearn.datasets._samples_generator.make_blobs.center_box",
          "default_value": "(-10.0, 10.0)",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple of float (min, max)",
            "default_value": "(-10.0, 10.0)",
            "description": "The bounding box for each cluster center when centers are\ngenerated at random."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of float (min, max)"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_blobs.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Shuffle the samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_blobs.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_blobs/return_centers",
          "name": "return_centers",
          "qname": "sklearn.datasets._samples_generator.make_blobs.return_centers",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, then return the centers of each cluster\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate isotropic Gaussian blobs for clustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate isotropic Gaussian blobs for clustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int or array-like, default=100\n    If int, it is the total number of points equally divided among\n    clusters.\n    If array-like, each element of the sequence indicates\n    the number of samples per cluster.\n\n    .. versionchanged:: v0.20\n        one can now pass an array-like to the ``n_samples`` parameter\n\nn_features : int, default=2\n    The number of features for each sample.\n\ncenters : int or ndarray of shape (n_centers, n_features), default=None\n    The number of centers to generate, or the fixed center locations.\n    If n_samples is an int and centers is None, 3 centers are generated.\n    If n_samples is array-like, centers must be\n    either None or an array of length equal to the length of n_samples.\n\ncluster_std : float or array-like of float, default=1.0\n    The standard deviation of the clusters.\n\ncenter_box : tuple of float (min, max), default=(-10.0, 10.0)\n    The bounding box for each cluster center when centers are\n    generated at random.\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_centers : bool, default=False\n    If True, then return the centers of each cluster\n\n    .. versionadded:: 0.23\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels for cluster membership of each sample.\n\ncenters : ndarray of shape (n_centers, n_features)\n    The centers of each cluster. Only returned if\n    ``return_centers=True``.\n\nExamples\n--------\n>>> from sklearn.datasets import make_blobs\n>>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n...                   random_state=0)\n>>> print(X.shape)\n(10, 2)\n>>> y\narray([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n>>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,\n...                   random_state=0)\n>>> print(X.shape)\n(10, 2)\n>>> y\narray([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])\n\nSee Also\n--------\nmake_classification : A more intricate variant.",
      "code": "@_deprecate_positional_args\ndef make_blobs(n_samples=100, n_features=2, *, centers=None, cluster_std=1.0,\n               center_box=(-10.0, 10.0), shuffle=True, random_state=None,\n               return_centers=False):\n    \"\"\"Generate isotropic Gaussian blobs for clustering.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int or array-like, default=100\n        If int, it is the total number of points equally divided among\n        clusters.\n        If array-like, each element of the sequence indicates\n        the number of samples per cluster.\n\n        .. versionchanged:: v0.20\n            one can now pass an array-like to the ``n_samples`` parameter\n\n    n_features : int, default=2\n        The number of features for each sample.\n\n    centers : int or ndarray of shape (n_centers, n_features), default=None\n        The number of centers to generate, or the fixed center locations.\n        If n_samples is an int and centers is None, 3 centers are generated.\n        If n_samples is array-like, centers must be\n        either None or an array of length equal to the length of n_samples.\n\n    cluster_std : float or array-like of float, default=1.0\n        The standard deviation of the clusters.\n\n    center_box : tuple of float (min, max), default=(-10.0, 10.0)\n        The bounding box for each cluster center when centers are\n        generated at random.\n\n    shuffle : bool, default=True\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_centers : bool, default=False\n        If True, then return the centers of each cluster\n\n        .. versionadded:: 0.23\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The generated samples.\n\n    y : ndarray of shape (n_samples,)\n        The integer labels for cluster membership of each sample.\n\n    centers : ndarray of shape (n_centers, n_features)\n        The centers of each cluster. Only returned if\n        ``return_centers=True``.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_blobs\n    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n    ...                   random_state=0)\n    >>> print(X.shape)\n    (10, 2)\n    >>> y\n    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n    >>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,\n    ...                   random_state=0)\n    >>> print(X.shape)\n    (10, 2)\n    >>> y\n    array([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])\n\n    See Also\n    --------\n    make_classification : A more intricate variant.\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    if isinstance(n_samples, numbers.Integral):\n        # Set n_centers by looking at centers arg\n        if centers is None:\n            centers = 3\n\n        if isinstance(centers, numbers.Integral):\n            n_centers = centers\n            centers = generator.uniform(center_box[0], center_box[1],\n                                        size=(n_centers, n_features))\n\n        else:\n            centers = check_array(centers)\n            n_features = centers.shape[1]\n            n_centers = centers.shape[0]\n\n    else:\n        # Set n_centers by looking at [n_samples] arg\n        n_centers = len(n_samples)\n        if centers is None:\n            centers = generator.uniform(center_box[0], center_box[1],\n                                        size=(n_centers, n_features))\n        try:\n            assert len(centers) == n_centers\n        except TypeError as e:\n            raise ValueError(\"Parameter `centers` must be array-like. \"\n                             \"Got {!r} instead\".format(centers)) from e\n        except AssertionError as e:\n            raise ValueError(\n                f\"Length of `n_samples` not consistent with number of \"\n                f\"centers. Got n_samples = {n_samples} and centers = {centers}\"\n            ) from e\n        else:\n            centers = check_array(centers)\n            n_features = centers.shape[1]\n\n    # stds: if cluster_std is given as list, it must be consistent\n    # with the n_centers\n    if (hasattr(cluster_std, \"__len__\") and len(cluster_std) != n_centers):\n        raise ValueError(\"Length of `clusters_std` not consistent with \"\n                         \"number of centers. Got centers = {} \"\n                         \"and cluster_std = {}\".format(centers, cluster_std))\n\n    if isinstance(cluster_std, numbers.Real):\n        cluster_std = np.full(len(centers), cluster_std)\n\n    X = []\n    y = []\n\n    if isinstance(n_samples, Iterable):\n        n_samples_per_center = n_samples\n    else:\n        n_samples_per_center = [int(n_samples // n_centers)] * n_centers\n\n        for i in range(n_samples % n_centers):\n            n_samples_per_center[i] += 1\n\n    for i, (n, std) in enumerate(zip(n_samples_per_center, cluster_std)):\n        X.append(generator.normal(loc=centers[i], scale=std,\n                                  size=(n, n_features)))\n        y += [i] * n\n\n    X = np.concatenate(X)\n    y = np.array(y)\n\n    if shuffle:\n        total_n_samples = np.sum(n_samples)\n        indices = np.arange(total_n_samples)\n        generator.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n\n    if return_centers:\n        return X, y, centers\n    else:\n        return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard",
      "name": "make_checkerboard",
      "qname": "sklearn.datasets._samples_generator.make_checkerboard",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/shape",
          "name": "shape",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.shape",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "tuple of shape (n_rows, n_cols)",
            "default_value": "",
            "description": "The shape of the result."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of shape (n_rows, n_cols)"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/n_clusters",
          "name": "n_clusters",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.n_clusters",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or array-like or shape (n_row_clusters, n_column_clusters)",
            "default_value": "",
            "description": "The number of row and column clusters."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_row_clusters, n_column_clusters)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/minval",
          "name": "minval",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.minval",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Minimum value of a bicluster."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/maxval",
          "name": "maxval",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.maxval",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum value of a bicluster."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Shuffle the samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_checkerboard/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_checkerboard.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate an array with block checkerboard structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate an array with block checkerboard structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nshape : tuple of shape (n_rows, n_cols)\n    The shape of the result.\n\nn_clusters : int or array-like or shape (n_row_clusters, n_column_clusters)\n    The number of row and column clusters.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nminval : int, default=10\n    Minimum value of a bicluster.\n\nmaxval : int, default=100\n    Maximum value of a bicluster.\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape `shape`\n    The generated array.\n\nrows : ndarray of shape (n_clusters, X.shape[0])\n    The indicators for cluster membership of each row.\n\ncols : ndarray of shape (n_clusters, X.shape[1])\n    The indicators for cluster membership of each column.\n\n\nReferences\n----------\n\n.. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).\n    Spectral biclustering of microarray data: coclustering genes\n    and conditions. Genome research, 13(4), 703-716.\n\nSee Also\n--------\nmake_biclusters",
      "code": "@_deprecate_positional_args\ndef make_checkerboard(shape, n_clusters, *, noise=0.0, minval=10,\n                      maxval=100, shuffle=True, random_state=None):\n    \"\"\"Generate an array with block checkerboard structure for\n    biclustering.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    shape : tuple of shape (n_rows, n_cols)\n        The shape of the result.\n\n    n_clusters : int or array-like or shape (n_row_clusters, n_column_clusters)\n        The number of row and column clusters.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise.\n\n    minval : int, default=10\n        Minimum value of a bicluster.\n\n    maxval : int, default=100\n        Maximum value of a bicluster.\n\n    shuffle : bool, default=True\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape `shape`\n        The generated array.\n\n    rows : ndarray of shape (n_clusters, X.shape[0])\n        The indicators for cluster membership of each row.\n\n    cols : ndarray of shape (n_clusters, X.shape[1])\n        The indicators for cluster membership of each column.\n\n\n    References\n    ----------\n\n    .. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).\n        Spectral biclustering of microarray data: coclustering genes\n        and conditions. Genome research, 13(4), 703-716.\n\n    See Also\n    --------\n    make_biclusters\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    if hasattr(n_clusters, \"__len__\"):\n        n_row_clusters, n_col_clusters = n_clusters\n    else:\n        n_row_clusters = n_col_clusters = n_clusters\n\n    # row and column clusters of approximately equal sizes\n    n_rows, n_cols = shape\n    row_sizes = generator.multinomial(n_rows,\n                                      np.repeat(1.0 / n_row_clusters,\n                                                n_row_clusters))\n    col_sizes = generator.multinomial(n_cols,\n                                      np.repeat(1.0 / n_col_clusters,\n                                                n_col_clusters))\n\n    row_labels = np.hstack(list(np.repeat(val, rep) for val, rep in\n                                zip(range(n_row_clusters), row_sizes)))\n    col_labels = np.hstack(list(np.repeat(val, rep) for val, rep in\n                                zip(range(n_col_clusters), col_sizes)))\n\n    result = np.zeros(shape, dtype=np.float64)\n    for i in range(n_row_clusters):\n        for j in range(n_col_clusters):\n            selector = np.outer(row_labels == i, col_labels == j)\n            result[selector] += generator.uniform(minval, maxval)\n\n    if noise > 0:\n        result += generator.normal(scale=noise, size=result.shape)\n\n    if shuffle:\n        result, row_idx, col_idx = _shuffle(result, random_state)\n        row_labels = row_labels[row_idx]\n        col_labels = col_labels[col_idx]\n\n    rows = np.vstack([row_labels == label\n                      for label in range(n_row_clusters)\n                      for _ in range(n_col_clusters)])\n    cols = np.vstack([col_labels == label\n                      for _ in range(n_row_clusters)\n                      for label in range(n_col_clusters)])\n\n    return result, rows, cols"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_circles",
      "name": "make_circles",
      "qname": "sklearn.datasets._samples_generator.make_circles",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_circles/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_circles.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or tuple of shape (2,), dtype=int",
            "default_value": "100",
            "description": "If int, it is the total number of points generated.\nFor odd numbers, the inner circle will have one point more than the\nouter circle.\nIf two-element tuple, number of points in outer circle and inner\ncircle.\n\n.. versionchanged:: 0.23\n   Added two-element tuple."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "tuple of shape (2,)"
              },
              {
                "kind": "NamedType",
                "name": "dtype=int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_circles/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_circles.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to shuffle the samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_circles/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_circles.noise",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Standard deviation of Gaussian noise added to the data."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_circles/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_circles.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset shuffling and noise.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_circles/factor",
          "name": "factor",
          "qname": "sklearn.datasets._samples_generator.make_circles.factor",
          "default_value": "0.8",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": ".8",
            "description": "Scale factor between inner and outer circle in the range `(0, 1)`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": false,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Make a large circle containing a smaller circle in 2d.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Make a large circle containing a smaller circle in 2d.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int or tuple of shape (2,), dtype=int, default=100\n    If int, it is the total number of points generated.\n    For odd numbers, the inner circle will have one point more than the\n    outer circle.\n    If two-element tuple, number of points in outer circle and inner\n    circle.\n\n    .. versionchanged:: 0.23\n       Added two-element tuple.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples.\n\nnoise : float, default=None\n    Standard deviation of Gaussian noise added to the data.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling and noise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nfactor : float, default=.8\n    Scale factor between inner and outer circle in the range `(0, 1)`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 2)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels (0 or 1) for class membership of each sample.",
      "code": "@_deprecate_positional_args\ndef make_circles(n_samples=100, *, shuffle=True, noise=None, random_state=None,\n                 factor=.8):\n    \"\"\"Make a large circle containing a smaller circle in 2d.\n\n    A simple toy dataset to visualize clustering and classification\n    algorithms.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int or tuple of shape (2,), dtype=int, default=100\n        If int, it is the total number of points generated.\n        For odd numbers, the inner circle will have one point more than the\n        outer circle.\n        If two-element tuple, number of points in outer circle and inner\n        circle.\n\n        .. versionchanged:: 0.23\n           Added two-element tuple.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples.\n\n    noise : float, default=None\n        Standard deviation of Gaussian noise added to the data.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset shuffling and noise.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    factor : float, default=.8\n        Scale factor between inner and outer circle in the range `(0, 1)`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 2)\n        The generated samples.\n\n    y : ndarray of shape (n_samples,)\n        The integer labels (0 or 1) for class membership of each sample.\n    \"\"\"\n\n    if factor >= 1 or factor < 0:\n        raise ValueError(\"'factor' has to be between 0 and 1.\")\n\n    if isinstance(n_samples, numbers.Integral):\n        n_samples_out = n_samples // 2\n        n_samples_in = n_samples - n_samples_out\n    else:\n        try:\n            n_samples_out, n_samples_in = n_samples\n        except ValueError as e:\n            raise ValueError('`n_samples` can be either an int or '\n                             'a two-element tuple.') from e\n\n    generator = check_random_state(random_state)\n    # so as not to have the first point = last point, we set endpoint=False\n    linspace_out = np.linspace(0, 2 * np.pi, n_samples_out, endpoint=False)\n    linspace_in = np.linspace(0, 2 * np.pi, n_samples_in, endpoint=False)\n    outer_circ_x = np.cos(linspace_out)\n    outer_circ_y = np.sin(linspace_out)\n    inner_circ_x = np.cos(linspace_in) * factor\n    inner_circ_y = np.sin(linspace_in) * factor\n\n    X = np.vstack([np.append(outer_circ_x, inner_circ_x),\n                   np.append(outer_circ_y, inner_circ_y)]).T\n    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),\n                   np.ones(n_samples_in, dtype=np.intp)])\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    if noise is not None:\n        X += generator.normal(scale=noise, size=X.shape)\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification",
      "name": "make_classification",
      "qname": "sklearn.datasets._samples_generator.make_classification",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_features",
          "default_value": "20",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "20",
            "description": "The total number of features. These comprise ``n_informative``\ninformative features, ``n_redundant`` redundant features,\n``n_repeated`` duplicated features and\n``n_features-n_informative-n_redundant-n_repeated`` useless features\ndrawn at random."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_informative",
          "name": "n_informative",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_informative",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of informative features. Each class is composed of a number\nof gaussian clusters each located around the vertices of a hypercube\nin a subspace of dimension ``n_informative``. For each cluster,\ninformative features are drawn independently from  N(0, 1) and then\nrandomly linearly combined within each cluster in order to add\ncovariance. The clusters are then placed on the vertices of the\nhypercube."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_redundant",
          "name": "n_redundant",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_redundant",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of redundant features. These features are generated as\nrandom linear combinations of the informative features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_repeated",
          "name": "n_repeated",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_repeated",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "The number of duplicated features, drawn randomly from the informative\nand the redundant features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_classes",
          "name": "n_classes",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_classes",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of classes (or labels) of the classification problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/n_clusters_per_class",
          "name": "n_clusters_per_class",
          "qname": "sklearn.datasets._samples_generator.make_classification.n_clusters_per_class",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of clusters per class."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/weights",
          "name": "weights",
          "qname": "sklearn.datasets._samples_generator.make_classification.weights",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_classes,) or (n_classes - 1,)",
            "default_value": "None",
            "description": "The proportions of samples assigned to each class. If None, then\nclasses are balanced. Note that if ``len(weights) == n_classes - 1``,\nthen the last class weight is automatically inferred.\nMore than ``n_samples`` samples may be returned if the sum of\n``weights`` exceeds 1. Note that the actual class proportions will\nnot exactly match ``weights`` when ``flip_y`` isn't 0."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_classes,) or (n_classes - 1,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/flip_y",
          "name": "flip_y",
          "qname": "sklearn.datasets._samples_generator.make_classification.flip_y",
          "default_value": "0.01",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "The fraction of samples whose class is assigned randomly. Larger\nvalues introduce noise in the labels and make the classification\ntask harder. Note that the default setting flip_y > 0 might lead\nto less than ``n_classes`` in y in some cases."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/class_sep",
          "name": "class_sep",
          "qname": "sklearn.datasets._samples_generator.make_classification.class_sep",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "The factor multiplying the hypercube size.  Larger values spread\nout the clusters/classes and make the classification task easier."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/hypercube",
          "name": "hypercube",
          "qname": "sklearn.datasets._samples_generator.make_classification.hypercube",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True, the clusters are put on the vertices of a hypercube. If\nFalse, the clusters are put on the vertices of a random polytope."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/shift",
          "name": "shift",
          "qname": "sklearn.datasets._samples_generator.make_classification.shift",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float, ndarray of shape (n_features,) or None",
            "default_value": "0.0",
            "description": "Shift features by the specified value. If None, then features\nare shifted by a random value drawn in [-class_sep, class_sep]."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_features,)"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/scale",
          "name": "scale",
          "qname": "sklearn.datasets._samples_generator.make_classification.scale",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float, ndarray of shape (n_features,) or None",
            "default_value": "1.0",
            "description": "Multiply features by the specified value. If None, then features\nare scaled by a random value drawn in [1, 100]. Note that scaling\nhappens after shifting."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_features,)"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_classification.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Shuffle the samples and the features."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_classification/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_classification.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a random n-class classification problem.\n\nThis initially creates clusters of points normally distributed (std=1)\nabout vertices of an ``n_informative``-dimensional hypercube with sides of\nlength ``2*class_sep`` and assigns an equal number of clusters to each\nclass. It introduces interdependence between these features and adds\nvarious types of further noise to the data.\n\nWithout shuffling, ``X`` horizontally stacks features in the following\norder: the primary ``n_informative`` features, followed by ``n_redundant``\nlinear combinations of the informative features, followed by ``n_repeated``\nduplicates, drawn randomly with replacement from the informative and\nredundant features. The remaining features are filled with random noise.\nThus, without shuffling, all useful features are contained in the columns\n``X[:, :n_informative + n_redundant + n_repeated]``.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a random n-class classification problem.\n\nThis initially creates clusters of points normally distributed (std=1)\nabout vertices of an ``n_informative``-dimensional hypercube with sides of\nlength ``2*class_sep`` and assigns an equal number of clusters to each\nclass. It introduces interdependence between these features and adds\nvarious types of further noise to the data.\n\nWithout shuffling, ``X`` horizontally stacks features in the following\norder: the primary ``n_informative`` features, followed by ``n_redundant``\nlinear combinations of the informative features, followed by ``n_repeated``\nduplicates, drawn randomly with replacement from the informative and\nredundant features. The remaining features are filled with random noise.\nThus, without shuffling, all useful features are contained in the columns\n``X[:, :n_informative + n_redundant + n_repeated]``.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=20\n    The total number of features. These comprise ``n_informative``\n    informative features, ``n_redundant`` redundant features,\n    ``n_repeated`` duplicated features and\n    ``n_features-n_informative-n_redundant-n_repeated`` useless features\n    drawn at random.\n\nn_informative : int, default=2\n    The number of informative features. Each class is composed of a number\n    of gaussian clusters each located around the vertices of a hypercube\n    in a subspace of dimension ``n_informative``. For each cluster,\n    informative features are drawn independently from  N(0, 1) and then\n    randomly linearly combined within each cluster in order to add\n    covariance. The clusters are then placed on the vertices of the\n    hypercube.\n\nn_redundant : int, default=2\n    The number of redundant features. These features are generated as\n    random linear combinations of the informative features.\n\nn_repeated : int, default=0\n    The number of duplicated features, drawn randomly from the informative\n    and the redundant features.\n\nn_classes : int, default=2\n    The number of classes (or labels) of the classification problem.\n\nn_clusters_per_class : int, default=2\n    The number of clusters per class.\n\nweights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None\n    The proportions of samples assigned to each class. If None, then\n    classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n    then the last class weight is automatically inferred.\n    More than ``n_samples`` samples may be returned if the sum of\n    ``weights`` exceeds 1. Note that the actual class proportions will\n    not exactly match ``weights`` when ``flip_y`` isn't 0.\n\nflip_y : float, default=0.01\n    The fraction of samples whose class is assigned randomly. Larger\n    values introduce noise in the labels and make the classification\n    task harder. Note that the default setting flip_y > 0 might lead\n    to less than ``n_classes`` in y in some cases.\n\nclass_sep : float, default=1.0\n    The factor multiplying the hypercube size.  Larger values spread\n    out the clusters/classes and make the classification task easier.\n\nhypercube : bool, default=True\n    If True, the clusters are put on the vertices of a hypercube. If\n    False, the clusters are put on the vertices of a random polytope.\n\nshift : float, ndarray of shape (n_features,) or None, default=0.0\n    Shift features by the specified value. If None, then features\n    are shifted by a random value drawn in [-class_sep, class_sep].\n\nscale : float, ndarray of shape (n_features,) or None, default=1.0\n    Multiply features by the specified value. If None, then features\n    are scaled by a random value drawn in [1, 100]. Note that scaling\n    happens after shifting.\n\nshuffle : bool, default=True\n    Shuffle the samples and the features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels for class membership of each sample.\n\nNotes\n-----\nThe algorithm is adapted from Guyon [1] and was designed to generate\nthe \"Madelon\" dataset.\n\nReferences\n----------\n.. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n       selection benchmark\", 2003.\n\nSee Also\n--------\nmake_blobs : Simplified variant.\nmake_multilabel_classification : Unrelated generator for multilabel tasks.",
      "code": "@_deprecate_positional_args\ndef make_classification(n_samples=100, n_features=20, *, n_informative=2,\n                        n_redundant=2, n_repeated=0, n_classes=2,\n                        n_clusters_per_class=2, weights=None, flip_y=0.01,\n                        class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,\n                        shuffle=True, random_state=None):\n    \"\"\"Generate a random n-class classification problem.\n\n    This initially creates clusters of points normally distributed (std=1)\n    about vertices of an ``n_informative``-dimensional hypercube with sides of\n    length ``2*class_sep`` and assigns an equal number of clusters to each\n    class. It introduces interdependence between these features and adds\n    various types of further noise to the data.\n\n    Without shuffling, ``X`` horizontally stacks features in the following\n    order: the primary ``n_informative`` features, followed by ``n_redundant``\n    linear combinations of the informative features, followed by ``n_repeated``\n    duplicates, drawn randomly with replacement from the informative and\n    redundant features. The remaining features are filled with random noise.\n    Thus, without shuffling, all useful features are contained in the columns\n    ``X[:, :n_informative + n_redundant + n_repeated]``.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    n_features : int, default=20\n        The total number of features. These comprise ``n_informative``\n        informative features, ``n_redundant`` redundant features,\n        ``n_repeated`` duplicated features and\n        ``n_features-n_informative-n_redundant-n_repeated`` useless features\n        drawn at random.\n\n    n_informative : int, default=2\n        The number of informative features. Each class is composed of a number\n        of gaussian clusters each located around the vertices of a hypercube\n        in a subspace of dimension ``n_informative``. For each cluster,\n        informative features are drawn independently from  N(0, 1) and then\n        randomly linearly combined within each cluster in order to add\n        covariance. The clusters are then placed on the vertices of the\n        hypercube.\n\n    n_redundant : int, default=2\n        The number of redundant features. These features are generated as\n        random linear combinations of the informative features.\n\n    n_repeated : int, default=0\n        The number of duplicated features, drawn randomly from the informative\n        and the redundant features.\n\n    n_classes : int, default=2\n        The number of classes (or labels) of the classification problem.\n\n    n_clusters_per_class : int, default=2\n        The number of clusters per class.\n\n    weights : array-like of shape (n_classes,) or (n_classes - 1,),\\\n              default=None\n        The proportions of samples assigned to each class. If None, then\n        classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n        then the last class weight is automatically inferred.\n        More than ``n_samples`` samples may be returned if the sum of\n        ``weights`` exceeds 1. Note that the actual class proportions will\n        not exactly match ``weights`` when ``flip_y`` isn't 0.\n\n    flip_y : float, default=0.01\n        The fraction of samples whose class is assigned randomly. Larger\n        values introduce noise in the labels and make the classification\n        task harder. Note that the default setting flip_y > 0 might lead\n        to less than ``n_classes`` in y in some cases.\n\n    class_sep : float, default=1.0\n        The factor multiplying the hypercube size.  Larger values spread\n        out the clusters/classes and make the classification task easier.\n\n    hypercube : bool, default=True\n        If True, the clusters are put on the vertices of a hypercube. If\n        False, the clusters are put on the vertices of a random polytope.\n\n    shift : float, ndarray of shape (n_features,) or None, default=0.0\n        Shift features by the specified value. If None, then features\n        are shifted by a random value drawn in [-class_sep, class_sep].\n\n    scale : float, ndarray of shape (n_features,) or None, default=1.0\n        Multiply features by the specified value. If None, then features\n        are scaled by a random value drawn in [1, 100]. Note that scaling\n        happens after shifting.\n\n    shuffle : bool, default=True\n        Shuffle the samples and the features.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The generated samples.\n\n    y : ndarray of shape (n_samples,)\n        The integer labels for class membership of each sample.\n\n    Notes\n    -----\n    The algorithm is adapted from Guyon [1] and was designed to generate\n    the \"Madelon\" dataset.\n\n    References\n    ----------\n    .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n           selection benchmark\", 2003.\n\n    See Also\n    --------\n    make_blobs : Simplified variant.\n    make_multilabel_classification : Unrelated generator for multilabel tasks.\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    # Count features, clusters and samples\n    if n_informative + n_redundant + n_repeated > n_features:\n        raise ValueError(\"Number of informative, redundant and repeated \"\n                         \"features must sum to less than the number of total\"\n                         \" features\")\n    # Use log2 to avoid overflow errors\n    if n_informative < np.log2(n_classes * n_clusters_per_class):\n        msg = \"n_classes({}) * n_clusters_per_class({}) must be\"\n        msg += \" smaller or equal 2**n_informative({})={}\"\n        raise ValueError(msg.format(n_classes, n_clusters_per_class,\n                                    n_informative, 2**n_informative))\n\n    if weights is not None:\n        if len(weights) not in [n_classes, n_classes - 1]:\n            raise ValueError(\"Weights specified but incompatible with number \"\n                             \"of classes.\")\n        if len(weights) == n_classes - 1:\n            if isinstance(weights, list):\n                weights = weights + [1.0 - sum(weights)]\n            else:\n                weights = np.resize(weights, n_classes)\n                weights[-1] = 1.0 - sum(weights[:-1])\n    else:\n        weights = [1.0 / n_classes] * n_classes\n\n    n_useless = n_features - n_informative - n_redundant - n_repeated\n    n_clusters = n_classes * n_clusters_per_class\n\n    # Distribute samples among clusters by weight\n    n_samples_per_cluster = [\n        int(n_samples * weights[k % n_classes] / n_clusters_per_class)\n        for k in range(n_clusters)]\n\n    for i in range(n_samples - sum(n_samples_per_cluster)):\n        n_samples_per_cluster[i % n_clusters] += 1\n\n    # Initialize X and y\n    X = np.zeros((n_samples, n_features))\n    y = np.zeros(n_samples, dtype=int)\n\n    # Build the polytope whose vertices become cluster centroids\n    centroids = _generate_hypercube(n_clusters, n_informative,\n                                    generator).astype(float, copy=False)\n    centroids *= 2 * class_sep\n    centroids -= class_sep\n    if not hypercube:\n        centroids *= generator.rand(n_clusters, 1)\n        centroids *= generator.rand(1, n_informative)\n\n    # Initially draw informative features from the standard normal\n    X[:, :n_informative] = generator.randn(n_samples, n_informative)\n\n    # Create each cluster; a variant of make_blobs\n    stop = 0\n    for k, centroid in enumerate(centroids):\n        start, stop = stop, stop + n_samples_per_cluster[k]\n        y[start:stop] = k % n_classes  # assign labels\n        X_k = X[start:stop, :n_informative]  # slice a view of the cluster\n\n        A = 2 * generator.rand(n_informative, n_informative) - 1\n        X_k[...] = np.dot(X_k, A)  # introduce random covariance\n\n        X_k += centroid  # shift the cluster to a vertex\n\n    # Create redundant features\n    if n_redundant > 0:\n        B = 2 * generator.rand(n_informative, n_redundant) - 1\n        X[:, n_informative:n_informative + n_redundant] = \\\n            np.dot(X[:, :n_informative], B)\n\n    # Repeat some features\n    if n_repeated > 0:\n        n = n_informative + n_redundant\n        indices = ((n - 1) * generator.rand(n_repeated) + 0.5).astype(np.intp)\n        X[:, n:n + n_repeated] = X[:, indices]\n\n    # Fill useless features\n    if n_useless > 0:\n        X[:, -n_useless:] = generator.randn(n_samples, n_useless)\n\n    # Randomly replace labels\n    if flip_y >= 0.0:\n        flip_mask = generator.rand(n_samples) < flip_y\n        y[flip_mask] = generator.randint(n_classes, size=flip_mask.sum())\n\n    # Randomly shift and scale\n    if shift is None:\n        shift = (2 * generator.rand(n_features) - 1) * class_sep\n    X += shift\n\n    if scale is None:\n        scale = 1 + 100 * generator.rand(n_features)\n    X *= scale\n\n    if shuffle:\n        # Randomly permute samples\n        X, y = util_shuffle(X, y, random_state=generator)\n\n        # Randomly permute features\n        indices = np.arange(n_features)\n        generator.shuffle(indices)\n        X[:, :] = X[:, indices]\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman1",
      "name": "make_friedman1",
      "qname": "sklearn.datasets._samples_generator.make_friedman1",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman1/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_friedman1.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman1/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_friedman1.n_features",
          "default_value": "10",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of features. Should be at least 5."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman1/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_friedman1.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise applied to the output."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman1/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_friedman1.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset noise. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate the \"Friedman #1\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are independent features uniformly distributed on the interval\n[0, 1]. The output `y` is created according to the formula::\n\n    y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n\nOut of the `n_features` features, only 5 are actually used to compute\n`y`. The remaining features are independent of `y`.\n\nThe number of features has to be >= 5.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate the \"Friedman #1\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are independent features uniformly distributed on the interval\n[0, 1]. The output `y` is created according to the formula::\n\n    y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n\nOut of the `n_features` features, only 5 are actually used to compute\n`y`. The remaining features are independent of `y`.\n\nThe number of features has to be >= 5.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=10\n    The number of features. Should be at least 5.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset noise. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n       of Statistics 19 (1), pages 1-67, 1991.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n       pages 123-140, 1996.",
      "code": "@_deprecate_positional_args\ndef make_friedman1(n_samples=100, n_features=10, *, noise=0.0,\n                   random_state=None):\n    \"\"\"Generate the \"Friedman #1\" regression problem.\n\n    This dataset is described in Friedman [1] and Breiman [2].\n\n    Inputs `X` are independent features uniformly distributed on the interval\n    [0, 1]. The output `y` is created according to the formula::\n\n        y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 \\\n+ 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n\n    Out of the `n_features` features, only 5 are actually used to compute\n    `y`. The remaining features are independent of `y`.\n\n    The number of features has to be >= 5.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    n_features : int, default=10\n        The number of features. Should be at least 5.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise applied to the output.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset noise. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The input samples.\n\n    y : ndarray of shape (n_samples,)\n        The output values.\n\n    References\n    ----------\n    .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n           of Statistics 19 (1), pages 1-67, 1991.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n           pages 123-140, 1996.\n    \"\"\"\n    if n_features < 5:\n        raise ValueError(\"n_features must be at least five.\")\n\n    generator = check_random_state(random_state)\n\n    X = generator.rand(n_samples, n_features)\n    y = 10 * np.sin(np.pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 \\\n        + 10 * X[:, 3] + 5 * X[:, 4] + noise * generator.randn(n_samples)\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman2",
      "name": "make_friedman2",
      "qname": "sklearn.datasets._samples_generator.make_friedman2",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman2/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_friedman2.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman2/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_friedman2.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise applied to the output."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman2/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_friedman2.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset noise. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate the \"Friedman #2\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n    0 <= X[:, 0] <= 100,\n    40 * pi <= X[:, 1] <= 560 * pi,\n    0 <= X[:, 2] <= 1,\n    1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\n    y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate the \"Friedman #2\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n    0 <= X[:, 0] <= 100,\n    40 * pi <= X[:, 1] <= 560 * pi,\n    0 <= X[:, 2] <= 1,\n    1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\n    y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset noise. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 4)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n       of Statistics 19 (1), pages 1-67, 1991.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n       pages 123-140, 1996.",
      "code": "@_deprecate_positional_args\ndef make_friedman2(n_samples=100, *, noise=0.0, random_state=None):\n    \"\"\"Generate the \"Friedman #2\" regression problem.\n\n    This dataset is described in Friedman [1] and Breiman [2].\n\n    Inputs `X` are 4 independent features uniformly distributed on the\n    intervals::\n\n        0 <= X[:, 0] <= 100,\n        40 * pi <= X[:, 1] <= 560 * pi,\n        0 <= X[:, 2] <= 1,\n        1 <= X[:, 3] <= 11.\n\n    The output `y` is created according to the formula::\n\n        y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2] \\\n - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise applied to the output.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset noise. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 4)\n        The input samples.\n\n    y : ndarray of shape (n_samples,)\n        The output values.\n\n    References\n    ----------\n    .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n           of Statistics 19 (1), pages 1-67, 1991.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n           pages 123-140, 1996.\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    X = generator.rand(n_samples, 4)\n    X[:, 0] *= 100\n    X[:, 1] *= 520 * np.pi\n    X[:, 1] += 40 * np.pi\n    X[:, 3] *= 10\n    X[:, 3] += 1\n\n    y = (X[:, 0] ** 2\n         + (X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 \\\n        + noise * generator.randn(n_samples)\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman3",
      "name": "make_friedman3",
      "qname": "sklearn.datasets._samples_generator.make_friedman3",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman3/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_friedman3.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman3/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_friedman3.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise applied to the output."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_friedman3/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_friedman3.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset noise. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate the \"Friedman #3\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n    0 <= X[:, 0] <= 100,\n    40 * pi <= X[:, 1] <= 560 * pi,\n    0 <= X[:, 2] <= 1,\n    1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\n    y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate the \"Friedman #3\" regression problem.\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n    0 <= X[:, 0] <= 100,\n    40 * pi <= X[:, 1] <= 560 * pi,\n    0 <= X[:, 2] <= 1,\n    1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\n    y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset noise. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 4)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n       of Statistics 19 (1), pages 1-67, 1991.\n\n.. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n       pages 123-140, 1996.",
      "code": "@_deprecate_positional_args\ndef make_friedman3(n_samples=100, *, noise=0.0, random_state=None):\n    \"\"\"Generate the \"Friedman #3\" regression problem.\n\n    This dataset is described in Friedman [1] and Breiman [2].\n\n    Inputs `X` are 4 independent features uniformly distributed on the\n    intervals::\n\n        0 <= X[:, 0] <= 100,\n        40 * pi <= X[:, 1] <= 560 * pi,\n        0 <= X[:, 2] <= 1,\n        1 <= X[:, 3] <= 11.\n\n    The output `y` is created according to the formula::\n\n        y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) \\\n/ X[:, 0]) + noise * N(0, 1).\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise applied to the output.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset noise. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 4)\n        The input samples.\n\n    y : ndarray of shape (n_samples,)\n        The output values.\n\n    References\n    ----------\n    .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n           of Statistics 19 (1), pages 1-67, 1991.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n           pages 123-140, 1996.\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    X = generator.rand(n_samples, 4)\n    X[:, 0] *= 100\n    X[:, 1] *= 520 * np.pi\n    X[:, 1] += 40 * np.pi\n    X[:, 3] *= 10\n    X[:, 3] += 1\n\n    y = np.arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) \\\n        + noise * generator.randn(n_samples)\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles",
      "name": "make_gaussian_quantiles",
      "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/mean",
          "name": "mean",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.mean",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features,)",
            "default_value": "None",
            "description": "The mean of the multi-dimensional normal distribution.\nIf None then use the origin (0, 0, ...)."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/cov",
          "name": "cov",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.cov",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "The covariance matrix will be this value times the unit matrix. This\ndataset only produces symmetric normal distributions."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.n_samples",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The total number of points equally divided among classes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.n_features",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The number of features for each sample."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/n_classes",
          "name": "n_classes",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.n_classes",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "The number of classes"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Shuffle the samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_gaussian_quantiles/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_gaussian_quantiles.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate isotropic Gaussian and label samples by quantile.\n\nThis classification dataset is constructed by taking a multi-dimensional\nstandard normal distribution and defining classes separated by nested\nconcentric multi-dimensional spheres such that roughly equal numbers of\nsamples are in each class (quantiles of the :math:`\\chi^2` distribution).\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate isotropic Gaussian and label samples by quantile.\n\nThis classification dataset is constructed by taking a multi-dimensional\nstandard normal distribution and defining classes separated by nested\nconcentric multi-dimensional spheres such that roughly equal numbers of\nsamples are in each class (quantiles of the :math:`\\chi^2` distribution).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nmean : ndarray of shape (n_features,), default=None\n    The mean of the multi-dimensional normal distribution.\n    If None then use the origin (0, 0, ...).\n\ncov : float, default=1.0\n    The covariance matrix will be this value times the unit matrix. This\n    dataset only produces symmetric normal distributions.\n\nn_samples : int, default=100\n    The total number of points equally divided among classes.\n\nn_features : int, default=2\n    The number of features for each sample.\n\nn_classes : int, default=3\n    The number of classes\n\nshuffle : bool, default=True\n    Shuffle the samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels for quantile membership of each sample.\n\nNotes\n-----\nThe dataset is from Zhu et al [1].\n\nReferences\n----------\n.. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.",
      "code": "@_deprecate_positional_args\ndef make_gaussian_quantiles(*, mean=None, cov=1., n_samples=100,\n                            n_features=2, n_classes=3,\n                            shuffle=True, random_state=None):\n    r\"\"\"Generate isotropic Gaussian and label samples by quantile.\n\n    This classification dataset is constructed by taking a multi-dimensional\n    standard normal distribution and defining classes separated by nested\n    concentric multi-dimensional spheres such that roughly equal numbers of\n    samples are in each class (quantiles of the :math:`\\chi^2` distribution).\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    mean : ndarray of shape (n_features,), default=None\n        The mean of the multi-dimensional normal distribution.\n        If None then use the origin (0, 0, ...).\n\n    cov : float, default=1.0\n        The covariance matrix will be this value times the unit matrix. This\n        dataset only produces symmetric normal distributions.\n\n    n_samples : int, default=100\n        The total number of points equally divided among classes.\n\n    n_features : int, default=2\n        The number of features for each sample.\n\n    n_classes : int, default=3\n        The number of classes\n\n    shuffle : bool, default=True\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The generated samples.\n\n    y : ndarray of shape (n_samples,)\n        The integer labels for quantile membership of each sample.\n\n    Notes\n    -----\n    The dataset is from Zhu et al [1].\n\n    References\n    ----------\n    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\n    \"\"\"\n    if n_samples < n_classes:\n        raise ValueError(\"n_samples must be at least n_classes\")\n\n    generator = check_random_state(random_state)\n\n    if mean is None:\n        mean = np.zeros(n_features)\n    else:\n        mean = np.array(mean)\n\n    # Build multivariate normal distribution\n    X = generator.multivariate_normal(mean, cov * np.identity(n_features),\n                                      (n_samples,))\n\n    # Sort by distance from origin\n    idx = np.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1))\n    X = X[idx, :]\n\n    # Label by quantile\n    step = n_samples // n_classes\n\n    y = np.hstack([np.repeat(np.arange(n_classes), step),\n                   np.repeat(n_classes - 1, n_samples - step * n_classes)])\n\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_hastie_10_2",
      "name": "make_hastie_10_2",
      "qname": "sklearn.datasets._samples_generator.make_hastie_10_2",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_hastie_10_2/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_hastie_10_2.n_samples",
          "default_value": "12000",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "12000",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_hastie_10_2/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_hastie_10_2.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generates data for binary classification used in\nHastie et al. 2009, Example 10.2.\n\nThe ten features are standard independent Gaussian and\nthe target ``y`` is defined by::\n\n  y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generates data for binary classification used in\nHastie et al. 2009, Example 10.2.\n\nThe ten features are standard independent Gaussian and\nthe target ``y`` is defined by::\n\n  y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=12000\n    The number of samples.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 10)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n       Learning Ed. 2\", Springer, 2009.\n\nSee Also\n--------\nmake_gaussian_quantiles : A generalization of this dataset approach.",
      "code": "@_deprecate_positional_args\ndef make_hastie_10_2(n_samples=12000, *, random_state=None):\n    \"\"\"Generates data for binary classification used in\n    Hastie et al. 2009, Example 10.2.\n\n    The ten features are standard independent Gaussian and\n    the target ``y`` is defined by::\n\n      y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=12000\n        The number of samples.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 10)\n        The input samples.\n\n    y : ndarray of shape (n_samples,)\n        The output values.\n\n    References\n    ----------\n    .. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n           Learning Ed. 2\", Springer, 2009.\n\n    See Also\n    --------\n    make_gaussian_quantiles : A generalization of this dataset approach.\n    \"\"\"\n    rs = check_random_state(random_state)\n\n    shape = (n_samples, 10)\n    X = rs.normal(size=shape).reshape(shape)\n    y = ((X ** 2.0).sum(axis=1) > 9.34).astype(np.float64, copy=False)\n    y[y == 0.0] = -1.0\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_low_rank_matrix",
      "name": "make_low_rank_matrix",
      "qname": "sklearn.datasets._samples_generator.make_low_rank_matrix",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_low_rank_matrix/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_low_rank_matrix.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_low_rank_matrix/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_low_rank_matrix.n_features",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_low_rank_matrix/effective_rank",
          "name": "effective_rank",
          "qname": "sklearn.datasets._samples_generator.make_low_rank_matrix.effective_rank",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The approximate number of singular vectors required to explain most of\nthe data by linear combinations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_low_rank_matrix/tail_strength",
          "name": "tail_strength",
          "qname": "sklearn.datasets._samples_generator.make_low_rank_matrix.tail_strength",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The relative importance of the fat noisy tail of the singular values\nprofile. The value should be between 0 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_low_rank_matrix/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_low_rank_matrix.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a mostly low rank matrix with bell-shaped singular values.\n\nMost of the variance can be explained by a bell-shaped curve of width\neffective_rank: the low rank part of the singular values profile is::\n\n    (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\nThe remaining singular values' tail is fat, decreasing as::\n\n    tail_strength * exp(-0.1 * i / effective_rank).\n\nThe low rank part of the profile can be considered the structured\nsignal part of the data while the tail can be considered the noisy\npart of the data that cannot be summarized by a low number of linear\ncomponents (singular vectors).\n\nThis kind of singular profiles is often seen in practice, for instance:\n - gray level pictures of faces\n - TF-IDF vectors of text documents crawled from the web\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a mostly low rank matrix with bell-shaped singular values.\n\nMost of the variance can be explained by a bell-shaped curve of width\neffective_rank: the low rank part of the singular values profile is::\n\n    (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\nThe remaining singular values' tail is fat, decreasing as::\n\n    tail_strength * exp(-0.1 * i / effective_rank).\n\nThe low rank part of the profile can be considered the structured\nsignal part of the data while the tail can be considered the noisy\npart of the data that cannot be summarized by a low number of linear\ncomponents (singular vectors).\n\nThis kind of singular profiles is often seen in practice, for instance:\n - gray level pictures of faces\n - TF-IDF vectors of text documents crawled from the web\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=100\n    The number of features.\n\neffective_rank : int, default=10\n    The approximate number of singular vectors required to explain most of\n    the data by linear combinations.\n\ntail_strength : float, default=0.5\n    The relative importance of the fat noisy tail of the singular values\n    profile. The value should be between 0 and 1.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The matrix.",
      "code": "@_deprecate_positional_args\ndef make_low_rank_matrix(n_samples=100, n_features=100, *, effective_rank=10,\n                         tail_strength=0.5, random_state=None):\n    \"\"\"Generate a mostly low rank matrix with bell-shaped singular values.\n\n    Most of the variance can be explained by a bell-shaped curve of width\n    effective_rank: the low rank part of the singular values profile is::\n\n        (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\n    The remaining singular values' tail is fat, decreasing as::\n\n        tail_strength * exp(-0.1 * i / effective_rank).\n\n    The low rank part of the profile can be considered the structured\n    signal part of the data while the tail can be considered the noisy\n    part of the data that cannot be summarized by a low number of linear\n    components (singular vectors).\n\n    This kind of singular profiles is often seen in practice, for instance:\n     - gray level pictures of faces\n     - TF-IDF vectors of text documents crawled from the web\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    n_features : int, default=100\n        The number of features.\n\n    effective_rank : int, default=10\n        The approximate number of singular vectors required to explain most of\n        the data by linear combinations.\n\n    tail_strength : float, default=0.5\n        The relative importance of the fat noisy tail of the singular values\n        profile. The value should be between 0 and 1.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The matrix.\n    \"\"\"\n    generator = check_random_state(random_state)\n    n = min(n_samples, n_features)\n\n    # Random (ortho normal) vectors\n    u, _ = linalg.qr(generator.randn(n_samples, n), mode='economic',\n                     check_finite=False)\n    v, _ = linalg.qr(generator.randn(n_features, n), mode='economic',\n                     check_finite=False)\n\n    # Index of the singular values\n    singular_ind = np.arange(n, dtype=np.float64)\n\n    # Build the singular profile by assembling signal and noise components\n    low_rank = ((1 - tail_strength) *\n                np.exp(-1.0 * (singular_ind / effective_rank) ** 2))\n    tail = tail_strength * np.exp(-0.1 * singular_ind / effective_rank)\n    s = np.identity(n) * (low_rank + tail)\n\n    return np.dot(np.dot(u, s), v.T)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_moons",
      "name": "make_moons",
      "qname": "sklearn.datasets._samples_generator.make_moons",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_moons/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_moons.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int or tuple of shape (2,), dtype=int",
            "default_value": "100",
            "description": "If int, the total number of points generated.\nIf two-element tuple, number of points in each of two moons.\n\n.. versionchanged:: 0.23\n   Added two-element tuple."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "tuple of shape (2,)"
              },
              {
                "kind": "NamedType",
                "name": "dtype=int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_moons/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_moons.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to shuffle the samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_moons/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_moons.noise",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Standard deviation of Gaussian noise added to the data."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_moons/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_moons.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset shuffling and noise.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Make two interleaving half circles.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms. Read more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Make two interleaving half circles.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms. Read more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int or tuple of shape (2,), dtype=int, default=100\n    If int, the total number of points generated.\n    If two-element tuple, number of points in each of two moons.\n\n    .. versionchanged:: 0.23\n       Added two-element tuple.\n\nshuffle : bool, default=True\n    Whether to shuffle the samples.\n\nnoise : float, default=None\n    Standard deviation of Gaussian noise added to the data.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling and noise.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 2)\n    The generated samples.\n\ny : ndarray of shape (n_samples,)\n    The integer labels (0 or 1) for class membership of each sample.",
      "code": "@_deprecate_positional_args\ndef make_moons(n_samples=100, *, shuffle=True, noise=None, random_state=None):\n    \"\"\"Make two interleaving half circles.\n\n    A simple toy dataset to visualize clustering and classification\n    algorithms. Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int or tuple of shape (2,), dtype=int, default=100\n        If int, the total number of points generated.\n        If two-element tuple, number of points in each of two moons.\n\n        .. versionchanged:: 0.23\n           Added two-element tuple.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples.\n\n    noise : float, default=None\n        Standard deviation of Gaussian noise added to the data.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset shuffling and noise.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 2)\n        The generated samples.\n\n    y : ndarray of shape (n_samples,)\n        The integer labels (0 or 1) for class membership of each sample.\n    \"\"\"\n\n    if isinstance(n_samples, numbers.Integral):\n        n_samples_out = n_samples // 2\n        n_samples_in = n_samples - n_samples_out\n    else:\n        try:\n            n_samples_out, n_samples_in = n_samples\n        except ValueError as e:\n            raise ValueError('`n_samples` can be either an int or '\n                             'a two-element tuple.') from e\n\n    generator = check_random_state(random_state)\n\n    outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))\n    outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))\n    inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))\n    inner_circ_y = 1 - np.sin(np.linspace(0, np.pi, n_samples_in)) - .5\n\n    X = np.vstack([np.append(outer_circ_x, inner_circ_x),\n                   np.append(outer_circ_y, inner_circ_y)]).T\n    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),\n                   np.ones(n_samples_in, dtype=np.intp)])\n\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    if noise is not None:\n        X += generator.normal(scale=noise, size=X.shape)\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification",
      "name": "make_multilabel_classification",
      "qname": "sklearn.datasets._samples_generator.make_multilabel_classification",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.n_features",
          "default_value": "20",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "20",
            "description": "The total number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/n_classes",
          "name": "n_classes",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.n_classes",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "The number of classes of the classification problem."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/n_labels",
          "name": "n_labels",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.n_labels",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "The average number of labels per instance. More precisely, the number\nof labels per sample is drawn from a Poisson distribution with\n``n_labels`` as its expected value, but samples are bounded (using\nrejection sampling) by ``n_classes``, and must be nonzero if\n``allow_unlabeled`` is False."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/length",
          "name": "length",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.length",
          "default_value": "50",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "50",
            "description": "The sum of the features (number of words if documents) is drawn from\na Poisson distribution with this expected value."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/allow_unlabeled",
          "name": "allow_unlabeled",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.allow_unlabeled",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If ``True``, some instances might not belong to any class."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/sparse",
          "name": "sparse",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.sparse",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If ``True``, return a sparse feature matrix\n\n.. versionadded:: 0.17\n   parameter to allow *sparse* output."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/return_indicator",
          "name": "return_indicator",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.return_indicator",
          "default_value": "'dense'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'dense', 'sparse'} or False",
            "default_value": "'dense'",
            "description": "If ``'dense'`` return ``Y`` in the dense binary indicator format. If\n``'sparse'`` return ``Y`` in the sparse binary indicator format.\n``False`` returns a list of lists of labels."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "dense",
                  "sparse"
                ]
              },
              {
                "kind": "NamedType",
                "name": "False"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/return_distributions",
          "name": "return_distributions",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.return_distributions",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If ``True``, return the prior class probability and conditional\nprobabilities of features given classes, from which the data was\ndrawn."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_multilabel_classification/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_multilabel_classification.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a random multilabel classification problem.\n\nFor each sample, the generative process is:\n    - pick the number of labels: n ~ Poisson(n_labels)\n    - n times, choose a class c: c ~ Multinomial(theta)\n    - pick the document length: k ~ Poisson(length)\n    - k times, choose a word: w ~ Multinomial(theta_c)\n\nIn the above process, rejection sampling is used to make sure that\nn is never zero or more than `n_classes`, and that the document length\nis never zero. Likewise, we reject classes which have already been chosen.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a random multilabel classification problem.\n\nFor each sample, the generative process is:\n    - pick the number of labels: n ~ Poisson(n_labels)\n    - n times, choose a class c: c ~ Multinomial(theta)\n    - pick the document length: k ~ Poisson(length)\n    - k times, choose a word: w ~ Multinomial(theta_c)\n\nIn the above process, rejection sampling is used to make sure that\nn is never zero or more than `n_classes`, and that the document length\nis never zero. Likewise, we reject classes which have already been chosen.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=20\n    The total number of features.\n\nn_classes : int, default=5\n    The number of classes of the classification problem.\n\nn_labels : int, default=2\n    The average number of labels per instance. More precisely, the number\n    of labels per sample is drawn from a Poisson distribution with\n    ``n_labels`` as its expected value, but samples are bounded (using\n    rejection sampling) by ``n_classes``, and must be nonzero if\n    ``allow_unlabeled`` is False.\n\nlength : int, default=50\n    The sum of the features (number of words if documents) is drawn from\n    a Poisson distribution with this expected value.\n\nallow_unlabeled : bool, default=True\n    If ``True``, some instances might not belong to any class.\n\nsparse : bool, default=False\n    If ``True``, return a sparse feature matrix\n\n    .. versionadded:: 0.17\n       parameter to allow *sparse* output.\n\nreturn_indicator : {'dense', 'sparse'} or False, default='dense'\n    If ``'dense'`` return ``Y`` in the dense binary indicator format. If\n    ``'sparse'`` return ``Y`` in the sparse binary indicator format.\n    ``False`` returns a list of lists of labels.\n\nreturn_distributions : bool, default=False\n    If ``True``, return the prior class probability and conditional\n    probabilities of features given classes, from which the data was\n    drawn.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The generated samples.\n\nY : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n    The label sets. Sparse matrix should be of CSR format.\n\np_c : ndarray of shape (n_classes,)\n    The probability of each class being drawn. Only returned if\n    ``return_distributions=True``.\n\np_w_c : ndarray of shape (n_features, n_classes)\n    The probability of each feature being drawn given each class.\n    Only returned if ``return_distributions=True``.",
      "code": "@_deprecate_positional_args\ndef make_multilabel_classification(n_samples=100, n_features=20, *,\n                                   n_classes=5,\n                                   n_labels=2, length=50, allow_unlabeled=True,\n                                   sparse=False, return_indicator='dense',\n                                   return_distributions=False,\n                                   random_state=None):\n    \"\"\"Generate a random multilabel classification problem.\n\n    For each sample, the generative process is:\n        - pick the number of labels: n ~ Poisson(n_labels)\n        - n times, choose a class c: c ~ Multinomial(theta)\n        - pick the document length: k ~ Poisson(length)\n        - k times, choose a word: w ~ Multinomial(theta_c)\n\n    In the above process, rejection sampling is used to make sure that\n    n is never zero or more than `n_classes`, and that the document length\n    is never zero. Likewise, we reject classes which have already been chosen.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    n_features : int, default=20\n        The total number of features.\n\n    n_classes : int, default=5\n        The number of classes of the classification problem.\n\n    n_labels : int, default=2\n        The average number of labels per instance. More precisely, the number\n        of labels per sample is drawn from a Poisson distribution with\n        ``n_labels`` as its expected value, but samples are bounded (using\n        rejection sampling) by ``n_classes``, and must be nonzero if\n        ``allow_unlabeled`` is False.\n\n    length : int, default=50\n        The sum of the features (number of words if documents) is drawn from\n        a Poisson distribution with this expected value.\n\n    allow_unlabeled : bool, default=True\n        If ``True``, some instances might not belong to any class.\n\n    sparse : bool, default=False\n        If ``True``, return a sparse feature matrix\n\n        .. versionadded:: 0.17\n           parameter to allow *sparse* output.\n\n    return_indicator : {'dense', 'sparse'} or False, default='dense'\n        If ``'dense'`` return ``Y`` in the dense binary indicator format. If\n        ``'sparse'`` return ``Y`` in the sparse binary indicator format.\n        ``False`` returns a list of lists of labels.\n\n    return_distributions : bool, default=False\n        If ``True``, return the prior class probability and conditional\n        probabilities of features given classes, from which the data was\n        drawn.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The generated samples.\n\n    Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n        The label sets. Sparse matrix should be of CSR format.\n\n    p_c : ndarray of shape (n_classes,)\n        The probability of each class being drawn. Only returned if\n        ``return_distributions=True``.\n\n    p_w_c : ndarray of shape (n_features, n_classes)\n        The probability of each feature being drawn given each class.\n        Only returned if ``return_distributions=True``.\n\n    \"\"\"\n    if n_classes < 1:\n        raise ValueError(\n            \"'n_classes' should be an integer greater than 0. Got {} instead.\"\n            .format(n_classes)\n            )\n    if length < 1:\n        raise ValueError(\n            \"'length' should be an integer greater than 0. Got {} instead.\"\n            .format(length)\n            )\n\n    generator = check_random_state(random_state)\n    p_c = generator.rand(n_classes)\n    p_c /= p_c.sum()\n    cumulative_p_c = np.cumsum(p_c)\n    p_w_c = generator.rand(n_features, n_classes)\n    p_w_c /= np.sum(p_w_c, axis=0)\n\n    def sample_example():\n        _, n_classes = p_w_c.shape\n\n        # pick a nonzero number of labels per document by rejection sampling\n        y_size = n_classes + 1\n        while (not allow_unlabeled and y_size == 0) or y_size > n_classes:\n            y_size = generator.poisson(n_labels)\n\n        # pick n classes\n        y = set()\n        while len(y) != y_size:\n            # pick a class with probability P(c)\n            c = np.searchsorted(cumulative_p_c,\n                                generator.rand(y_size - len(y)))\n            y.update(c)\n        y = list(y)\n\n        # pick a non-zero document length by rejection sampling\n        n_words = 0\n        while n_words == 0:\n            n_words = generator.poisson(length)\n\n        # generate a document of length n_words\n        if len(y) == 0:\n            # if sample does not belong to any class, generate noise word\n            words = generator.randint(n_features, size=n_words)\n            return words, y\n\n        # sample words with replacement from selected classes\n        cumulative_p_w_sample = p_w_c.take(y, axis=1).sum(axis=1).cumsum()\n        cumulative_p_w_sample /= cumulative_p_w_sample[-1]\n        words = np.searchsorted(cumulative_p_w_sample, generator.rand(n_words))\n        return words, y\n\n    X_indices = array.array('i')\n    X_indptr = array.array('i', [0])\n    Y = []\n    for i in range(n_samples):\n        words, y = sample_example()\n        X_indices.extend(words)\n        X_indptr.append(len(X_indices))\n        Y.append(y)\n    X_data = np.ones(len(X_indices), dtype=np.float64)\n    X = sp.csr_matrix((X_data, X_indices, X_indptr),\n                      shape=(n_samples, n_features))\n    X.sum_duplicates()\n    if not sparse:\n        X = X.toarray()\n\n    # return_indicator can be True due to backward compatibility\n    if return_indicator in (True, 'sparse', 'dense'):\n        lb = MultiLabelBinarizer(sparse_output=(return_indicator == 'sparse'))\n        Y = lb.fit([range(n_classes)]).transform(Y)\n    elif return_indicator is not False:\n        raise ValueError(\"return_indicator must be either 'sparse', 'dense' \"\n                         'or False.')\n    if return_distributions:\n        return X, Y, p_c, p_w_c\n    return X, Y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression",
      "name": "make_regression",
      "qname": "sklearn.datasets._samples_generator.make_regression",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_regression.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_regression.n_features",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/n_informative",
          "name": "n_informative",
          "qname": "sklearn.datasets._samples_generator.make_regression.n_informative",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of informative features, i.e., the number of features used\nto build the linear model used to generate the output."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/n_targets",
          "name": "n_targets",
          "qname": "sklearn.datasets._samples_generator.make_regression.n_targets",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "The number of regression targets, i.e., the dimension of the y output\nvector associated with a sample. By default, the output is a scalar."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/bias",
          "name": "bias",
          "qname": "sklearn.datasets._samples_generator.make_regression.bias",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The bias term in the underlying linear model."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/effective_rank",
          "name": "effective_rank",
          "qname": "sklearn.datasets._samples_generator.make_regression.effective_rank",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "if not None:\n    The approximate number of singular vectors required to explain most\n    of the input data by linear combinations. Using this kind of\n    singular spectrum in the input allows the generator to reproduce\n    the correlations often observed in practice.\nif None:\n    The input set is well conditioned, centered and gaussian with\n    unit variance."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/tail_strength",
          "name": "tail_strength",
          "qname": "sklearn.datasets._samples_generator.make_regression.tail_strength",
          "default_value": "0.5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.5",
            "description": "The relative importance of the fat noisy tail of the singular values\nprofile if `effective_rank` is not None. When a float, it should be\nbetween 0 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_regression.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise applied to the output."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._samples_generator.make_regression.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Shuffle the samples and the features."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/coef",
          "name": "coef",
          "qname": "sklearn.datasets._samples_generator.make_regression.coef",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the coefficients of the underlying linear model are returned."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_regression/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_regression.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a random regression problem.\n\nThe input set can either be well conditioned (by default) or have a low\nrank-fat tail singular profile. See :func:`make_low_rank_matrix` for\nmore details.\n\nThe output is generated by applying a (potentially biased) random linear\nregression model with `n_informative` nonzero regressors to the previously\ngenerated input and some gaussian centered noise with some adjustable\nscale.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a random regression problem.\n\nThe input set can either be well conditioned (by default) or have a low\nrank-fat tail singular profile. See :func:`make_low_rank_matrix` for\nmore details.\n\nThe output is generated by applying a (potentially biased) random linear\nregression model with `n_informative` nonzero regressors to the previously\ngenerated input and some gaussian centered noise with some adjustable\nscale.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=100\n    The number of features.\n\nn_informative : int, default=10\n    The number of informative features, i.e., the number of features used\n    to build the linear model used to generate the output.\n\nn_targets : int, default=1\n    The number of regression targets, i.e., the dimension of the y output\n    vector associated with a sample. By default, the output is a scalar.\n\nbias : float, default=0.0\n    The bias term in the underlying linear model.\n\neffective_rank : int, default=None\n    if not None:\n        The approximate number of singular vectors required to explain most\n        of the input data by linear combinations. Using this kind of\n        singular spectrum in the input allows the generator to reproduce\n        the correlations often observed in practice.\n    if None:\n        The input set is well conditioned, centered and gaussian with\n        unit variance.\n\ntail_strength : float, default=0.5\n    The relative importance of the fat noisy tail of the singular values\n    profile if `effective_rank` is not None. When a float, it should be\n    between 0 and 1.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise applied to the output.\n\nshuffle : bool, default=True\n    Shuffle the samples and the features.\n\ncoef : bool, default=False\n    If True, the coefficients of the underlying linear model are returned.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The input samples.\n\ny : ndarray of shape (n_samples,) or (n_samples, n_targets)\n    The output values.\n\ncoef : ndarray of shape (n_features,) or (n_features, n_targets)\n    The coefficient of the underlying linear model. It is returned only if\n    coef is True.",
      "code": "@_deprecate_positional_args\ndef make_regression(n_samples=100, n_features=100, *, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    n_features : int, default=100\n        The number of features.\n\n    n_informative : int, default=10\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, default=1\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, default=0.0\n        The bias term in the underlying linear model.\n\n    effective_rank : int, default=None\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float, default=0.5\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None. When a float, it should be\n        between 0 and 1.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : bool, default=True\n        Shuffle the samples and the features.\n\n    coef : bool, default=False\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The input samples.\n\n    y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n        The output values.\n\n    coef : ndarray of shape (n_features,) or (n_features, n_targets)\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"\n    n_informative = min(n_features, n_informative)\n    generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples,\n                                 n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non\n    # zeros (the other features are not correlated to y and should be ignored\n    # by a sparsifying regularizers such as L1 or elastic net)\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative,\n                                                           n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Randomly permute samples and features\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n        indices = np.arange(n_features)\n        generator.shuffle(indices)\n        X[:, :] = X[:, indices]\n        ground_truth = ground_truth[indices]\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    else:\n        return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_s_curve",
      "name": "make_s_curve",
      "qname": "sklearn.datasets._samples_generator.make_s_curve",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_s_curve/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_s_curve.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of sample points on the S curve."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_s_curve/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_s_curve.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_s_curve/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_s_curve.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate an S curve dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate an S curve dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of sample points on the S curve.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 3)\n    The points.\n\nt : ndarray of shape (n_samples,)\n    The univariate position of the sample according to the main dimension\n    of the points in the manifold.",
      "code": "@_deprecate_positional_args\ndef make_s_curve(n_samples=100, *, noise=0.0, random_state=None):\n    \"\"\"Generate an S curve dataset.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of sample points on the S curve.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 3)\n        The points.\n\n    t : ndarray of shape (n_samples,)\n        The univariate position of the sample according to the main dimension\n        of the points in the manifold.\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    t = 3 * np.pi * (generator.rand(1, n_samples) - 0.5)\n    x = np.sin(t)\n    y = 2.0 * generator.rand(1, n_samples)\n    z = np.sign(t) * (np.cos(t) - 1)\n\n    X = np.concatenate((x, y, z))\n    X += noise * generator.randn(3, n_samples)\n    X = X.T\n    t = np.squeeze(t)\n\n    return X, t"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_coded_signal",
      "name": "make_sparse_coded_signal",
      "qname": "sklearn.datasets._samples_generator.make_sparse_coded_signal",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_coded_signal/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_sparse_coded_signal.n_samples",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Number of samples to generate"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_coded_signal/n_components",
          "name": "n_components",
          "qname": "sklearn.datasets._samples_generator.make_sparse_coded_signal.n_components",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Number of components in the dictionary"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_coded_signal/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_sparse_coded_signal.n_features",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Number of features of the dataset to generate"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_coded_signal/n_nonzero_coefs",
          "name": "n_nonzero_coefs",
          "qname": "sklearn.datasets._samples_generator.make_sparse_coded_signal.n_nonzero_coefs",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Number of active (non-zero) coefficients in each sample"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_coded_signal/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_sparse_coded_signal.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a signal as a sparse combination of dictionary elements.\n\nReturns a matrix Y = DX, such as D is (n_features, n_components),\nX is (n_components, n_samples) and each column of X has exactly\nn_nonzero_coefs non-zero elements.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a signal as a sparse combination of dictionary elements.\n\nReturns a matrix Y = DX, such as D is (n_features, n_components),\nX is (n_components, n_samples) and each column of X has exactly\nn_nonzero_coefs non-zero elements.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int\n    Number of samples to generate\n\nn_components : int\n    Number of components in the dictionary\n\nn_features : int\n    Number of features of the dataset to generate\n\nn_nonzero_coefs : int\n    Number of active (non-zero) coefficients in each sample\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\ndata : ndarray of shape (n_features, n_samples)\n    The encoded signal (Y).\n\ndictionary : ndarray of shape (n_features, n_components)\n    The dictionary with normalized components (D).\n\ncode : ndarray of shape (n_components, n_samples)\n    The sparse code such that each column of this matrix has exactly\n    n_nonzero_coefs non-zero items (X).",
      "code": "@_deprecate_positional_args\ndef make_sparse_coded_signal(n_samples, *, n_components, n_features,\n                             n_nonzero_coefs, random_state=None):\n    \"\"\"Generate a signal as a sparse combination of dictionary elements.\n\n    Returns a matrix Y = DX, such as D is (n_features, n_components),\n    X is (n_components, n_samples) and each column of X has exactly\n    n_nonzero_coefs non-zero elements.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples to generate\n\n    n_components : int\n        Number of components in the dictionary\n\n    n_features : int\n        Number of features of the dataset to generate\n\n    n_nonzero_coefs : int\n        Number of active (non-zero) coefficients in each sample\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    data : ndarray of shape (n_features, n_samples)\n        The encoded signal (Y).\n\n    dictionary : ndarray of shape (n_features, n_components)\n        The dictionary with normalized components (D).\n\n    code : ndarray of shape (n_components, n_samples)\n        The sparse code such that each column of this matrix has exactly\n        n_nonzero_coefs non-zero items (X).\n\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    # generate dictionary\n    D = generator.randn(n_features, n_components)\n    D /= np.sqrt(np.sum((D ** 2), axis=0))\n\n    # generate code\n    X = np.zeros((n_components, n_samples))\n    for i in range(n_samples):\n        idx = np.arange(n_components)\n        generator.shuffle(idx)\n        idx = idx[:n_nonzero_coefs]\n        X[idx, i] = generator.randn(n_nonzero_coefs)\n\n    # encode signal\n    Y = np.dot(D, X)\n\n    return map(np.squeeze, (Y, D, X))"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix",
      "name": "make_sparse_spd_matrix",
      "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix/dim",
          "name": "dim",
          "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix.dim",
          "default_value": "1",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "The size of the random matrix to generate."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix/alpha",
          "name": "alpha",
          "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix.alpha",
          "default_value": "0.95",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.95",
            "description": "The probability that a coefficient is zero (see notes). Larger values\nenforce more sparsity. The value should be in the range 0 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix/norm_diag",
          "name": "norm_diag",
          "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix.norm_diag",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to normalize the output matrix to make the leading diagonal\nelements all 1"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix/smallest_coef",
          "name": "smallest_coef",
          "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix.smallest_coef",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The value of the smallest coefficient between 0 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix/largest_coef",
          "name": "largest_coef",
          "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix.largest_coef",
          "default_value": "0.9",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.9",
            "description": "The value of the largest coefficient between 0 and 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_spd_matrix/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_sparse_spd_matrix.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a sparse symmetric definite positive matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a sparse symmetric definite positive matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\ndim : int, default=1\n    The size of the random matrix to generate.\n\nalpha : float, default=0.95\n    The probability that a coefficient is zero (see notes). Larger values\n    enforce more sparsity. The value should be in the range 0 and 1.\n\nnorm_diag : bool, default=False\n    Whether to normalize the output matrix to make the leading diagonal\n    elements all 1\n\nsmallest_coef : float, default=0.1\n    The value of the smallest coefficient between 0 and 1.\n\nlargest_coef : float, default=0.9\n    The value of the largest coefficient between 0 and 1.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nprec : sparse matrix of shape (dim, dim)\n    The generated matrix.\n\nNotes\n-----\nThe sparsity is actually imposed on the cholesky factor of the matrix.\nThus alpha does not translate directly into the filling fraction of\nthe matrix itself.\n\nSee Also\n--------\nmake_spd_matrix",
      "code": "@_deprecate_positional_args\ndef make_sparse_spd_matrix(dim=1, *, alpha=0.95, norm_diag=False,\n                           smallest_coef=.1, largest_coef=.9,\n                           random_state=None):\n    \"\"\"Generate a sparse symmetric definite positive matrix.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    dim : int, default=1\n        The size of the random matrix to generate.\n\n    alpha : float, default=0.95\n        The probability that a coefficient is zero (see notes). Larger values\n        enforce more sparsity. The value should be in the range 0 and 1.\n\n    norm_diag : bool, default=False\n        Whether to normalize the output matrix to make the leading diagonal\n        elements all 1\n\n    smallest_coef : float, default=0.1\n        The value of the smallest coefficient between 0 and 1.\n\n    largest_coef : float, default=0.9\n        The value of the largest coefficient between 0 and 1.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    prec : sparse matrix of shape (dim, dim)\n        The generated matrix.\n\n    Notes\n    -----\n    The sparsity is actually imposed on the cholesky factor of the matrix.\n    Thus alpha does not translate directly into the filling fraction of\n    the matrix itself.\n\n    See Also\n    --------\n    make_spd_matrix\n    \"\"\"\n    random_state = check_random_state(random_state)\n\n    chol = -np.eye(dim)\n    aux = random_state.rand(dim, dim)\n    aux[aux < alpha] = 0\n    aux[aux > alpha] = (smallest_coef\n                        + (largest_coef - smallest_coef)\n                        * random_state.rand(np.sum(aux > alpha)))\n    aux = np.tril(aux, k=-1)\n\n    # Permute the lines: we don't want to have asymmetries in the final\n    # SPD matrix\n    permutation = random_state.permutation(dim)\n    aux = aux[permutation].T[permutation]\n    chol += aux\n    prec = np.dot(chol.T, chol)\n\n    if norm_diag:\n        # Form the diagonal vector into a row matrix\n        d = np.diag(prec).reshape(1, prec.shape[0])\n        d = 1. / np.sqrt(d)\n\n        prec *= d\n        prec *= d.T\n\n    return prec"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_uncorrelated",
      "name": "make_sparse_uncorrelated",
      "qname": "sklearn.datasets._samples_generator.make_sparse_uncorrelated",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_uncorrelated/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_sparse_uncorrelated.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_uncorrelated/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._samples_generator.make_sparse_uncorrelated.n_features",
          "default_value": "10",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_sparse_uncorrelated/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_sparse_uncorrelated.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a random regression problem with sparse uncorrelated design.\n\nThis dataset is described in Celeux et al [1]. as::\n\n    X ~ N(0, 1)\n    y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n\nOnly the first 4 features are informative. The remaining features are\nuseless.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a random regression problem with sparse uncorrelated design.\n\nThis dataset is described in Celeux et al [1]. as::\n\n    X ~ N(0, 1)\n    y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n\nOnly the first 4 features are informative. The remaining features are\nuseless.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of samples.\n\nn_features : int, default=10\n    The number of features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, n_features)\n    The input samples.\n\ny : ndarray of shape (n_samples,)\n    The output values.\n\nReferences\n----------\n.. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,\n       \"Regularization in regression: comparing Bayesian and frequentist\n       methods in a poorly informative situation\", 2009.",
      "code": "@_deprecate_positional_args\ndef make_sparse_uncorrelated(n_samples=100, n_features=10, *,\n                             random_state=None):\n    \"\"\"Generate a random regression problem with sparse uncorrelated design.\n\n    This dataset is described in Celeux et al [1]. as::\n\n        X ~ N(0, 1)\n        y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n\n    Only the first 4 features are informative. The remaining features are\n    useless.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of samples.\n\n    n_features : int, default=10\n        The number of features.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_features)\n        The input samples.\n\n    y : ndarray of shape (n_samples,)\n        The output values.\n\n    References\n    ----------\n    .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,\n           \"Regularization in regression: comparing Bayesian and frequentist\n           methods in a poorly informative situation\", 2009.\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    X = generator.normal(loc=0, scale=1, size=(n_samples, n_features))\n    y = generator.normal(loc=(X[:, 0] +\n                              2 * X[:, 1] -\n                              2 * X[:, 2] -\n                              1.5 * X[:, 3]), scale=np.ones(n_samples))\n\n    return X, y"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_spd_matrix",
      "name": "make_spd_matrix",
      "qname": "sklearn.datasets._samples_generator.make_spd_matrix",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_spd_matrix/n_dim",
          "name": "n_dim",
          "qname": "sklearn.datasets._samples_generator.make_spd_matrix.n_dim",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "The matrix dimension."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_spd_matrix/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_spd_matrix.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a random symmetric, positive-definite matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a random symmetric, positive-definite matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_dim : int\n    The matrix dimension.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_dim, n_dim)\n    The random symmetric, positive-definite matrix.\n\nSee Also\n--------\nmake_sparse_spd_matrix",
      "code": "@_deprecate_positional_args\ndef make_spd_matrix(n_dim, *, random_state=None):\n    \"\"\"Generate a random symmetric, positive-definite matrix.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_dim : int\n        The matrix dimension.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_dim, n_dim)\n        The random symmetric, positive-definite matrix.\n\n    See Also\n    --------\n    make_sparse_spd_matrix\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    A = generator.rand(n_dim, n_dim)\n    U, _, Vt = linalg.svd(np.dot(A.T, A), check_finite=False)\n    X = np.dot(np.dot(U, 1.0 + np.diag(generator.rand(n_dim))), Vt)\n\n    return X"
    },
    {
      "id": "scikit-learn/sklearn.datasets._samples_generator/make_swiss_roll",
      "name": "make_swiss_roll",
      "qname": "sklearn.datasets._samples_generator.make_swiss_roll",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_swiss_roll/n_samples",
          "name": "n_samples",
          "qname": "sklearn.datasets._samples_generator.make_swiss_roll.n_samples",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of sample points on the S curve."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_swiss_roll/noise",
          "name": "noise",
          "qname": "sklearn.datasets._samples_generator.make_swiss_roll.noise",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The standard deviation of the gaussian noise."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._samples_generator/make_swiss_roll/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._samples_generator.make_swiss_roll.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset creation. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Generate a swiss roll dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.",
      "docstring": "Generate a swiss roll dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters\n----------\nn_samples : int, default=100\n    The number of sample points on the S curve.\n\nnoise : float, default=0.0\n    The standard deviation of the gaussian noise.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset creation. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nX : ndarray of shape (n_samples, 3)\n    The points.\n\nt : ndarray of shape (n_samples,)\n    The univariate position of the sample according to the main dimension\n    of the points in the manifold.\n\nNotes\n-----\nThe algorithm is from Marsland [1].\n\nReferences\n----------\n.. [1] S. Marsland, \"Machine Learning: An Algorithmic Perspective\",\n       Chapter 10, 2009.\n       http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/lle.py",
      "code": "@_deprecate_positional_args\ndef make_swiss_roll(n_samples=100, *, noise=0.0, random_state=None):\n    \"\"\"Generate a swiss roll dataset.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        The number of sample points on the S curve.\n\n    noise : float, default=0.0\n        The standard deviation of the gaussian noise.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, 3)\n        The points.\n\n    t : ndarray of shape (n_samples,)\n        The univariate position of the sample according to the main dimension\n        of the points in the manifold.\n\n    Notes\n    -----\n    The algorithm is from Marsland [1].\n\n    References\n    ----------\n    .. [1] S. Marsland, \"Machine Learning: An Algorithmic Perspective\",\n           Chapter 10, 2009.\n           http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/lle.py\n    \"\"\"\n    generator = check_random_state(random_state)\n\n    t = 1.5 * np.pi * (1 + 2 * generator.rand(1, n_samples))\n    x = t * np.cos(t)\n    y = 21 * generator.rand(1, n_samples)\n    z = t * np.sin(t)\n\n    X = np.concatenate((x, y, z))\n    X += noise * generator.randn(3, n_samples)\n    X = X.T\n    t = np.squeeze(t)\n\n    return X, t"
    },
    {
      "id": "scikit-learn/sklearn.datasets._species_distributions/fetch_species_distributions",
      "name": "fetch_species_distributions",
      "qname": "sklearn.datasets._species_distributions.fetch_species_distributions",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._species_distributions/fetch_species_distributions/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._species_distributions.fetch_species_distributions.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify another download and cache folder for the datasets. By default\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._species_distributions/fetch_species_distributions/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._species_distributions.fetch_species_distributions.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise a IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Loader for species distribution dataset from Phillips et. al. (2006)\n\nRead more in the :ref:`User Guide <datasets>`.",
      "docstring": "Loader for species distribution dataset from Phillips et. al. (2006)\n\nRead more in the :ref:`User Guide <datasets>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify another download and cache folder for the datasets. By default\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise a IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nReturns\n-------\ndata : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    coverages : array, shape = [14, 1592, 1212]\n        These represent the 14 features measured\n        at each point of the map grid.\n        The latitude/longitude values for the grid are discussed below.\n        Missing data is represented by the value -9999.\n    train : record array, shape = (1624,)\n        The training points for the data.  Each point has three fields:\n\n        - train['species'] is the species name\n        - train['dd long'] is the longitude, in degrees\n        - train['dd lat'] is the latitude, in degrees\n    test : record array, shape = (620,)\n        The test points for the data.  Same format as the training data.\n    Nx, Ny : integers\n        The number of longitudes (x) and latitudes (y) in the grid\n    x_left_lower_corner, y_left_lower_corner : floats\n        The (x,y) position of the lower-left corner, in degrees\n    grid_size : float\n        The spacing between points of the grid, in degrees\n\nReferences\n----------\n\n* `\"Maximum entropy modeling of species geographic distributions\"\n  <http://rob.schapire.net/papers/ecolmod.pdf>`_\n  S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n  190:231-259, 2006.\n\nNotes\n-----\n\nThis dataset represents the geographic distribution of species.\nThe dataset is provided by Phillips et. al. (2006).\n\nThe two species are:\n\n- `\"Bradypus variegatus\"\n  <http://www.iucnredlist.org/details/3038/0>`_ ,\n  the Brown-throated Sloth.\n\n- `\"Microryzomys minutus\"\n  <http://www.iucnredlist.org/details/13408/0>`_ ,\n  also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n  Colombia, Ecuador, Peru, and Venezuela.\n\n- For an example of using this dataset with scikit-learn, see\n  :ref:`examples/applications/plot_species_distribution_modeling.py\n  <sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py>`.",
      "code": "@_deprecate_positional_args\ndef fetch_species_distributions(*, data_home=None,\n                                download_if_missing=True):\n    \"\"\"Loader for species distribution dataset from Phillips et. al. (2006)\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify another download and cache folder for the datasets. By default\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        coverages : array, shape = [14, 1592, 1212]\n            These represent the 14 features measured\n            at each point of the map grid.\n            The latitude/longitude values for the grid are discussed below.\n            Missing data is represented by the value -9999.\n        train : record array, shape = (1624,)\n            The training points for the data.  Each point has three fields:\n\n            - train['species'] is the species name\n            - train['dd long'] is the longitude, in degrees\n            - train['dd lat'] is the latitude, in degrees\n        test : record array, shape = (620,)\n            The test points for the data.  Same format as the training data.\n        Nx, Ny : integers\n            The number of longitudes (x) and latitudes (y) in the grid\n        x_left_lower_corner, y_left_lower_corner : floats\n            The (x,y) position of the lower-left corner, in degrees\n        grid_size : float\n            The spacing between points of the grid, in degrees\n\n    References\n    ----------\n\n    * `\"Maximum entropy modeling of species geographic distributions\"\n      <http://rob.schapire.net/papers/ecolmod.pdf>`_\n      S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n      190:231-259, 2006.\n\n    Notes\n    -----\n\n    This dataset represents the geographic distribution of species.\n    The dataset is provided by Phillips et. al. (2006).\n\n    The two species are:\n\n    - `\"Bradypus variegatus\"\n      <http://www.iucnredlist.org/details/3038/0>`_ ,\n      the Brown-throated Sloth.\n\n    - `\"Microryzomys minutus\"\n      <http://www.iucnredlist.org/details/13408/0>`_ ,\n      also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n      Colombia, Ecuador, Peru, and Venezuela.\n\n    - For an example of using this dataset with scikit-learn, see\n      :ref:`examples/applications/plot_species_distribution_modeling.py\n      <sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py>`.\n    \"\"\"\n    data_home = get_data_home(data_home)\n    if not exists(data_home):\n        makedirs(data_home)\n\n    # Define parameters for the data files.  These should not be changed\n    # unless the data model changes.  They will be saved in the npz file\n    # with the downloaded data.\n    extra_params = dict(x_left_lower_corner=-94.8,\n                        Nx=1212,\n                        y_left_lower_corner=-56.05,\n                        Ny=1592,\n                        grid_size=0.05)\n    dtype = np.int16\n\n    archive_path = _pkl_filepath(data_home, DATA_ARCHIVE_NAME)\n\n    if not exists(archive_path):\n        if not download_if_missing:\n            raise IOError(\"Data not found and `download_if_missing` is False\")\n        logger.info('Downloading species data from %s to %s' % (\n            SAMPLES.url, data_home))\n        samples_path = _fetch_remote(SAMPLES, dirname=data_home)\n        with np.load(samples_path) as X:  # samples.zip is a valid npz\n            for f in X.files:\n                fhandle = BytesIO(X[f])\n                if 'train' in f:\n                    train = _load_csv(fhandle)\n                if 'test' in f:\n                    test = _load_csv(fhandle)\n        remove(samples_path)\n\n        logger.info('Downloading coverage data from %s to %s' % (\n            COVERAGES.url, data_home))\n        coverages_path = _fetch_remote(COVERAGES, dirname=data_home)\n        with np.load(coverages_path) as X:  # coverages.zip is a valid npz\n            coverages = []\n            for f in X.files:\n                fhandle = BytesIO(X[f])\n                logger.debug(' - converting {}'.format(f))\n                coverages.append(_load_coverage(fhandle))\n            coverages = np.asarray(coverages, dtype=dtype)\n        remove(coverages_path)\n\n        bunch = Bunch(coverages=coverages,\n                      test=test,\n                      train=train,\n                      **extra_params)\n        joblib.dump(bunch, archive_path, compress=9)\n    else:\n        bunch = joblib.load(archive_path)\n\n    return bunch"
    },
    {
      "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file",
      "name": "dump_svmlight_file",
      "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/X",
          "name": "X",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/y",
          "name": "y",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix}, shape = [n_samples (, n_labels)]",
            "default_value": "",
            "description": "Target values. Class labels must be an\ninteger or float, or array-like objects of integer or float for\nmultilabel classifications."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "shape = [n_samples (, n_labels)]"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/f",
          "name": "f",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.f",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "string or file-like in binary mode",
            "default_value": "",
            "description": "If string, specifies the path that will contain the data.\nIf file-like, data will be written to f. f should be opened in binary\nmode."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "string"
              },
              {
                "kind": "NamedType",
                "name": "file-like in binary mode"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/zero_based",
          "name": "zero_based",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.zero_based",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "boolean",
            "default_value": "True",
            "description": "Whether column indices should be written zero-based (True) or one-based\n(False)."
          },
          "type": {
            "kind": "NamedType",
            "name": "boolean"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/comment",
          "name": "comment",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.comment",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string",
            "default_value": "None",
            "description": "Comment to insert at the top of the file. This should be either a\nUnicode string, which will be encoded as UTF-8, or an ASCII byte\nstring.\nIf a comment is given, then it will be preceded by one that identifies\nthe file as having been dumped by scikit-learn. Note that not all\ntools grok comments in SVMlight files."
          },
          "type": {
            "kind": "NamedType",
            "name": "string"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/query_id",
          "name": "query_id",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.query_id",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Array containing pairwise preference constraints (qid in svmlight\nformat)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/dump_svmlight_file/multilabel",
          "name": "multilabel",
          "qname": "sklearn.datasets._svmlight_format_io.dump_svmlight_file.multilabel",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "boolean",
            "default_value": "False",
            "description": "Samples may have several labels each (see\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n.. versionadded:: 0.17\n   parameter *multilabel* to support multilabel datasets."
          },
          "type": {
            "kind": "NamedType",
            "name": "boolean"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Dump the dataset in svmlight / libsvm file format.\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.",
      "docstring": "Dump the dataset in svmlight / libsvm file format.\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : {array-like, sparse matrix}, shape = [n_samples (, n_labels)]\n    Target values. Class labels must be an\n    integer or float, or array-like objects of integer or float for\n    multilabel classifications.\n\nf : string or file-like in binary mode\n    If string, specifies the path that will contain the data.\n    If file-like, data will be written to f. f should be opened in binary\n    mode.\n\nzero_based : boolean, default=True\n    Whether column indices should be written zero-based (True) or one-based\n    (False).\n\ncomment : string, default=None\n    Comment to insert at the top of the file. This should be either a\n    Unicode string, which will be encoded as UTF-8, or an ASCII byte\n    string.\n    If a comment is given, then it will be preceded by one that identifies\n    the file as having been dumped by scikit-learn. Note that not all\n    tools grok comments in SVMlight files.\n\nquery_id : array-like of shape (n_samples,), default=None\n    Array containing pairwise preference constraints (qid in svmlight\n    format).\n\nmultilabel : boolean, default=False\n    Samples may have several labels each (see\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n    .. versionadded:: 0.17\n       parameter *multilabel* to support multilabel datasets.",
      "code": "@_deprecate_positional_args\ndef dump_svmlight_file(X, y, f, *, zero_based=True, comment=None,\n                       query_id=None,\n                       multilabel=False):\n    \"\"\"Dump the dataset in svmlight / libsvm file format.\n\n    This format is a text-based format, with one sample per line. It does\n    not store zero valued features hence is suitable for sparse dataset.\n\n    The first element of each line can be used to store a target variable\n    to predict.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training vectors, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : {array-like, sparse matrix}, shape = [n_samples (, n_labels)]\n        Target values. Class labels must be an\n        integer or float, or array-like objects of integer or float for\n        multilabel classifications.\n\n    f : string or file-like in binary mode\n        If string, specifies the path that will contain the data.\n        If file-like, data will be written to f. f should be opened in binary\n        mode.\n\n    zero_based : boolean, default=True\n        Whether column indices should be written zero-based (True) or one-based\n        (False).\n\n    comment : string, default=None\n        Comment to insert at the top of the file. This should be either a\n        Unicode string, which will be encoded as UTF-8, or an ASCII byte\n        string.\n        If a comment is given, then it will be preceded by one that identifies\n        the file as having been dumped by scikit-learn. Note that not all\n        tools grok comments in SVMlight files.\n\n    query_id : array-like of shape (n_samples,), default=None\n        Array containing pairwise preference constraints (qid in svmlight\n        format).\n\n    multilabel : boolean, default=False\n        Samples may have several labels each (see\n        https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n        .. versionadded:: 0.17\n           parameter *multilabel* to support multilabel datasets.\n    \"\"\"\n    if comment is not None:\n        # Convert comment string to list of lines in UTF-8.\n        # If a byte string is passed, then check whether it's ASCII;\n        # if a user wants to get fancy, they'll have to decode themselves.\n        # Avoid mention of str and unicode types for Python 3.x compat.\n        if isinstance(comment, bytes):\n            comment.decode(\"ascii\")  # just for the exception\n        else:\n            comment = comment.encode(\"utf-8\")\n        if b\"\\0\" in comment:\n            raise ValueError(\"comment string contains NUL byte\")\n\n    yval = check_array(y, accept_sparse='csr', ensure_2d=False)\n    if sp.issparse(yval):\n        if yval.shape[1] != 1 and not multilabel:\n            raise ValueError(\"expected y of shape (n_samples, 1),\"\n                             \" got %r\" % (yval.shape,))\n    else:\n        if yval.ndim != 1 and not multilabel:\n            raise ValueError(\"expected y of shape (n_samples,), got %r\"\n                             % (yval.shape,))\n\n    Xval = check_array(X, accept_sparse='csr')\n    if Xval.shape[0] != yval.shape[0]:\n        raise ValueError(\n            \"X.shape[0] and y.shape[0] should be the same, got\"\n            \" %r and %r instead.\" % (Xval.shape[0], yval.shape[0])\n        )\n\n    # We had some issues with CSR matrices with unsorted indices (e.g. #1501),\n    # so sort them here, but first make sure we don't modify the user's X.\n    # TODO We can do this cheaper; sorted_indices copies the whole matrix.\n    if yval is y and hasattr(yval, \"sorted_indices\"):\n        y = yval.sorted_indices()\n    else:\n        y = yval\n        if hasattr(y, \"sort_indices\"):\n            y.sort_indices()\n\n    if Xval is X and hasattr(Xval, \"sorted_indices\"):\n        X = Xval.sorted_indices()\n    else:\n        X = Xval\n        if hasattr(X, \"sort_indices\"):\n            X.sort_indices()\n\n    if query_id is not None:\n        query_id = np.asarray(query_id)\n        if query_id.shape[0] != y.shape[0]:\n            raise ValueError(\"expected query_id of shape (n_samples,), got %r\"\n                             % (query_id.shape,))\n\n    one_based = not zero_based\n\n    if hasattr(f, \"write\"):\n        _dump_svmlight(X, y, f, multilabel, one_based, comment, query_id)\n    else:\n        with open(f, \"wb\") as f:\n            _dump_svmlight(X, y, f, multilabel, one_based, comment, query_id)"
    },
    {
      "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file",
      "name": "load_svmlight_file",
      "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/f",
          "name": "f",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.f",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "str, file-like or int",
            "default_value": "",
            "description": "(Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\nbe uncompressed on the fly. If an integer is passed, it is assumed to\nbe a file descriptor. A file-like or file descriptor will not be closed\nby this function. A file-like object must be opened in binary mode."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "file-like"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.n_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of features to use. If None, it will be inferred. This\nargument is useful to load several files that are subsets of a\nbigger sliced dataset: each subset might not have examples of\nevery feature, hence the inferred shape might vary from one\nslice to another.\nn_features is only required if ``offset`` or ``length`` are passed a\nnon-default value."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/dtype",
          "name": "dtype",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.dtype",
          "default_value": "np.float64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "numpy data type",
            "default_value": "np.float64",
            "description": "Data type of dataset to be loaded. This will be the data type of the\noutput numpy arrays ``X`` and ``y``."
          },
          "type": {
            "kind": "NamedType",
            "name": "numpy data type"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/multilabel",
          "name": "multilabel",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.multilabel",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Samples may have several labels each (see\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/zero_based",
          "name": "zero_based",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.zero_based",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool or \"auto\"",
            "default_value": "\"auto\"",
            "description": "Whether column indices in f are zero-based (True) or one-based\n(False). If column indices are one-based, they are transformed to\nzero-based to match Python/NumPy conventions.\nIf set to \"auto\", a heuristic check is applied to determine this from\nthe file contents. Both kinds of files occur \"in the wild\", but they\nare unfortunately not self-identifying. Using \"auto\" or True should\nalways be safe when no ``offset`` or ``length`` is passed.\nIf ``offset`` or ``length`` are passed, the \"auto\" mode falls back\nto ``zero_based=True`` to avoid having the heuristic check yield\ninconsistent results on different segments of the file."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "bool"
              },
              {
                "kind": "NamedType",
                "name": "\"auto\""
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/query_id",
          "name": "query_id",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.query_id",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, will return the query_id array for each file."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/offset",
          "name": "offset",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.offset",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Ignore the offset first bytes by seeking forward, then\ndiscarding the following bytes up until the next new line\ncharacter."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_file/length",
          "name": "length",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_file.length",
          "default_value": "-1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "-1",
            "description": "If strictly positive, stop reading any new line of data once the\nposition in the file has reached the (offset + length) bytes threshold."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load datasets in the svmlight / libsvm format into sparse CSR matrix\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n\nThis format is used as the default format for both svmlight and the\nlibsvm command line programs.\n\nParsing a text based source can be expensive. When working on\nrepeatedly on the same dataset, it is recommended to wrap this\nloader with joblib.Memory.cache to store a memmapped backup of the\nCSR results of the first call and benefit from the near instantaneous\nloading of memmapped structures for the subsequent calls.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n\nThis implementation is written in Cython and is reasonably fast.\nHowever, a faster API-compatible loader is also available at:\n\n  https://github.com/mblondel/svmlight-loader",
      "docstring": "Load datasets in the svmlight / libsvm format into sparse CSR matrix\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n\nThis format is used as the default format for both svmlight and the\nlibsvm command line programs.\n\nParsing a text based source can be expensive. When working on\nrepeatedly on the same dataset, it is recommended to wrap this\nloader with joblib.Memory.cache to store a memmapped backup of the\nCSR results of the first call and benefit from the near instantaneous\nloading of memmapped structures for the subsequent calls.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n\nThis implementation is written in Cython and is reasonably fast.\nHowever, a faster API-compatible loader is also available at:\n\n  https://github.com/mblondel/svmlight-loader\n\nParameters\n----------\nf : str, file-like or int\n    (Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\n    be uncompressed on the fly. If an integer is passed, it is assumed to\n    be a file descriptor. A file-like or file descriptor will not be closed\n    by this function. A file-like object must be opened in binary mode.\n\nn_features : int, default=None\n    The number of features to use. If None, it will be inferred. This\n    argument is useful to load several files that are subsets of a\n    bigger sliced dataset: each subset might not have examples of\n    every feature, hence the inferred shape might vary from one\n    slice to another.\n    n_features is only required if ``offset`` or ``length`` are passed a\n    non-default value.\n\ndtype : numpy data type, default=np.float64\n    Data type of dataset to be loaded. This will be the data type of the\n    output numpy arrays ``X`` and ``y``.\n\nmultilabel : bool, default=False\n    Samples may have several labels each (see\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\nzero_based : bool or \"auto\", default=\"auto\"\n    Whether column indices in f are zero-based (True) or one-based\n    (False). If column indices are one-based, they are transformed to\n    zero-based to match Python/NumPy conventions.\n    If set to \"auto\", a heuristic check is applied to determine this from\n    the file contents. Both kinds of files occur \"in the wild\", but they\n    are unfortunately not self-identifying. Using \"auto\" or True should\n    always be safe when no ``offset`` or ``length`` is passed.\n    If ``offset`` or ``length`` are passed, the \"auto\" mode falls back\n    to ``zero_based=True`` to avoid having the heuristic check yield\n    inconsistent results on different segments of the file.\n\nquery_id : bool, default=False\n    If True, will return the query_id array for each file.\n\noffset : int, default=0\n    Ignore the offset first bytes by seeking forward, then\n    discarding the following bytes up until the next new line\n    character.\n\nlength : int, default=-1\n    If strictly positive, stop reading any new line of data once the\n    position in the file has reached the (offset + length) bytes threshold.\n\nReturns\n-------\nX : scipy.sparse matrix of shape (n_samples, n_features)\n\ny : ndarray of shape (n_samples,), or, in the multilabel a list of\n    tuples of length n_samples.\n\nquery_id : array of shape (n_samples,)\n   query_id for each sample. Only returned when query_id is set to\n   True.\n\nSee Also\n--------\nload_svmlight_files : Similar function for loading multiple files in this\n    format, enforcing the same number of features/columns on all of them.\n\nExamples\n--------\nTo use joblib.Memory to cache the svmlight file::\n\n    from joblib import Memory\n    from .datasets import load_svmlight_file\n    mem = Memory(\"./mycache\")\n\n    @mem.cache\n    def get_data():\n        data = load_svmlight_file(\"mysvmlightfile\")\n        return data[0], data[1]\n\n    X, y = get_data()",
      "code": "@_deprecate_positional_args\ndef load_svmlight_file(f, *, n_features=None, dtype=np.float64,\n                       multilabel=False, zero_based=\"auto\", query_id=False,\n                       offset=0, length=-1):\n    \"\"\"Load datasets in the svmlight / libsvm format into sparse CSR matrix\n\n    This format is a text-based format, with one sample per line. It does\n    not store zero valued features hence is suitable for sparse dataset.\n\n    The first element of each line can be used to store a target variable\n    to predict.\n\n    This format is used as the default format for both svmlight and the\n    libsvm command line programs.\n\n    Parsing a text based source can be expensive. When working on\n    repeatedly on the same dataset, it is recommended to wrap this\n    loader with joblib.Memory.cache to store a memmapped backup of the\n    CSR results of the first call and benefit from the near instantaneous\n    loading of memmapped structures for the subsequent calls.\n\n    In case the file contains a pairwise preference constraint (known\n    as \"qid\" in the svmlight format) these are ignored unless the\n    query_id parameter is set to True. These pairwise preference\n    constraints can be used to constraint the combination of samples\n    when using pairwise loss functions (as is the case in some\n    learning to rank problems) so that only pairs with the same\n    query_id value are considered.\n\n    This implementation is written in Cython and is reasonably fast.\n    However, a faster API-compatible loader is also available at:\n\n      https://github.com/mblondel/svmlight-loader\n\n    Parameters\n    ----------\n    f : str, file-like or int\n        (Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\n        be uncompressed on the fly. If an integer is passed, it is assumed to\n        be a file descriptor. A file-like or file descriptor will not be closed\n        by this function. A file-like object must be opened in binary mode.\n\n    n_features : int, default=None\n        The number of features to use. If None, it will be inferred. This\n        argument is useful to load several files that are subsets of a\n        bigger sliced dataset: each subset might not have examples of\n        every feature, hence the inferred shape might vary from one\n        slice to another.\n        n_features is only required if ``offset`` or ``length`` are passed a\n        non-default value.\n\n    dtype : numpy data type, default=np.float64\n        Data type of dataset to be loaded. This will be the data type of the\n        output numpy arrays ``X`` and ``y``.\n\n    multilabel : bool, default=False\n        Samples may have several labels each (see\n        https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n    zero_based : bool or \"auto\", default=\"auto\"\n        Whether column indices in f are zero-based (True) or one-based\n        (False). If column indices are one-based, they are transformed to\n        zero-based to match Python/NumPy conventions.\n        If set to \"auto\", a heuristic check is applied to determine this from\n        the file contents. Both kinds of files occur \"in the wild\", but they\n        are unfortunately not self-identifying. Using \"auto\" or True should\n        always be safe when no ``offset`` or ``length`` is passed.\n        If ``offset`` or ``length`` are passed, the \"auto\" mode falls back\n        to ``zero_based=True`` to avoid having the heuristic check yield\n        inconsistent results on different segments of the file.\n\n    query_id : bool, default=False\n        If True, will return the query_id array for each file.\n\n    offset : int, default=0\n        Ignore the offset first bytes by seeking forward, then\n        discarding the following bytes up until the next new line\n        character.\n\n    length : int, default=-1\n        If strictly positive, stop reading any new line of data once the\n        position in the file has reached the (offset + length) bytes threshold.\n\n    Returns\n    -------\n    X : scipy.sparse matrix of shape (n_samples, n_features)\n\n    y : ndarray of shape (n_samples,), or, in the multilabel a list of\n        tuples of length n_samples.\n\n    query_id : array of shape (n_samples,)\n       query_id for each sample. Only returned when query_id is set to\n       True.\n\n    See Also\n    --------\n    load_svmlight_files : Similar function for loading multiple files in this\n        format, enforcing the same number of features/columns on all of them.\n\n    Examples\n    --------\n    To use joblib.Memory to cache the svmlight file::\n\n        from joblib import Memory\n        from .datasets import load_svmlight_file\n        mem = Memory(\"./mycache\")\n\n        @mem.cache\n        def get_data():\n            data = load_svmlight_file(\"mysvmlightfile\")\n            return data[0], data[1]\n\n        X, y = get_data()\n    \"\"\"\n    return tuple(load_svmlight_files([f], n_features=n_features,\n                                     dtype=dtype,\n                                     multilabel=multilabel,\n                                     zero_based=zero_based,\n                                     query_id=query_id,\n                                     offset=offset,\n                                     length=length))"
    },
    {
      "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files",
      "name": "load_svmlight_files",
      "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/files",
          "name": "files",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.files",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, dtype=str, file-like or int",
            "default_value": "",
            "description": "(Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\nbe uncompressed on the fly. If an integer is passed, it is assumed to\nbe a file descriptor. File-likes and file descriptors will not be\nclosed by this function. File-like objects must be opened in binary\nmode."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "dtype=str"
              },
              {
                "kind": "NamedType",
                "name": "file-like"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/n_features",
          "name": "n_features",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.n_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of features to use. If None, it will be inferred from the\nmaximum column index occurring in any of the files.\n\nThis can be set to a higher value than the actual number of features\nin any of the input files, but setting it to a lower value will cause\nan exception to be raised."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/dtype",
          "name": "dtype",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.dtype",
          "default_value": "np.float64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "numpy data type",
            "default_value": "np.float64",
            "description": "Data type of dataset to be loaded. This will be the data type of the\noutput numpy arrays ``X`` and ``y``."
          },
          "type": {
            "kind": "NamedType",
            "name": "numpy data type"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/multilabel",
          "name": "multilabel",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.multilabel",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Samples may have several labels each (see\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/zero_based",
          "name": "zero_based",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.zero_based",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool or \"auto\"",
            "default_value": "\"auto\"",
            "description": "Whether column indices in f are zero-based (True) or one-based\n(False). If column indices are one-based, they are transformed to\nzero-based to match Python/NumPy conventions.\nIf set to \"auto\", a heuristic check is applied to determine this from\nthe file contents. Both kinds of files occur \"in the wild\", but they\nare unfortunately not self-identifying. Using \"auto\" or True should\nalways be safe when no offset or length is passed.\nIf offset or length are passed, the \"auto\" mode falls back\nto zero_based=True to avoid having the heuristic check yield\ninconsistent results on different segments of the file."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "bool"
              },
              {
                "kind": "NamedType",
                "name": "\"auto\""
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/query_id",
          "name": "query_id",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.query_id",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, will return the query_id array for each file."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/offset",
          "name": "offset",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.offset",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Ignore the offset first bytes by seeking forward, then\ndiscarding the following bytes up until the next new line\ncharacter."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._svmlight_format_io/load_svmlight_files/length",
          "name": "length",
          "qname": "sklearn.datasets._svmlight_format_io.load_svmlight_files.length",
          "default_value": "-1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "-1",
            "description": "If strictly positive, stop reading any new line of data once the\nposition in the file has reached the (offset + length) bytes threshold."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load dataset from multiple files in SVMlight format\n\nThis function is equivalent to mapping load_svmlight_file over a list of\nfiles, except that the results are concatenated into a single, flat list\nand the samples vectors are constrained to all have the same number of\nfeatures.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.",
      "docstring": "Load dataset from multiple files in SVMlight format\n\nThis function is equivalent to mapping load_svmlight_file over a list of\nfiles, except that the results are concatenated into a single, flat list\nand the samples vectors are constrained to all have the same number of\nfeatures.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n\nParameters\n----------\nfiles : array-like, dtype=str, file-like or int\n    (Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\n    be uncompressed on the fly. If an integer is passed, it is assumed to\n    be a file descriptor. File-likes and file descriptors will not be\n    closed by this function. File-like objects must be opened in binary\n    mode.\n\nn_features : int, default=None\n    The number of features to use. If None, it will be inferred from the\n    maximum column index occurring in any of the files.\n\n    This can be set to a higher value than the actual number of features\n    in any of the input files, but setting it to a lower value will cause\n    an exception to be raised.\n\ndtype : numpy data type, default=np.float64\n    Data type of dataset to be loaded. This will be the data type of the\n    output numpy arrays ``X`` and ``y``.\n\nmultilabel : bool, default=False\n    Samples may have several labels each (see\n    https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\nzero_based : bool or \"auto\", default=\"auto\"\n    Whether column indices in f are zero-based (True) or one-based\n    (False). If column indices are one-based, they are transformed to\n    zero-based to match Python/NumPy conventions.\n    If set to \"auto\", a heuristic check is applied to determine this from\n    the file contents. Both kinds of files occur \"in the wild\", but they\n    are unfortunately not self-identifying. Using \"auto\" or True should\n    always be safe when no offset or length is passed.\n    If offset or length are passed, the \"auto\" mode falls back\n    to zero_based=True to avoid having the heuristic check yield\n    inconsistent results on different segments of the file.\n\nquery_id : bool, default=False\n    If True, will return the query_id array for each file.\n\noffset : int, default=0\n    Ignore the offset first bytes by seeking forward, then\n    discarding the following bytes up until the next new line\n    character.\n\nlength : int, default=-1\n    If strictly positive, stop reading any new line of data once the\n    position in the file has reached the (offset + length) bytes threshold.\n\nReturns\n-------\n[X1, y1, ..., Xn, yn]\nwhere each (Xi, yi) pair is the result from load_svmlight_file(files[i]).\n\nIf query_id is set to True, this will return instead [X1, y1, q1,\n..., Xn, yn, qn] where (Xi, yi, qi) is the result from\nload_svmlight_file(files[i])\n\nNotes\n-----\nWhen fitting a model to a matrix X_train and evaluating it against a\nmatrix X_test, it is essential that X_train and X_test have the same\nnumber of features (X_train.shape[1] == X_test.shape[1]). This may not\nbe the case if you load the files individually with load_svmlight_file.\n\nSee Also\n--------\nload_svmlight_file",
      "code": "@_deprecate_positional_args\ndef load_svmlight_files(files, *, n_features=None, dtype=np.float64,\n                        multilabel=False, zero_based=\"auto\", query_id=False,\n                        offset=0, length=-1):\n    \"\"\"Load dataset from multiple files in SVMlight format\n\n    This function is equivalent to mapping load_svmlight_file over a list of\n    files, except that the results are concatenated into a single, flat list\n    and the samples vectors are constrained to all have the same number of\n    features.\n\n    In case the file contains a pairwise preference constraint (known\n    as \"qid\" in the svmlight format) these are ignored unless the\n    query_id parameter is set to True. These pairwise preference\n    constraints can be used to constraint the combination of samples\n    when using pairwise loss functions (as is the case in some\n    learning to rank problems) so that only pairs with the same\n    query_id value are considered.\n\n    Parameters\n    ----------\n    files : array-like, dtype=str, file-like or int\n        (Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\n        be uncompressed on the fly. If an integer is passed, it is assumed to\n        be a file descriptor. File-likes and file descriptors will not be\n        closed by this function. File-like objects must be opened in binary\n        mode.\n\n    n_features : int, default=None\n        The number of features to use. If None, it will be inferred from the\n        maximum column index occurring in any of the files.\n\n        This can be set to a higher value than the actual number of features\n        in any of the input files, but setting it to a lower value will cause\n        an exception to be raised.\n\n    dtype : numpy data type, default=np.float64\n        Data type of dataset to be loaded. This will be the data type of the\n        output numpy arrays ``X`` and ``y``.\n\n    multilabel : bool, default=False\n        Samples may have several labels each (see\n        https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n    zero_based : bool or \"auto\", default=\"auto\"\n        Whether column indices in f are zero-based (True) or one-based\n        (False). If column indices are one-based, they are transformed to\n        zero-based to match Python/NumPy conventions.\n        If set to \"auto\", a heuristic check is applied to determine this from\n        the file contents. Both kinds of files occur \"in the wild\", but they\n        are unfortunately not self-identifying. Using \"auto\" or True should\n        always be safe when no offset or length is passed.\n        If offset or length are passed, the \"auto\" mode falls back\n        to zero_based=True to avoid having the heuristic check yield\n        inconsistent results on different segments of the file.\n\n    query_id : bool, default=False\n        If True, will return the query_id array for each file.\n\n    offset : int, default=0\n        Ignore the offset first bytes by seeking forward, then\n        discarding the following bytes up until the next new line\n        character.\n\n    length : int, default=-1\n        If strictly positive, stop reading any new line of data once the\n        position in the file has reached the (offset + length) bytes threshold.\n\n    Returns\n    -------\n    [X1, y1, ..., Xn, yn]\n    where each (Xi, yi) pair is the result from load_svmlight_file(files[i]).\n\n    If query_id is set to True, this will return instead [X1, y1, q1,\n    ..., Xn, yn, qn] where (Xi, yi, qi) is the result from\n    load_svmlight_file(files[i])\n\n    Notes\n    -----\n    When fitting a model to a matrix X_train and evaluating it against a\n    matrix X_test, it is essential that X_train and X_test have the same\n    number of features (X_train.shape[1] == X_test.shape[1]). This may not\n    be the case if you load the files individually with load_svmlight_file.\n\n    See Also\n    --------\n    load_svmlight_file\n    \"\"\"\n    if (offset != 0 or length > 0) and zero_based == \"auto\":\n        # disable heuristic search to avoid getting inconsistent results on\n        # different segments of the file\n        zero_based = True\n\n    if (offset != 0 or length > 0) and n_features is None:\n        raise ValueError(\n            \"n_features is required when offset or length is specified.\")\n\n    r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n                        offset=offset, length=length)\n         for f in files]\n\n    if (zero_based is False or\n            zero_based == \"auto\" and all(len(tmp[1]) and np.min(tmp[1]) > 0\n                                         for tmp in r)):\n        for _, indices, _, _, _ in r:\n            indices -= 1\n\n    n_f = max(ind[1].max() if len(ind[1]) else 0 for ind in r) + 1\n\n    if n_features is None:\n        n_features = n_f\n    elif n_features < n_f:\n        raise ValueError(\"n_features was set to {},\"\n                         \" but input file contains {} features\"\n                         .format(n_features, n_f))\n\n    result = []\n    for data, indices, indptr, y, query_values in r:\n        shape = (indptr.shape[0] - 1, n_features)\n        X = sp.csr_matrix((data, indices, indptr), shape)\n        X.sort_indices()\n        result += X, y\n        if query_id:\n            result.append(query_values)\n\n    return result"
    },
    {
      "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups",
      "name": "fetch_20newsgroups",
      "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify a download and cache folder for the datasets. If None,\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/subset",
          "name": "subset",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.subset",
          "default_value": "'train'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'train', 'test', 'all'}",
            "default_value": "'train'",
            "description": "Select the dataset to load: 'train' for the training set, 'test'\nfor the test set, 'all' for both, with shuffled ordering."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "test",
              "train",
              "all"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/categories",
          "name": "categories",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.categories",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like, dtype=str or unicode",
            "default_value": "None",
            "description": "If None (default), load all the categories.\nIf not None, list of category names to load (other categories\nignored)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "dtype=str"
              },
              {
                "kind": "NamedType",
                "name": "unicode"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/shuffle",
          "name": "shuffle",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to shuffle the data: might be important for models that\nmake the assumption that the samples are independent and identically\ndistributed (i.i.d.), such as stochastic gradient descent."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/random_state",
          "name": "random_state",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.random_state",
          "default_value": "42",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for dataset shuffling. Pass an int\nfor reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/remove",
          "name": "remove",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.remove",
          "default_value": "()",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple",
            "default_value": "()",
            "description": "May contain any subset of ('headers', 'footers', 'quotes'). Each of\nthese are kinds of text that will be detected and removed from the\nnewsgroup posts, preventing classifiers from overfitting on\nmetadata.\n\n'headers' removes newsgroup headers, 'footers' removes blocks at the\nends of posts that look like signatures, and 'quotes' removes lines\nthat appear to be quoting another post.\n\n'headers' follows an exact standard; the other filters are not always\ncorrect."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise an IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns `(data.data, data.target)` instead of a Bunch\nobject.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load the filenames and data from the 20 newsgroups dataset (classification).\n\nDownload it if necessary.\n\n=================   ==========\nClasses                     20\nSamples total            18846\nDimensionality               1\nFeatures                  text\n=================   ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.",
      "docstring": "Load the filenames and data from the 20 newsgroups dataset (classification).\n\nDownload it if necessary.\n\n=================   ==========\nClasses                     20\nSamples total            18846\nDimensionality               1\nFeatures                  text\n=================   ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.\n\nParameters\n----------\ndata_home : str, default=None\n    Specify a download and cache folder for the datasets. If None,\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nsubset : {'train', 'test', 'all'}, default='train'\n    Select the dataset to load: 'train' for the training set, 'test'\n    for the test set, 'all' for both, with shuffled ordering.\n\ncategories : array-like, dtype=str or unicode, default=None\n    If None (default), load all the categories.\n    If not None, list of category names to load (other categories\n    ignored).\n\nshuffle : bool, default=True\n    Whether or not to shuffle the data: might be important for models that\n    make the assumption that the samples are independent and identically\n    distributed (i.i.d.), such as stochastic gradient descent.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for dataset shuffling. Pass an int\n    for reproducible output across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nremove : tuple, default=()\n    May contain any subset of ('headers', 'footers', 'quotes'). Each of\n    these are kinds of text that will be detected and removed from the\n    newsgroup posts, preventing classifiers from overfitting on\n    metadata.\n\n    'headers' removes newsgroup headers, 'footers' removes blocks at the\n    ends of posts that look like signatures, and 'quotes' removes lines\n    that appear to be quoting another post.\n\n    'headers' follows an exact standard; the other filters are not always\n    correct.\n\ndownload_if_missing : bool, default=True\n    If False, raise an IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns `(data.data, data.target)` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\nbunch : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data : list of shape (n_samples,)\n        The data list to learn.\n    target: ndarray of shape (n_samples,)\n        The target labels.\n    filenames: list of shape (n_samples,)\n        The path to the location of the data.\n    DESCR: str\n        The full description of the dataset.\n    target_names: list of shape (n_classes,)\n        The names of target classes.\n\n(data, target) : tuple if `return_X_y=True`\n    .. versionadded:: 0.22",
      "code": "@_deprecate_positional_args\ndef fetch_20newsgroups(*, data_home=None, subset='train', categories=None,\n                       shuffle=True, random_state=42,\n                       remove=(),\n                       download_if_missing=True, return_X_y=False):\n    \"\"\"Load the filenames and data from the 20 newsgroups dataset \\\n(classification).\n\n    Download it if necessary.\n\n    =================   ==========\n    Classes                     20\n    Samples total            18846\n    Dimensionality               1\n    Features                  text\n    =================   ==========\n\n    Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n\n    Parameters\n    ----------\n    data_home : str, default=None\n        Specify a download and cache folder for the datasets. If None,\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    subset : {'train', 'test', 'all'}, default='train'\n        Select the dataset to load: 'train' for the training set, 'test'\n        for the test set, 'all' for both, with shuffled ordering.\n\n    categories : array-like, dtype=str or unicode, default=None\n        If None (default), load all the categories.\n        If not None, list of category names to load (other categories\n        ignored).\n\n    shuffle : bool, default=True\n        Whether or not to shuffle the data: might be important for models that\n        make the assumption that the samples are independent and identically\n        distributed (i.i.d.), such as stochastic gradient descent.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for dataset shuffling. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    remove : tuple, default=()\n        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n        these are kinds of text that will be detected and removed from the\n        newsgroup posts, preventing classifiers from overfitting on\n        metadata.\n\n        'headers' removes newsgroup headers, 'footers' removes blocks at the\n        ends of posts that look like signatures, and 'quotes' removes lines\n        that appear to be quoting another post.\n\n        'headers' follows an exact standard; the other filters are not always\n        correct.\n\n    download_if_missing : bool, default=True\n        If False, raise an IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : bool, default=False\n        If True, returns `(data.data, data.target)` instead of a Bunch\n        object.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    bunch : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : list of shape (n_samples,)\n            The data list to learn.\n        target: ndarray of shape (n_samples,)\n            The target labels.\n        filenames: list of shape (n_samples,)\n            The path to the location of the data.\n        DESCR: str\n            The full description of the dataset.\n        target_names: list of shape (n_classes,)\n            The names of target classes.\n\n    (data, target) : tuple if `return_X_y=True`\n        .. versionadded:: 0.22\n    \"\"\"\n\n    data_home = get_data_home(data_home=data_home)\n    cache_path = _pkl_filepath(data_home, CACHE_NAME)\n    twenty_home = os.path.join(data_home, \"20news_home\")\n    cache = None\n    if os.path.exists(cache_path):\n        try:\n            with open(cache_path, 'rb') as f:\n                compressed_content = f.read()\n            uncompressed_content = codecs.decode(\n                compressed_content, 'zlib_codec')\n            cache = pickle.loads(uncompressed_content)\n        except Exception as e:\n            print(80 * '_')\n            print('Cache loading failed')\n            print(80 * '_')\n            print(e)\n\n    if cache is None:\n        if download_if_missing:\n            logger.info(\"Downloading 20news dataset. \"\n                        \"This may take a few minutes.\")\n            cache = _download_20newsgroups(target_dir=twenty_home,\n                                           cache_path=cache_path)\n        else:\n            raise IOError('20Newsgroups dataset not found')\n\n    if subset in ('train', 'test'):\n        data = cache[subset]\n    elif subset == 'all':\n        data_lst = list()\n        target = list()\n        filenames = list()\n        for subset in ('train', 'test'):\n            data = cache[subset]\n            data_lst.extend(data.data)\n            target.extend(data.target)\n            filenames.extend(data.filenames)\n\n        data.data = data_lst\n        data.target = np.array(target)\n        data.filenames = np.array(filenames)\n    else:\n        raise ValueError(\n            \"subset can only be 'train', 'test' or 'all', got '%s'\" % subset)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'twenty_newsgroups.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    data.DESCR = fdescr\n\n    if 'headers' in remove:\n        data.data = [strip_newsgroup_header(text) for text in data.data]\n    if 'footers' in remove:\n        data.data = [strip_newsgroup_footer(text) for text in data.data]\n    if 'quotes' in remove:\n        data.data = [strip_newsgroup_quoting(text) for text in data.data]\n\n    if categories is not None:\n        labels = [(data.target_names.index(cat), cat) for cat in categories]\n        # Sort the categories to have the ordering of the labels\n        labels.sort()\n        labels, categories = zip(*labels)\n        mask = np.in1d(data.target, labels)\n        data.filenames = data.filenames[mask]\n        data.target = data.target[mask]\n        # searchsorted to have continuous labels\n        data.target = np.searchsorted(labels, data.target)\n        data.target_names = list(categories)\n        # Use an object array to shuffle: avoids memory copy\n        data_lst = np.array(data.data, dtype=object)\n        data_lst = data_lst[mask]\n        data.data = data_lst.tolist()\n\n    if shuffle:\n        random_state = check_random_state(random_state)\n        indices = np.arange(data.target.shape[0])\n        random_state.shuffle(indices)\n        data.filenames = data.filenames[indices]\n        data.target = data.target[indices]\n        # Use an object array to shuffle: avoids memory copy\n        data_lst = np.array(data.data, dtype=object)\n        data_lst = data_lst[indices]\n        data.data = data_lst.tolist()\n\n    if return_X_y:\n        return data.data, data.target\n\n    return data"
    },
    {
      "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized",
      "name": "fetch_20newsgroups_vectorized",
      "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/subset",
          "name": "subset",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.subset",
          "default_value": "'train'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'train', 'test', 'all'}",
            "default_value": "'train'",
            "description": "Select the dataset to load: 'train' for the training set, 'test'\nfor the test set, 'all' for both, with shuffled ordering."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "test",
              "train",
              "all"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/remove",
          "name": "remove",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.remove",
          "default_value": "()",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple",
            "default_value": "()",
            "description": "May contain any subset of ('headers', 'footers', 'quotes'). Each of\nthese are kinds of text that will be detected and removed from the\nnewsgroup posts, preventing classifiers from overfitting on\nmetadata.\n\n'headers' removes newsgroup headers, 'footers' removes blocks at the\nends of posts that look like signatures, and 'quotes' removes lines\nthat appear to be quoting another post."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/data_home",
          "name": "data_home",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.data_home",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "None",
            "description": "Specify an download and cache folder for the datasets. If None,\nall scikit-learn data is stored in '~/scikit_learn_data' subfolders."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/download_if_missing",
          "name": "download_if_missing",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.download_if_missing",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, raise an IOError if the data is not locally available\ninstead of trying to download the data from the source site."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/return_X_y",
          "name": "return_X_y",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.return_X_y",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, returns ``(data.data, data.target)`` instead of a Bunch\nobject.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/normalize",
          "name": "normalize",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.normalize",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True, normalizes each document's feature vector to unit norm using\n:func:`sklearn.preprocessing.normalize`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.datasets._twenty_newsgroups/fetch_20newsgroups_vectorized/as_frame",
          "name": "as_frame",
          "qname": "sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized.as_frame",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the data is a pandas DataFrame including columns with\nappropriate dtypes (numeric, string, or categorical). The target is\na pandas DataFrame or Series depending on the number of\n`target_columns`.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.datasets"
      ],
      "description": "Load and vectorize the 20 newsgroups dataset (classification).\n\nDownload it if necessary.\n\nThis is a convenience function; the transformation is done using the\ndefault settings for\n:class:`~sklearn.feature_extraction.text.CountVectorizer`. For more\nadvanced usage (stopword filtering, n-gram extraction, etc.), combine\nfetch_20newsgroups with a custom\n:class:`~sklearn.feature_extraction.text.CountVectorizer`,\n:class:`~sklearn.feature_extraction.text.HashingVectorizer`,\n:class:`~sklearn.feature_extraction.text.TfidfTransformer` or\n:class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n\nThe resulting counts are normalized using\n:func:`sklearn.preprocessing.normalize` unless normalize is set to False.\n\n=================   ==========\nClasses                     20\nSamples total            18846\nDimensionality          130107\nFeatures                  real\n=================   ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.",
      "docstring": "Load and vectorize the 20 newsgroups dataset (classification).\n\nDownload it if necessary.\n\nThis is a convenience function; the transformation is done using the\ndefault settings for\n:class:`~sklearn.feature_extraction.text.CountVectorizer`. For more\nadvanced usage (stopword filtering, n-gram extraction, etc.), combine\nfetch_20newsgroups with a custom\n:class:`~sklearn.feature_extraction.text.CountVectorizer`,\n:class:`~sklearn.feature_extraction.text.HashingVectorizer`,\n:class:`~sklearn.feature_extraction.text.TfidfTransformer` or\n:class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n\nThe resulting counts are normalized using\n:func:`sklearn.preprocessing.normalize` unless normalize is set to False.\n\n=================   ==========\nClasses                     20\nSamples total            18846\nDimensionality          130107\nFeatures                  real\n=================   ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.\n\nParameters\n----------\nsubset : {'train', 'test', 'all'}, default='train'\n    Select the dataset to load: 'train' for the training set, 'test'\n    for the test set, 'all' for both, with shuffled ordering.\n\nremove : tuple, default=()\n    May contain any subset of ('headers', 'footers', 'quotes'). Each of\n    these are kinds of text that will be detected and removed from the\n    newsgroup posts, preventing classifiers from overfitting on\n    metadata.\n\n    'headers' removes newsgroup headers, 'footers' removes blocks at the\n    ends of posts that look like signatures, and 'quotes' removes lines\n    that appear to be quoting another post.\n\ndata_home : str, default=None\n    Specify an download and cache folder for the datasets. If None,\n    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True\n    If False, raise an IOError if the data is not locally available\n    instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False\n    If True, returns ``(data.data, data.target)`` instead of a Bunch\n    object.\n\n    .. versionadded:: 0.20\n\nnormalize : bool, default=True\n    If True, normalizes each document's feature vector to unit norm using\n    :func:`sklearn.preprocessing.normalize`.\n\n    .. versionadded:: 0.22\n\nas_frame : bool, default=False\n    If True, the data is a pandas DataFrame including columns with\n    appropriate dtypes (numeric, string, or categorical). The target is\n    a pandas DataFrame or Series depending on the number of\n    `target_columns`.\n\n    .. versionadded:: 0.24\n\nReturns\n-------\nbunch : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    data: {sparse matrix, dataframe} of shape (n_samples, n_features)\n        The input data matrix. If ``as_frame`` is `True`, ``data`` is\n        a pandas DataFrame with sparse columns.\n    target: {ndarray, series} of shape (n_samples,)\n        The target labels. If ``as_frame`` is `True`, ``target`` is a\n        pandas Series.\n    target_names: list of shape (n_classes,)\n        The names of target classes.\n    DESCR: str\n        The full description of the dataset.\n    frame: dataframe of shape (n_samples, n_features + 1)\n        Only present when `as_frame=True`. Pandas DataFrame with ``data``\n        and ``target``.\n\n        .. versionadded:: 0.24\n\n(data, target) : tuple if ``return_X_y`` is True\n    `data` and `target` would be of the format defined in the `Bunch`\n    description above.\n\n    .. versionadded:: 0.20",
      "code": "@_deprecate_positional_args\ndef fetch_20newsgroups_vectorized(*, subset=\"train\", remove=(), data_home=None,\n                                  download_if_missing=True, return_X_y=False,\n                                  normalize=True, as_frame=False):\n    \"\"\"Load and vectorize the 20 newsgroups dataset (classification).\n\n    Download it if necessary.\n\n    This is a convenience function; the transformation is done using the\n    default settings for\n    :class:`~sklearn.feature_extraction.text.CountVectorizer`. For more\n    advanced usage (stopword filtering, n-gram extraction, etc.), combine\n    fetch_20newsgroups with a custom\n    :class:`~sklearn.feature_extraction.text.CountVectorizer`,\n    :class:`~sklearn.feature_extraction.text.HashingVectorizer`,\n    :class:`~sklearn.feature_extraction.text.TfidfTransformer` or\n    :class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n\n    The resulting counts are normalized using\n    :func:`sklearn.preprocessing.normalize` unless normalize is set to False.\n\n    =================   ==========\n    Classes                     20\n    Samples total            18846\n    Dimensionality          130107\n    Features                  real\n    =================   ==========\n\n    Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n\n    Parameters\n    ----------\n    subset : {'train', 'test', 'all'}, default='train'\n        Select the dataset to load: 'train' for the training set, 'test'\n        for the test set, 'all' for both, with shuffled ordering.\n\n    remove : tuple, default=()\n        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n        these are kinds of text that will be detected and removed from the\n        newsgroup posts, preventing classifiers from overfitting on\n        metadata.\n\n        'headers' removes newsgroup headers, 'footers' removes blocks at the\n        ends of posts that look like signatures, and 'quotes' removes lines\n        that appear to be quoting another post.\n\n    data_home : str, default=None\n        Specify an download and cache folder for the datasets. If None,\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : bool, default=True\n        If False, raise an IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    return_X_y : bool, default=False\n        If True, returns ``(data.data, data.target)`` instead of a Bunch\n        object.\n\n        .. versionadded:: 0.20\n\n    normalize : bool, default=True\n        If True, normalizes each document's feature vector to unit norm using\n        :func:`sklearn.preprocessing.normalize`.\n\n        .. versionadded:: 0.22\n\n    as_frame : bool, default=False\n        If True, the data is a pandas DataFrame including columns with\n        appropriate dtypes (numeric, string, or categorical). The target is\n        a pandas DataFrame or Series depending on the number of\n        `target_columns`.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n    bunch : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data: {sparse matrix, dataframe} of shape (n_samples, n_features)\n            The input data matrix. If ``as_frame`` is `True`, ``data`` is\n            a pandas DataFrame with sparse columns.\n        target: {ndarray, series} of shape (n_samples,)\n            The target labels. If ``as_frame`` is `True`, ``target`` is a\n            pandas Series.\n        target_names: list of shape (n_classes,)\n            The names of target classes.\n        DESCR: str\n            The full description of the dataset.\n        frame: dataframe of shape (n_samples, n_features + 1)\n            Only present when `as_frame=True`. Pandas DataFrame with ``data``\n            and ``target``.\n\n            .. versionadded:: 0.24\n\n    (data, target) : tuple if ``return_X_y`` is True\n        `data` and `target` would be of the format defined in the `Bunch`\n        description above.\n\n        .. versionadded:: 0.20\n    \"\"\"\n    data_home = get_data_home(data_home=data_home)\n    filebase = '20newsgroup_vectorized'\n    if remove:\n        filebase += 'remove-' + ('-'.join(remove))\n    target_file = _pkl_filepath(data_home, filebase + \".pkl\")\n\n    # we shuffle but use a fixed seed for the memoization\n    data_train = fetch_20newsgroups(data_home=data_home,\n                                    subset='train',\n                                    categories=None,\n                                    shuffle=True,\n                                    random_state=12,\n                                    remove=remove,\n                                    download_if_missing=download_if_missing)\n\n    data_test = fetch_20newsgroups(data_home=data_home,\n                                   subset='test',\n                                   categories=None,\n                                   shuffle=True,\n                                   random_state=12,\n                                   remove=remove,\n                                   download_if_missing=download_if_missing)\n\n    if os.path.exists(target_file):\n        try:\n            X_train, X_test, feature_names = joblib.load(target_file)\n        except ValueError as e:\n            raise ValueError(\n                f\"The cached dataset located in {target_file} was fetched \"\n                f\"with an older scikit-learn version and it is not compatible \"\n                f\"with the scikit-learn version imported. You need to \"\n                f\"manually delete the file: {target_file}.\"\n            ) from e\n    else:\n        vectorizer = CountVectorizer(dtype=np.int16)\n        X_train = vectorizer.fit_transform(data_train.data).tocsr()\n        X_test = vectorizer.transform(data_test.data).tocsr()\n        feature_names = vectorizer.get_feature_names()\n\n        joblib.dump((X_train, X_test, feature_names), target_file, compress=9)\n\n    # the data is stored as int16 for compactness\n    # but normalize needs floats\n    if normalize:\n        X_train = X_train.astype(np.float64)\n        X_test = X_test.astype(np.float64)\n        preprocessing.normalize(X_train, copy=False)\n        preprocessing.normalize(X_test, copy=False)\n\n    target_names = data_train.target_names\n\n    if subset == \"train\":\n        data = X_train\n        target = data_train.target\n    elif subset == \"test\":\n        data = X_test\n        target = data_test.target\n    elif subset == \"all\":\n        data = sp.vstack((X_train, X_test)).tocsr()\n        target = np.concatenate((data_train.target, data_test.target))\n    else:\n        raise ValueError(\"%r is not a valid subset: should be one of \"\n                         \"['train', 'test', 'all']\" % subset)\n\n    module_path = dirname(__file__)\n    with open(join(module_path, 'descr', 'twenty_newsgroups.rst')) as rst_file:\n        fdescr = rst_file.read()\n\n    frame = None\n    target_name = ['category_class']\n\n    if as_frame:\n        frame, data, target = _convert_data_dataframe(\n            \"fetch_20newsgroups_vectorized\",\n            data,\n            target,\n            feature_names,\n            target_names=target_name,\n            sparse_data=True\n        )\n\n    if return_X_y:\n        return data, target\n\n    return Bunch(data=data,\n                 target=target,\n                 frame=frame,\n                 target_names=target_names,\n                 feature_names=feature_names,\n                 DESCR=fdescr)"
    },
    {
      "id": "scikit-learn/sklearn.datasets.setup/configuration",
      "name": "configuration",
      "qname": "sklearn.datasets.setup.configuration",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.datasets.setup/configuration/parent_package",
          "name": "parent_package",
          "qname": "sklearn.datasets.setup.configuration.parent_package",
          "default_value": "''",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.datasets.setup/configuration/top_path",
          "name": "top_path",
          "qname": "sklearn.datasets.setup.configuration.top_path",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def configuration(parent_package='', top_path=None):\n    from numpy.distutils.misc_util import Configuration\n    config = Configuration('datasets', parent_package, top_path)\n    config.add_data_dir('data')\n    config.add_data_dir('descr')\n    config.add_data_dir('images')\n    config.add_data_dir(os.path.join('tests', 'data'))\n    if platform.python_implementation() != 'PyPy':\n        config.add_extension('_svmlight_format_fast',\n                             sources=['_svmlight_format_fast.pyx'],\n                             include_dirs=[numpy.get_include()])\n    config.add_subpackage('tests')\n    return config"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "n_features",
            "description": "Number of dictionary elements to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.alpha",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "Sparsity controlling parameter."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.tol",
          "default_value": "1e-08",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-8",
            "description": "Tolerance for numerical error."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/fit_algorithm",
          "name": "fit_algorithm",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.fit_algorithm",
          "default_value": "'lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lars', 'cd'}",
            "default_value": "'lars'",
            "description": "* `'lars'`: uses the least angle regression method to solve the lasso\n  problem (:func:`~sklearn.linear_model.lars_path`);\n* `'cd'`: uses the coordinate descent method to compute the\n  Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n  faster if the estimated components are sparse.\n\n.. versionadded:: 0.17\n   *cd* coordinate descent method to improve speed."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/transform_algorithm",
          "name": "transform_algorithm",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.transform_algorithm",
          "default_value": "'omp'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}",
            "default_value": "'omp'",
            "description": "Algorithm used to transform the data:\n\n- `'lars'`: uses the least angle regression method\n  (:func:`~sklearn.linear_model.lars_path`);\n- `'lasso_lars'`: uses Lars to compute the Lasso solution.\n- `'lasso_cd'`: uses the coordinate descent method to compute the\n  Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n  will be faster if the estimated components are sparse.\n- `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n  solution.\n- `'threshold'`: squashes to zero all coefficients less than alpha from\n  the projection ``dictionary * X'``.\n\n.. versionadded:: 0.17\n   *lasso_cd* coordinate descent method to improve speed."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "lasso_cd",
              "threshold",
              "omp",
              "lasso_lars"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/transform_n_nonzero_coefs",
          "name": "transform_n_nonzero_coefs",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.transform_n_nonzero_coefs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of nonzero coefficients to target in each column of the\nsolution. This is only used by `algorithm='lars'` and `algorithm='omp'`\nand is overridden by `alpha` in the `omp` case. If `None`, then\n`transform_n_nonzero_coefs=int(n_features / 10)`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/transform_alpha",
          "name": "transform_alpha",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.transform_alpha",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\npenalty applied to the L1 norm.\nIf `algorithm='threshold'`, `alpha` is the absolute value of the\nthreshold below which coefficients will be squashed to zero.\nIf `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\nthe reconstruction error targeted. In this case, it overrides\n`n_nonzero_coefs`.\nIf `None`, default to 1.0"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/code_init",
          "name": "code_init",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.code_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_components)",
            "default_value": "None",
            "description": "Initial value for the code, for warm restart. Only used if `code_init`\nand `dict_init` are not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/dict_init",
          "name": "dict_init",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.dict_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "None",
            "description": "Initial values for the dictionary, for warm restart. Only used if\n`code_init` and `dict_init` are not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "To control the verbosity of the procedure."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/split_sign",
          "name": "split_sign",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.split_sign",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to split the sparse feature vector into the concatenation of\nits negative part and its positive part. This can improve the\nperformance of downstream classifiers."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for initializing the dictionary when ``dict_init`` is not\nspecified, randomly shuffling the data when ``shuffle`` is set to\n``True``, and updating the dictionary. Pass an int for reproducible\nresults across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/positive_code",
          "name": "positive_code",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.positive_code",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the code.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/positive_dict",
          "name": "positive_dict",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.positive_dict",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the dictionary\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/__init__/transform_max_iter",
          "name": "transform_max_iter",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.__init__.transform_max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n`'lasso_lars'`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n    (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, max_iter=1000, tol=1e-8,\n                 fit_algorithm='lars', transform_algorithm='omp',\n                 transform_n_nonzero_coefs=None, transform_alpha=None,\n                 n_jobs=None, code_init=None, dict_init=None, verbose=False,\n                 split_sign=False, random_state=None, positive_code=False,\n                 positive_dict=False, transform_max_iter=1000):\n\n        super().__init__(\n            transform_algorithm, transform_n_nonzero_coefs,\n            transform_alpha, split_sign, n_jobs, positive_code,\n            transform_max_iter\n        )\n        self.n_components = n_components\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.fit_algorithm = fit_algorithm\n        self.code_init = code_init\n        self.dict_init = dict_init\n        self.verbose = verbose\n        self.random_state = random_state\n        self.positive_dict = positive_dict"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where `n_samples` in the number of samples\nand `n_features` is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/DictionaryLearning/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._dict_learning.DictionaryLearning.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model from data in X.",
      "docstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where `n_samples` in the number of samples\n    and `n_features` is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the object itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` in the number of samples\n            and `n_features` is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the object itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n        if self.n_components is None:\n            n_components = X.shape[1]\n        else:\n            n_components = self.n_components\n\n        V, U, E, self.n_iter_ = dict_learning(\n            X, n_components, alpha=self.alpha,\n            tol=self.tol, max_iter=self.max_iter,\n            method=self.fit_algorithm,\n            method_max_iter=self.transform_max_iter,\n            n_jobs=self.n_jobs,\n            code_init=self.code_init,\n            dict_init=self.dict_init,\n            verbose=self.verbose,\n            random_state=random_state,\n            return_n_iter=True,\n            positive_dict=self.positive_dict,\n            positive_code=self.positive_code)\n        self.components_ = U\n        self.error_ = E\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of dictionary elements to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.alpha",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1",
            "description": "Sparsity controlling parameter."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/n_iter",
          "name": "n_iter",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.n_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Total number of iterations to perform."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/fit_algorithm",
          "name": "fit_algorithm",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.fit_algorithm",
          "default_value": "'lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lars', 'cd'}",
            "default_value": "'lars'",
            "description": "The algorithm used:\n\n- `'lars'`: uses the least angle regression method to solve the lasso\n  problem (`linear_model.lars_path`)\n- `'cd'`: uses the coordinate descent method to compute the\n  Lasso solution (`linear_model.Lasso`). Lars will be faster if\n  the estimated components are sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/batch_size",
          "name": "batch_size",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.batch_size",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of samples in each mini-batch."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/shuffle",
          "name": "shuffle",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to shuffle the samples before forming batches."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/dict_init",
          "name": "dict_init",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.dict_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "None",
            "description": "initial value of the dictionary for warm restart scenarios"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/transform_algorithm",
          "name": "transform_algorithm",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.transform_algorithm",
          "default_value": "'omp'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}",
            "default_value": "'omp'",
            "description": "Algorithm used to transform the data:\n\n- `'lars'`: uses the least angle regression method\n  (`linear_model.lars_path`);\n- `'lasso_lars'`: uses Lars to compute the Lasso solution.\n- `'lasso_cd'`: uses the coordinate descent method to compute the\n  Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n  if the estimated components are sparse.\n- `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n  solution.\n- `'threshold'`: squashes to zero all coefficients less than alpha from\n  the projection ``dictionary * X'``."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "lasso_cd",
              "threshold",
              "omp",
              "lasso_lars"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/transform_n_nonzero_coefs",
          "name": "transform_n_nonzero_coefs",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.transform_n_nonzero_coefs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of nonzero coefficients to target in each column of the\nsolution. This is only used by `algorithm='lars'` and `algorithm='omp'`\nand is overridden by `alpha` in the `omp` case. If `None`, then\n`transform_n_nonzero_coefs=int(n_features / 10)`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/transform_alpha",
          "name": "transform_alpha",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.transform_alpha",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\npenalty applied to the L1 norm.\nIf `algorithm='threshold'`, `alpha` is the absolute value of the\nthreshold below which coefficients will be squashed to zero.\nIf `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\nthe reconstruction error targeted. In this case, it overrides\n`n_nonzero_coefs`.\nIf `None`, default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "To control the verbosity of the procedure."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/split_sign",
          "name": "split_sign",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.split_sign",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to split the sparse feature vector into the concatenation of\nits negative part and its positive part. This can improve the\nperformance of downstream classifiers."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for initializing the dictionary when ``dict_init`` is not\nspecified, randomly shuffling the data when ``shuffle`` is set to\n``True``, and updating the dictionary. Pass an int for reproducible\nresults across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/positive_code",
          "name": "positive_code",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.positive_code",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the code.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/positive_dict",
          "name": "positive_dict",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.positive_dict",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the dictionary.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/__init__/transform_max_iter",
          "name": "transform_max_iter",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__.transform_max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n`'lasso_lars'`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Mini-batch dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n   (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, n_iter=1000,\n                 fit_algorithm='lars', n_jobs=None, batch_size=3, shuffle=True,\n                 dict_init=None, transform_algorithm='omp',\n                 transform_n_nonzero_coefs=None, transform_alpha=None,\n                 verbose=False, split_sign=False, random_state=None,\n                 positive_code=False, positive_dict=False,\n                 transform_max_iter=1000):\n\n        super().__init__(\n            transform_algorithm, transform_n_nonzero_coefs, transform_alpha,\n            split_sign, n_jobs, positive_code, transform_max_iter\n        )\n        self.n_components = n_components\n        self.alpha = alpha\n        self.n_iter = n_iter\n        self.fit_algorithm = fit_algorithm\n        self.dict_init = dict_init\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.split_sign = split_sign\n        self.random_state = random_state\n        self.positive_dict = positive_dict"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model from data in X.",
      "docstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n\n        U, (A, B), self.n_iter_ = dict_learning_online(\n            X, self.n_components, alpha=self.alpha,\n            n_iter=self.n_iter, return_code=False,\n            method=self.fit_algorithm,\n            method_max_iter=self.transform_max_iter,\n            n_jobs=self.n_jobs, dict_init=self.dict_init,\n            batch_size=self.batch_size, shuffle=self.shuffle,\n            verbose=self.verbose, random_state=random_state,\n            return_inner_stats=True,\n            return_n_iter=True,\n            positive_dict=self.positive_dict,\n            positive_code=self.positive_code)\n        self.components_ = U\n        # Keep track of the state of the algorithm to be able to do\n        # some online fitting (partial_fit)\n        self.inner_stats_ = (A, B)\n        self.iter_offset_ = self.n_iter\n        self.random_state_ = random_state\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/partial_fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/partial_fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/partial_fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/MiniBatchDictionaryLearning/partial_fit/iter_offset",
          "name": "iter_offset",
          "qname": "sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.partial_fit.iter_offset",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of iteration on data batches that has been\nperformed before this call to partial_fit. This is optional:\nif no number is passed, the memory of the object is\nused."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Updates the model using the data in X as a mini-batch.",
      "docstring": "Updates the model using the data in X as a mini-batch.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\niter_offset : int, default=None\n    The number of iteration on data batches that has been\n    performed before this call to partial_fit. This is optional:\n    if no number is passed, the memory of the object is\n    used.\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def partial_fit(self, X, y=None, iter_offset=None):\n        \"\"\"Updates the model using the data in X as a mini-batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        iter_offset : int, default=None\n            The number of iteration on data batches that has been\n            performed before this call to partial_fit. This is optional:\n            if no number is passed, the memory of the object is\n            used.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        if not hasattr(self, 'random_state_'):\n            self.random_state_ = check_random_state(self.random_state)\n        if hasattr(self, 'components_'):\n            dict_init = self.components_\n        else:\n            dict_init = self.dict_init\n        inner_stats = getattr(self, 'inner_stats_', None)\n        if iter_offset is None:\n            iter_offset = getattr(self, 'iter_offset_', 0)\n        X = self._validate_data(X, reset=(iter_offset == 0))\n        U, (A, B) = dict_learning_online(\n            X, self.n_components, alpha=self.alpha,\n            n_iter=1, method=self.fit_algorithm,\n            method_max_iter=self.transform_max_iter,\n            n_jobs=self.n_jobs, dict_init=dict_init,\n            batch_size=len(X), shuffle=False,\n            verbose=self.verbose, return_code=False,\n            iter_offset=iter_offset, random_state=self.random_state_,\n            return_inner_stats=True, inner_stats=inner_stats,\n            positive_dict=self.positive_dict,\n            positive_code=self.positive_code)\n        self.components_ = U\n\n        # Keep track of the state of the algorithm to be able to do\n        # some online fitting (partial_fit)\n        self.inner_stats_ = (A, B)\n        self.iter_offset_ = iter_offset + 1\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/dictionary",
          "name": "dictionary",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.dictionary",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "",
            "description": "The dictionary atoms used for sparse coding. Lines are assumed to be\nnormalized to unit norm."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/transform_algorithm",
          "name": "transform_algorithm",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.transform_algorithm",
          "default_value": "'omp'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}",
            "default_value": "'omp'",
            "description": "Algorithm used to transform the data:\n\n- `'lars'`: uses the least angle regression method\n  (`linear_model.lars_path`);\n- `'lasso_lars'`: uses Lars to compute the Lasso solution;\n- `'lasso_cd'`: uses the coordinate descent method to compute the\n  Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n  the estimated components are sparse;\n- `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n  solution;\n- `'threshold'`: squashes to zero all coefficients less than alpha from\n  the projection ``dictionary * X'``."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "lasso_cd",
              "threshold",
              "omp",
              "lasso_lars"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/transform_n_nonzero_coefs",
          "name": "transform_n_nonzero_coefs",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.transform_n_nonzero_coefs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of nonzero coefficients to target in each column of the\nsolution. This is only used by `algorithm='lars'` and `algorithm='omp'`\nand is overridden by `alpha` in the `omp` case. If `None`, then\n`transform_n_nonzero_coefs=int(n_features / 10)`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/transform_alpha",
          "name": "transform_alpha",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.transform_alpha",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\npenalty applied to the L1 norm.\nIf `algorithm='threshold'`, `alpha` is the absolute value of the\nthreshold below which coefficients will be squashed to zero.\nIf `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\nthe reconstruction error targeted. In this case, it overrides\n`n_nonzero_coefs`.\nIf `None`, default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/split_sign",
          "name": "split_sign",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.split_sign",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to split the sparse feature vector into the concatenation of\nits negative part and its positive part. This can improve the\nperformance of downstream classifiers."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/positive_code",
          "name": "positive_code",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.positive_code",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the code.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/__init__/transform_max_iter",
          "name": "transform_max_iter",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.__init__.transform_max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n`lasso_lars`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Sparse coding\n\nFinds a sparse representation of data against a fixed, precomputed\ndictionary.\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, dictionary, *, transform_algorithm='omp',\n                 transform_n_nonzero_coefs=None, transform_alpha=None,\n                 split_sign=False, n_jobs=None, positive_code=False,\n                 transform_max_iter=1000):\n        super().__init__(\n            transform_algorithm, transform_n_nonzero_coefs,\n            transform_alpha, split_sign, n_jobs, positive_code,\n            transform_max_iter\n        )\n        self.dictionary = dictionary"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/components_@getter",
      "name": "components_",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder.components_",
      "decorators": [
        "deprecated(\"The attribute 'components_' is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Use the 'dictionary' instead.\")",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/components_@getter/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.components_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"The attribute 'components_' is deprecated \"  # type: ignore\n                \"in 0.24 and will be removed in 1.1 (renaming of 0.26). Use \"\n                \"the 'dictionary' instead.\")\n    @property\n    def components_(self):\n        return self.dictionary"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.",
      "docstring": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n\nParameters\n----------\nX : Ignored\n\ny : Ignored\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n\n        Parameters\n        ----------\n        X : Ignored\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/n_components_@getter",
      "name": "n_components_",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder.n_components_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/n_components_@getter/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.n_components_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def n_components_(self):\n        return self.dictionary.shape[0]"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/n_features_in_@getter",
      "name": "n_features_in_",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder.n_features_in_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/n_features_in_@getter/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.n_features_in_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def n_features_in_(self):\n        return self.dictionary.shape[1]"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._dict_learning.SparseCoder.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Test data to be transformed, must have the same number of\nfeatures as the data used to train the model."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/SparseCoder/transform/y",
          "name": "y",
          "qname": "sklearn.decomposition._dict_learning.SparseCoder.transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Encode the data as a sparse combination of the dictionary atoms.\n\nCoding method is determined by the object parameter\n`transform_algorithm`.",
      "docstring": "Encode the data as a sparse combination of the dictionary atoms.\n\nCoding method is determined by the object parameter\n`transform_algorithm`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Test data to be transformed, must have the same number of\n    features as the data used to train the model.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed data.",
      "code": "    def transform(self, X, y=None):\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\n\n        Coding method is determined by the object parameter\n        `transform_algorithm`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        return super()._transform(X, self.dictionary)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning",
      "name": "dict_learning",
      "qname": "sklearn.decomposition._dict_learning.dict_learning",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.n_components",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Number of dictionary atoms to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.alpha",
          "default_value": null,
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Sparsity controlling parameter."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Maximum number of iterations to perform."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.tol",
          "default_value": "1e-08",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-8",
            "description": "Tolerance for the stopping condition."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/method",
          "name": "method",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.method",
          "default_value": "'lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lars', 'cd'}",
            "default_value": "'lars'",
            "description": "The method used:\n\n* `'lars'`: uses the least angle regression method to solve the lasso\n   problem (`linear_model.lars_path`);\n* `'cd'`: uses the coordinate descent method to compute the\n  Lasso solution (`linear_model.Lasso`). Lars will be faster if\n  the estimated components are sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/dict_init",
          "name": "dict_init",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.dict_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "None",
            "description": "Initial value for the dictionary for warm restart scenarios. Only used\nif `code_init` and `dict_init` are not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/code_init",
          "name": "code_init",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.code_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_components)",
            "default_value": "None",
            "description": "Initial value for the sparse code for warm restart scenarios. Only used\nif `code_init` and `dict_init` are not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/callback",
          "name": "callback",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.callback",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Callable that gets invoked every five iterations"
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "To control the verbosity of the procedure."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for randomly initializing the dictionary. Pass an int for\nreproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/positive_dict",
          "name": "positive_dict",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.positive_dict",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the dictionary.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/positive_code",
          "name": "positive_code",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.positive_code",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the code.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning/method_max_iter",
          "name": "method_max_iter",
          "qname": "sklearn.decomposition._dict_learning.dict_learning.method_max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Solves a dictionary learning matrix factorization problem.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n    (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                 (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.",
      "docstring": "Solves a dictionary learning matrix factorization problem.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n    (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                 (U,V)\n                with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data matrix.\n\nn_components : int\n    Number of dictionary atoms to extract.\n\nalpha : int\n    Sparsity controlling parameter.\n\nmax_iter : int, default=100\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-8\n    Tolerance for the stopping condition.\n\nmethod : {'lars', 'cd'}, default='lars'\n    The method used:\n\n    * `'lars'`: uses the least angle regression method to solve the lasso\n       problem (`linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial value for the dictionary for warm restart scenarios. Only used\n    if `code_init` and `dict_init` are not None.\n\ncode_init : ndarray of shape (n_samples, n_components), default=None\n    Initial value for the sparse code for warm restart scenarios. Only used\n    if `code_init` and `dict_init` are not None.\n\ncallback : callable, default=None\n    Callable that gets invoked every five iterations\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for randomly initializing the dictionary. Pass an int for\n    reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\nmethod_max_iter : int, default=1000\n    Maximum number of iterations to perform.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\ncode : ndarray of shape (n_samples, n_components)\n    The sparse code factor in the matrix factorization.\n\ndictionary : ndarray of shape (n_components, n_features),\n    The dictionary factor in the matrix factorization.\n\nerrors : array\n    Vector of errors at each iteration.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to True.\n\nSee Also\n--------\ndict_learning_online\nDictionaryLearning\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA",
      "code": "@_deprecate_positional_args\ndef dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-8,\n                  method='lars', n_jobs=None, dict_init=None, code_init=None,\n                  callback=None, verbose=False, random_state=None,\n                  return_n_iter=False, positive_dict=False,\n                  positive_code=False, method_max_iter=1000):\n    \"\"\"Solves a dictionary learning matrix factorization problem.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                     (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int\n        Number of dictionary atoms to extract.\n\n    alpha : int\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        The method used:\n\n        * `'lars'`: uses the least angle regression method to solve the lasso\n           problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value for the dictionary for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the sparse code for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for randomly initializing the dictionary. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See Also\n    --------\n    dict_learning_online\n    DictionaryLearning\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n    \"\"\"\n    if method not in ('lars', 'cd'):\n        raise ValueError('Coding method %r not supported as a fit algorithm.'\n                         % method)\n\n    _check_positive_coding(method, positive_code)\n\n    method = 'lasso_' + method\n\n    t0 = time.time()\n    # Avoid integer division problems\n    alpha = float(alpha)\n    random_state = check_random_state(random_state)\n\n    # Init the code and the dictionary with SVD of Y\n    if code_init is not None and dict_init is not None:\n        code = np.array(code_init, order='F')\n        # Don't copy V, it will happen below\n        dictionary = dict_init\n    else:\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\n        dictionary = S[:, np.newaxis] * dictionary\n    r = len(dictionary)\n    if n_components <= r:  # True even if n_components=None\n        code = code[:, :n_components]\n        dictionary = dictionary[:n_components, :]\n    else:\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\n        dictionary = np.r_[dictionary,\n                           np.zeros((n_components - r, dictionary.shape[1]))]\n\n    # Fortran-order dict, as we are going to access its row vectors\n    dictionary = np.array(dictionary, order='F')\n\n    residuals = 0\n\n    errors = []\n    current_cost = np.nan\n\n    if verbose == 1:\n        print('[dict_learning]', end=' ')\n\n    # If max_iter is 0, number of iterations returned should be zero\n    ii = -1\n\n    for ii in range(max_iter):\n        dt = (time.time() - t0)\n        if verbose == 1:\n            sys.stdout.write(\".\")\n            sys.stdout.flush()\n        elif verbose:\n            print(\"Iteration % 3i \"\n                  \"(elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\n                  % (ii, dt, dt / 60, current_cost))\n\n        # Update code\n        code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,\n                             init=code, n_jobs=n_jobs, positive=positive_code,\n                             max_iter=method_max_iter, verbose=verbose)\n        # Update dictionary\n        dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,\n                                             verbose=verbose, return_r2=True,\n                                             random_state=random_state,\n                                             positive=positive_dict)\n        dictionary = dictionary.T\n\n        # Cost function\n        current_cost = 0.5 * residuals + alpha * np.sum(np.abs(code))\n        errors.append(current_cost)\n\n        if ii > 0:\n            dE = errors[-2] - errors[-1]\n            # assert(dE >= -tol * errors[-1])\n            if dE < tol * errors[-1]:\n                if verbose == 1:\n                    # A line return\n                    print(\"\")\n                elif verbose:\n                    print(\"--- Convergence reached after %d iterations\" % ii)\n                break\n        if ii % 5 == 0 and callback is not None:\n            callback(locals())\n\n    if return_n_iter:\n        return code, dictionary, errors, ii + 1\n    else:\n        return code, dictionary, errors"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online",
      "name": "dict_learning_online",
      "qname": "sklearn.decomposition._dict_learning.dict_learning_online",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Number of dictionary atoms to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.alpha",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1",
            "description": "Sparsity controlling parameter."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/n_iter",
          "name": "n_iter",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.n_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Number of mini-batch iterations to perform."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/return_code",
          "name": "return_code",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.return_code",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to also return the code U or just the dictionary `V`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/dict_init",
          "name": "dict_init",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.dict_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "None",
            "description": "Initial value for the dictionary for warm restart scenarios."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/callback",
          "name": "callback",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.callback",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "callable that gets invoked every five iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/batch_size",
          "name": "batch_size",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.batch_size",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "The number of samples to take in each batch."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "To control the verbosity of the procedure."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/shuffle",
          "name": "shuffle",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to shuffle the data before splitting it in batches."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/method",
          "name": "method",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.method",
          "default_value": "'lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lars', 'cd'}",
            "default_value": "'lars'",
            "description": "* `'lars'`: uses the least angle regression method to solve the lasso\n  problem (`linear_model.lars_path`);\n* `'cd'`: uses the coordinate descent method to compute the\n  Lasso solution (`linear_model.Lasso`). Lars will be faster if\n  the estimated components are sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/iter_offset",
          "name": "iter_offset",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.iter_offset",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Number of previous iterations completed on the dictionary used for\ninitialization."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for initializing the dictionary when ``dict_init`` is not\nspecified, randomly shuffling the data when ``shuffle`` is set to\n``True``, and updating the dictionary. Pass an int for reproducible\nresults across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/return_inner_stats",
          "name": "return_inner_stats",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.return_inner_stats",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Return the inner statistics A (dictionary covariance) and B\n(data approximation). Useful to restart the algorithm in an\nonline setting. If `return_inner_stats` is `True`, `return_code` is\nignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/inner_stats",
          "name": "inner_stats",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.inner_stats",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple of (A, B) ndarrays",
            "default_value": "None",
            "description": "Inner sufficient statistics that are kept by the algorithm.\nPassing them at initialization is useful in online settings, to\navoid losing the history of the evolution.\n`A` `(n_components, n_components)` is the dictionary covariance matrix.\n`B` `(n_features, n_components)` is the data approximation matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of (A, B) ndarrays"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/positive_dict",
          "name": "positive_dict",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.positive_dict",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the dictionary.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/positive_code",
          "name": "positive_code",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.positive_code",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the code.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/dict_learning_online/method_max_iter",
          "name": "method_max_iter",
          "qname": "sklearn.decomposition._dict_learning.dict_learning_online.method_max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform when solving the lasso problem.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Solves a dictionary learning matrix factorization problem online.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n    (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                 (U,V)\n                 with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code. This is\naccomplished by repeatedly iterating over mini-batches by slicing\nthe input data.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.",
      "docstring": "Solves a dictionary learning matrix factorization problem online.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n    (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                 (U,V)\n                 with || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code. This is\naccomplished by repeatedly iterating over mini-batches by slicing\nthe input data.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data matrix.\n\nn_components : int, default=2\n    Number of dictionary atoms to extract.\n\nalpha : float, default=1\n    Sparsity controlling parameter.\n\nn_iter : int, default=100\n    Number of mini-batch iterations to perform.\n\nreturn_code : bool, default=True\n    Whether to also return the code U or just the dictionary `V`.\n\ndict_init : ndarray of shape (n_components, n_features), default=None\n    Initial value for the dictionary for warm restart scenarios.\n\ncallback : callable, default=None\n    callable that gets invoked every five iterations.\n\nbatch_size : int, default=3\n    The number of samples to take in each batch.\n\nverbose : bool, default=False\n    To control the verbosity of the procedure.\n\nshuffle : bool, default=True\n    Whether to shuffle the data before splitting it in batches.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\nmethod : {'lars', 'cd'}, default='lars'\n    * `'lars'`: uses the least angle regression method to solve the lasso\n      problem (`linear_model.lars_path`);\n    * `'cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\n      the estimated components are sparse.\n\niter_offset : int, default=0\n    Number of previous iterations completed on the dictionary used for\n    initialization.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for initializing the dictionary when ``dict_init`` is not\n    specified, randomly shuffling the data when ``shuffle`` is set to\n    ``True``, and updating the dictionary. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_inner_stats : bool, default=False\n    Return the inner statistics A (dictionary covariance) and B\n    (data approximation). Useful to restart the algorithm in an\n    online setting. If `return_inner_stats` is `True`, `return_code` is\n    ignored.\n\ninner_stats : tuple of (A, B) ndarrays, default=None\n    Inner sufficient statistics that are kept by the algorithm.\n    Passing them at initialization is useful in online settings, to\n    avoid losing the history of the evolution.\n    `A` `(n_components, n_components)` is the dictionary covariance matrix.\n    `B` `(n_features, n_components)` is the data approximation matrix.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\npositive_dict : bool, default=False\n    Whether to enforce positivity when finding the dictionary.\n\n    .. versionadded:: 0.20\n\npositive_code : bool, default=False\n    Whether to enforce positivity when finding the code.\n\n    .. versionadded:: 0.20\n\nmethod_max_iter : int, default=1000\n    Maximum number of iterations to perform when solving the lasso problem.\n\n    .. versionadded:: 0.22\n\nReturns\n-------\ncode : ndarray of shape (n_samples, n_components),\n    The sparse code (only returned if `return_code=True`).\n\ndictionary : ndarray of shape (n_components, n_features),\n    The solutions to the dictionary learning problem.\n\nn_iter : int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to `True`.\n\nSee Also\n--------\ndict_learning\nDictionaryLearning\nMiniBatchDictionaryLearning\nSparsePCA\nMiniBatchSparsePCA",
      "code": "@_deprecate_positional_args\ndef dict_learning_online(X, n_components=2, *, alpha=1, n_iter=100,\n                         return_code=True, dict_init=None, callback=None,\n                         batch_size=3, verbose=False, shuffle=True,\n                         n_jobs=None, method='lars', iter_offset=0,\n                         random_state=None, return_inner_stats=False,\n                         inner_stats=None, return_n_iter=False,\n                         positive_dict=False, positive_code=False,\n                         method_max_iter=1000):\n    \"\"\"Solves a dictionary learning matrix factorization problem online.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                     (U,V)\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. This is\n    accomplished by repeatedly iterating over mini-batches by slicing\n    the input data.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int, default=2\n        Number of dictionary atoms to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    n_iter : int, default=100\n        Number of mini-batch iterations to perform.\n\n    return_code : bool, default=True\n        Whether to also return the code U or just the dictionary `V`.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value for the dictionary for warm restart scenarios.\n\n    callback : callable, default=None\n        callable that gets invoked every five iterations.\n\n    batch_size : int, default=3\n        The number of samples to take in each batch.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        * `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    iter_offset : int, default=0\n        Number of previous iterations completed on the dictionary used for\n        initialization.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_inner_stats : bool, default=False\n        Return the inner statistics A (dictionary covariance) and B\n        (data approximation). Useful to restart the algorithm in an\n        online setting. If `return_inner_stats` is `True`, `return_code` is\n        ignored.\n\n    inner_stats : tuple of (A, B) ndarrays, default=None\n        Inner sufficient statistics that are kept by the algorithm.\n        Passing them at initialization is useful in online settings, to\n        avoid losing the history of the evolution.\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\n        `B` `(n_features, n_components)` is the data approximation matrix.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform when solving the lasso problem.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components),\n        The sparse code (only returned if `return_code=True`).\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The solutions to the dictionary learning problem.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to `True`.\n\n    See Also\n    --------\n    dict_learning\n    DictionaryLearning\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n    \"\"\"\n    if n_components is None:\n        n_components = X.shape[1]\n\n    if method not in ('lars', 'cd'):\n        raise ValueError('Coding method not supported as a fit algorithm.')\n\n    _check_positive_coding(method, positive_code)\n\n    method = 'lasso_' + method\n\n    t0 = time.time()\n    n_samples, n_features = X.shape\n    # Avoid integer division problems\n    alpha = float(alpha)\n    random_state = check_random_state(random_state)\n\n    # Init V with SVD of X\n    if dict_init is not None:\n        dictionary = dict_init\n    else:\n        _, S, dictionary = randomized_svd(X, n_components,\n                                          random_state=random_state)\n        dictionary = S[:, np.newaxis] * dictionary\n    r = len(dictionary)\n    if n_components <= r:\n        dictionary = dictionary[:n_components, :]\n    else:\n        dictionary = np.r_[dictionary,\n                           np.zeros((n_components - r, dictionary.shape[1]))]\n\n    if verbose == 1:\n        print('[dict_learning]', end=' ')\n\n    if shuffle:\n        X_train = X.copy()\n        random_state.shuffle(X_train)\n    else:\n        X_train = X\n\n    dictionary = check_array(dictionary.T, order='F', dtype=np.float64,\n                             copy=False)\n    dictionary = np.require(dictionary, requirements='W')\n\n    X_train = check_array(X_train, order='C', dtype=np.float64, copy=False)\n\n    batches = gen_batches(n_samples, batch_size)\n    batches = itertools.cycle(batches)\n\n    # The covariance of the dictionary\n    if inner_stats is None:\n        A = np.zeros((n_components, n_components))\n        # The data approximation\n        B = np.zeros((n_features, n_components))\n    else:\n        A = inner_stats[0].copy()\n        B = inner_stats[1].copy()\n\n    # If n_iter is zero, we need to return zero.\n    ii = iter_offset - 1\n\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\n        this_X = X_train[batch]\n        dt = (time.time() - t0)\n        if verbose == 1:\n            sys.stdout.write(\".\")\n            sys.stdout.flush()\n        elif verbose:\n            if verbose > 10 or ii % ceil(100. / verbose) == 0:\n                print(\"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\"\n                      % (ii, dt, dt / 60))\n\n        this_code = sparse_encode(this_X, dictionary.T, algorithm=method,\n                                  alpha=alpha, n_jobs=n_jobs,\n                                  check_input=False,\n                                  positive=positive_code,\n                                  max_iter=method_max_iter, verbose=verbose).T\n\n        # Update the auxiliary variables\n        if ii < batch_size - 1:\n            theta = float((ii + 1) * batch_size)\n        else:\n            theta = float(batch_size ** 2 + ii + 1 - batch_size)\n        beta = (theta + 1 - batch_size) / (theta + 1)\n\n        A *= beta\n        A += np.dot(this_code, this_code.T)\n        B *= beta\n        B += np.dot(this_X.T, this_code.T)\n\n        # Update dictionary\n        dictionary = _update_dict(dictionary, B, A, verbose=verbose,\n                                  random_state=random_state,\n                                  positive=positive_dict)\n        # XXX: Can the residuals be of any use?\n\n        # Maybe we need a stopping criteria based on the amount of\n        # modification in the dictionary\n        if callback is not None:\n            callback(locals())\n\n    if return_inner_stats:\n        if return_n_iter:\n            return dictionary.T, (A, B), ii - iter_offset + 1\n        else:\n            return dictionary.T, (A, B)\n    if return_code:\n        if verbose > 1:\n            print('Learning code...', end=' ')\n        elif verbose == 1:\n            print('|', end=' ')\n        code = sparse_encode(X, dictionary.T, algorithm=method, alpha=alpha,\n                             n_jobs=n_jobs, check_input=False,\n                             positive=positive_code, max_iter=method_max_iter,\n                             verbose=verbose)\n        if verbose > 1:\n            dt = (time.time() - t0)\n            print('done (total time: % 3is, % 4.1fmn)' % (dt, dt / 60))\n        if return_n_iter:\n            return code, dictionary.T, ii - iter_offset + 1\n        else:\n            return code, dictionary.T\n\n    if return_n_iter:\n        return dictionary.T, ii - iter_offset + 1\n    else:\n        return dictionary.T"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode",
      "name": "sparse_encode",
      "qname": "sklearn.decomposition._dict_learning.sparse_encode",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/X",
          "name": "X",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/dictionary",
          "name": "dictionary",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.dictionary",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "",
            "description": "The dictionary matrix against which to solve the sparse coding of\nthe data. Some of the algorithms assume normalized rows for meaningful\noutput."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/gram",
          "name": "gram",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.gram",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_components)",
            "default_value": "None",
            "description": "Precomputed Gram matrix, `dictionary * dictionary'`."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/cov",
          "name": "cov",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.cov",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_samples)",
            "default_value": "None",
            "description": "Precomputed covariance, `dictionary' * X`."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_samples)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/algorithm",
          "name": "algorithm",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.algorithm",
          "default_value": "'lasso_lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}",
            "default_value": "'lasso_lars'",
            "description": "The algorithm used:\n\n* `'lars'`: uses the least angle regression method\n  (`linear_model.lars_path`);\n* `'lasso_lars'`: uses Lars to compute the Lasso solution;\n* `'lasso_cd'`: uses the coordinate descent method to compute the\n  Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\n  the estimated components are sparse;\n* `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n  solution;\n* `'threshold'`: squashes to zero all coefficients less than\n  regularization from the projection `dictionary * data'`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "lasso_cd",
              "threshold",
              "omp",
              "lasso_lars"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/n_nonzero_coefs",
          "name": "n_nonzero_coefs",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.n_nonzero_coefs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of nonzero coefficients to target in each column of the\nsolution. This is only used by `algorithm='lars'` and `algorithm='omp'`\nand is overridden by `alpha` in the `omp` case. If `None`, then\n`n_nonzero_coefs=int(n_features / 10)`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.alpha",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\npenalty applied to the L1 norm.\nIf `algorithm='threshold'`, `alpha` is the absolute value of the\nthreshold below which coefficients will be squashed to zero.\nIf `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\nthe reconstruction error targeted. In this case, it overrides\n`n_nonzero_coefs`.\nIf `None`, default to 1."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/copy_cov",
          "name": "copy_cov",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.copy_cov",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy the precomputed covariance matrix; if `False`, it may\nbe overwritten."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/init",
          "name": "init",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_components)",
            "default_value": "None",
            "description": "Initialization value of the sparse codes. Only used if\n`algorithm='lasso_cd'`."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n`'lasso_lars'`."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/check_input",
          "name": "check_input",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.check_input",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If `False`, the input arrays X and dictionary will not be checked."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity; the higher, the more messages."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._dict_learning/sparse_encode/positive",
          "name": "positive",
          "qname": "sklearn.decomposition._dict_learning.sparse_encode.positive",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to enforce positivity when finding the encoding.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Sparse coding\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.",
      "docstring": "Sparse coding\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\n    X ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Data matrix.\n\ndictionary : ndarray of shape (n_components, n_features)\n    The dictionary matrix against which to solve the sparse coding of\n    the data. Some of the algorithms assume normalized rows for meaningful\n    output.\n\ngram : ndarray of shape (n_components, n_components), default=None\n    Precomputed Gram matrix, `dictionary * dictionary'`.\n\ncov : ndarray of shape (n_components, n_samples), default=None\n    Precomputed covariance, `dictionary' * X`.\n\nalgorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'},             default='lasso_lars'\n    The algorithm used:\n\n    * `'lars'`: uses the least angle regression method\n      (`linear_model.lars_path`);\n    * `'lasso_lars'`: uses Lars to compute the Lasso solution;\n    * `'lasso_cd'`: uses the coordinate descent method to compute the\n      Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\n      the estimated components are sparse;\n    * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n      solution;\n    * `'threshold'`: squashes to zero all coefficients less than\n      regularization from the projection `dictionary * data'`.\n\nn_nonzero_coefs : int, default=None\n    Number of nonzero coefficients to target in each column of the\n    solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n    and is overridden by `alpha` in the `omp` case. If `None`, then\n    `n_nonzero_coefs=int(n_features / 10)`.\n\nalpha : float, default=None\n    If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n    penalty applied to the L1 norm.\n    If `algorithm='threshold'`, `alpha` is the absolute value of the\n    threshold below which coefficients will be squashed to zero.\n    If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n    the reconstruction error targeted. In this case, it overrides\n    `n_nonzero_coefs`.\n    If `None`, default to 1.\n\ncopy_cov : bool, default=True\n    Whether to copy the precomputed covariance matrix; if `False`, it may\n    be overwritten.\n\ninit : ndarray of shape (n_samples, n_components), default=None\n    Initialization value of the sparse codes. Only used if\n    `algorithm='lasso_cd'`.\n\nmax_iter : int, default=1000\n    Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n    `'lasso_lars'`.\n\nn_jobs : int, default=None\n    Number of parallel jobs to run.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\ncheck_input : bool, default=True\n    If `False`, the input arrays X and dictionary will not be checked.\n\nverbose : int, default=0\n    Controls the verbosity; the higher, the more messages.\n\npositive : bool, default=False\n    Whether to enforce positivity when finding the encoding.\n\n    .. versionadded:: 0.20\n\nReturns\n-------\ncode : ndarray of shape (n_samples, n_components)\n    The sparse codes\n\nSee Also\n--------\nsklearn.linear_model.lars_path\nsklearn.linear_model.orthogonal_mp\nsklearn.linear_model.Lasso\nSparseCoder",
      "code": "@_deprecate_positional_args\ndef sparse_encode(X, dictionary, *, gram=None, cov=None,\n                  algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None,\n                  copy_cov=True, init=None, max_iter=1000, n_jobs=None,\n                  check_input=True, verbose=0, positive=False):\n    \"\"\"Sparse coding\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data matrix.\n\n    dictionary : ndarray of shape (n_components, n_features)\n        The dictionary matrix against which to solve the sparse coding of\n        the data. Some of the algorithms assume normalized rows for meaningful\n        output.\n\n    gram : ndarray of shape (n_components, n_components), default=None\n        Precomputed Gram matrix, `dictionary * dictionary'`.\n\n    cov : ndarray of shape (n_components, n_samples), default=None\n        Precomputed covariance, `dictionary' * X`.\n\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\n            default='lasso_lars'\n        The algorithm used:\n\n        * `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\n          the estimated components are sparse;\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution;\n        * `'threshold'`: squashes to zero all coefficients less than\n          regularization from the projection `dictionary * data'`.\n\n    n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `n_nonzero_coefs=int(n_features / 10)`.\n\n    alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    copy_cov : bool, default=True\n        Whether to copy the precomputed covariance matrix; if `False`, it may\n        be overwritten.\n\n    init : ndarray of shape (n_samples, n_components), default=None\n        Initialization value of the sparse codes. Only used if\n        `algorithm='lasso_cd'`.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    check_input : bool, default=True\n        If `False`, the input arrays X and dictionary will not be checked.\n\n    verbose : int, default=0\n        Controls the verbosity; the higher, the more messages.\n\n    positive : bool, default=False\n        Whether to enforce positivity when finding the encoding.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse codes\n\n    See Also\n    --------\n    sklearn.linear_model.lars_path\n    sklearn.linear_model.orthogonal_mp\n    sklearn.linear_model.Lasso\n    SparseCoder\n    \"\"\"\n    if check_input:\n        if algorithm == 'lasso_cd':\n            dictionary = check_array(dictionary, order='C', dtype='float64')\n            X = check_array(X, order='C', dtype='float64')\n        else:\n            dictionary = check_array(dictionary)\n            X = check_array(X)\n\n    n_samples, n_features = X.shape\n    n_components = dictionary.shape[0]\n\n    if gram is None and algorithm != 'threshold':\n        gram = np.dot(dictionary, dictionary.T)\n\n    if cov is None and algorithm != 'lasso_cd':\n        copy_cov = False\n        cov = np.dot(dictionary, X.T)\n\n    if algorithm in ('lars', 'omp'):\n        regularization = n_nonzero_coefs\n        if regularization is None:\n            regularization = min(max(n_features / 10, 1), n_components)\n    else:\n        regularization = alpha\n        if regularization is None:\n            regularization = 1.\n\n    if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':\n        code = _sparse_encode(X,\n                              dictionary, gram, cov=cov,\n                              algorithm=algorithm,\n                              regularization=regularization, copy_cov=copy_cov,\n                              init=init,\n                              max_iter=max_iter,\n                              check_input=False,\n                              verbose=verbose,\n                              positive=positive)\n        return code\n\n    # Enter parallel code block\n    code = np.empty((n_samples, n_components))\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\n\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        delayed(_sparse_encode)(\n            X[this_slice], dictionary, gram,\n            cov[:, this_slice] if cov is not None else None,\n            algorithm,\n            regularization=regularization, copy_cov=copy_cov,\n            init=init[this_slice] if init is not None else None,\n            max_iter=max_iter,\n            check_input=False,\n            verbose=verbose,\n            positive=positive)\n        for this_slice in slices)\n    for this_slice, this_view in zip(slices, code_views):\n        code[this_slice] = this_view\n    return code"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Dimensionality of latent space, the number of components\nof ``X`` that are obtained after ``transform``.\nIf None, n_components is set to the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.tol",
          "default_value": "0.01",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float, defaul=1e-2",
            "default_value": "",
            "description": "Stopping tolerance for log-likelihood increase."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "defaul=1e-2"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/copy",
          "name": "copy",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to make a copy of X. If ``False``, the input X gets overwritten\nduring fitting."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/noise_variance_init",
          "name": "noise_variance_init",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.noise_variance_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_features,)",
            "default_value": "None",
            "description": "The initial guess of the noise variance for each feature.\nIf None, it defaults to np.ones(n_features)."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_features,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/svd_method",
          "name": "svd_method",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.svd_method",
          "default_value": "'randomized'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lapack', 'randomized'}",
            "default_value": "'randomized'",
            "description": "Which SVD method to use. If 'lapack' use standard SVD from\nscipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\nDefaults to 'randomized'. For most applications 'randomized' will\nbe sufficiently precise while providing significant speed gains.\nAccuracy can also be improved by setting higher values for\n`iterated_power`. If this is not sufficient, for maximum precision\nyou should choose 'lapack'."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lapack",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/iterated_power",
          "name": "iterated_power",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.iterated_power",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of iterations for the power method. 3 by default. Only used\nif ``svd_method`` equals 'randomized'."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/rotation",
          "name": "rotation",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.rotation",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'varimax', 'quartimax'}",
            "default_value": "None",
            "description": "If not None, apply the indicated rotation. Currently, varimax and\nquartimax are implemented. See\n`\"The varimax criterion for analytic rotation in factor analysis\"\n<https://link.springer.com/article/10.1007%2FBF02289233>`_\nH. F. Kaiser, 1958.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "varimax",
              "quartimax"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.__init__.random_state",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or RandomState instance",
            "default_value": "0",
            "description": "Only used when ``svd_method`` equals 'randomized'. Pass an int for\nreproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Factor Analysis (FA).\n\nA simple linear generative model with Gaussian latent variables.\n\nThe observations are assumed to be caused by a linear transformation of\nlower dimensional latent factors and added Gaussian noise.\nWithout loss of generality the factors are distributed according to a\nGaussian with zero mean and unit covariance. The noise is also zero mean\nand has an arbitrary diagonal covariance matrix.\n\nIf we would restrict the model further, by assuming that the Gaussian\nnoise is even isotropic (all diagonal entries are the same) we would obtain\n:class:`PPCA`.\n\nFactorAnalysis performs a maximum likelihood estimate of the so-called\n`loading` matrix, the transformation of the latent variables to the\nobserved ones, using SVD based approach.\n\nRead more in the :ref:`User Guide <FA>`.\n\n.. versionadded:: 0.13",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, tol=1e-2, copy=True,\n                 max_iter=1000,\n                 noise_variance_init=None, svd_method='randomized',\n                 iterated_power=3, rotation=None, random_state=0):\n        self.n_components = n_components\n        self.copy = copy\n        self.tol = tol\n        self.max_iter = max_iter\n        if svd_method not in ['lapack', 'randomized']:\n            raise ValueError('SVD method %s is not supported. Please consider'\n                             ' the documentation' % svd_method)\n        self.svd_method = svd_method\n\n        self.noise_variance_init = noise_variance_init\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n        self.rotation = rotation"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the FactorAnalysis model to X using SVD based approach",
      "docstring": "Fit the FactorAnalysis model to X using SVD based approach\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the FactorAnalysis model to X using SVD based approach\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, copy=self.copy, dtype=np.float64)\n\n        n_samples, n_features = X.shape\n        n_components = self.n_components\n        if n_components is None:\n            n_components = n_features\n\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        # some constant terms\n        nsqrt = sqrt(n_samples)\n        llconst = n_features * log(2. * np.pi) + n_components\n        var = np.var(X, axis=0)\n\n        if self.noise_variance_init is None:\n            psi = np.ones(n_features, dtype=X.dtype)\n        else:\n            if len(self.noise_variance_init) != n_features:\n                raise ValueError(\"noise_variance_init dimension does not \"\n                                 \"with number of features : %d != %d\" %\n                                 (len(self.noise_variance_init), n_features))\n            psi = np.array(self.noise_variance_init)\n\n        loglike = []\n        old_ll = -np.inf\n        SMALL = 1e-12\n\n        # we'll modify svd outputs to return unexplained variance\n        # to allow for unified computation of loglikelihood\n        if self.svd_method == 'lapack':\n            def my_svd(X):\n                _, s, Vt = linalg.svd(X,\n                                      full_matrices=False,\n                                      check_finite=False)\n                return (s[:n_components], Vt[:n_components],\n                        squared_norm(s[n_components:]))\n        elif self.svd_method == 'randomized':\n            random_state = check_random_state(self.random_state)\n\n            def my_svd(X):\n                _, s, Vt = randomized_svd(X, n_components,\n                                          random_state=random_state,\n                                          n_iter=self.iterated_power)\n                return s, Vt, squared_norm(X) - squared_norm(s)\n        else:\n            raise ValueError('SVD method %s is not supported. Please consider'\n                             ' the documentation' % self.svd_method)\n\n        for i in range(self.max_iter):\n            # SMALL helps numerics\n            sqrt_psi = np.sqrt(psi) + SMALL\n            s, Vt, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n            s **= 2\n            # Use 'maximum' here to avoid sqrt problems.\n            W = np.sqrt(np.maximum(s - 1., 0.))[:, np.newaxis] * Vt\n            del Vt\n            W *= sqrt_psi\n\n            # loglikelihood\n            ll = llconst + np.sum(np.log(s))\n            ll += unexp_var + np.sum(np.log(psi))\n            ll *= -n_samples / 2.\n            loglike.append(ll)\n            if (ll - old_ll) < self.tol:\n                break\n            old_ll = ll\n\n            psi = np.maximum(var - np.sum(W ** 2, axis=0), SMALL)\n        else:\n            warnings.warn('FactorAnalysis did not converge.' +\n                          ' You might want' +\n                          ' to increase the number of iterations.',\n                          ConvergenceWarning)\n\n        self.components_ = W\n        if self.rotation is not None:\n            self.components_ = self._rotate(W)\n        self.noise_variance_ = psi\n        self.loglike_ = loglike\n        self.n_iter_ = i + 1\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/get_covariance",
      "name": "get_covariance",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.get_covariance",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/get_covariance/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.get_covariance.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute data covariance with the FactorAnalysis model.\n\n``cov = components_.T * components_ + diag(noise_variance)``",
      "docstring": "Compute data covariance with the FactorAnalysis model.\n\n``cov = components_.T * components_ + diag(noise_variance)``\n\nReturns\n-------\ncov : ndarray of shape (n_features, n_features)\n    Estimated covariance of data.",
      "code": "    def get_covariance(self):\n        \"\"\"Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``\n\n        Returns\n        -------\n        cov : ndarray of shape (n_features, n_features)\n            Estimated covariance of data.\n        \"\"\"\n        check_is_fitted(self)\n\n        cov = np.dot(self.components_.T, self.components_)\n        cov.flat[::len(cov) + 1] += self.noise_variance_  # modify diag inplace\n        return cov"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/get_precision",
      "name": "get_precision",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.get_precision",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/get_precision/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.get_precision.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute data precision matrix with the FactorAnalysis model.",
      "docstring": "Compute data precision matrix with the FactorAnalysis model.\n\nReturns\n-------\nprecision : ndarray of shape (n_features, n_features)\n    Estimated precision of data.",
      "code": "    def get_precision(self):\n        \"\"\"Compute data precision matrix with the FactorAnalysis model.\n\n        Returns\n        -------\n        precision : ndarray of shape (n_features, n_features)\n            Estimated precision of data.\n        \"\"\"\n        check_is_fitted(self)\n\n        n_features = self.components_.shape[1]\n\n        # handle corner cases first\n        if self.n_components == 0:\n            return np.diag(1. / self.noise_variance_)\n        if self.n_components == n_features:\n            return linalg.inv(self.get_covariance())\n\n        # Get precision using matrix inversion lemma\n        components_ = self.components_\n        precision = np.dot(components_ / self.noise_variance_, components_.T)\n        precision.flat[::len(precision) + 1] += 1.\n        precision = np.dot(components_.T,\n                           np.dot(linalg.inv(precision), components_))\n        precision /= self.noise_variance_[:, np.newaxis]\n        precision /= -self.noise_variance_[np.newaxis, :]\n        precision.flat[::len(precision) + 1] += 1. / self.noise_variance_\n        return precision"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score",
      "name": "score",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score/X",
          "name": "X",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score/y",
          "name": "y",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the average log-likelihood of the samples",
      "docstring": "Compute the average log-likelihood of the samples\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    The data\n\ny : Ignored\n\nReturns\n-------\nll : float\n    Average log-likelihood of the samples under the current model",
      "code": "    def score(self, X, y=None):\n        \"\"\"Compute the average log-likelihood of the samples\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model\n        \"\"\"\n        return np.mean(self.score_samples(X))"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score_samples",
      "name": "score_samples",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score_samples",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score_samples/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score_samples.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/score_samples/X",
          "name": "X",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.score_samples.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data"
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the log-likelihood of each sample",
      "docstring": "Compute the log-likelihood of each sample\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    The data\n\nReturns\n-------\nll : ndarray of shape (n_samples,)\n    Log-likelihood of each sample under the current model",
      "code": "    def score_samples(self, X):\n        \"\"\"Compute the log-likelihood of each sample\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, reset=False)\n        Xr = X - self.mean_\n        precision = self.get_precision()\n        n_features = X.shape[1]\n        log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= .5 * (n_features * log(2. * np.pi)\n                          - fast_logdet(precision))\n        return log_like"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._factor_analysis/FactorAnalysis/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._factor_analysis.FactorAnalysis.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply dimensionality reduction to X using the model.\n\nCompute the expected mean of the latent variables.\nSee Barber, 21.2.33 (or Bishop, 12.66).",
      "docstring": "Apply dimensionality reduction to X using the model.\n\nCompute the expected mean of the latent variables.\nSee Barber, 21.2.33 (or Bishop, 12.66).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    The latent variables of X.",
      "code": "    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            The latent variables of X.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, reset=False)\n        Ih = np.eye(len(self.components_))\n\n        X_transformed = X - self.mean_\n\n        Wpsi = self.components_ / self.noise_variance_\n        cov_z = linalg.inv(Ih + np.dot(Wpsi, self.components_.T))\n        tmp = np.dot(X_transformed, Wpsi.T)\n        X_transformed = np.dot(tmp, cov_z)\n\n        return X_transformed"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._fastica.FastICA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of components to use. If None is passed, all are used."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.algorithm",
          "default_value": "'parallel'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'parallel', 'deflation'}",
            "default_value": "'parallel'",
            "description": "Apply parallel or deflational algorithm for FastICA."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "parallel",
              "deflation"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/whiten",
          "name": "whiten",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.whiten",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If whiten is false, the data is already considered to be\nwhitened, and no whitening is performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/fun",
          "name": "fun",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.fun",
          "default_value": "'logcosh'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'logcosh', 'exp', 'cube'} or callable",
            "default_value": "'logcosh'",
            "description": "The functional form of the G function used in the\napproximation to neg-entropy. Could be either 'logcosh', 'exp',\nor 'cube'.\nYou can also provide your own function. It should return a tuple\ncontaining the value of the function, and of its derivative, in the\npoint. Example::\n\n    def my_g(x):\n        return x ** 3, (3 * x ** 2).mean(axis=-1)"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "exp",
                  "logcosh",
                  "cube"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/fun_args",
          "name": "fun_args",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.fun_args",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Arguments to send to the functional form.\nIf empty and if fun='logcosh', fun_args will take value\n{'alpha' : 1.0}."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations during fit."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Tolerance on update at each iteration."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/w_init",
          "name": "w_init",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.w_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_components)",
            "default_value": "None",
            "description": "The mixing matrix to be used to initialize the algorithm."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._fastica.FastICA.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used to initialize ``w_init`` when not specified, with a\nnormal distribution. Pass an int, for reproducible results\nacross multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "FastICA: a fast algorithm for Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, algorithm='parallel', whiten=True,\n                 fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,\n                 w_init=None, random_state=None):\n        super().__init__()\n        if max_iter < 1:\n            raise ValueError(\"max_iter should be greater than 1, got \"\n                             \"(max_iter={})\".format(max_iter))\n        self.n_components = n_components\n        self.algorithm = algorithm\n        self.whiten = whiten\n        self.fun = fun\n        self.fun_args = fun_args\n        self.max_iter = max_iter\n        self.tol = tol\n        self.w_init = w_init\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._fastica.FastICA.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._fastica.FastICA.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._fastica.FastICA.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._fastica.FastICA.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model to X.",
      "docstring": "Fit the model to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._fit(X, compute_sources=False)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.decomposition._fastica.FastICA.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._fastica.FastICA.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._fastica.FastICA.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/fit_transform/y",
          "name": "y",
          "qname": "sklearn.decomposition._fastica.FastICA.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model and recover the sources from X.",
      "docstring": "Fit the model and recover the sources from X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model and recover the sources from X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        return self._fit(X, compute_sources=True)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.decomposition._fastica.FastICA.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._fastica.FastICA.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/inverse_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._fastica.FastICA.inverse_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_components)",
            "default_value": "",
            "description": "Sources, where n_samples is the number of samples\nand n_components is the number of components."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/inverse_transform/copy",
          "name": "copy",
          "qname": "sklearn.decomposition._fastica.FastICA.inverse_transform.copy",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, data passed to fit are overwritten. Defaults to True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform the sources back to the mixed data (apply mixing matrix).",
      "docstring": "Transform the sources back to the mixed data (apply mixing matrix).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_components)\n    Sources, where n_samples is the number of samples\n    and n_components is the number of components.\ncopy : bool, default=True\n    If False, data passed to fit are overwritten. Defaults to True.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_features)",
      "code": "    def inverse_transform(self, X, copy=True):\n        \"\"\"Transform the sources back to the mixed data (apply mixing matrix).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            Sources, where n_samples is the number of samples\n            and n_components is the number of components.\n        copy : bool, default=True\n            If False, data passed to fit are overwritten. Defaults to True.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_features)\n        \"\"\"\n        check_is_fitted(self)\n\n        X = check_array(X, copy=(copy and self.whiten), dtype=FLOAT_DTYPES)\n        X = np.dot(X, self.mixing_.T)\n        if self.whiten:\n            X += self.mean_\n\n        return X"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._fastica.FastICA.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._fastica.FastICA.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._fastica.FastICA.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data to transform, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/FastICA/transform/copy",
          "name": "copy",
          "qname": "sklearn.decomposition._fastica.FastICA.transform.copy",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, data passed to fit can be overwritten. Defaults to True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Recover the sources from X (apply the unmixing matrix).",
      "docstring": "Recover the sources from X (apply the unmixing matrix).\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Data to transform, where n_samples is the number of samples\n    and n_features is the number of features.\n\ncopy : bool, default=True\n    If False, data passed to fit can be overwritten. Defaults to True.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)",
      "code": "    def transform(self, X, copy=True):\n        \"\"\"Recover the sources from X (apply the unmixing matrix).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Data to transform, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        copy : bool, default=True\n            If False, data passed to fit can be overwritten. Defaults to True.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, copy=(copy and self.whiten),\n                                dtype=FLOAT_DTYPES, reset=False)\n        if self.whiten:\n            X -= self.mean_\n\n        return np.dot(X, self.components_.T)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._fastica/fastica",
      "name": "fastica",
      "qname": "sklearn.decomposition._fastica.fastica",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/X",
          "name": "X",
          "qname": "sklearn.decomposition._fastica.fastica.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._fastica.fastica.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of components to extract. If None no dimension reduction\nis performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/algorithm",
          "name": "algorithm",
          "qname": "sklearn.decomposition._fastica.fastica.algorithm",
          "default_value": "'parallel'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'parallel', 'deflation'}",
            "default_value": "'parallel'",
            "description": "Apply a parallel or deflational FASTICA algorithm."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "parallel",
              "deflation"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/whiten",
          "name": "whiten",
          "qname": "sklearn.decomposition._fastica.fastica.whiten",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True perform an initial whitening of the data.\nIf False, the data is assumed to have already been\npreprocessed: it should be centered, normed and white.\nOtherwise you will get incorrect results.\nIn this case the parameter n_components will be ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/fun",
          "name": "fun",
          "qname": "sklearn.decomposition._fastica.fastica.fun",
          "default_value": "'logcosh'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'logcosh', 'exp', 'cube'} or callable",
            "default_value": "'logcosh'",
            "description": "The functional form of the G function used in the\napproximation to neg-entropy. Could be either 'logcosh', 'exp',\nor 'cube'.\nYou can also provide your own function. It should return a tuple\ncontaining the value of the function, and of its derivative, in the\npoint. The derivative should be averaged along its last dimension.\nExample:\n\ndef my_g(x):\n    return x ** 3, np.mean(3 * x ** 2, axis=-1)"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "exp",
                  "logcosh",
                  "cube"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/fun_args",
          "name": "fun_args",
          "qname": "sklearn.decomposition._fastica.fastica.fun_args",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Arguments to send to the functional form.\nIf empty or None and if fun='logcosh', fun_args will take value\n{'alpha' : 1.0}"
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._fastica.fastica.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations to perform."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._fastica.fastica.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-04",
            "description": "A positive scalar giving the tolerance at which the\nun-mixing matrix is considered to have converged."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/w_init",
          "name": "w_init",
          "qname": "sklearn.decomposition._fastica.fastica.w_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_components)",
            "default_value": "None",
            "description": "Initial un-mixing array of dimension (n.comp,n.comp).\nIf None (default) then an array of normal r.v.'s is used."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._fastica.fastica.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used to initialize ``w_init`` when not specified, with a\nnormal distribution. Pass an int, for reproducible results\nacross multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/return_X_mean",
          "name": "return_X_mean",
          "qname": "sklearn.decomposition._fastica.fastica.return_X_mean",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, X_mean is returned too."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/compute_sources",
          "name": "compute_sources",
          "qname": "sklearn.decomposition._fastica.fastica.compute_sources",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, sources are not computed, but only the rotation matrix.\nThis can save memory when working with big data. Defaults to True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._fastica/fastica/return_n_iter",
          "name": "return_n_iter",
          "qname": "sklearn.decomposition._fastica.fastica.return_n_iter",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether or not to return the number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Perform Fast Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.",
      "docstring": "Perform Fast Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\nn_components : int, default=None\n    Number of components to extract. If None no dimension reduction\n    is performed.\n\nalgorithm : {'parallel', 'deflation'}, default='parallel'\n    Apply a parallel or deflational FASTICA algorithm.\n\nwhiten : bool, default=True\n    If True perform an initial whitening of the data.\n    If False, the data is assumed to have already been\n    preprocessed: it should be centered, normed and white.\n    Otherwise you will get incorrect results.\n    In this case the parameter n_components will be ignored.\n\nfun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n    The functional form of the G function used in the\n    approximation to neg-entropy. Could be either 'logcosh', 'exp',\n    or 'cube'.\n    You can also provide your own function. It should return a tuple\n    containing the value of the function, and of its derivative, in the\n    point. The derivative should be averaged along its last dimension.\n    Example:\n\n    def my_g(x):\n        return x ** 3, np.mean(3 * x ** 2, axis=-1)\n\nfun_args : dict, default=None\n    Arguments to send to the functional form.\n    If empty or None and if fun='logcosh', fun_args will take value\n    {'alpha' : 1.0}\n\nmax_iter : int, default=200\n    Maximum number of iterations to perform.\n\ntol : float, default=1e-04\n    A positive scalar giving the tolerance at which the\n    un-mixing matrix is considered to have converged.\n\nw_init : ndarray of shape (n_components, n_components), default=None\n    Initial un-mixing array of dimension (n.comp,n.comp).\n    If None (default) then an array of normal r.v.'s is used.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used to initialize ``w_init`` when not specified, with a\n    normal distribution. Pass an int, for reproducible results\n    across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nreturn_X_mean : bool, default=False\n    If True, X_mean is returned too.\n\ncompute_sources : bool, default=True\n    If False, sources are not computed, but only the rotation matrix.\n    This can save memory when working with big data. Defaults to True.\n\nreturn_n_iter : bool, default=False\n    Whether or not to return the number of iterations.\n\nReturns\n-------\nK : ndarray of shape (n_components, n_features) or None\n    If whiten is 'True', K is the pre-whitening matrix that projects data\n    onto the first n_components principal components. If whiten is 'False',\n    K is 'None'.\n\nW : ndarray of shape (n_components, n_components)\n    The square matrix that unmixes the data after whitening.\n    The mixing matrix is the pseudo-inverse of matrix ``W K``\n    if K is not None, else it is the inverse of W.\n\nS : ndarray of shape (n_samples, n_components) or None\n    Estimated source matrix\n\nX_mean : ndarray of shape (n_features,)\n    The mean over features. Returned only if return_X_mean is True.\n\nn_iter : int\n    If the algorithm is \"deflation\", n_iter is the\n    maximum number of iterations run across all components. Else\n    they are just the number of iterations taken to converge. This is\n    returned only when return_n_iter is set to `True`.\n\nNotes\n-----\n\nThe data matrix X is considered to be a linear combination of\nnon-Gaussian (independent) components i.e. X = AS where columns of S\ncontain the independent components and A is a linear mixing\nmatrix. In short ICA attempts to `un-mix' the data by estimating an\nun-mixing matrix W where ``S = W K X.``\nWhile FastICA was proposed to estimate as many sources\nas features, it is possible to estimate less by setting\nn_components < n_features. It this case K is not a square matrix\nand the estimated A is the pseudo-inverse of ``W K``.\n\nThis implementation was originally made for data of shape\n[n_features, n_samples]. Now the input is transposed\nbefore the algorithm is applied. This makes it slightly\nfaster for Fortran-ordered input.\n\nImplemented using FastICA:\n*A. Hyvarinen and E. Oja, Independent Component Analysis:\nAlgorithms and Applications, Neural Networks, 13(4-5), 2000,\npp. 411-430*",
      "code": "@_deprecate_positional_args\ndef fastica(X, n_components=None, *, algorithm=\"parallel\", whiten=True,\n            fun=\"logcosh\", fun_args=None, max_iter=200, tol=1e-04, w_init=None,\n            random_state=None, return_X_mean=False, compute_sources=True,\n            return_n_iter=False):\n    \"\"\"Perform Fast Independent Component Analysis.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    n_components : int, default=None\n        Number of components to extract. If None no dimension reduction\n        is performed.\n\n    algorithm : {'parallel', 'deflation'}, default='parallel'\n        Apply a parallel or deflational FASTICA algorithm.\n\n    whiten : bool, default=True\n        If True perform an initial whitening of the data.\n        If False, the data is assumed to have already been\n        preprocessed: it should be centered, normed and white.\n        Otherwise you will get incorrect results.\n        In this case the parameter n_components will be ignored.\n\n    fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. The derivative should be averaged along its last dimension.\n        Example:\n\n        def my_g(x):\n            return x ** 3, np.mean(3 * x ** 2, axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}\n\n    max_iter : int, default=200\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-04\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : ndarray of shape (n_components, n_components), default=None\n        Initial un-mixing array of dimension (n.comp,n.comp).\n        If None (default) then an array of normal r.v.'s is used.\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_X_mean : bool, default=False\n        If True, X_mean is returned too.\n\n    compute_sources : bool, default=True\n        If False, sources are not computed, but only the rotation matrix.\n        This can save memory when working with big data. Defaults to True.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    K : ndarray of shape (n_components, n_features) or None\n        If whiten is 'True', K is the pre-whitening matrix that projects data\n        onto the first n_components principal components. If whiten is 'False',\n        K is 'None'.\n\n    W : ndarray of shape (n_components, n_components)\n        The square matrix that unmixes the data after whitening.\n        The mixing matrix is the pseudo-inverse of matrix ``W K``\n        if K is not None, else it is the inverse of W.\n\n    S : ndarray of shape (n_samples, n_components) or None\n        Estimated source matrix\n\n    X_mean : ndarray of shape (n_features,)\n        The mean over features. Returned only if return_X_mean is True.\n\n    n_iter : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge. This is\n        returned only when return_n_iter is set to `True`.\n\n    Notes\n    -----\n\n    The data matrix X is considered to be a linear combination of\n    non-Gaussian (independent) components i.e. X = AS where columns of S\n    contain the independent components and A is a linear mixing\n    matrix. In short ICA attempts to `un-mix' the data by estimating an\n    un-mixing matrix W where ``S = W K X.``\n    While FastICA was proposed to estimate as many sources\n    as features, it is possible to estimate less by setting\n    n_components < n_features. It this case K is not a square matrix\n    and the estimated A is the pseudo-inverse of ``W K``.\n\n    This implementation was originally made for data of shape\n    [n_features, n_samples]. Now the input is transposed\n    before the algorithm is applied. This makes it slightly\n    faster for Fortran-ordered input.\n\n    Implemented using FastICA:\n    *A. Hyvarinen and E. Oja, Independent Component Analysis:\n    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n    pp. 411-430*\n\n    \"\"\"\n\n    est = FastICA(n_components=n_components, algorithm=algorithm,\n                  whiten=whiten, fun=fun, fun_args=fun_args,\n                  max_iter=max_iter, tol=tol, w_init=w_init,\n                  random_state=random_state)\n    sources = est._fit(X, compute_sources=compute_sources)\n\n    if whiten:\n        if return_X_mean:\n            if return_n_iter:\n                return (est.whitening_, est._unmixing, sources, est.mean_,\n                        est.n_iter_)\n            else:\n                return est.whitening_, est._unmixing, sources, est.mean_\n        else:\n            if return_n_iter:\n                return est.whitening_, est._unmixing, sources, est.n_iter_\n            else:\n                return est.whitening_, est._unmixing, sources\n\n    else:\n        if return_X_mean:\n            if return_n_iter:\n                return None, est._unmixing, sources, None, est.n_iter_\n            else:\n                return None, est._unmixing, sources, None\n        else:\n            if return_n_iter:\n                return None, est._unmixing, sources, est.n_iter_\n            else:\n                return None, est._unmixing, sources"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of components to keep. If ``n_components`` is ``None``,\nthen ``n_components`` is set to ``min(n_samples, n_features)``."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__/whiten",
          "name": "whiten",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.__init__.whiten",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When True (False by default) the ``components_`` vectors are divided\nby ``n_samples`` times ``components_`` to ensure uncorrelated outputs\nwith unit component-wise variances.\n\nWhitening will remove some information from the transformed signal\n(the relative variance scales of the components) but can sometimes\nimprove the predictive accuracy of the downstream estimators by\nmaking data respect some hard-wired assumptions."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__/copy",
          "name": "copy",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, X will be overwritten. ``copy=False`` can be used to\nsave memory but is unsafe for general use."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/__init__/batch_size",
          "name": "batch_size",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.__init__.batch_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of samples to use for each batch. Only used when calling\n``fit``. If ``batch_size`` is ``None``, then ``batch_size``\nis inferred from the data and set to ``5 * n_features``, to provide a\nbalance between approximation accuracy and memory consumption."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Incremental principal components analysis (IPCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of\nthe data, keeping only the most significant singular vectors to\nproject the data to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nDepending on the size of the input data, this algorithm can be much more\nmemory efficient than a PCA, and allows sparse input.\n\nThis algorithm has constant memory complexity, on the order\nof ``batch_size * n_features``, enabling use of np.memmap files without\nloading the entire file into memory. For sparse matrices, the input\nis converted to dense in batches (in order to be able to subtract the\nmean) which avoids storing the entire dense matrix at any one time.\n\nThe computational overhead of each SVD is\n``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\nremain in memory at a time. There will be ``n_samples / batch_size`` SVD\ncomputations to get the principal components, versus 1 large SVD of\ncomplexity ``O(n_samples * n_features ** 2)`` for PCA.\n\nRead more in the :ref:`User Guide <IncrementalPCA>`.\n\n.. versionadded:: 0.16",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, whiten=False, copy=True,\n                 batch_size=None):\n        self.n_components = n_components\n        self.whiten = whiten\n        self.copy = copy\n        self.batch_size = batch_size"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model with X, using minibatches of size batch_size.",
      "docstring": "Fit the model with X, using minibatches of size batch_size.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model with X, using minibatches of size batch_size.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self.components_ = None\n        self.n_samples_seen_ = 0\n        self.mean_ = .0\n        self.var_ = .0\n        self.singular_values_ = None\n        self.explained_variance_ = None\n        self.explained_variance_ratio_ = None\n        self.noise_variance_ = None\n\n        X = self._validate_data(X, accept_sparse=['csr', 'csc', 'lil'],\n                                copy=self.copy, dtype=[np.float64, np.float32])\n        n_samples, n_features = X.shape\n\n        if self.batch_size is None:\n            self.batch_size_ = 5 * n_features\n        else:\n            self.batch_size_ = self.batch_size\n\n        for batch in gen_batches(n_samples, self.batch_size_,\n                                 min_batch_size=self.n_components or 0):\n            X_batch = X[batch]\n            if sparse.issparse(X_batch):\n                X_batch = X_batch.toarray()\n            self.partial_fit(X_batch, check_input=False)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/partial_fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/partial_fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/partial_fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/partial_fit/check_input",
          "name": "check_input",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.partial_fit.check_input",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Run check_array on X."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Incremental fit with X. All of X is processed as a single batch.",
      "docstring": "Incremental fit with X. All of X is processed as a single batch.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples and\n    n_features is the number of features.\n\ncheck_input : bool, default=True\n    Run check_array on X.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def partial_fit(self, X, y=None, check_input=True):\n        \"\"\"Incremental fit with X. All of X is processed as a single batch.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        check_input : bool, default=True\n            Run check_array on X.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        first_pass = not hasattr(self, \"components_\")\n        if check_input:\n            if sparse.issparse(X):\n                raise TypeError(\n                    \"IncrementalPCA.partial_fit does not support \"\n                    \"sparse input. Either convert data to dense \"\n                    \"or use IncrementalPCA.fit to do so in batches.\")\n            X = self._validate_data(\n                X, copy=self.copy, dtype=[np.float64, np.float32],\n                reset=first_pass)\n        n_samples, n_features = X.shape\n        if first_pass:\n            self.components_ = None\n\n        if self.n_components is None:\n            if self.components_ is None:\n                self.n_components_ = min(n_samples, n_features)\n            else:\n                self.n_components_ = self.components_.shape[0]\n        elif not 1 <= self.n_components <= n_features:\n            raise ValueError(\"n_components=%r invalid for n_features=%d, need \"\n                             \"more rows than columns for IncrementalPCA \"\n                             \"processing\" % (self.n_components, n_features))\n        elif not self.n_components <= n_samples:\n            raise ValueError(\"n_components=%r must be less or equal to \"\n                             \"the batch number of samples \"\n                             \"%d.\" % (self.n_components, n_samples))\n        else:\n            self.n_components_ = self.n_components\n\n        if (self.components_ is not None) and (self.components_.shape[0] !=\n                                               self.n_components_):\n            raise ValueError(\"Number of input features has changed from %i \"\n                             \"to %i between calls to partial_fit! Try \"\n                             \"setting n_components to a fixed value.\" %\n                             (self.components_.shape[0], self.n_components_))\n\n        # This is the first partial_fit\n        if not hasattr(self, 'n_samples_seen_'):\n            self.n_samples_seen_ = 0\n            self.mean_ = .0\n            self.var_ = .0\n\n        # Update stats - they are 0 if this is the first step\n        col_mean, col_var, n_total_samples = \\\n            _incremental_mean_and_var(\n                X, last_mean=self.mean_, last_variance=self.var_,\n                last_sample_count=np.repeat(self.n_samples_seen_, X.shape[1]))\n        n_total_samples = n_total_samples[0]\n\n        # Whitening\n        if self.n_samples_seen_ == 0:\n            # If it is the first step, simply whiten X\n            X -= col_mean\n        else:\n            col_batch_mean = np.mean(X, axis=0)\n            X -= col_batch_mean\n            # Build matrix of combined previous basis and new data\n            mean_correction = \\\n                np.sqrt((self.n_samples_seen_ / n_total_samples) *\n                        n_samples) * (self.mean_ - col_batch_mean)\n            X = np.vstack((self.singular_values_.reshape((-1, 1)) *\n                           self.components_, X, mean_correction))\n\n        U, S, Vt = linalg.svd(X, full_matrices=False, check_finite=False)\n        U, Vt = svd_flip(U, Vt, u_based_decision=False)\n        explained_variance = S ** 2 / (n_total_samples - 1)\n        explained_variance_ratio = S ** 2 / np.sum(col_var * n_total_samples)\n\n        self.n_samples_seen_ = n_total_samples\n        self.components_ = Vt[:self.n_components_]\n        self.singular_values_ = S[:self.n_components_]\n        self.mean_ = col_mean\n        self.var_ = col_var\n        self.explained_variance_ = explained_variance[:self.n_components_]\n        self.explained_variance_ratio_ = \\\n            explained_variance_ratio[:self.n_components_]\n        if self.n_components_ < n_features:\n            self.noise_variance_ = \\\n                explained_variance[self.n_components_:].mean()\n        else:\n            self.noise_variance_ = 0.\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._incremental_pca/IncrementalPCA/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._incremental_pca.IncrementalPCA.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply dimensionality reduction to X.\n\nX is projected on the first principal components previously extracted\nfrom a training set, using minibatches of size batch_size if X is\nsparse.",
      "docstring": "Apply dimensionality reduction to X.\n\nX is projected on the first principal components previously extracted\nfrom a training set, using minibatches of size batch_size if X is\nsparse.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data, where n_samples is the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from sklearn.decomposition import IncrementalPCA\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n...               [1, 1], [2, 1], [3, 2]])\n>>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n>>> ipca.fit(X)\nIncrementalPCA(batch_size=3, n_components=2)\n>>> ipca.transform(X) # doctest: +SKIP",
      "code": "    def transform(self, X):\n        \"\"\"Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set, using minibatches of size batch_size if X is\n        sparse.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> from sklearn.decomposition import IncrementalPCA\n        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n        ...               [1, 1], [2, 1], [3, 2]])\n        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n        >>> ipca.fit(X)\n        IncrementalPCA(batch_size=3, n_components=2)\n        >>> ipca.transform(X) # doctest: +SKIP\n        \"\"\"\n        if sparse.issparse(X):\n            n_samples = X.shape[0]\n            output = []\n            for batch in gen_batches(n_samples, self.batch_size_,\n                                     min_batch_size=self.n_components or 0):\n                output.append(super().transform(X[batch].toarray()))\n            return np.vstack(output)\n        else:\n            return super().transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of components. If None, all non-zero components are kept."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/kernel",
          "name": "kernel",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.kernel",
          "default_value": "'linear'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'linear', 'poly',             'rbf', 'sigmoid', 'cosine', 'precomputed'}",
            "default_value": "'linear'",
            "description": "Kernel used for PCA."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "sigmoid",
              "rbf",
              "precomputed",
              "cosine",
              "poly",
              "linear"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/gamma",
          "name": "gamma",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.gamma",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\nkernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/degree",
          "name": "degree",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.degree",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Degree for poly kernels. Ignored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/coef0",
          "name": "coef0",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.coef0",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1",
            "description": "Independent term in poly and sigmoid kernels.\nIgnored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/kernel_params",
          "name": "kernel_params",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.kernel_params",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dict",
            "default_value": "None",
            "description": "Parameters (keyword arguments) and\nvalues for kernel passed as callable object.\nIgnored by other kernels."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.alpha",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "Hyperparameter of the ridge regression that learns the\ninverse transform (when fit_inverse_transform=True)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/fit_inverse_transform",
          "name": "fit_inverse_transform",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.fit_inverse_transform",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Learn the inverse transform for non-precomputed kernels.\n(i.e. learn to find the pre-image of a point)"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/eigen_solver",
          "name": "eigen_solver",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.eigen_solver",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'dense', 'arpack'}",
            "default_value": "'auto'",
            "description": "Select eigensolver to use. If n_components is much less than\nthe number of training samples, arpack may be more efficient\nthan the dense eigensolver."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "arpack",
              "auto",
              "dense"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.tol",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0",
            "description": "Convergence tolerance for arpack.\nIf 0, optimal value will be chosen by arpack."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.max_iter",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Maximum number of iterations for arpack.\nIf None, optimal value will be chosen by arpack."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/remove_zero_eig",
          "name": "remove_zero_eig",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.remove_zero_eig",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, then all components with zero eigenvalues are removed, so\nthat the number of components in the output may be < n_components\n(and sometimes even zero due to numerical instability).\nWhen n_components is None, this parameter is ignored and components\nwith zero eigenvalues are removed regardless."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducible\nresults across multiple function calls.\nSee :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/copy_X",
          "name": "copy_X",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.copy_X",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If True, input X is copied and stored by the model in the `X_fit_`\nattribute. If no further changes will be done to X, setting\n`copy_X=False` saves memory by storing a reference.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Kernel Principal component analysis (KPCA).\n\nNon-linear dimensionality reduction through the use of kernels (see\n:ref:`metrics`).\n\nRead more in the :ref:`User Guide <kernel_PCA>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, kernel=\"linear\",\n                 gamma=None, degree=3, coef0=1, kernel_params=None,\n                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',\n                 tol=0, max_iter=None, remove_zero_eig=False,\n                 random_state=None, copy_X=True, n_jobs=None):\n        if fit_inverse_transform and kernel == 'precomputed':\n            raise ValueError(\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.remove_zero_eig = remove_zero_eig\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model from data in X.",
      "docstring": "Fit the model from data in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr', copy=self.copy_X)\n        self._centerer = KernelCenterer()\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n\n        if self.fit_inverse_transform:\n            # no need to use the kernel to transform X, use shortcut expression\n            X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n            self._fit_inverse_transform(X_transformed, X)\n\n        self.X_fit_ = X\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit_transform/y",
          "name": "y",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/fit_transform/params",
          "name": "params",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.fit_transform.params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model from data in X and transform X.",
      "docstring": "Fit the model from data in X and transform X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)",
      "code": "    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        # no need to use the kernel to transform X, use shortcut expression\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.decomposition._kernel_pca.KernelPCA.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/inverse_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.inverse_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_components)",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_components)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X back to original space.\n\n``inverse_transform`` approximates the inverse transformation using\na learned pre-image. The pre-image is learned by kernel ridge\nregression of the original data on their low-dimensional representation\nvectors.\n\n.. note:\n    :meth:`~sklearn.decomposition.fit` internally uses a centered\n    kernel. As the centered kernel no longer contains the information\n    of the mean of kernel features, such information is not taken into\n    account in reconstruction.\n\n.. note::\n    When users want to compute inverse transformation for 'linear'\n    kernel, it is recommended that they use\n    :class:`~sklearn.decomposition.PCA` instead. Unlike\n    :class:`~sklearn.decomposition.PCA`,\n    :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n    does not reconstruct the mean of data when 'linear' kernel is used\n    due to the use of centered kernel.",
      "docstring": "Transform X back to original space.\n\n``inverse_transform`` approximates the inverse transformation using\na learned pre-image. The pre-image is learned by kernel ridge\nregression of the original data on their low-dimensional representation\nvectors.\n\n.. note:\n    :meth:`~sklearn.decomposition.fit` internally uses a centered\n    kernel. As the centered kernel no longer contains the information\n    of the mean of kernel features, such information is not taken into\n    account in reconstruction.\n\n.. note::\n    When users want to compute inverse transformation for 'linear'\n    kernel, it is recommended that they use\n    :class:`~sklearn.decomposition.PCA` instead. Unlike\n    :class:`~sklearn.decomposition.PCA`,\n    :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n    does not reconstruct the mean of data when 'linear' kernel is used\n    due to the use of centered kernel.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_components)\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_features)\n\nReferences\n----------\n\"Learning to Find Pre-Images\", G BakIr et al, 2004.",
      "code": "    def inverse_transform(self, X):\n        \"\"\"Transform X back to original space.\n\n        ``inverse_transform`` approximates the inverse transformation using\n        a learned pre-image. The pre-image is learned by kernel ridge\n        regression of the original data on their low-dimensional representation\n        vectors.\n\n        .. note:\n            :meth:`~sklearn.decomposition.fit` internally uses a centered\n            kernel. As the centered kernel no longer contains the information\n            of the mean of kernel features, such information is not taken into\n            account in reconstruction.\n\n        .. note::\n            When users want to compute inverse transformation for 'linear'\n            kernel, it is recommended that they use\n            :class:`~sklearn.decomposition.PCA` instead. Unlike\n            :class:`~sklearn.decomposition.PCA`,\n            :class:`~sklearn.decomposition.KernelPCA`'s ``inverse_transform``\n            does not reconstruct the mean of data when 'linear' kernel is used\n            due to the use of centered kernel.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_components)\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_features)\n\n        References\n        ----------\n        \"Learning to Find Pre-Images\", G BakIr et al, 2004.\n        \"\"\"\n        if not self.fit_inverse_transform:\n            raise NotFittedError(\"The fit_inverse_transform parameter was not\"\n                                 \" set to True when instantiating and hence \"\n                                 \"the inverse transform is not available.\")\n\n        K = self._get_kernel(X, self.X_transformed_fit_)\n        return np.dot(K, self.dual_coef_)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._kernel_pca.KernelPCA.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._kernel_pca/KernelPCA/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._kernel_pca.KernelPCA.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X.",
      "docstring": "Transform X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)",
      "code": "    def transform(self, X):\n        \"\"\"Transform X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n\n        # Compute centered gram matrix between X and training data X_fit_\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n\n        # scale eigenvectors (properly account for null-space for dot product)\n        non_zeros = np.flatnonzero(self.lambdas_)\n        scaled_alphas = np.zeros_like(self.alphas_)\n        scaled_alphas[:, non_zeros] = (self.alphas_[:, non_zeros]\n                                       / np.sqrt(self.lambdas_[non_zeros]))\n\n        # Project with a scalar product between K and the scaled eigenvectors\n        return np.dot(K, scaled_alphas)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.n_components",
          "default_value": "10",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Number of topics.\n\n.. versionchanged:: 0.19\n    ``n_topics`` was renamed to ``n_components``"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/doc_topic_prior",
          "name": "doc_topic_prior",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.doc_topic_prior",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Prior of document topic distribution `theta`. If the value is None,\ndefaults to `1 / n_components`.\nIn [1]_, this is called `alpha`."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/topic_word_prior",
          "name": "topic_word_prior",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.topic_word_prior",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Prior of topic word distribution `beta`. If the value is None, defaults\nto `1 / n_components`.\nIn [1]_, this is called `eta`."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/learning_method",
          "name": "learning_method",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.learning_method",
          "default_value": "'batch'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'batch', 'online'}",
            "default_value": "'batch'",
            "description": "Method used to update `_component`. Only used in :meth:`fit` method.\nIn general, if the data size is large, the online update will be much\nfaster than the batch update.\n\nValid options::\n\n    'batch': Batch variational Bayes method. Use all training data in\n        each EM update.\n        Old `components_` will be overwritten in each iteration.\n    'online': Online variational Bayes method. In each EM update, use\n        mini-batch of training data to update the ``components_``\n        variable incrementally. The learning rate is controlled by the\n        ``learning_decay`` and the ``learning_offset`` parameters.\n\n.. versionchanged:: 0.20\n    The default learning method is now ``\"batch\"``."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "online",
              "batch"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/learning_decay",
          "name": "learning_decay",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.learning_decay",
          "default_value": "0.7",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.7",
            "description": "It is a parameter that control learning rate in the online learning\nmethod. The value should be set between (0.5, 1.0] to guarantee\nasymptotic convergence. When the value is 0.0 and batch_size is\n``n_samples``, the update method is same as batch learning. In the\nliterature, this is called kappa."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/learning_offset",
          "name": "learning_offset",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.learning_offset",
          "default_value": "10.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "10.",
            "description": "A (positive) parameter that downweights early iterations in online\nlearning.  It should be greater than 1.0. In the literature, this is\ncalled tau_0."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.max_iter",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The maximum number of iterations."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/batch_size",
          "name": "batch_size",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.batch_size",
          "default_value": "128",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "128",
            "description": "Number of documents to use in each EM iteration. Only used in online\nlearning."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/evaluate_every",
          "name": "evaluate_every",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.evaluate_every",
          "default_value": "-1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "-1",
            "description": "How often to evaluate perplexity. Only used in `fit` method.\nset it to 0 or negative number to not evaluate perplexity in\ntraining at all. Evaluating perplexity can help you check convergence\nin training process, but it will also increase total training time.\nEvaluating perplexity in every iteration might increase training time\nup to two-fold."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/total_samples",
          "name": "total_samples",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.total_samples",
          "default_value": "1000000.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1e6",
            "description": "Total number of documents. Only used in the :meth:`partial_fit` method."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/perp_tol",
          "name": "perp_tol",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.perp_tol",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-1",
            "description": "Perplexity tolerance in batch learning. Only used when\n``evaluate_every`` is greater than 0."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/mean_change_tol",
          "name": "mean_change_tol",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.mean_change_tol",
          "default_value": "0.001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-3",
            "description": "Stopping tolerance for updating document topic distribution in E-step."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/max_doc_update_iter",
          "name": "max_doc_update_iter",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.max_doc_update_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Max number of iterations for updating document topic distribution in\nthe E-step."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to use in the E-step.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity level."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Pass an int for reproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Latent Dirichlet Allocation with online variational Bayes algorithm\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <LatentDirichletAllocation>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=10, *, doc_topic_prior=None,\n                 topic_word_prior=None, learning_method='batch',\n                 learning_decay=.7, learning_offset=10., max_iter=10,\n                 batch_size=128, evaluate_every=-1, total_samples=1e6,\n                 perp_tol=1e-1, mean_change_tol=1e-3, max_doc_update_iter=100,\n                 n_jobs=None, verbose=0, random_state=None):\n        self.n_components = n_components\n        self.doc_topic_prior = doc_topic_prior\n        self.topic_word_prior = topic_word_prior\n        self.learning_method = learning_method\n        self.learning_decay = learning_decay\n        self.learning_offset = learning_offset\n        self.max_iter = max_iter\n        self.batch_size = batch_size\n        self.evaluate_every = evaluate_every\n        self.total_samples = total_samples\n        self.perp_tol = perp_tol\n        self.mean_change_tol = mean_change_tol\n        self.max_doc_update_iter = max_doc_update_iter\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Document word matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn model for the data X with variational Bayes method.\n\nWhen `learning_method` is 'online', use mini-batch update.\nOtherwise, use batch update.",
      "docstring": "Learn model for the data X with variational Bayes method.\n\nWhen `learning_method` is 'online', use mini-batch update.\nOtherwise, use batch update.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Learn model for the data X with variational Bayes method.\n\n        When `learning_method` is 'online', use mini-batch update.\n        Otherwise, use batch update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._check_params()\n        X = self._check_non_neg_array(X, reset_n_features=True,\n                                      whom=\"LatentDirichletAllocation.fit\")\n        n_samples, n_features = X.shape\n        max_iter = self.max_iter\n        evaluate_every = self.evaluate_every\n        learning_method = self.learning_method\n\n        batch_size = self.batch_size\n\n        # initialize parameters\n        self._init_latent_vars(n_features)\n        # change to perplexity later\n        last_bound = None\n        n_jobs = effective_n_jobs(self.n_jobs)\n        with Parallel(n_jobs=n_jobs,\n                      verbose=max(0, self.verbose - 1)) as parallel:\n            for i in range(max_iter):\n                if learning_method == 'online':\n                    for idx_slice in gen_batches(n_samples, batch_size):\n                        self._em_step(X[idx_slice, :], total_samples=n_samples,\n                                      batch_update=False, parallel=parallel)\n                else:\n                    # batch update\n                    self._em_step(X, total_samples=n_samples,\n                                  batch_update=True, parallel=parallel)\n\n                # check perplexity\n                if evaluate_every > 0 and (i + 1) % evaluate_every == 0:\n                    doc_topics_distr, _ = self._e_step(X, cal_sstats=False,\n                                                       random_init=False,\n                                                       parallel=parallel)\n                    bound = self._perplexity_precomp_distr(X, doc_topics_distr,\n                                                           sub_sampling=False)\n                    if self.verbose:\n                        print('iteration: %d of max_iter: %d, perplexity: %.4f'\n                              % (i + 1, max_iter, bound))\n\n                    if last_bound and abs(last_bound - bound) < self.perp_tol:\n                        break\n                    last_bound = bound\n\n                elif self.verbose:\n                    print('iteration: %d of max_iter: %d' % (i + 1, max_iter))\n                self.n_iter_ += 1\n\n        # calculate final perplexity value on train set\n        doc_topics_distr, _ = self._e_step(X, cal_sstats=False,\n                                           random_init=False,\n                                           parallel=parallel)\n        self.bound_ = self._perplexity_precomp_distr(X, doc_topics_distr,\n                                                     sub_sampling=False)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/partial_fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/partial_fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Document word matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/partial_fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Online VB with Mini-Batch update.",
      "docstring": "Online VB with Mini-Batch update.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def partial_fit(self, X, y=None):\n        \"\"\"Online VB with Mini-Batch update.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._check_params()\n        first_time = not hasattr(self, 'components_')\n        X = self._check_non_neg_array(\n            X, reset_n_features=first_time,\n            whom=\"LatentDirichletAllocation.partial_fit\")\n        n_samples, n_features = X.shape\n        batch_size = self.batch_size\n\n        # initialize parameters or check\n        if first_time:\n            self._init_latent_vars(n_features)\n\n        if n_features != self.components_.shape[1]:\n            raise ValueError(\n                \"The provided data has %d dimensions while \"\n                \"the model was trained with feature size %d.\" %\n                (n_features, self.components_.shape[1]))\n\n        n_jobs = effective_n_jobs(self.n_jobs)\n        with Parallel(n_jobs=n_jobs,\n                      verbose=max(0, self.verbose - 1)) as parallel:\n            for idx_slice in gen_batches(n_samples, batch_size):\n                self._em_step(X[idx_slice, :],\n                              total_samples=self.total_samples,\n                              batch_update=False,\n                              parallel=parallel)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/perplexity",
      "name": "perplexity",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.perplexity",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/perplexity/self",
          "name": "self",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.perplexity.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/perplexity/X",
          "name": "X",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.perplexity.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Document word matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/perplexity/sub_sampling",
          "name": "sub_sampling",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.perplexity.sub_sampling",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "",
            "description": "Do sub-sampling or not."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Calculate approximate perplexity for data X.\n\nPerplexity is defined as exp(-1. * log-likelihood per word)\n\n.. versionchanged:: 0.19\n   *doc_topic_distr* argument has been deprecated and is ignored\n   because user no longer has access to unnormalized distribution",
      "docstring": "Calculate approximate perplexity for data X.\n\nPerplexity is defined as exp(-1. * log-likelihood per word)\n\n.. versionchanged:: 0.19\n   *doc_topic_distr* argument has been deprecated and is ignored\n   because user no longer has access to unnormalized distribution\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\nsub_sampling : bool\n    Do sub-sampling or not.\n\nReturns\n-------\nscore : float\n    Perplexity score.",
      "code": "    def perplexity(self, X, sub_sampling=False):\n        \"\"\"Calculate approximate perplexity for data X.\n\n        Perplexity is defined as exp(-1. * log-likelihood per word)\n\n        .. versionchanged:: 0.19\n           *doc_topic_distr* argument has been deprecated and is ignored\n           because user no longer has access to unnormalized distribution\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        sub_sampling : bool\n            Do sub-sampling or not.\n\n        Returns\n        -------\n        score : float\n            Perplexity score.\n        \"\"\"\n        return self._perplexity_precomp_distr(X, sub_sampling=sub_sampling)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/score",
      "name": "score",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/score/self",
          "name": "self",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/score/X",
          "name": "X",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Document word matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/score/y",
          "name": "y",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Calculate approximate log-likelihood as score.",
      "docstring": "Calculate approximate log-likelihood as score.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\ny : Ignored\n\nReturns\n-------\nscore : float\n    Use approximate bound as score.",
      "code": "    def score(self, X, y=None):\n        \"\"\"Calculate approximate log-likelihood as score.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        y : Ignored\n\n        Returns\n        -------\n        score : float\n            Use approximate bound as score.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_non_neg_array(X, reset_n_features=False,\n                                      whom=\"LatentDirichletAllocation.score\")\n\n        doc_topic_distr = self._unnormalized_transform(X)\n        score = self._approx_bound(X, doc_topic_distr, sub_sampling=False)\n        return score"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._lda/LatentDirichletAllocation/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._lda.LatentDirichletAllocation.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Document word matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform data X according to the fitted model.\n\n   .. versionchanged:: 0.18\n      *doc_topic_distr* is now normalized",
      "docstring": "Transform data X according to the fitted model.\n\n   .. versionchanged:: 0.18\n      *doc_topic_distr* is now normalized\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document word matrix.\n\nReturns\n-------\ndoc_topic_distr : ndarray of shape (n_samples, n_components)\n    Document topic distribution for X.",
      "code": "    def transform(self, X):\n        \"\"\"Transform data X according to the fitted model.\n\n           .. versionchanged:: 0.18\n              *doc_topic_distr* is now normalized\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document word matrix.\n\n        Returns\n        -------\n        doc_topic_distr : ndarray of shape (n_samples, n_components)\n            Document topic distribution for X.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_non_neg_array(\n            X, reset_n_features=False,\n            whom=\"LatentDirichletAllocation.transform\")\n        doc_topic_distr = self._unnormalized_transform(X)\n        doc_topic_distr /= doc_topic_distr.sum(axis=1)[:, np.newaxis]\n        return doc_topic_distr"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._nmf.NMF.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of components, if n_components is not set all features\nare kept."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/init",
          "name": "init",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.init",
          "default_value": "'warn'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}",
            "default_value": "None",
            "description": "Method used to initialize the procedure.\nDefault: None.\nValid options:\n\n- `None`: 'nndsvd' if n_components <= min(n_samples, n_features),\n  otherwise random.\n\n- `'random'`: non-negative random matrices, scaled with:\n  sqrt(X.mean() / n_components)\n\n- `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n  initialization (better for sparseness)\n\n- `'nndsvda'`: NNDSVD with zeros filled with the average of X\n  (better when sparsity is not desired)\n\n- `'nndsvdar'` NNDSVD with zeros filled with small random values\n  (generally faster, less accurate alternative to NNDSVDa\n  for when sparsity is not desired)\n\n- `'custom'`: use custom matrices W and H"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "nndsvdar",
              "random",
              "nndsvda",
              "custom",
              "nndsvd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/solver",
          "name": "solver",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.solver",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'mu'}",
            "default_value": "'cd'",
            "description": "Numerical solver to use:\n'cd' is a Coordinate Descent solver.\n'mu' is a Multiplicative Update solver.\n\n.. versionadded:: 0.17\n   Coordinate Descent solver.\n\n.. versionadded:: 0.19\n   Multiplicative Update solver."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "mu",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/beta_loss",
          "name": "beta_loss",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.beta_loss",
          "default_value": "'frobenius'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or {'frobenius', 'kullback-leibler',             'itakura-saito'}",
            "default_value": "'frobenius'",
            "description": "Beta divergence to be minimized, measuring the distance between X\nand the dot product WH. Note that values different from 'frobenius'\n(or 2) and 'kullback-leibler' (or 1) lead to significantly slower\nfits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\nmatrix X cannot contain zeros. Used only in 'mu' solver.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "kullback-leibler",
                  "itakura-saito",
                  "frobenius"
                ]
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Tolerance of the stopping condition."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations before timing out."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for initialisation (when ``init`` == 'nndsvdar' or\n'random'), and in Coordinate Descent. Pass an int for reproducible\nresults across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.",
            "description": "Constant that multiplies the regularization terms. Set it to zero to\nhave no regularization.\n\n.. versionadded:: 0.17\n   *alpha* used in the Coordinate Descent solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/l1_ratio",
          "name": "l1_ratio",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.l1_ratio",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.",
            "description": "The regularization mixing parameter, with 0 <= l1_ratio <= 1.\nFor l1_ratio = 0 the penalty is an elementwise L2 penalty\n(aka Frobenius Norm).\nFor l1_ratio = 1 it is an elementwise L1 penalty.\nFor 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n.. versionadded:: 0.17\n   Regularization parameter *l1_ratio* used in the Coordinate Descent\n   solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Whether to be verbose."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/shuffle",
          "name": "shuffle",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.shuffle",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If true, randomize the order of coordinates in the CD solver.\n\n.. versionadded:: 0.17\n   *shuffle* parameter used in the Coordinate Descent solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/__init__/regularization",
          "name": "regularization",
          "qname": "sklearn.decomposition._nmf.NMF.__init__.regularization",
          "default_value": "'both'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'both', 'components', 'transformation', None}",
            "default_value": "'both'",
            "description": "Select whether the regularization affects the components (H), the\ntransformation (W), both or none of them.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "transformation",
              "components",
              "both"
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Non-Negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nRead more in the :ref:`User Guide <NMF>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, init='warn', solver='cd',\n                 beta_loss='frobenius', tol=1e-4, max_iter=200,\n                 random_state=None, alpha=0., l1_ratio=0., verbose=0,\n                 shuffle=False, regularization='both'):\n        self.n_components = n_components\n        self.init = init\n        self.solver = solver\n        self.beta_loss = beta_loss\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.alpha = alpha\n        self.l1_ratio = l1_ratio\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.regularization = regularization"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._nmf.NMF.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._nmf.NMF.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._nmf.NMF.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data matrix to be decomposed"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._nmf.NMF.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit/params",
          "name": "params",
          "qname": "sklearn.decomposition._nmf.NMF.fit.params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn a NMF model for the data X.",
      "docstring": "Learn a NMF model for the data X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data matrix to be decomposed\n\ny : Ignored\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None, **params):\n        \"\"\"Learn a NMF model for the data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.fit_transform(X, **params)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.decomposition._nmf.NMF.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._nmf.NMF.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._nmf.NMF.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data matrix to be decomposed"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform/y",
          "name": "y",
          "qname": "sklearn.decomposition._nmf.NMF.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform/W",
          "name": "W",
          "qname": "sklearn.decomposition._nmf.NMF.fit_transform.W",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_components)",
            "default_value": "",
            "description": "If init='custom', it is used as initial guess for the solution."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/fit_transform/H",
          "name": "H",
          "qname": "sklearn.decomposition._nmf.NMF.fit_transform.H",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_components, n_features)",
            "default_value": "",
            "description": "If init='custom', it is used as initial guess for the solution."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_components, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn a NMF model for the data X and returns the transformed data.\n\nThis is more efficient than calling fit followed by transform.",
      "docstring": "Learn a NMF model for the data X and returns the transformed data.\n\nThis is more efficient than calling fit followed by transform.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data matrix to be decomposed\n\ny : Ignored\n\nW : array-like of shape (n_samples, n_components)\n    If init='custom', it is used as initial guess for the solution.\n\nH : array-like of shape (n_components, n_features)\n    If init='custom', it is used as initial guess for the solution.\n\nReturns\n-------\nW : ndarray of shape (n_samples, n_components)\n    Transformed data.",
      "code": "    def fit_transform(self, X, y=None, W=None, H=None):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        y : Ignored\n\n        W : array-like of shape (n_samples, n_components)\n            If init='custom', it is used as initial guess for the solution.\n\n        H : array-like of shape (n_components, n_features)\n            If init='custom', it is used as initial guess for the solution.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n                                dtype=[np.float64, np.float32])\n\n        with config_context(assume_finite=True):\n            W, H, n_iter_ = non_negative_factorization(\n                X=X, W=W, H=H, n_components=self.n_components, init=self.init,\n                update_H=True, solver=self.solver, beta_loss=self.beta_loss,\n                tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,\n                l1_ratio=self.l1_ratio, regularization=self.regularization,\n                random_state=self.random_state, verbose=self.verbose,\n                shuffle=self.shuffle)\n\n        self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n                                                    square_root=True)\n\n        self.n_components_ = H.shape[0]\n        self.components_ = H\n        self.n_iter_ = n_iter_\n\n        return W"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/NMF/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.decomposition._nmf.NMF.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._nmf.NMF.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/inverse_transform/W",
          "name": "W",
          "qname": "sklearn.decomposition._nmf.NMF.inverse_transform.W",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{ndarray, sparse matrix} of shape (n_samples, n_components)",
            "default_value": "",
            "description": "Transformed data matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_components)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform data back to its original space.",
      "docstring": "Transform data back to its original space.\n\nParameters\n----------\nW : {ndarray, sparse matrix} of shape (n_samples, n_components)\n    Transformed data matrix.\n\nReturns\n-------\nX : {ndarray, sparse matrix} of shape (n_samples, n_features)\n    Data matrix of original shape.\n\n.. versionadded:: 0.18",
      "code": "    def inverse_transform(self, W):\n        \"\"\"Transform data back to its original space.\n\n        Parameters\n        ----------\n        W : {ndarray, sparse matrix} of shape (n_samples, n_components)\n            Transformed data matrix.\n\n        Returns\n        -------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Data matrix of original shape.\n\n        .. versionadded:: 0.18\n        \"\"\"\n        check_is_fitted(self)\n        return np.dot(W, self.components_)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/NMF/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._nmf.NMF.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._nmf.NMF.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/NMF/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._nmf.NMF.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Data matrix to be transformed by the model."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform the data X according to the fitted NMF model.",
      "docstring": "Transform the data X according to the fitted NMF model.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Data matrix to be transformed by the model.\n\nReturns\n-------\nW : ndarray of shape (n_samples, n_components)\n    Transformed data.",
      "code": "    def transform(self, X):\n        \"\"\"Transform the data X according to the fitted NMF model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data matrix to be transformed by the model.\n\n        Returns\n        -------\n        W : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n                                dtype=[np.float64, np.float32],\n                                reset=False)\n\n        with config_context(assume_finite=True):\n            W, _, n_iter_ = non_negative_factorization(\n                X=X, W=None, H=self.components_,\n                n_components=self.n_components_,\n                init=self.init, update_H=False, solver=self.solver,\n                beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,\n                alpha=self.alpha, l1_ratio=self.l1_ratio,\n                regularization=self.regularization,\n                random_state=self.random_state,\n                verbose=self.verbose, shuffle=self.shuffle)\n\n        return W"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization",
      "name": "non_negative_factorization",
      "qname": "sklearn.decomposition._nmf.non_negative_factorization",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/X",
          "name": "X",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Constant matrix."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/W",
          "name": "W",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.W",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_components)",
            "default_value": "None",
            "description": "If init='custom', it is used as initial guess for the solution."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/H",
          "name": "H",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.H",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_components, n_features)",
            "default_value": "None",
            "description": "If init='custom', it is used as initial guess for the solution.\nIf update_H=False, it is used as a constant, to solve for W only."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of components, if n_components is not set all features\nare kept."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/init",
          "name": "init",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.init",
          "default_value": "'warn'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}",
            "default_value": "None",
            "description": "Method used to initialize the procedure.\n\nValid options:\n\n- None: 'nndsvd' if n_components < n_features, otherwise 'random'.\n\n- 'random': non-negative random matrices, scaled with:\n    sqrt(X.mean() / n_components)\n\n- 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n    initialization (better for sparseness)\n\n- 'nndsvda': NNDSVD with zeros filled with the average of X\n    (better when sparsity is not desired)\n\n- 'nndsvdar': NNDSVD with zeros filled with small random values\n    (generally faster, less accurate alternative to NNDSVDa\n    for when sparsity is not desired)\n\n- 'custom': use custom matrices W and H if `update_H=True`. If\n  `update_H=False`, then only custom matrix H is used.\n\n.. versionchanged:: 0.23\n    The default value of `init` changed from 'random' to None in 0.23."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "nndsvdar",
              "random",
              "nndsvda",
              "custom",
              "nndsvd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/update_H",
          "name": "update_H",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.update_H",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Set to True, both W and H will be estimated from initial guesses.\nSet to False, only W will be estimated."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/solver",
          "name": "solver",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.solver",
          "default_value": "'cd'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'cd', 'mu'}",
            "default_value": "'cd'",
            "description": "Numerical solver to use:\n\n- 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n    Alternating Least Squares (Fast HALS).\n\n- 'mu' is a Multiplicative Update solver.\n\n.. versionadded:: 0.17\n   Coordinate Descent solver.\n\n.. versionadded:: 0.19\n   Multiplicative Update solver."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "mu",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/beta_loss",
          "name": "beta_loss",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.beta_loss",
          "default_value": "'frobenius'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or {'frobenius', 'kullback-leibler',             'itakura-saito'}",
            "default_value": "'frobenius'",
            "description": "Beta divergence to be minimized, measuring the distance between X\nand the dot product WH. Note that values different from 'frobenius'\n(or 2) and 'kullback-leibler' (or 1) lead to significantly slower\nfits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\nmatrix X cannot contain zeros. Used only in 'mu' solver.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "kullback-leibler",
                  "itakura-saito",
                  "frobenius"
                ]
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Tolerance of the stopping condition."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.max_iter",
          "default_value": "200",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "200",
            "description": "Maximum number of iterations before timing out."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.",
            "description": "Constant that multiplies the regularization terms."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/l1_ratio",
          "name": "l1_ratio",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.l1_ratio",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.",
            "description": "The regularization mixing parameter, with 0 <= l1_ratio <= 1.\nFor l1_ratio = 0 the penalty is an elementwise L2 penalty\n(aka Frobenius Norm).\nFor l1_ratio = 1 it is an elementwise L1 penalty.\nFor 0 < l1_ratio < 1, the penalty is a combination of L1 and L2."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/regularization",
          "name": "regularization",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.regularization",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'both', 'components', 'transformation'}",
            "default_value": "None",
            "description": "Select whether the regularization affects the components (H), the\ntransformation (W), both or none of them."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "transformation",
              "components",
              "both"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n'random'), and in Coordinate Descent. Pass an int for reproducible\nresults across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "The verbosity level."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._nmf/non_negative_factorization/shuffle",
          "name": "shuffle",
          "qname": "sklearn.decomposition._nmf.non_negative_factorization.shuffle",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If true, randomize the order of coordinates in the CD solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.decomposition"
      ],
      "description": "Compute Non-negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}^2` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H. If H is given and update_H=False, it solves for W only.",
      "docstring": "Compute Non-negative Matrix Factorization (NMF).\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is:\n\n    .. math::\n\n        0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n        + alpha * l1_{ratio} * ||vec(H)||_1\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n        + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\nWhere:\n\n:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\nThe generic norm :math:`||X - WH||_{loss}^2` may represent\nthe Frobenius norm or another supported beta-divergence loss.\nThe choice between options is controlled by the `beta_loss` parameter.\n\nThe objective function is minimized with an alternating minimization of W\nand H. If H is given and update_H=False, it solves for W only.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Constant matrix.\n\nW : array-like of shape (n_samples, n_components), default=None\n    If init='custom', it is used as initial guess for the solution.\n\nH : array-like of shape (n_components, n_features), default=None\n    If init='custom', it is used as initial guess for the solution.\n    If update_H=False, it is used as a constant, to solve for W only.\n\nn_components : int, default=None\n    Number of components, if n_components is not set all features\n    are kept.\n\ninit : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n    Method used to initialize the procedure.\n\n    Valid options:\n\n    - None: 'nndsvd' if n_components < n_features, otherwise 'random'.\n\n    - 'random': non-negative random matrices, scaled with:\n        sqrt(X.mean() / n_components)\n\n    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n        initialization (better for sparseness)\n\n    - 'nndsvda': NNDSVD with zeros filled with the average of X\n        (better when sparsity is not desired)\n\n    - 'nndsvdar': NNDSVD with zeros filled with small random values\n        (generally faster, less accurate alternative to NNDSVDa\n        for when sparsity is not desired)\n\n    - 'custom': use custom matrices W and H if `update_H=True`. If\n      `update_H=False`, then only custom matrix H is used.\n\n    .. versionchanged:: 0.23\n        The default value of `init` changed from 'random' to None in 0.23.\n\nupdate_H : bool, default=True\n    Set to True, both W and H will be estimated from initial guesses.\n    Set to False, only W will be estimated.\n\nsolver : {'cd', 'mu'}, default='cd'\n    Numerical solver to use:\n\n    - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n        Alternating Least Squares (Fast HALS).\n\n    - 'mu' is a Multiplicative Update solver.\n\n    .. versionadded:: 0.17\n       Coordinate Descent solver.\n\n    .. versionadded:: 0.19\n       Multiplicative Update solver.\n\nbeta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n    Beta divergence to be minimized, measuring the distance between X\n    and the dot product WH. Note that values different from 'frobenius'\n    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n    fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n    matrix X cannot contain zeros. Used only in 'mu' solver.\n\n    .. versionadded:: 0.19\n\ntol : float, default=1e-4\n    Tolerance of the stopping condition.\n\nmax_iter : int, default=200\n    Maximum number of iterations before timing out.\n\nalpha : float, default=0.\n    Constant that multiplies the regularization terms.\n\nl1_ratio : float, default=0.\n    The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n    For l1_ratio = 0 the penalty is an elementwise L2 penalty\n    (aka Frobenius Norm).\n    For l1_ratio = 1 it is an elementwise L1 penalty.\n    For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\nregularization : {'both', 'components', 'transformation'}, default=None\n    Select whether the regularization affects the components (H), the\n    transformation (W), both or none of them.\n\nrandom_state : int, RandomState instance or None, default=None\n    Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n    'random'), and in Coordinate Descent. Pass an int for reproducible\n    results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nverbose : int, default=0\n    The verbosity level.\n\nshuffle : bool, default=False\n    If true, randomize the order of coordinates in the CD solver.\n\nReturns\n-------\nW : ndarray of shape (n_samples, n_components)\n    Solution to the non-negative least squares problem.\n\nH : ndarray of shape (n_components, n_features)\n    Solution to the non-negative least squares problem.\n\nn_iter : int\n    Actual number of iterations.\n\nExamples\n--------\n>>> import numpy as np\n>>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import non_negative_factorization\n>>> W, H, n_iter = non_negative_factorization(X, n_components=2,\n... init='random', random_state=0)\n\nReferences\n----------\nCichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\nlarge scale nonnegative matrix and tensor factorizations.\"\nIEICE transactions on fundamentals of electronics, communications and\ncomputer sciences 92.3: 708-721, 2009.\n\nFevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\nfactorization with the beta-divergence. Neural Computation, 23(9).",
      "code": "@_deprecate_positional_args\ndef non_negative_factorization(X, W=None, H=None, n_components=None, *,\n                               init='warn', update_H=True, solver='cd',\n                               beta_loss='frobenius', tol=1e-4,\n                               max_iter=200, alpha=0., l1_ratio=0.,\n                               regularization=None, random_state=None,\n                               verbose=0, shuffle=False):\n    \"\"\"Compute Non-negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n            + alpha * l1_{ratio} * ||vec(H)||_1\n\n            + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n            + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The objective function is minimized with an alternating minimization of W\n    and H. If H is given and update_H=False, it solves for W only.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like of shape (n_samples, n_components), default=None\n        If init='custom', it is used as initial guess for the solution.\n\n    H : array-like of shape (n_components, n_features), default=None\n        If init='custom', it is used as initial guess for the solution.\n        If update_H=False, it is used as a constant, to solve for W only.\n\n    n_components : int, default=None\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n\n        Valid options:\n\n        - None: 'nndsvd' if n_components < n_features, otherwise 'random'.\n\n        - 'random': non-negative random matrices, scaled with:\n            sqrt(X.mean() / n_components)\n\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n            initialization (better for sparseness)\n\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n            (better when sparsity is not desired)\n\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n            (generally faster, less accurate alternative to NNDSVDa\n            for when sparsity is not desired)\n\n        - 'custom': use custom matrices W and H if `update_H=True`. If\n          `update_H=False`, then only custom matrix H is used.\n\n        .. versionchanged:: 0.23\n            The default value of `init` changed from 'random' to None in 0.23.\n\n    update_H : bool, default=True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n            Alternating Least Squares (Fast HALS).\n\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\\n            'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    alpha : float, default=0.\n        Constant that multiplies the regularization terms.\n\n    l1_ratio : float, default=0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    regularization : {'both', 'components', 'transformation'}, default=None\n        Select whether the regularization affects the components (H), the\n        transformation (W), both or none of them.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import non_negative_factorization\n    >>> W, H, n_iter = non_negative_factorization(X, n_components=2,\n    ... init='random', random_state=0)\n\n    References\n    ----------\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\n    factorization with the beta-divergence. Neural Computation, 23(9).\n    \"\"\"\n    X = check_array(X, accept_sparse=('csr', 'csc'),\n                    dtype=[np.float64, np.float32])\n    check_non_negative(X, \"NMF (input X)\")\n    beta_loss = _check_string_param(solver, regularization, beta_loss, init)\n\n    if X.min() == 0 and beta_loss <= 0:\n        raise ValueError(\"When beta_loss <= 0 and X contains zeros, \"\n                         \"the solver may diverge. Please add small values to \"\n                         \"X, or use a positive beta_loss.\")\n\n    n_samples, n_features = X.shape\n    if n_components is None:\n        n_components = n_features\n\n    if not isinstance(n_components, numbers.Integral) or n_components <= 0:\n        raise ValueError(\"Number of components must be a positive integer;\"\n                         \" got (n_components=%r)\" % n_components)\n    if not isinstance(max_iter, numbers.Integral) or max_iter < 0:\n        raise ValueError(\"Maximum number of iterations must be a positive \"\n                         \"integer; got (max_iter=%r)\" % max_iter)\n    if not isinstance(tol, numbers.Number) or tol < 0:\n        raise ValueError(\"Tolerance for stopping criteria must be \"\n                         \"positive; got (tol=%r)\" % tol)\n\n    # check W and H, or initialize them\n    if init == 'custom' and update_H:\n        _check_init(H, (n_components, n_features), \"NMF (input H)\")\n        _check_init(W, (n_samples, n_components), \"NMF (input W)\")\n        if H.dtype != X.dtype or W.dtype != X.dtype:\n            raise TypeError(\"H and W should have the same dtype as X. Got \"\n                            \"H.dtype = {} and W.dtype = {}.\"\n                            .format(H.dtype, W.dtype))\n    elif not update_H:\n        _check_init(H, (n_components, n_features), \"NMF (input H)\")\n        if H.dtype != X.dtype:\n            raise TypeError(\"H should have the same dtype as X. Got H.dtype = \"\n                            \"{}.\".format(H.dtype))\n        # 'mu' solver should not be initialized by zeros\n        if solver == 'mu':\n            avg = np.sqrt(X.mean() / n_components)\n            W = np.full((n_samples, n_components), avg, dtype=X.dtype)\n        else:\n            W = np.zeros((n_samples, n_components), dtype=X.dtype)\n    else:\n        W, H = _initialize_nmf(X, n_components, init=init,\n                               random_state=random_state)\n\n    l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = _compute_regularization(\n        alpha, l1_ratio, regularization)\n\n    if solver == 'cd':\n        W, H, n_iter = _fit_coordinate_descent(X, W, H, tol, max_iter,\n                                               l1_reg_W, l1_reg_H,\n                                               l2_reg_W, l2_reg_H,\n                                               update_H=update_H,\n                                               verbose=verbose,\n                                               shuffle=shuffle,\n                                               random_state=random_state)\n    elif solver == 'mu':\n        W, H, n_iter = _fit_multiplicative_update(X, W, H, beta_loss, max_iter,\n                                                  tol, l1_reg_W, l1_reg_H,\n                                                  l2_reg_W, l2_reg_H, update_H,\n                                                  verbose)\n\n    else:\n        raise ValueError(\"Invalid solver parameter '%s'.\" % solver)\n\n    if n_iter == max_iter and tol > 0:\n        warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n                      \" improve convergence.\" % max_iter, ConvergenceWarning)\n\n    return W, H, n_iter"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._pca.PCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._pca.PCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._pca.PCA.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int, float or 'mle'",
            "default_value": "None",
            "description": "Number of components to keep.\nif n_components is not set all components are kept::\n\n    n_components == min(n_samples, n_features)\n\nIf ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\nMLE is used to guess the dimension. Use of ``n_components == 'mle'``\nwill interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\nIf ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\nnumber of components such that the amount of variance that needs to be\nexplained is greater than the percentage specified by n_components.\n\nIf ``svd_solver == 'arpack'``, the number of components must be\nstrictly less than the minimum of n_features and n_samples.\n\nHence, the None case results in::\n\n    n_components == min(n_samples, n_features) - 1"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "'mle'"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/copy",
          "name": "copy",
          "qname": "sklearn.decomposition._pca.PCA.__init__.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If False, data passed to fit are overwritten and running\nfit(X).transform(X) will not yield the expected results,\nuse fit_transform(X) instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/whiten",
          "name": "whiten",
          "qname": "sklearn.decomposition._pca.PCA.__init__.whiten",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When True (False by default) the `components_` vectors are multiplied\nby the square root of n_samples and then divided by the singular values\nto ensure uncorrelated outputs with unit component-wise variances.\n\nWhitening will remove some information from the transformed signal\n(the relative variance scales of the components) but can sometime\nimprove the predictive accuracy of the downstream estimators by\nmaking their data respect some hard-wired assumptions."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/svd_solver",
          "name": "svd_solver",
          "qname": "sklearn.decomposition._pca.PCA.__init__.svd_solver",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'full', 'arpack', 'randomized'}",
            "default_value": "'auto'",
            "description": "If auto :\n    The solver is selected by a default policy based on `X.shape` and\n    `n_components`: if the input data is larger than 500x500 and the\n    number of components to extract is lower than 80% of the smallest\n    dimension of the data, then the more efficient 'randomized'\n    method is enabled. Otherwise the exact full SVD is computed and\n    optionally truncated afterwards.\nIf full :\n    run exact full SVD calling the standard LAPACK solver via\n    `scipy.linalg.svd` and select the components by postprocessing\nIf arpack :\n    run SVD truncated to n_components calling ARPACK solver via\n    `scipy.sparse.linalg.svds`. It requires strictly\n    0 < n_components < min(X.shape)\nIf randomized :\n    run randomized SVD by the method of Halko et al.\n\n.. versionadded:: 0.18.0"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto",
              "arpack",
              "full",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._pca.PCA.__init__.tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "Tolerance for singular values computed by svd_solver == 'arpack'.\nMust be of range [0.0, infinity).\n\n.. versionadded:: 0.18.0"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": "Infinity",
                "min_inclusive": true,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/iterated_power",
          "name": "iterated_power",
          "qname": "sklearn.decomposition._pca.PCA.__init__.iterated_power",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or 'auto'",
            "default_value": "'auto'",
            "description": "Number of iterations for the power method computed by\nsvd_solver == 'randomized'.\nMust be of range [0, infinity).\n\n.. versionadded:: 0.18.0"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": "Infinity",
                "min_inclusive": true,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "'auto'"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._pca.PCA.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used when the 'arpack' or 'randomized' solvers are used. Pass an int\nfor reproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.18.0"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Principal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space. The input data is centered\nbut not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nRead more in the :ref:`User Guide <PCA>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, copy=True, whiten=False,\n                 svd_solver='auto', tol=0.0, iterated_power='auto',\n                 random_state=None):\n        self.n_components = n_components\n        self.copy = copy\n        self.whiten = whiten\n        self.svd_solver = svd_solver\n        self.tol = tol\n        self.iterated_power = iterated_power\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._pca.PCA.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._pca.PCA.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._pca.PCA.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._pca.PCA.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model with X.",
      "docstring": "Fit the model with X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model with X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._fit(X)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.decomposition._pca.PCA.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._pca.PCA.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._pca.PCA.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data, where n_samples is the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/fit_transform/y",
          "name": "y",
          "qname": "sklearn.decomposition._pca.PCA.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model with X and apply the dimensionality reduction on X.",
      "docstring": "Fit the model with X and apply the dimensionality reduction on X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data, where n_samples is the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed values.\n\nNotes\n-----\nThis method returns a Fortran-ordered array. To convert it to a\nC-ordered array, use 'np.ascontiguousarray'.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed values.\n\n        Notes\n        -----\n        This method returns a Fortran-ordered array. To convert it to a\n        C-ordered array, use 'np.ascontiguousarray'.\n        \"\"\"\n        U, S, Vt = self._fit(X)\n        U = U[:, :self.n_components_]\n\n        if self.whiten:\n            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\n            U *= sqrt(X.shape[0] - 1)\n        else:\n            # X_new = X * V = U * S * Vt * V = U * S\n            U *= S[:self.n_components_]\n\n        return U"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._pca/PCA/score",
      "name": "score",
      "qname": "sklearn.decomposition._pca.PCA.score",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/score/self",
          "name": "self",
          "qname": "sklearn.decomposition._pca.PCA.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/score/X",
          "name": "X",
          "qname": "sklearn.decomposition._pca.PCA.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/score/y",
          "name": "y",
          "qname": "sklearn.decomposition._pca.PCA.score.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return the average log-likelihood of all samples.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf",
      "docstring": "Return the average log-likelihood of all samples.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data.\n\ny : Ignored\n\nReturns\n-------\nll : float\n    Average log-likelihood of the samples under the current model.",
      "code": "    def score(self, X, y=None):\n        \"\"\"Return the average log-likelihood of all samples.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model.\n        \"\"\"\n        return np.mean(self.score_samples(X))"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._pca/PCA/score_samples",
      "name": "score_samples",
      "qname": "sklearn.decomposition._pca.PCA.score_samples",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/score_samples/self",
          "name": "self",
          "qname": "sklearn.decomposition._pca.PCA.score_samples.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._pca/PCA/score_samples/X",
          "name": "X",
          "qname": "sklearn.decomposition._pca.PCA.score_samples.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return the log-likelihood of each sample.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf",
      "docstring": "Return the log-likelihood of each sample.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data.\n\nReturns\n-------\nll : ndarray of shape (n_samples,)\n    Log-likelihood of each sample under the current model.",
      "code": "    def score_samples(self, X):\n        \"\"\"Return the log-likelihood of each sample.\n\n        See. \"Pattern Recognition and Machine Learning\"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, dtype=[np.float64, np.float32], reset=False)\n        Xr = X - self.mean_\n        n_features = X.shape[1]\n        precision = self.get_precision()\n        log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= .5 * (n_features * log(2. * np.pi) -\n                          fast_logdet(precision))\n        return log_like"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "number of sparse atoms to extract"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.alpha",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "Sparsity controlling parameter. Higher values lead to sparser\ncomponents."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/ridge_alpha",
          "name": "ridge_alpha",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.ridge_alpha",
          "default_value": "0.01",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "Amount of ridge shrinkage to apply in order to improve\nconditioning when calling the transform method."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/n_iter",
          "name": "n_iter",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.n_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "number of iterations to perform for each mini batch"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/callback",
          "name": "callback",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.callback",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "callable that gets invoked every five iterations"
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/batch_size",
          "name": "batch_size",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.batch_size",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "the number of features to take in each mini batch"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or bool",
            "default_value": "False",
            "description": "Controls the verbosity; the higher, the more messages. Defaults to 0."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/shuffle",
          "name": "shuffle",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.shuffle",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "whether to shuffle the data before splitting it in batches"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/method",
          "name": "method",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.method",
          "default_value": "'lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lars', 'cd'}",
            "default_value": "'lars'",
            "description": "lars: uses the least angle regression method to solve the lasso problem\n(linear_model.lars_path)\ncd: uses the coordinate descent method to compute the\nLasso solution (linear_model.Lasso). Lars will be faster if\nthe estimated components are sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used for random shuffling when ``shuffle`` is set to ``True``,\nduring online dictionary learning. Pass an int for reproducible results\nacross multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Mini-batch Sparse Principal Components Analysis\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01,\n                 n_iter=100, callback=None, batch_size=3, verbose=False,\n                 shuffle=True, n_jobs=None, method='lars', random_state=None):\n        super().__init__(\n            n_components=n_components, alpha=alpha, verbose=verbose,\n            ridge_alpha=ridge_alpha, n_jobs=n_jobs, method=method,\n            random_state=random_state)\n        self.n_iter = n_iter\n        self.callback = callback\n        self.batch_size = batch_size\n        self.shuffle = shuffle"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/MiniBatchSparsePCA/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model from data in X.",
      "docstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n\n        self.mean_ = X.mean(axis=0)\n        X = X - self.mean_\n\n        if self.n_components is None:\n            n_components = X.shape[1]\n        else:\n            n_components = self.n_components\n        Vt, _, self.n_iter_ = dict_learning_online(\n            X.T, n_components, alpha=self.alpha,\n            n_iter=self.n_iter, return_code=True,\n            dict_init=None, verbose=self.verbose,\n            callback=self.callback,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            n_jobs=self.n_jobs, method=self.method,\n            random_state=random_state,\n            return_n_iter=True)\n        self.components_ = Vt.T\n\n        components_norm = np.linalg.norm(\n            self.components_, axis=1)[:, np.newaxis]\n        components_norm[components_norm == 0] = 1\n        self.components_ /= components_norm\n        self.n_components_ = len(self.components_)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.n_components",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of sparse atoms to extract."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.alpha",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1",
            "description": "Sparsity controlling parameter. Higher values lead to sparser\ncomponents."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/ridge_alpha",
          "name": "ridge_alpha",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.ridge_alpha",
          "default_value": "0.01",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.01",
            "description": "Amount of ridge shrinkage to apply in order to improve\nconditioning when calling the transform method."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.max_iter",
          "default_value": "1000",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1000",
            "description": "Maximum number of iterations to perform."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.tol",
          "default_value": "1e-08",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-8",
            "description": "Tolerance for the stopping condition."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/method",
          "name": "method",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.method",
          "default_value": "'lars'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'lars', 'cd'}",
            "default_value": "'lars'",
            "description": "lars: uses the least angle regression method to solve the lasso problem\n(linear_model.lars_path)\ncd: uses the coordinate descent method to compute the\nLasso solution (linear_model.Lasso). Lars will be faster if\nthe estimated components are sparse."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "lars",
              "cd"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of parallel jobs to run.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/U_init",
          "name": "U_init",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.U_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_components)",
            "default_value": "None",
            "description": "Initial values for the loadings for warm restart scenarios. Only used\nif `U_init` and `V_init` are not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_components)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/V_init",
          "name": "V_init",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.V_init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_components, n_features)",
            "default_value": "None",
            "description": "Initial values for the components for warm restart scenarios. Only used\nif `U_init` and `V_init` are not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_components, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or bool",
            "default_value": "False",
            "description": "Controls the verbosity; the higher, the more messages. Defaults to 0."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used during dictionary learning. Pass an int for reproducible results\nacross multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Sparse Principal Components Analysis (SparsePCA).\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01,\n                 max_iter=1000, tol=1e-8, method='lars', n_jobs=None,\n                 U_init=None, V_init=None, verbose=False, random_state=None):\n        self.n_components = n_components\n        self.alpha = alpha\n        self.ridge_alpha = ridge_alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.method = method\n        self.n_jobs = n_jobs\n        self.U_init = U_init\n        self.V_init = V_init\n        self.verbose = verbose\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._sparse_pca.SparsePCA.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where n_samples in the number of samples\nand n_features is the number of features."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the model from data in X.",
      "docstring": "Fit the model from data in X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vector, where n_samples in the number of samples\n    and n_features is the number of features.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the instance itself.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        random_state = check_random_state(self.random_state)\n        X = self._validate_data(X)\n\n        self.mean_ = X.mean(axis=0)\n        X = X - self.mean_\n\n        if self.n_components is None:\n            n_components = X.shape[1]\n        else:\n            n_components = self.n_components\n        code_init = self.V_init.T if self.V_init is not None else None\n        dict_init = self.U_init.T if self.U_init is not None else None\n        Vt, _, E, self.n_iter_ = dict_learning(X.T, n_components,\n                                               alpha=self.alpha,\n                                               tol=self.tol,\n                                               max_iter=self.max_iter,\n                                               method=self.method,\n                                               n_jobs=self.n_jobs,\n                                               verbose=self.verbose,\n                                               random_state=random_state,\n                                               code_init=code_init,\n                                               dict_init=dict_init,\n                                               return_n_iter=True)\n        self.components_ = Vt.T\n        components_norm = np.linalg.norm(\n            self.components_, axis=1)[:, np.newaxis]\n        components_norm[components_norm == 0] = 1\n        self.components_ /= components_norm\n        self.n_components_ = len(self.components_)\n\n        self.error_ = E\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._sparse_pca.SparsePCA.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._sparse_pca/SparsePCA/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._sparse_pca.SparsePCA.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Test data to be transformed, must have the same number of\nfeatures as the data used to train the model."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Least Squares projection of the data onto the sparse components.\n\nTo avoid instability issues in case the system is under-determined,\nregularization can be applied (Ridge regression) via the\n`ridge_alpha` parameter.\n\nNote that Sparse PCA components orthogonality is not enforced as in PCA\nhence one cannot use a simple linear projection.",
      "docstring": "Least Squares projection of the data onto the sparse components.\n\nTo avoid instability issues in case the system is under-determined,\nregularization can be applied (Ridge regression) via the\n`ridge_alpha` parameter.\n\nNote that Sparse PCA components orthogonality is not enforced as in PCA\nhence one cannot use a simple linear projection.\n\nParameters\n----------\nX : ndarray of shape (n_samples, n_features)\n    Test data to be transformed, must have the same number of\n    features as the data used to train the model.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Transformed data.",
      "code": "    def transform(self, X):\n        \"\"\"Least Squares projection of the data onto the sparse components.\n\n        To avoid instability issues in case the system is under-determined,\n        regularization can be applied (Ridge regression) via the\n        `ridge_alpha` parameter.\n\n        Note that Sparse PCA components orthogonality is not enforced as in PCA\n        hence one cannot use a simple linear projection.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Test data to be transformed, must have the same number of\n            features as the data used to train the model.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, reset=False)\n        X = X - self.mean_\n\n        U = ridge_regression(self.components_.T, X.T, self.ridge_alpha,\n                             solver='cholesky')\n\n        return U"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__",
      "name": "__init__",
      "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__/self",
          "name": "self",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__/n_components",
          "name": "n_components",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__.n_components",
          "default_value": "2",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2",
            "description": "Desired dimensionality of output data.\nMust be strictly less than the number of features.\nThe default value is useful for visualisation. For LSA, a value of\n100 is recommended."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__.algorithm",
          "default_value": "'randomized'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'arpack', 'randomized'}",
            "default_value": "'randomized'",
            "description": "SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n(scipy.sparse.linalg.svds), or \"randomized\" for the randomized\nalgorithm due to Halko (2009)."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "arpack",
              "randomized"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__/n_iter",
          "name": "n_iter",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__.n_iter",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "Number of iterations for randomized SVD solver. Not used by ARPACK. The\ndefault is larger than the default in\n:func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\nmatrices that may have large slowly decaying spectrum."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Used during randomized svd. Pass an int for reproducible results across\nmultiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/__init__/tol",
          "name": "tol",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.__init__.tol",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.",
            "description": "Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\nSVD solver."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Dimensionality reduction using truncated SVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of\ntruncated singular value decomposition (SVD). Contrary to PCA, this\nestimator does not center the data before computing the singular value\ndecomposition. This means it can work with sparse matrices\nefficiently.\n\nIn particular, truncated SVD works on term count/tf-idf matrices as\nreturned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\nthat context, it is known as latent semantic analysis (LSA).\n\nThis estimator supports two algorithms: a fast randomized SVD solver, and\na \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n`X.T * X`, whichever is more efficient.\n\nRead more in the :ref:`User Guide <LSA>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_components=2, *, algorithm=\"randomized\", n_iter=5,\n                 random_state=None, tol=0.):\n        self.algorithm = algorithm\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.random_state = random_state\n        self.tol = tol"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit",
      "name": "fit",
      "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit/self",
          "name": "self",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit/X",
          "name": "X",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit/y",
          "name": "y",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit model on training data X.",
      "docstring": "Fit model on training data X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n\nReturns\n-------\nself : object\n    Returns the transformer object.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Fit model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the transformer object.\n        \"\"\"\n        self.fit_transform(X)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/fit_transform/y",
          "name": "y",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit model to X and perform dimensionality reduction on X.",
      "docstring": "Fit model to X and perform dimensionality reduction on X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\n\ny : Ignored\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Reduced version of X. This will always be a dense array.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Fit model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'],\n                                ensure_min_features=2)\n        random_state = check_random_state(self.random_state)\n\n        if self.algorithm == \"arpack\":\n            v0 = _init_arpack_v0(min(X.shape), random_state)\n            U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol, v0=v0)\n            # svds doesn't abide by scipy.linalg.svd/randomized_svd\n            # conventions, so reverse its outputs.\n            Sigma = Sigma[::-1]\n            U, VT = svd_flip(U[:, ::-1], VT[::-1])\n\n        elif self.algorithm == \"randomized\":\n            k = self.n_components\n            n_features = X.shape[1]\n            if k >= n_features:\n                raise ValueError(\"n_components must be < n_features;\"\n                                 \" got %d >= %d\" % (k, n_features))\n            U, Sigma, VT = randomized_svd(X, self.n_components,\n                                          n_iter=self.n_iter,\n                                          random_state=random_state)\n        else:\n            raise ValueError(\"unknown algorithm %r\" % self.algorithm)\n\n        self.components_ = VT\n\n        # As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\n        # X @ V is not the same as U @ Sigma\n        if self.algorithm == \"randomized\" or \\\n                (self.algorithm == \"arpack\" and self.tol > 0):\n            X_transformed = safe_sparse_dot(X, self.components_.T)\n        else:\n            X_transformed = U * Sigma\n\n        # Calculate explained variance & explained variance ratio\n        self.explained_variance_ = exp_var = np.var(X_transformed, axis=0)\n        if sp.issparse(X):\n            _, full_var = mean_variance_axis(X, axis=0)\n            full_var = full_var.sum()\n        else:\n            full_var = np.var(X, axis=0).sum()\n        self.explained_variance_ratio_ = exp_var / full_var\n        self.singular_values_ = Sigma  # Store the singular values.\n\n        return X_transformed"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/inverse_transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.inverse_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_components)",
            "default_value": "",
            "description": "New data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_components)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform X back to its original space.\n\nReturns an array X_original whose transform would be X.",
      "docstring": "Transform X back to its original space.\n\nReturns an array X_original whose transform would be X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_components)\n    New data.\n\nReturns\n-------\nX_original : ndarray of shape (n_samples, n_features)\n    Note that this is always a dense array.",
      "code": "    def inverse_transform(self, X):\n        \"\"\"Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : ndarray of shape (n_samples, n_features)\n            Note that this is always a dense array.\n        \"\"\"\n        X = check_array(X)\n        return np.dot(X, self.components_)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/transform",
      "name": "transform",
      "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/transform/self",
          "name": "self",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition._truncated_svd/TruncatedSVD/transform/X",
          "name": "X",
          "qname": "sklearn.decomposition._truncated_svd.TruncatedSVD.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "New data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Perform dimensionality reduction on X.",
      "docstring": "Perform dimensionality reduction on X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    New data.\n\nReturns\n-------\nX_new : ndarray of shape (n_samples, n_components)\n    Reduced version of X. This will always be a dense array.",
      "code": "    def transform(self, X):\n        \"\"\"Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            New data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._validate_data(X, accept_sparse=['csr', 'csc'], reset=False)\n        return safe_sparse_dot(X, self.components_.T)"
    },
    {
      "id": "scikit-learn/sklearn.decomposition.setup/configuration",
      "name": "configuration",
      "qname": "sklearn.decomposition.setup.configuration",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.decomposition.setup/configuration/parent_package",
          "name": "parent_package",
          "qname": "sklearn.decomposition.setup.configuration.parent_package",
          "default_value": "''",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.decomposition.setup/configuration/top_path",
          "name": "top_path",
          "qname": "sklearn.decomposition.setup.configuration.top_path",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def configuration(parent_package=\"\", top_path=None):\n    config = Configuration(\"decomposition\", parent_package, top_path)\n\n    libraries = []\n    if os.name == 'posix':\n        libraries.append('m')\n\n    config.add_extension(\"_online_lda_fast\",\n                         sources=[\"_online_lda_fast.pyx\"],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_extension('_cdnmf_fast',\n                         sources=['_cdnmf_fast.pyx'],\n                         include_dirs=[numpy.get_include()],\n                         libraries=libraries)\n\n    config.add_subpackage(\"tests\")\n\n    return config"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/base_estimator",
          "name": "base_estimator",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.base_estimator",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "The base estimator to fit on random subsets of the dataset.\nIf None, then the base estimator is a\n:class:`~sklearn.tree.DecisionTreeClassifier`."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.n_estimators",
          "default_value": "10",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of base estimators in the ensemble."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.max_samples",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1.0",
            "description": "The number of samples to draw from X to train each base estimator (with\nreplacement by default, see `bootstrap` for more details).\n\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.max_features",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1.0",
            "description": "The number of features to draw from X to train each base estimator (\nwithout replacement by default, see `bootstrap_features` for more\ndetails).\n\n- If int, then draw `max_features` features.\n- If float, then draw `max_features * X.shape[1]` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.bootstrap",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether samples are drawn with replacement. If False, sampling\nwithout replacement is performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/bootstrap_features",
          "name": "bootstrap_features",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.bootstrap_features",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether features are drawn with replacement."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/oob_score",
          "name": "oob_score",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.oob_score",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use out-of-bag samples to estimate\nthe generalization error. Only available if bootstrap=True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit\na whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n.. versionadded:: 0.17\n   *warm_start* constructor parameter."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel for both :meth:`fit` and\n:meth:`predict`. ``None`` means 1 unless in a\n:obj:`joblib.parallel_backend` context. ``-1`` means using all\nprocessors. See :term:`Glossary <n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the random resampling of the original dataset\n(sample wise and feature wise).\nIf the base estimator accepts a `random_state` attribute, a different\nseed is generated for each instance in the ensemble.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "A Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base\nclassifiers each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None,\n                 n_estimators=10, *,\n                 max_samples=1.0,\n                 max_features=1.0,\n                 bootstrap=True,\n                 bootstrap_features=False,\n                 oob_score=False,\n                 warm_start=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0):\n\n        super().__init__(\n            base_estimator,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            bootstrap=bootstrap,\n            bootstrap_features=bootstrap_features,\n            oob_score=oob_score,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/decision_function",
      "name": "decision_function",
      "qname": "sklearn.ensemble._bagging.BaggingClassifier.decision_function",
      "decorators": [
        "if_delegate_has_method(delegate='base_estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Average of the decision functions of the base classifiers.",
      "docstring": "Average of the decision functions of the base classifiers.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\nscore : ndarray of shape (n_samples, k)\n    The decision function of the input samples. The columns correspond\n    to the classes in sorted order, as they appear in the attribute\n    ``classes_``. Regression and binary classification are special\n    cases with ``k == 1``, otherwise ``k==n_classes``.",
      "code": "    @if_delegate_has_method(delegate='base_estimator')\n    def decision_function(self, X):\n        \"\"\"Average of the decision functions of the base classifiers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        score : ndarray of shape (n_samples, k)\n            The decision function of the input samples. The columns correspond\n            to the classes in sorted order, as they appear in the attribute\n            ``classes_``. Regression and binary classification are special\n            cases with ``k == 1``, otherwise ``k==n_classes``.\n\n        \"\"\"\n        check_is_fitted(self)\n\n        # Check data\n        X = check_array(\n            X, accept_sparse=['csr', 'csc'], dtype=None,\n            force_all_finite=False\n        )\n\n        if self.n_features_ != X.shape[1]:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features is {0} and \"\n                             \"input n_features is {1} \"\n                             \"\".format(self.n_features_, X.shape[1]))\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n                                                             self.n_jobs)\n\n        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n            delayed(_parallel_decision_function)(\n                self.estimators_[starts[i]:starts[i + 1]],\n                self.estimators_features_[starts[i]:starts[i + 1]],\n                X)\n            for i in range(n_jobs))\n\n        # Reduce\n        decisions = sum(all_decisions) / self.n_estimators\n\n        return decisions"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class for X.\n\nThe predicted class of an input sample is computed as the class with\nthe highest mean predicted probability. If base estimators do not\nimplement a ``predict_proba`` method, then it resorts to voting.",
      "docstring": "Predict class for X.\n\nThe predicted class of an input sample is computed as the class with\nthe highest mean predicted probability. If base estimators do not\nimplement a ``predict_proba`` method, then it resorts to voting.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted classes.",
      "code": "    def predict(self, X):\n        \"\"\"Predict class for X.\n\n        The predicted class of an input sample is computed as the class with\n        the highest mean predicted probability. If base estimators do not\n        implement a ``predict_proba`` method, then it resorts to voting.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        predicted_probabilitiy = self.predict_proba(X)\n        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n                                  axis=0)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_log_proba",
      "name": "predict_log_proba",
      "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict_log_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_log_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict_log_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_log_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict_log_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the base\nestimators in the ensemble.",
      "docstring": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the base\nestimators in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the base\n        estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n            # Check data\n            X = check_array(\n                X, accept_sparse=['csr', 'csc'], dtype=None,\n                force_all_finite=False\n            )\n\n            if self.n_features_ != X.shape[1]:\n                raise ValueError(\"Number of features of the model must \"\n                                 \"match the input. Model n_features is {0} \"\n                                 \"and input n_features is {1} \"\n                                 \"\".format(self.n_features_, X.shape[1]))\n\n            # Parallel loop\n            n_jobs, n_estimators, starts = _partition_estimators(\n                self.n_estimators, self.n_jobs)\n\n            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n                delayed(_parallel_predict_log_proba)(\n                    self.estimators_[starts[i]:starts[i + 1]],\n                    self.estimators_features_[starts[i]:starts[i + 1]],\n                    X,\n                    self.n_classes_)\n                for i in range(n_jobs))\n\n            # Reduce\n            log_proba = all_log_proba[0]\n\n            for j in range(1, len(all_log_proba)):\n                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n\n            log_proba -= np.log(self.n_estimators)\n\n            return log_proba\n\n        else:\n            return np.log(self.predict_proba(X))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_proba",
      "name": "predict_proba",
      "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingClassifier/predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._bagging.BaggingClassifier.predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe mean predicted class probabilities of the base estimators in the\nensemble. If base estimators do not implement a ``predict_proba``\nmethod, then it resorts to voting and the predicted class probabilities\nof an input sample represents the proportion of estimators predicting\neach class.",
      "docstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe mean predicted class probabilities of the base estimators in the\nensemble. If base estimators do not implement a ``predict_proba``\nmethod, then it resorts to voting and the predicted class probabilities\nof an input sample represents the proportion of estimators predicting\neach class.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the mean predicted class probabilities of the base estimators in the\n        ensemble. If base estimators do not implement a ``predict_proba``\n        method, then it resorts to voting and the predicted class probabilities\n        of an input sample represents the proportion of estimators predicting\n        each class.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = check_array(\n            X, accept_sparse=['csr', 'csc'], dtype=None,\n            force_all_finite=False\n        )\n\n        if self.n_features_ != X.shape[1]:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features is {0} and \"\n                             \"input n_features is {1}.\"\n                             \"\".format(self.n_features_, X.shape[1]))\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n                                                             self.n_jobs)\n\n        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n                             **self._parallel_args())(\n            delayed(_parallel_predict_proba)(\n                self.estimators_[starts[i]:starts[i + 1]],\n                self.estimators_features_[starts[i]:starts[i + 1]],\n                X,\n                self.n_classes_)\n            for i in range(n_jobs))\n\n        # Reduce\n        proba = sum(all_proba) / self.n_estimators\n\n        return proba"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/base_estimator",
          "name": "base_estimator",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.base_estimator",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "The base estimator to fit on random subsets of the dataset.\nIf None, then the base estimator is a\n:class:`~sklearn.tree.DecisionTreeRegressor`."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.n_estimators",
          "default_value": "10",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of base estimators in the ensemble."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.max_samples",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1.0",
            "description": "The number of samples to draw from X to train each base estimator (with\nreplacement by default, see `bootstrap` for more details).\n\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.max_features",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1.0",
            "description": "The number of features to draw from X to train each base estimator (\nwithout replacement by default, see `bootstrap_features` for more\ndetails).\n\n- If int, then draw `max_features` features.\n- If float, then draw `max_features * X.shape[1]` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.bootstrap",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether samples are drawn with replacement. If False, sampling\nwithout replacement is performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/bootstrap_features",
          "name": "bootstrap_features",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.bootstrap_features",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether features are drawn with replacement."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/oob_score",
          "name": "oob_score",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.oob_score",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use out-of-bag samples to estimate\nthe generalization error. Only available if bootstrap=True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit\na whole new ensemble. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel for both :meth:`fit` and\n:meth:`predict`. ``None`` means 1 unless in a\n:obj:`joblib.parallel_backend` context. ``-1`` means using all\nprocessors. See :term:`Glossary <n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the random resampling of the original dataset\n(sample wise and feature wise).\nIf the base estimator accepts a `random_state` attribute, a different\nseed is generated for each instance in the ensemble.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "A Bagging regressor.\n\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n\n.. versionadded:: 0.15",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None,\n                 n_estimators=10, *,\n                 max_samples=1.0,\n                 max_features=1.0,\n                 bootstrap=True,\n                 bootstrap_features=False,\n                 oob_score=False,\n                 warm_start=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0):\n        super().__init__(\n            base_estimator,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            bootstrap=bootstrap,\n            bootstrap_features=bootstrap_features,\n            oob_score=oob_score,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._bagging.BaggingRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._bagging/BaggingRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._bagging.BaggingRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrices are accepted only if\nthey are supported by the base estimator."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.",
      "docstring": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrices are accepted only if\n    they are supported by the base estimator.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        # Check data\n        X = check_array(\n            X, accept_sparse=['csr', 'csc'], dtype=None,\n            force_all_finite=False\n        )\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n                                                             self.n_jobs)\n\n        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n            delayed(_parallel_predict_regression)(\n                self.estimators_[starts[i]:starts[i + 1]],\n                self.estimators_features_[starts[i]:starts[i + 1]],\n                X)\n            for i in range(n_jobs))\n\n        # Reduce\n        y_hat = sum(all_y_hat) / self.n_estimators\n\n        return y_hat"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__getitem__",
      "name": "__getitem__",
      "qname": "sklearn.ensemble._base.BaseEnsemble.__getitem__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__getitem__/self",
          "name": "self",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__getitem__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__getitem__/index",
          "name": "index",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__getitem__.index",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return the index'th estimator in the ensemble.",
      "docstring": "Return the index'th estimator in the ensemble.",
      "code": "    def __getitem__(self, index):\n        \"\"\"Return the index'th estimator in the ensemble.\"\"\"\n        return self.estimators_[index]"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._base.BaseEnsemble.__init__",
      "decorators": [
        "abstractmethod"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__init__/base_estimator",
          "name": "base_estimator",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__init__.base_estimator",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "",
            "description": "The base estimator from which the ensemble is built."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__init__.n_estimators",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "The number of estimators in the ensemble."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__init__/estimator_params",
          "name": "estimator_params",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__init__.estimator_params",
          "default_value": "tuple()",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "list of str",
            "default_value": "tuple()",
            "description": "The list of attributes to use as parameters when instantiating a\nnew base estimator. If none are given, default parameters are used."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of str"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Base class for all ensemble classes.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.",
      "docstring": "",
      "code": "    @abstractmethod\n    def __init__(self, base_estimator, *, n_estimators=10,\n                 estimator_params=tuple()):\n        # Set parameters\n        self.base_estimator = base_estimator\n        self.n_estimators = n_estimators\n        self.estimator_params = estimator_params"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__iter__",
      "name": "__iter__",
      "qname": "sklearn.ensemble._base.BaseEnsemble.__iter__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__iter__/self",
          "name": "self",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__iter__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return iterator over estimators in the ensemble.",
      "docstring": "Return iterator over estimators in the ensemble.",
      "code": "    def __iter__(self):\n        \"\"\"Return iterator over estimators in the ensemble.\"\"\"\n        return iter(self.estimators_)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__len__",
      "name": "__len__",
      "qname": "sklearn.ensemble._base.BaseEnsemble.__len__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._base/BaseEnsemble/__len__/self",
          "name": "self",
          "qname": "sklearn.ensemble._base.BaseEnsemble.__len__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return the number of estimators in the ensemble.",
      "docstring": "Return the number of estimators in the ensemble.",
      "code": "    def __len__(self):\n        \"\"\"Return the number of estimators in the ensemble.\"\"\"\n        return len(self.estimators_)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.22\n   The default value of ``n_estimators`` changed from 10 to 100\n   in 0.22."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/criterion",
          "name": "criterion",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.criterion",
          "default_value": "'gini'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"gini\", \"entropy\"}",
            "default_value": "\"gini\"",
            "description": "The function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "entropy",
              "gini"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.max_depth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.max_features",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"sqrt\", \"log2\"}, int or float",
            "default_value": "\"auto\"",
            "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `round(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "auto",
                  "sqrt",
                  "log2"
                ]
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.bootstrap",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/oob_score",
          "name": "oob_score",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.oob_score",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use out-of-bag samples to estimate the generalization score.\nOnly available if bootstrap=True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n:meth:`decision_path` and :meth:`apply` are all parallelized over the\ntrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors. See :term:`Glossary\n<n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls 3 sources of randomness:\n\n- the bootstrapping of the samples used when building trees\n  (if ``bootstrap=True``)\n- the sampling of the features to consider when looking for the best\n  split at each node (if ``max_features < n_features``)\n- the draw of the splits for each of the `max_features`\n\nSee :term:`Glossary <random_state>` for details."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/class_weight",
          "name": "class_weight",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.class_weight",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"balanced\", \"balanced_subsample\"}, dict or list of dicts",
            "default_value": "None",
            "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that\nweights are computed based on the bootstrap sample for every tree\ngrown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "balanced_subsample",
                  "balanced"
                ]
              },
              {
                "kind": "NamedType",
                "name": "dict"
              },
              {
                "kind": "NamedType",
                "name": "list of dicts"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/ccp_alpha",
          "name": "ccp_alpha",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.ccp_alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-negative float",
            "default_value": "0.0",
            "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "non-negative float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesClassifier/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._forest.ExtraTreesClassifier.__init__.max_samples",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "If bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\n- If None (default), then draw `X.shape[0]` samples.\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples. Thus,\n  `max_samples` should be in the interval `(0, 1)`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": false,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"gini\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=False,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 class_weight=None,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=ExtraTreeClassifier(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            class_weight=class_weight,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.22\n   The default value of ``n_estimators`` changed from 10 to 100\n   in 0.22."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/criterion",
          "name": "criterion",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.criterion",
          "default_value": "'mse'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"mse\", \"mae\"}",
            "default_value": "\"mse\"",
            "description": "The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "mse",
              "mae"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.max_depth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.max_features",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"sqrt\", \"log2\"}, int or float",
            "default_value": "\"auto\"",
            "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `round(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=n_features`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "auto",
                  "sqrt",
                  "log2"
                ]
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.bootstrap",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/oob_score",
          "name": "oob_score",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.oob_score",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use out-of-bag samples to estimate the generalization score.\nOnly available if bootstrap=True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n:meth:`decision_path` and :meth:`apply` are all parallelized over the\ntrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors. See :term:`Glossary\n<n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls 3 sources of randomness:\n\n- the bootstrapping of the samples used when building trees\n  (if ``bootstrap=True``)\n- the sampling of the features to consider when looking for the best\n  split at each node (if ``max_features < n_features``)\n- the draw of the splits for each of the `max_features`\n\nSee :term:`Glossary <random_state>` for details."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/ccp_alpha",
          "name": "ccp_alpha",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.ccp_alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-negative float",
            "default_value": "0.0",
            "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "non-negative float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/ExtraTreesRegressor/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._forest.ExtraTreesRegressor.__init__.max_samples",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "If bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\n- If None (default), then draw `X.shape[0]` samples.\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples. Thus,\n  `max_samples` should be in the interval `(0, 1)`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": false,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An extra-trees regressor.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"mse\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=False,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.22\n   The default value of ``n_estimators`` changed from 10 to 100\n   in 0.22."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/criterion",
          "name": "criterion",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.criterion",
          "default_value": "'gini'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"gini\", \"entropy\"}",
            "default_value": "\"gini\"",
            "description": "The function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.\nNote: this parameter is tree-specific."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "entropy",
              "gini"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.max_depth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.max_features",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"sqrt\", \"log2\"}, int or float",
            "default_value": "\"auto\"",
            "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `round(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "auto",
                  "sqrt",
                  "log2"
                ]
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.bootstrap",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/oob_score",
          "name": "oob_score",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.oob_score",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use out-of-bag samples to estimate the generalization score.\nOnly available if bootstrap=True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n:meth:`decision_path` and :meth:`apply` are all parallelized over the\ntrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors. See :term:`Glossary\n<n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls both the randomness of the bootstrapping of the samples used\nwhen building trees (if ``bootstrap=True``) and the sampling of the\nfeatures to consider when looking for the best split at each node\n(if ``max_features < n_features``).\nSee :term:`Glossary <random_state>` for details."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/class_weight",
          "name": "class_weight",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.class_weight",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"balanced\", \"balanced_subsample\"}, dict or list of dicts",
            "default_value": "None",
            "description": "Weights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that\nweights are computed based on the bootstrap sample for every tree\ngrown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "balanced_subsample",
                  "balanced"
                ]
              },
              {
                "kind": "NamedType",
                "name": "dict"
              },
              {
                "kind": "NamedType",
                "name": "list of dicts"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/ccp_alpha",
          "name": "ccp_alpha",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.ccp_alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-negative float",
            "default_value": "0.0",
            "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "non-negative float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestClassifier/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._forest.RandomForestClassifier.__init__.max_samples",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "If bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\n- If None (default), then draw `X.shape[0]` samples.\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples. Thus,\n  `max_samples` should be in the interval `(0, 1)`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": false,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"gini\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=True,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 class_weight=None,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=DecisionTreeClassifier(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            class_weight=class_weight,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of trees in the forest.\n\n.. versionchanged:: 0.22\n   The default value of ``n_estimators`` changed from 10 to 100\n   in 0.22."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/criterion",
          "name": "criterion",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.criterion",
          "default_value": "'mse'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"mse\", \"mae\"}",
            "default_value": "\"mse\"",
            "description": "The function to measure the quality of a split. Supported criteria\nare \"mse\" for the mean squared error, which is equal to variance\nreduction as feature selection criterion, and \"mae\" for the mean\nabsolute error.\n\n.. versionadded:: 0.18\n   Mean Absolute Error (MAE) criterion."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "mse",
              "mae"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.max_depth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.max_features",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"auto\", \"sqrt\", \"log2\"}, int or float",
            "default_value": "\"auto\"",
            "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `round(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=n_features`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "auto",
                  "sqrt",
                  "log2"
                ]
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.bootstrap",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/oob_score",
          "name": "oob_score",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.oob_score",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether to use out-of-bag samples to estimate the generalization score.\nOnly available if bootstrap=True."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n:meth:`decision_path` and :meth:`apply` are all parallelized over the\ntrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors. See :term:`Glossary\n<n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls both the randomness of the bootstrapping of the samples used\nwhen building trees (if ``bootstrap=True``) and the sampling of the\nfeatures to consider when looking for the best split at each node\n(if ``max_features < n_features``).\nSee :term:`Glossary <random_state>` for details."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/ccp_alpha",
          "name": "ccp_alpha",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.ccp_alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-negative float",
            "default_value": "0.0",
            "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "non-negative float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomForestRegressor/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._forest.RandomForestRegressor.__init__.max_samples",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "If bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\n- If None (default), then draw `X.shape[0]` samples.\n- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples. Thus,\n  `max_samples` should be in the interval `(0, 1)`.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": false,
                "max_inclusive": false
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "A random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and uses averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide <forest>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 criterion=\"mse\",\n                 max_depth=None,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_features=\"auto\",\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 bootstrap=True,\n                 oob_score=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False,\n                 ccp_alpha=0.0,\n                 max_samples=None):\n        super().__init__(\n            base_estimator=DecisionTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\", \"ccp_alpha\"),\n            bootstrap=bootstrap,\n            oob_score=oob_score,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=max_samples)\n\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_features = max_features\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.ccp_alpha = ccp_alpha"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "Number of trees in the forest.\n\n.. versionchanged:: 0.22\n   The default value of ``n_estimators`` changed from 10 to 100\n   in 0.22."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.max_depth",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "5",
            "description": "The maximum depth of each tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` is the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` is the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/sparse_output",
          "name": "sparse_output",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.sparse_output",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether or not to return a sparse CSR matrix, as default behavior,\nor to return a dense array compatible with dense pipeline operators."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n:meth:`decision_path` and :meth:`apply` are all parallelized over the\ntrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors. See :term:`Glossary\n<n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the generation of the random `y` used to fit the trees\nand the draw of the splits for each feature at the trees' nodes.\nSee :term:`Glossary <random_state>` for details."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity when fitting and predicting."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An ensemble of totally random trees.\n\nAn unsupervised transformation of a dataset to a high-dimensional\nsparse representation. A datapoint is coded according to which leaf of\neach tree it is sorted into. Using a one-hot encoding of the leaves,\nthis leads to a binary coding with as many ones as there are trees in\nthe forest.\n\nThe dimensionality of the resulting representation is\n``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\nthe number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\nRead more in the :ref:`User Guide <random_trees_embedding>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 n_estimators=100, *,\n                 max_depth=5,\n                 min_samples_split=2,\n                 min_samples_leaf=1,\n                 min_weight_fraction_leaf=0.,\n                 max_leaf_nodes=None,\n                 min_impurity_decrease=0.,\n                 min_impurity_split=None,\n                 sparse_output=True,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(),\n            n_estimators=n_estimators,\n            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n                              \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n                              \"max_features\", \"max_leaf_nodes\",\n                              \"min_impurity_decrease\", \"min_impurity_split\",\n                              \"random_state\"),\n            bootstrap=False,\n            oob_score=False,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose,\n            warm_start=warm_start,\n            max_samples=None)\n\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_impurity_decrease = min_impurity_decrease\n        self.min_impurity_split = min_impurity_split\n        self.sparse_output = sparse_output"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Use ``dtype=np.float32`` for maximum\nefficiency. Sparse matrices are also supported, use sparse\n``csc_matrix`` for maximum efficiency."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit estimator.",
      "docstring": "Fit estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Use ``dtype=np.float32`` for maximum\n    efficiency. Sparse matrices are also supported, use sparse\n    ``csc_matrix`` for maximum efficiency.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. In the case of\n    classification, splits are also ignored if they would result in any\n    single class carrying a negative weight in either child node.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        self.fit_transform(X, y, sample_weight=sample_weight)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit_transform/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit_transform/X",
          "name": "X",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data used to build forests. Use ``dtype=np.float32`` for\nmaximum efficiency."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit_transform/y",
          "name": "y",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/fit_transform/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.fit_transform.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted. Splits\nthat would create child nodes with net zero or negative weight are\nignored while searching for a split in each node. In the case of\nclassification, splits are also ignored if they would result in any\nsingle class carrying a negative weight in either child node."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit estimator and transform dataset.",
      "docstring": "Fit estimator and transform dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data used to build forests. Use ``dtype=np.float32`` for\n    maximum efficiency.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. In the case of\n    classification, splits are also ignored if they would result in any\n    single class carrying a negative weight in either child node.\n\nReturns\n-------\nX_transformed : sparse matrix of shape (n_samples, n_out)\n    Transformed dataset.",
      "code": "    def fit_transform(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator and transform dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data used to build forests. Use ``dtype=np.float32`` for\n            maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        Returns\n        -------\n        X_transformed : sparse matrix of shape (n_samples, n_out)\n            Transformed dataset.\n        \"\"\"\n        X = check_array(X, accept_sparse=['csc'])\n        if issparse(X):\n            # Pre-sort indices to avoid that each individual tree of the\n            # ensemble sorts the indices.\n            X.sort_indices()\n\n        rnd = check_random_state(self.random_state)\n        y = rnd.uniform(size=X.shape[0])\n        super().fit(X, y, sample_weight=sample_weight)\n\n        self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output)\n        return self.one_hot_encoder_.fit_transform(self.apply(X))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/transform",
      "name": "transform",
      "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/transform/self",
          "name": "self",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._forest/RandomTreesEmbedding/transform/X",
          "name": "X",
          "qname": "sklearn.ensemble._forest.RandomTreesEmbedding.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Input data to be transformed. Use ``dtype=np.float32`` for maximum\nefficiency. Sparse matrices are also supported, use sparse\n``csr_matrix`` for maximum efficiency."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform dataset.",
      "docstring": "Transform dataset.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Input data to be transformed. Use ``dtype=np.float32`` for maximum\n    efficiency. Sparse matrices are also supported, use sparse\n    ``csr_matrix`` for maximum efficiency.\n\nReturns\n-------\nX_transformed : sparse matrix of shape (n_samples, n_out)\n    Transformed dataset.",
      "code": "    def transform(self, X):\n        \"\"\"\n        Transform dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data to be transformed. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csr_matrix`` for maximum efficiency.\n\n        Returns\n        -------\n        X_transformed : sparse matrix of shape (n_samples, n_out)\n            Transformed dataset.\n        \"\"\"\n        check_is_fitted(self)\n        return self.one_hot_encoder_.transform(self.apply(X))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/loss",
          "name": "loss",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.loss",
          "default_value": "'deviance'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'deviance', 'exponential'}",
            "default_value": "'deviance'",
            "description": "The loss function to be optimized. 'deviance' refers to\ndeviance (= logistic regression) for classification\nwith probabilistic outputs. For loss 'exponential' gradient\nboosting recovers the AdaBoost algorithm."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "exponential",
              "deviance"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/learning_rate",
          "name": "learning_rate",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.learning_rate",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "Learning rate shrinks the contribution of each tree by `learning_rate`.\nThere is a trade-off between learning_rate and n_estimators."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of boosting stages to perform. Gradient boosting\nis fairly robust to over-fitting so a large number usually\nresults in better performance."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/subsample",
          "name": "subsample",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.subsample",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "The fraction of samples to be used for fitting the individual base\nlearners. If smaller than 1.0 this results in Stochastic Gradient\nBoosting. `subsample` interacts with the parameter `n_estimators`.\nChoosing `subsample < 1.0` leads to a reduction of variance\nand an increase in bias."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/criterion",
          "name": "criterion",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.criterion",
          "default_value": "'friedman_mse'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'friedman_mse', 'mse', 'mae'}",
            "default_value": "'friedman_mse'",
            "description": "The function to measure the quality of a split. Supported criteria\nare 'friedman_mse' for the mean squared error with improvement\nscore by Friedman, 'mse' for mean squared error, and 'mae' for\nthe mean absolute error. The default value of 'friedman_mse' is\ngenerally the best as it can provide a better approximation in\nsome cases.\n\n.. versionadded:: 0.18\n.. deprecated:: 0.24\n    `criterion='mae'` is deprecated and will be removed in version\n    1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n    instead, as trees should use a least-square criterion in\n    Gradient Boosting."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "mse",
              "friedman_mse",
              "mae"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.max_depth",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "The maximum depth of the individual regression estimators. The maximum\ndepth limits the number of nodes in the tree. Tune this parameter\nfor best performance; the best value depends on the interaction\nof the input variables."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/init",
          "name": "init",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "estimator or 'zero'",
            "default_value": "None",
            "description": "An estimator object that is used to compute the initial predictions.\n``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n'zero', the initial raw predictions are set to zero. By default, a\n``DummyEstimator`` predicting the classes priors is used."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "estimator"
              },
              {
                "kind": "NamedType",
                "name": "'zero'"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the random seed given to each Tree estimator at each\nboosting iteration.\nIn addition, it controls the random permutation of the features at\neach split (see Notes for more details).\nIt also controls the random spliting of the training data to obtain a\nvalidation set if `n_iter_no_change` is not None.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.max_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'sqrt', 'log2'}, int or float",
            "default_value": "None",
            "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `int(max_features * n_features)` features are considered at each\n  split.\n- If 'auto', then `max_features=sqrt(n_features)`.\n- If 'sqrt', then `max_features=sqrt(n_features)`.\n- If 'log2', then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nChoosing `max_features < n_features` leads to a reduction of variance\nand an increase in bias.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "auto",
                  "sqrt",
                  "log2"
                ]
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Enable verbose output. If 1 then it prints progress and performance\nonce in a while (the more trees the lower the frequency). If greater\nthan 1 then it prints progress and performance for every tree."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just erase the\nprevious solution. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/validation_fraction",
          "name": "validation_fraction",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.validation_fraction",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The proportion of training data to set aside as validation set for\nearly stopping. Must be between 0 and 1.\nOnly used if ``n_iter_no_change`` is set to an integer.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/n_iter_no_change",
          "name": "n_iter_no_change",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.n_iter_no_change",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "``n_iter_no_change`` is used to decide if early stopping will be used\nto terminate training when validation score is not improving. By\ndefault it is set to None to disable early stopping. If set to a\nnumber, it will set aside ``validation_fraction`` size of the training\ndata as validation and terminate training when validation score is not\nimproving in all of the previous ``n_iter_no_change`` numbers of\niterations. The split is stratified.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/tol",
          "name": "tol",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Tolerance for the early stopping. When the loss is not improving\nby at least tol for ``n_iter_no_change`` iterations (if set to a\nnumber), the training stops.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/__init__/ccp_alpha",
          "name": "ccp_alpha",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.__init__.ccp_alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-negative float",
            "default_value": "0.0",
            "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "non-negative float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Gradient Boosting for classification.\n\nGB builds an additive model in a\nforward stage-wise fashion; it allows for the optimization of\narbitrary differentiable loss functions. In each stage ``n_classes_``\nregression trees are fit on the negative gradient of the\nbinomial or multinomial deviance loss function. Binary classification\nis a special case where only a single regression tree is induced.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100,\n                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n                 max_depth=3, min_impurity_decrease=0.,\n                 min_impurity_split=None, init=None,\n                 random_state=None, max_features=None, verbose=0,\n                 max_leaf_nodes=None, warm_start=False,\n                 validation_fraction=0.1, n_iter_no_change=None, tol=1e-4,\n                 ccp_alpha=0.0):\n\n        super().__init__(\n            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n            criterion=criterion, min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_depth=max_depth, init=init, subsample=subsample,\n            max_features=max_features,\n            random_state=random_state, verbose=verbose,\n            max_leaf_nodes=max_leaf_nodes,\n            min_impurity_decrease=min_impurity_decrease,\n            min_impurity_split=min_impurity_split,\n            warm_start=warm_start, validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, ccp_alpha=ccp_alpha)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/decision_function",
      "name": "decision_function",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the decision function of ``X``.",
      "docstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscore : ndarray of shape (n_samples, n_classes) or (n_samples,)\n    The decision function of the input samples, which corresponds to\n    the raw values predicted from the trees of the ensemble . The\n    order of the classes corresponds to that in the attribute\n    :term:`classes_`. Regression and binary classification produce an\n    array of shape (n_samples,).",
      "code": "    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            order of the classes corresponds to that in the attribute\n            :term:`classes_`. Regression and binary classification produce an\n            array of shape (n_samples,).\n        \"\"\"\n        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n        raw_predictions = self._raw_predict(X)\n        if raw_predictions.shape[1] == 1:\n            return raw_predictions.ravel()\n        return raw_predictions"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class for X.",
      "docstring": "Predict class for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict class for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        raw_predictions = self.decision_function(X)\n        encoded_labels = \\\n            self.loss_._raw_prediction_to_decision(raw_predictions)\n        return self.classes_.take(encoded_labels, axis=0)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_log_proba",
      "name": "predict_log_proba",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict_log_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_log_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict_log_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_log_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict_log_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class log-probabilities for X.",
      "docstring": "Predict class log-probabilities for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nRaises\n------\nAttributeError\n    If the ``loss`` does not support probabilities.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        proba = self.predict_proba(X)\n        return np.log(proba)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_proba",
      "name": "predict_proba",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X.",
      "docstring": "Predict class probabilities for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nRaises\n------\nAttributeError\n    If the ``loss`` does not support probabilities.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        raw_predictions = self.decision_function(X)\n        try:\n            return self.loss_._raw_prediction_to_proba(raw_predictions)\n        except NotFittedError:\n            raise\n        except AttributeError as e:\n            raise AttributeError('loss=%r does not support predict_proba' %\n                                 self.loss) from e"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_decision_function",
      "name": "staged_decision_function",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.",
      "docstring": "Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscore : generator of ndarray of shape (n_samples, k)\n    The decision function of the input samples, which corresponds to\n    the raw values predicted from the trees of the ensemble . The\n    classes corresponds to that in the attribute :term:`classes_`.\n    Regression and binary classification are special cases with\n    ``k == 1``, otherwise ``k==n_classes``.",
      "code": "    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        yield from self._staged_raw_predict(X)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict",
      "name": "staged_predict",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.",
      "docstring": "Predict class at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted value of the input samples.",
      "code": "    def staged_predict(self, X):\n        \"\"\"Predict class at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            encoded_labels = \\\n                self.loss_._raw_prediction_to_decision(raw_predictions)\n            yield self.classes_.take(encoded_labels, axis=0)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict_proba",
      "name": "staged_predict_proba",
      "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingClassifier/staged_predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.",
      "docstring": "Predict class probabilities at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted value of the input samples.",
      "code": "    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        try:\n            for raw_predictions in self._staged_raw_predict(X):\n                yield self.loss_._raw_prediction_to_proba(raw_predictions)\n        except NotFittedError:\n            raise\n        except AttributeError as e:\n            raise AttributeError('loss=%r does not support predict_proba' %\n                                 self.loss) from e"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/loss",
          "name": "loss",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.loss",
          "default_value": "'ls'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ls', 'lad', 'huber', 'quantile'}",
            "default_value": "'ls'",
            "description": "Loss function to be optimized. 'ls' refers to least squares\nregression. 'lad' (least absolute deviation) is a highly robust\nloss function solely based on order information of the input\nvariables. 'huber' is a combination of the two. 'quantile'\nallows quantile regression (use `alpha` to specify the quantile)."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "huber",
              "quantile",
              "lad",
              "ls"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/learning_rate",
          "name": "learning_rate",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.learning_rate",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "Learning rate shrinks the contribution of each tree by `learning_rate`.\nThere is a trade-off between learning_rate and n_estimators."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of boosting stages to perform. Gradient boosting\nis fairly robust to over-fitting so a large number usually\nresults in better performance."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/subsample",
          "name": "subsample",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.subsample",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.0",
            "description": "The fraction of samples to be used for fitting the individual base\nlearners. If smaller than 1.0 this results in Stochastic Gradient\nBoosting. `subsample` interacts with the parameter `n_estimators`.\nChoosing `subsample < 1.0` leads to a reduction of variance\nand an increase in bias."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/criterion",
          "name": "criterion",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.criterion",
          "default_value": "'friedman_mse'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'friedman_mse', 'mse', 'mae'}",
            "default_value": "'friedman_mse'",
            "description": "The function to measure the quality of a split. Supported criteria\nare \"friedman_mse\" for the mean squared error with improvement\nscore by Friedman, \"mse\" for mean squared error, and \"mae\" for\nthe mean absolute error. The default value of \"friedman_mse\" is\ngenerally the best as it can provide a better approximation in\nsome cases.\n\n.. versionadded:: 0.18\n.. deprecated:: 0.24\n    `criterion='mae'` is deprecated and will be removed in version\n    1.1 (renaming of 0.26). The correct way of minimizing the absolute\n    error is to use `loss='lad'` instead."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "mse",
              "friedman_mse",
              "mae"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/min_samples_split",
          "name": "min_samples_split",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.min_samples_split",
          "default_value": "2",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "2",
            "description": "The minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.min_samples_leaf",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.\n\n.. versionchanged:: 0.18\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/min_weight_fraction_leaf",
          "name": "min_weight_fraction_leaf",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.min_weight_fraction_leaf",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.max_depth",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Maximum depth of the individual regression estimators. The maximum\ndepth limits the number of nodes in the tree. Tune this parameter\nfor best performance; the best value depends on the interaction\nof the input variables."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/min_impurity_decrease",
          "name": "min_impurity_decrease",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.min_impurity_decrease",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.0",
            "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t / N * (impurity - N_t_R / N_t * right_impurity\n                        - N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/min_impurity_split",
          "name": "min_impurity_split",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.min_impurity_split",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "None",
            "description": "Threshold for early stopping in tree growth. A node will split\nif its impurity is above the threshold, otherwise it is a leaf.\n\n.. deprecated:: 0.19\n   ``min_impurity_split`` has been deprecated in favor of\n   ``min_impurity_decrease`` in 0.19. The default value of\n   ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n   will be removed in 1.0 (renaming of 0.25).\n   Use ``min_impurity_decrease`` instead."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/init",
          "name": "init",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.init",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "estimator or 'zero'",
            "default_value": "None",
            "description": "An estimator object that is used to compute the initial predictions.\n``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\ninitial raw predictions are set to zero. By default a\n``DummyEstimator`` is used, predicting either the average target value\n(for loss='ls'), or a quantile for the other losses."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "estimator"
              },
              {
                "kind": "NamedType",
                "name": "'zero'"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the random seed given to each Tree estimator at each\nboosting iteration.\nIn addition, it controls the random permutation of the features at\neach split (see Notes for more details).\nIt also controls the random spliting of the training data to obtain a\nvalidation set if `n_iter_no_change` is not None.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.max_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'sqrt', 'log2'}, int or float",
            "default_value": "None",
            "description": "The number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `int(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=n_features`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nChoosing `max_features < n_features` leads to a reduction of variance\nand an increase in bias.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "auto",
                  "sqrt",
                  "log2"
                ]
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.alpha",
          "default_value": "0.9",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.9",
            "description": "The alpha-quantile of the huber loss function and the quantile\nloss function. Only if ``loss='huber'`` or ``loss='quantile'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Enable verbose output. If 1 then it prints progress and performance\nonce in a while (the more trees the lower the frequency). If greater\nthan 1 then it prints progress and performance for every tree."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.max_leaf_nodes",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just erase the\nprevious solution. See :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/validation_fraction",
          "name": "validation_fraction",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.validation_fraction",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The proportion of training data to set aside as validation set for\nearly stopping. Must be between 0 and 1.\nOnly used if ``n_iter_no_change`` is set to an integer.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/n_iter_no_change",
          "name": "n_iter_no_change",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.n_iter_no_change",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "``n_iter_no_change`` is used to decide if early stopping will be used\nto terminate training when validation score is not improving. By\ndefault it is set to None to disable early stopping. If set to a\nnumber, it will set aside ``validation_fraction`` size of the training\ndata as validation and terminate training when validation score is not\nimproving in all of the previous ``n_iter_no_change`` numbers of\niterations.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/tol",
          "name": "tol",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.tol",
          "default_value": "0.0001",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1e-4",
            "description": "Tolerance for the early stopping. When the loss is not improving\nby at least tol for ``n_iter_no_change`` iterations (if set to a\nnumber), the training stops.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/__init__/ccp_alpha",
          "name": "ccp_alpha",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.__init__.ccp_alpha",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-negative float",
            "default_value": "0.0",
            "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details.\n\n.. versionadded:: 0.22"
          },
          "type": {
            "kind": "NamedType",
            "name": "non-negative float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Gradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion;\nit allows for the optimization of arbitrary differentiable loss functions.\nIn each stage a regression tree is fit on the negative gradient of the\ngiven loss function.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, loss='ls', learning_rate=0.1, n_estimators=100,\n                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n                 max_depth=3, min_impurity_decrease=0.,\n                 min_impurity_split=None, init=None, random_state=None,\n                 max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n                 warm_start=False, validation_fraction=0.1,\n                 n_iter_no_change=None, tol=1e-4, ccp_alpha=0.0):\n\n        super().__init__(\n            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n            criterion=criterion, min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_depth=max_depth, init=init, subsample=subsample,\n            max_features=max_features,\n            min_impurity_decrease=min_impurity_decrease,\n            min_impurity_split=min_impurity_split,\n            random_state=random_state, alpha=alpha, verbose=verbose,\n            max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, ccp_alpha=ccp_alpha)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/apply",
      "name": "apply",
      "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.apply",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/apply/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.apply.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/apply/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.apply.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, its dtype will be converted to\n``dtype=np.float32``. If a sparse matrix is provided, it will\nbe converted to a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17",
      "docstring": "Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, its dtype will be converted to\n    ``dtype=np.float32``. If a sparse matrix is provided, it will\n    be converted to a sparse ``csr_matrix``.\n\nReturns\n-------\nX_leaves : array-like of shape (n_samples, n_estimators)\n    For each datapoint x in X and for each tree in the ensemble,\n    return the index of the leaf x ends up in each estimator.",
      "code": "    def apply(self, X):\n        \"\"\"Apply trees in the ensemble to X, return leaf indices.\n\n        .. versionadded:: 0.17\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will\n            be converted to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        X_leaves : array-like of shape (n_samples, n_estimators)\n            For each datapoint x in X and for each tree in the ensemble,\n            return the index of the leaf x ends up in each estimator.\n        \"\"\"\n\n        leaves = super().apply(X)\n        leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n        return leaves"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/n_classes_@getter",
      "name": "n_classes_",
      "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.n_classes_",
      "decorators": [
        "deprecated('Attribute n_classes_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).')",
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/n_classes_@getter/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.n_classes_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @deprecated(\"Attribute n_classes_ was deprecated \"  # type: ignore\n                \"in version 0.24 and will be removed in 1.1 \"\n                \"(renaming of 0.26).\")\n    @property\n    def n_classes_(self):\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_classes_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n        return 1"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict regression target for X.",
      "docstring": "Predict regression target for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n        # In regression we can directly return the raw value from the trees.\n        return self._raw_predict(X).ravel()"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/staged_predict",
      "name": "staged_predict",
      "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.staged_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/staged_predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.staged_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._gb/GradientBoostingRegressor/staged_predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._gb.GradientBoostingRegressor.staged_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict regression target at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.",
      "docstring": "Predict regression target at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted value of the input samples.",
      "code": "    def staged_predict(self, X):\n        \"\"\"Predict regression target at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield raw_predictions.ravel()"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/loss",
          "name": "loss",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.loss",
          "default_value": "'auto'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'binary_crossentropy', 'categorical_crossentropy'}",
            "default_value": "'auto'",
            "description": "The loss function to use in the boosting process. 'binary_crossentropy'\n(also known as logistic loss) is used for binary classification and\ngeneralizes to 'categorical_crossentropy' for multiclass\nclassification. 'auto' will automatically choose either loss depending\non the nature of the problem."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto",
              "binary_crossentropy",
              "categorical_crossentropy"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/learning_rate",
          "name": "learning_rate",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.learning_rate",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The learning rate, also known as *shrinkage*. This is used as a\nmultiplicative factor for the leaves values. Use ``1`` for no\nshrinkage."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The maximum number of iterations of the boosting process, i.e. the\nmaximum number of trees for binary classification. For multiclass\nclassification, `n_classes` trees per iteration are built."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.max_leaf_nodes",
          "default_value": "31",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "31",
            "description": "The maximum number of leaves for each tree. Must be strictly greater\nthan 1. If None, there is no maximum limit."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.max_depth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "None",
            "description": "The maximum depth of each tree. The depth of a tree is the number of\nedges to go from the root to the deepest leaf.\nDepth isn't constrained by default."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.min_samples_leaf",
          "default_value": "20",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "20",
            "description": "The minimum number of samples per leaf. For small datasets with less\nthan a few hundred samples, it is recommended to lower this value\nsince only very shallow trees would be built."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/l2_regularization",
          "name": "l2_regularization",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.l2_regularization",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0",
            "description": "The L2 regularization parameter. Use 0 for no regularization."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/max_bins",
          "name": "max_bins",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.max_bins",
          "default_value": "255",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "255",
            "description": "The maximum number of bins to use for non-missing values. Before\ntraining, each feature of the input array `X` is binned into\ninteger-valued bins, which allows for a much faster training stage.\nFeatures with a small number of unique values may use less than\n``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\nis always reserved for missing values. Must be no larger than 255."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/categorical_features",
          "name": "categorical_features",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.categorical_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,)",
            "default_value": "None.",
            "description": "Indicates the categorical features.\n\n- None : no feature will be considered categorical.\n- boolean array-like : boolean mask indicating categorical features.\n- integer array-like : integer indices indicating categorical\n  features.\n\nFor each categorical feature, there must be at most `max_bins` unique\ncategories, and each categorical value must be in [0, max_bins -1].\n\nRead more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "array-like of   of shape (n_features)"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_categorical_features,)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/monotonic_cst",
          "name": "monotonic_cst",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.monotonic_cst",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of int of shape (n_features)",
            "default_value": "None",
            "description": "Indicates the monotonic constraint to enforce on each feature. -1, 1\nand 0 respectively correspond to a negative constraint, positive\nconstraint and no constraint. Read more in the :ref:`User Guide\n<monotonic_cst_gbdt>`.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of int of shape (n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble. For results to be valid, the\nestimator should be re-trained on the same data only.\nSee :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/early_stopping",
          "name": "early_stopping",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.early_stopping",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or bool",
            "default_value": "'auto'",
            "description": "If 'auto', early stopping is enabled if the sample size is larger than\n10000. If True, early stopping is enabled, otherwise early stopping is\ndisabled.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/scoring",
          "name": "scoring",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.scoring",
          "default_value": "'loss'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable or None",
            "default_value": "'loss'",
            "description": "Scoring parameter to use for early stopping. It can be a single\nstring (see :ref:`scoring_parameter`) or a callable (see\n:ref:`scoring`). If None, the estimator's default scorer\nis used. If ``scoring='loss'``, early stopping is checked\nw.r.t the loss value. Only used if early stopping is performed."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/validation_fraction",
          "name": "validation_fraction",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.validation_fraction",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float or None",
            "default_value": "0.1",
            "description": "Proportion (or absolute size) of training data to set aside as\nvalidation data for early stopping. If None, early stopping is done on\nthe training data. Only used if early stopping is performed."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/n_iter_no_change",
          "name": "n_iter_no_change",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.n_iter_no_change",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Used to determine when to \"early stop\". The fitting process is\nstopped when none of the last ``n_iter_no_change`` scores are better\nthan the ``n_iter_no_change - 1`` -th-to-last one, up to some\ntolerance. Only used if early stopping is performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/tol",
          "name": "tol",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.tol",
          "default_value": "1e-07",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or None",
            "default_value": "1e-7",
            "description": "The absolute tolerance to use when comparing scores. The higher the\ntolerance, the more likely we are to early stop: higher tolerance\nmeans that it will be harder for subsequent iterations to be\nconsidered an improvement upon the reference score."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "The verbosity level. If not zero, print some information about the\nfitting process."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Pseudo-random number generator to control the subsampling in the\nbinning process, and the train/validation data split if early stopping\nis enabled.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Histogram-based Gradient Boosting Classification Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, loss='auto', *, learning_rate=0.1, max_iter=100,\n                 max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,\n                 l2_regularization=0., max_bins=255,\n                 categorical_features=None,  monotonic_cst=None,\n                 warm_start=False, early_stopping='auto', scoring='loss',\n                 validation_fraction=0.1, n_iter_no_change=10, tol=1e-7,\n                 verbose=0, random_state=None):\n        super(HistGradientBoostingClassifier, self).__init__(\n            loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n            max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n            min_samples_leaf=min_samples_leaf,\n            l2_regularization=l2_regularization, max_bins=max_bins,\n            categorical_features=categorical_features,\n            monotonic_cst=monotonic_cst,\n            warm_start=warm_start,\n            early_stopping=early_stopping, scoring=scoring,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n            random_state=random_state)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/decision_function",
      "name": "decision_function",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the decision function of ``X``.",
      "docstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ndecision : ndarray, shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n    The raw predicted values (i.e. the sum of the trees leaves) for\n    each sample. n_trees_per_iteration is equal to the number of\n    classes in multiclass classification.",
      "code": "    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        decision : ndarray, shape (n_samples,) or \\\n                (n_samples, n_trees_per_iteration)\n            The raw predicted values (i.e. the sum of the trees leaves) for\n            each sample. n_trees_per_iteration is equal to the number of\n            classes in multiclass classification.\n        \"\"\"\n        decision = self._raw_predict(X)\n        if decision.shape[0] == 1:\n            decision = decision.ravel()\n        return decision.T"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict classes for X.",
      "docstring": "Predict classes for X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ny : ndarray, shape (n_samples,)\n    The predicted classes.",
      "code": "    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        # TODO: This could be done in parallel\n        encoded_classes = np.argmax(self.predict_proba(X), axis=1)\n        return self.classes_[encoded_classes]"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict_proba",
      "name": "predict_proba",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X.",
      "docstring": "Predict class probabilities for X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\np : ndarray, shape (n_samples, n_classes)\n    The class probabilities of the input samples.",
      "code": "    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        p : ndarray, shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n        \"\"\"\n        raw_predictions = self._raw_predict(X)\n        return self._loss.predict_proba(raw_predictions)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_decision_function",
      "name": "staged_decision_function",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.",
      "docstring": "Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ndecision : generator of ndarray of shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n    The decision function of the input samples, which corresponds to\n    the raw values predicted from the trees of the ensemble . The\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        decision : generator of ndarray of shape (n_samples,) or \\\n                (n_samples, n_trees_per_iteration)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        for staged_decision in self._staged_raw_predict(X):\n            if staged_decision.shape[0] == 1:\n                staged_decision = staged_decision.ravel()\n            yield staged_decision.T"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict",
      "name": "staged_predict",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict classes at each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\n.. versionadded:: 0.24",
      "docstring": "Predict classes at each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted classes of the input samples, for each iteration.",
      "code": "    def staged_predict(self, X):\n        \"\"\"Predict classes at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes of the input samples, for each iteration.\n        \"\"\"\n        for proba in self.staged_predict_proba(X):\n            encoded_classes = np.argmax(proba, axis=1)\n            yield self.classes_.take(encoded_classes, axis=0)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict_proba",
      "name": "staged_predict_proba",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingClassifier/staged_predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities at each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.",
      "docstring": "Predict class probabilities at each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted class probabilities of the input samples,\n    for each iteration.",
      "code": "    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities at each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted class probabilities of the input samples,\n            for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield self._loss.predict_proba(raw_predictions)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/loss",
          "name": "loss",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.loss",
          "default_value": "'least_squares'",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{'least_squares', 'least_absolute_deviation', 'poisson'}",
            "default_value": "'least_squares'",
            "description": "The loss function to use in the boosting process. Note that the\n\"least squares\" and \"poisson\" losses actually implement\n\"half least squares loss\" and \"half poisson deviance\" to simplify the\ncomputation of the gradient. Furthermore, \"poisson\" loss internally\nuses a log-link and requires ``y >= 0``\n\n.. versionchanged:: 0.23\n   Added option 'poisson'."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "poisson",
              "least_absolute_deviation",
              "least_squares"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/learning_rate",
          "name": "learning_rate",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.learning_rate",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0.1",
            "description": "The learning rate, also known as *shrinkage*. This is used as a\nmultiplicative factor for the leaves values. Use ``1`` for no\nshrinkage."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/max_iter",
          "name": "max_iter",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.max_iter",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The maximum number of iterations of the boosting process, i.e. the\nmaximum number of trees."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/max_leaf_nodes",
          "name": "max_leaf_nodes",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.max_leaf_nodes",
          "default_value": "31",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "31",
            "description": "The maximum number of leaves for each tree. Must be strictly greater\nthan 1. If None, there is no maximum limit."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/max_depth",
          "name": "max_depth",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.max_depth",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "None",
            "description": "The maximum depth of each tree. The depth of a tree is the number of\nedges to go from the root to the deepest leaf.\nDepth isn't constrained by default."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/min_samples_leaf",
          "name": "min_samples_leaf",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.min_samples_leaf",
          "default_value": "20",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "20",
            "description": "The minimum number of samples per leaf. For small datasets with less\nthan a few hundred samples, it is recommended to lower this value\nsince only very shallow trees would be built."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/l2_regularization",
          "name": "l2_regularization",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.l2_regularization",
          "default_value": "0.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0",
            "description": "The L2 regularization parameter. Use ``0`` for no regularization\n(default)."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/max_bins",
          "name": "max_bins",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.max_bins",
          "default_value": "255",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "255",
            "description": "The maximum number of bins to use for non-missing values. Before\ntraining, each feature of the input array `X` is binned into\ninteger-valued bins, which allows for a much faster training stage.\nFeatures with a small number of unique values may use less than\n``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\nis always reserved for missing values. Must be no larger than 255."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/categorical_features",
          "name": "categorical_features",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.categorical_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of {bool, int} of shape (n_features)             or shape (n_categorical_features,)",
            "default_value": "None.",
            "description": "Indicates the categorical features.\n\n- None : no feature will be considered categorical.\n- boolean array-like : boolean mask indicating categorical features.\n- integer array-like : integer indices indicating categorical\n  features.\n\nFor each categorical feature, there must be at most `max_bins` unique\ncategories, and each categorical value must be in [0, max_bins -1].\n\nRead more in the :ref:`User Guide <categorical_support_gbdt>`.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "array-like of   of shape (n_features)"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_categorical_features,)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/monotonic_cst",
          "name": "monotonic_cst",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.monotonic_cst",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of int of shape (n_features)",
            "default_value": "None",
            "description": "Indicates the monotonic constraint to enforce on each feature. -1, 1\nand 0 respectively correspond to a negative constraint, positive\nconstraint and no constraint. Read more in the :ref:`User Guide\n<monotonic_cst_gbdt>`.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of int of shape (n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble. For results to be valid, the\nestimator should be re-trained on the same data only.\nSee :term:`the Glossary <warm_start>`."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/early_stopping",
          "name": "early_stopping",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.early_stopping",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or bool",
            "default_value": "'auto'",
            "description": "If 'auto', early stopping is enabled if the sample size is larger than\n10000. If True, early stopping is enabled, otherwise early stopping is\ndisabled.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/scoring",
          "name": "scoring",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.scoring",
          "default_value": "'loss'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable or None",
            "default_value": "'loss'",
            "description": "Scoring parameter to use for early stopping. It can be a single\nstring (see :ref:`scoring_parameter`) or a callable (see\n:ref:`scoring`). If None, the estimator's default scorer is used. If\n``scoring='loss'``, early stopping is checked w.r.t the loss value.\nOnly used if early stopping is performed."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/validation_fraction",
          "name": "validation_fraction",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.validation_fraction",
          "default_value": "0.1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float or None",
            "default_value": "0.1",
            "description": "Proportion (or absolute size) of training data to set aside as\nvalidation data for early stopping. If None, early stopping is done on\nthe training data. Only used if early stopping is performed."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/n_iter_no_change",
          "name": "n_iter_no_change",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.n_iter_no_change",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Used to determine when to \"early stop\". The fitting process is\nstopped when none of the last ``n_iter_no_change`` scores are better\nthan the ``n_iter_no_change - 1`` -th-to-last one, up to some\ntolerance. Only used if early stopping is performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/tol",
          "name": "tol",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.tol",
          "default_value": "1e-07",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or None",
            "default_value": "1e-7",
            "description": "The absolute tolerance to use when comparing scores during early\nstopping. The higher the tolerance, the more likely we are to early\nstop: higher tolerance means that it will be harder for subsequent\niterations to be considered an improvement upon the reference score."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "The verbosity level. If not zero, print some information about the\nfitting process."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Pseudo-random number generator to control the subsampling in the\nbinning process, and the train/validation data split if early stopping\nis enabled.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Histogram-based Gradient Boosting Regression Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\nfor big datasets (n_samples >= 10 000).\n\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\n\nThis implementation is inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`_.\n\n.. note::\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingRegressor\n\nRead more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n\n.. versionadded:: 0.21",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, loss='least_squares', *, learning_rate=0.1,\n                 max_iter=100, max_leaf_nodes=31, max_depth=None,\n                 min_samples_leaf=20, l2_regularization=0., max_bins=255,\n                 categorical_features=None, monotonic_cst=None,\n                 warm_start=False, early_stopping='auto',\n                 scoring='loss', validation_fraction=0.1,\n                 n_iter_no_change=10, tol=1e-7,\n                 verbose=0, random_state=None):\n        super(HistGradientBoostingRegressor, self).__init__(\n            loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n            max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n            min_samples_leaf=min_samples_leaf,\n            l2_regularization=l2_regularization, max_bins=max_bins,\n            monotonic_cst=monotonic_cst,\n            categorical_features=categorical_features,\n            early_stopping=early_stopping,\n            warm_start=warm_start, scoring=scoring,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n            random_state=random_state)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like, shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict values for X.",
      "docstring": "Predict values for X.\n\nParameters\n----------\nX : array-like, shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ny : ndarray, shape (n_samples,)\n    The predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict values for X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        # Return inverse link of raw predictions after converting\n        # shape (n_samples, 1) to (n_samples,)\n        return self._loss.inverse_link_function(self._raw_predict(X).ravel())"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/staged_predict",
      "name": "staged_predict",
      "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.staged_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/staged_predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.staged_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._hist_gradient_boosting.gradient_boosting/HistGradientBoostingRegressor/staged_predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.staged_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict regression target for each iteration\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\n.. versionadded:: 0.24",
      "docstring": "Predict regression target for each iteration\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n\n.. versionadded:: 0.24\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted values of the input samples, for each iteration.",
      "code": "    def staged_predict(self, X):\n        \"\"\"Predict regression target for each iteration\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        .. versionadded:: 0.24\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted values of the input samples, for each iteration.\n        \"\"\"\n        for raw_predictions in self._staged_raw_predict(X):\n            yield self._loss.inverse_link_function(raw_predictions.ravel())"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._iforest.IsolationForest.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.n_estimators",
          "default_value": "100",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "100",
            "description": "The number of base estimators in the ensemble."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/max_samples",
          "name": "max_samples",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.max_samples",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "\"auto\", int or float",
            "default_value": "\"auto\"",
            "description": "The number of samples to draw from X to train each base estimator.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n    - If \"auto\", then `max_samples=min(256, n_samples)`.\n\nIf max_samples is larger than the number of samples provided,\nall samples will be used for all trees (no sampling)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "\"auto\""
              },
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/contamination",
          "name": "contamination",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.contamination",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "'auto' or float",
            "default_value": "'auto'",
            "description": "The amount of contamination of the data set, i.e. the proportion\nof outliers in the data set. Used when fitting to define the threshold\non the scores of the samples.\n\n    - If 'auto', the threshold is determined as in the\n      original paper.\n    - If float, the contamination should be in the range [0, 0.5].\n\n.. versionchanged:: 0.22\n   The default value of ``contamination`` changed from 0.1\n   to ``'auto'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 0.5,
                "min_inclusive": true,
                "max_inclusive": true
              },
              {
                "kind": "NamedType",
                "name": "'auto'"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.max_features",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1.0",
            "description": "The number of features to draw from X to train each base estimator.\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/bootstrap",
          "name": "bootstrap",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.bootstrap",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, individual trees are fit on random subsets of the training\ndata sampled with replacement. If False, sampling without replacement\nis performed."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel for both :meth:`fit` and\n:meth:`predict`. ``None`` means 1 unless in a\n:obj:`joblib.parallel_backend` context. ``-1`` means using all\nprocessors. See :term:`Glossary <n_jobs>` for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the pseudo-randomness of the selection of the feature\nand split values for each branching step and each tree in the forest.\n\nPass an int for reproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls the verbosity of the tree building process."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/__init__/warm_start",
          "name": "warm_start",
          "qname": "sklearn.ensemble._iforest.IsolationForest.__init__.warm_start",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`.\n\n.. versionadded:: 0.21"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Isolation Forest Algorithm.\n\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest 'isolates' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nRead more in the :ref:`User Guide <isolation_forest>`.\n\n.. versionadded:: 0.18",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"auto\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 random_state=None,\n                 verbose=0,\n                 warm_start=False):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n        self.contamination = contamination"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/decision_function",
      "name": "decision_function",
      "qname": "sklearn.ensemble._iforest.IsolationForest.decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._iforest.IsolationForest.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._iforest.IsolationForest.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Average anomaly score of X of the base classifiers.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.",
      "docstring": "Average anomaly score of X of the base classifiers.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscores : ndarray of shape (n_samples,)\n    The anomaly score of the input samples.\n    The lower, the more abnormal. Negative scores represent outliers,\n    positive scores represent inliers.",
      "code": "    def decision_function(self, X):\n        \"\"\"\n        Average anomaly score of X of the base classifiers.\n\n        The anomaly score of an input sample is computed as\n        the mean anomaly score of the trees in the forest.\n\n        The measure of normality of an observation given a tree is the depth\n        of the leaf containing this observation, which is equivalent to\n        the number of splittings required to isolate this point. In case of\n        several observations n_left in the leaf, the average path length of\n        a n_left samples isolation tree is added.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        scores : ndarray of shape (n_samples,)\n            The anomaly score of the input samples.\n            The lower, the more abnormal. Negative scores represent outliers,\n            positive scores represent inliers.\n        \"\"\"\n        # We subtract self.offset_ to make 0 be the threshold value for being\n        # an outlier:\n\n        return self.score_samples(X) - self.offset_"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._iforest.IsolationForest.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._iforest.IsolationForest.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._iforest.IsolationForest.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Use ``dtype=np.float32`` for maximum\nefficiency. Sparse matrices are also supported, use sparse\n``csc_matrix`` for maximum efficiency."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._iforest.IsolationForest.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Ignored",
            "default_value": "",
            "description": "Not used, present for API consistency by convention."
          },
          "type": {
            "kind": "NamedType",
            "name": "Ignored"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._iforest.IsolationForest.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit estimator.",
      "docstring": "Fit estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Use ``dtype=np.float32`` for maximum\n    efficiency. Sparse matrices are also supported, use sparse\n    ``csc_matrix`` for maximum efficiency.\n\ny : Ignored\n    Not used, present for API consistency by convention.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n\nReturns\n-------\nself : object\n    Fitted estimator.",
      "code": "    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"\n        Fit estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        X = check_array(X, accept_sparse=['csc'])\n        if issparse(X):\n            # Pre-sort indices to avoid that each individual tree of the\n            # ensemble sorts the indices.\n            X.sort_indices()\n\n        rnd = check_random_state(self.random_state)\n        y = rnd.uniform(size=X.shape[0])\n\n        # ensure that max_sample is in [1, n_samples]:\n        n_samples = X.shape[0]\n\n        if isinstance(self.max_samples, str):\n            if self.max_samples == 'auto':\n                max_samples = min(256, n_samples)\n            else:\n                raise ValueError('max_samples (%s) is not supported.'\n                                 'Valid choices are: \"auto\", int or'\n                                 'float' % self.max_samples)\n\n        elif isinstance(self.max_samples, numbers.Integral):\n            if self.max_samples > n_samples:\n                warn(\"max_samples (%s) is greater than the \"\n                     \"total number of samples (%s). max_samples \"\n                     \"will be set to n_samples for estimation.\"\n                     % (self.max_samples, n_samples))\n                max_samples = n_samples\n            else:\n                max_samples = self.max_samples\n        else:  # float\n            if not 0. < self.max_samples <= 1.:\n                raise ValueError(\"max_samples must be in (0, 1], got %r\"\n                                 % self.max_samples)\n            max_samples = int(self.max_samples * X.shape[0])\n\n        self.max_samples_ = max_samples\n        max_depth = int(np.ceil(np.log2(max(max_samples, 2))))\n        super()._fit(X, y, max_samples,\n                     max_depth=max_depth,\n                     sample_weight=sample_weight)\n\n        if self.contamination == \"auto\":\n            # 0.5 plays a special role as described in the original paper.\n            # we take the opposite as we consider the opposite of their score.\n            self.offset_ = -0.5\n            return self\n\n        # else, define offset_ wrt contamination parameter\n        self.offset_ = np.percentile(self.score_samples(X),\n                                     100. * self.contamination)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._iforest.IsolationForest.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._iforest.IsolationForest.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._iforest.IsolationForest.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict if a particular sample is an outlier or not.",
      "docstring": "Predict if a particular sample is an outlier or not.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nis_inlier : ndarray of shape (n_samples,)\n    For each observation, tells whether or not (+1 or -1) it should\n    be considered as an inlier according to the fitted model.",
      "code": "    def predict(self, X):\n        \"\"\"\n        Predict if a particular sample is an outlier or not.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        is_inlier : ndarray of shape (n_samples,)\n            For each observation, tells whether or not (+1 or -1) it should\n            be considered as an inlier according to the fitted model.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, accept_sparse='csr')\n        is_inlier = np.ones(X.shape[0], dtype=int)\n        is_inlier[self.decision_function(X) < 0] = -1\n        return is_inlier"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/score_samples",
      "name": "score_samples",
      "qname": "sklearn.ensemble._iforest.IsolationForest.score_samples",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/score_samples/self",
          "name": "self",
          "qname": "sklearn.ensemble._iforest.IsolationForest.score_samples.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._iforest/IsolationForest/score_samples/X",
          "name": "X",
          "qname": "sklearn.ensemble._iforest.IsolationForest.score_samples.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Opposite of the anomaly score defined in the original paper.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.",
      "docstring": "Opposite of the anomaly score defined in the original paper.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\nscores : ndarray of shape (n_samples,)\n    The anomaly score of the input samples.\n    The lower, the more abnormal.",
      "code": "    def score_samples(self, X):\n        \"\"\"\n        Opposite of the anomaly score defined in the original paper.\n\n        The anomaly score of an input sample is computed as\n        the mean anomaly score of the trees in the forest.\n\n        The measure of normality of an observation given a tree is the depth\n        of the leaf containing this observation, which is equivalent to\n        the number of splittings required to isolate this point. In case of\n        several observations n_left in the leaf, the average path length of\n        a n_left samples isolation tree is added.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        scores : ndarray of shape (n_samples,)\n            The anomaly score of the input samples.\n            The lower, the more abnormal.\n        \"\"\"\n        # code structure from ForestClassifier/predict_proba\n\n        check_is_fitted(self)\n\n        # Check data\n        X = check_array(X, accept_sparse='csr')\n        if self.n_features_ != X.shape[1]:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features is {0} and \"\n                             \"input n_features is {1}.\"\n                             \"\".format(self.n_features_, X.shape[1]))\n\n        # Take the opposite of the scores as bigger is better (here less\n        # abnormal)\n        return -self._compute_chunked_score_samples(X)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/estimators",
          "name": "estimators",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.estimators",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "list of (str, estimator)",
            "default_value": "",
            "description": "Base estimators which will be stacked together. Each element of the\nlist is defined as a tuple of string (i.e. name) and an estimator\ninstance. An estimator can be set to 'drop' using `set_params`."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of (str, estimator)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/final_estimator",
          "name": "final_estimator",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.final_estimator",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "estimator",
            "default_value": "None",
            "description": "A classifier which will be used to combine the base estimators.\nThe default classifier is a\n:class:`~sklearn.linear_model.LogisticRegression`."
          },
          "type": {
            "kind": "NamedType",
            "name": "estimator"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/cv",
          "name": "cv",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.cv",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, cross-validation generator or an iterable",
            "default_value": "None",
            "description": "Determines the cross-validation splitting strategy used in\n`cross_val_predict` to train `final_estimator`. Possible inputs for\ncv are:\n\n* None, to use the default 5-fold cross validation,\n* integer, to specify the number of folds in a (Stratified) KFold,\n* An object to be used as a cross-validation generator,\n* An iterable yielding train, test splits.\n\nFor integer/None inputs, if the estimator is a classifier and y is\neither binary or multiclass,\n:class:`~sklearn.model_selection.StratifiedKFold` is used.\nIn all other cases, :class:`~sklearn.model_selection.KFold` is used.\nThese splitters are instantiated with `shuffle=False` so the splits\nwill be the same across calls.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here.\n\n.. note::\n   A larger number of split will provide no benefits if the number\n   of training samples is large enough. Indeed, the training time\n   will increase. ``cv`` is not used for model evaluation but for\n   prediction."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "cross-validation generator"
              },
              {
                "kind": "NamedType",
                "name": "an iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/stack_method",
          "name": "stack_method",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.stack_method",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', 'predict_proba', 'decision_function', 'predict'}",
            "default_value": "'auto'",
            "description": "Methods called for each base estimator. It can be:\n\n* if 'auto', it will try to invoke, for each estimator,\n  `'predict_proba'`, `'decision_function'` or `'predict'` in that\n  order.\n* otherwise, one of `'predict_proba'`, `'decision_function'` or\n  `'predict'`. If the method is not implemented by the estimator, it\n  will raise an error."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto",
              "predict_proba",
              "predict",
              "decision_function"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel all `estimators` `fit`.\n`None` means 1 unless in a `joblib.parallel_backend` context. -1 means\nusing all processors. See Glossary for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/passthrough",
          "name": "passthrough",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.passthrough",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When False, only the predictions of estimators will be used as\ntraining data for `final_estimator`. When True, the\n`final_estimator` is trained on the predictions as well as the\noriginal training data."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity level."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Stack of estimators with a final classifier.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a classifier to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimators, final_estimator=None, *, cv=None,\n                 stack_method='auto', n_jobs=None, passthrough=False,\n                 verbose=0):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            stack_method=stack_method,\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose\n        )"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/decision_function",
      "name": "decision_function",
      "qname": "sklearn.ensemble._stacking.StackingClassifier.decision_function",
      "decorators": [
        "if_delegate_has_method(delegate='final_estimator_')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict decision function for samples in X using\n`final_estimator_.decision_function`.",
      "docstring": "Predict decision function for samples in X using\n`final_estimator_.decision_function`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\ndecisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n    The decision function computed the final estimator.",
      "code": "    @if_delegate_has_method(delegate='final_estimator_')\n    def decision_function(self, X):\n        \"\"\"Predict decision function for samples in X using\n        `final_estimator_.decision_function`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,), (n_samples, n_classes), \\\n            or (n_samples, n_classes * (n_classes-1) / 2)\n            The decision function computed the final estimator.\n        \"\"\"\n        check_is_fitted(self)\n        return self.final_estimator_.decision_function(self.transform(X))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._stacking.StackingClassifier.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where `n_samples` is the number of samples and\n`n_features` is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the estimators.",
      "docstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        check_classification_targets(y)\n        self._le = LabelEncoder().fit(y)\n        self.classes_ = self._le.classes_\n        return super().fit(X, self._le.transform(y), sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._stacking.StackingClassifier.predict",
      "decorators": [
        "if_delegate_has_method(delegate='final_estimator_')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict/predict_params",
          "name": "predict_params",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.predict.predict_params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "dict of str -> obj",
            "default_value": "",
            "description": "Parameters to the `predict` called by the `final_estimator`. Note\nthat this may be used to return uncertainties from some estimators\nwith `return_std` or `return_cov`. Be aware that it will only\naccounts for uncertainty in the final estimator."
          },
          "type": {
            "kind": "NamedType",
            "name": "dict of str -> obj"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict target for X.",
      "docstring": "Predict target for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\n**predict_params : dict of str -> obj\n    Parameters to the `predict` called by the `final_estimator`. Note\n    that this may be used to return uncertainties from some estimators\n    with `return_std` or `return_cov`. Be aware that it will only\n    accounts for uncertainty in the final estimator.\n\nReturns\n-------\ny_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n    Predicted targets.",
      "code": "    @if_delegate_has_method(delegate='final_estimator_')\n    def predict(self, X, **predict_params):\n        \"\"\"Predict target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        **predict_params : dict of str -> obj\n            Parameters to the `predict` called by the `final_estimator`. Note\n            that this may be used to return uncertainties from some estimators\n            with `return_std` or `return_cov`. Be aware that it will only\n            accounts for uncertainty in the final estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n            Predicted targets.\n        \"\"\"\n        y_pred = super().predict(X, **predict_params)\n        return self._le.inverse_transform(y_pred)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict_proba",
      "name": "predict_proba",
      "qname": "sklearn.ensemble._stacking.StackingClassifier.predict_proba",
      "decorators": [
        "if_delegate_has_method(delegate='final_estimator_')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X using\n`final_estimator_.predict_proba`.",
      "docstring": "Predict class probabilities for X using\n`final_estimator_.predict_proba`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\nprobabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n    The class probabilities of the input samples.",
      "code": "    @if_delegate_has_method(delegate='final_estimator_')\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X using\n        `final_estimator_.predict_proba`.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes) or \\\n            list of ndarray of shape (n_output,)\n            The class probabilities of the input samples.\n        \"\"\"\n        check_is_fitted(self)\n        return self.final_estimator_.predict_proba(self.transform(X))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/transform",
      "name": "transform",
      "qname": "sklearn.ensemble._stacking.StackingClassifier.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/transform/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingClassifier/transform/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingClassifier.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where `n_samples` is the number of samples and\n`n_features` is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return class labels or probabilities for X for each estimator.",
      "docstring": "Return class labels or probabilities for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\ny_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n    Prediction outputs for each estimator.",
      "code": "    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators) or \\\n                (n_samples, n_classes * n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/estimators",
          "name": "estimators",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.estimators",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "list of (str, estimator)",
            "default_value": "",
            "description": "Base estimators which will be stacked together. Each element of the\nlist is defined as a tuple of string (i.e. name) and an estimator\ninstance. An estimator can be set to 'drop' using `set_params`."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of (str, estimator)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/final_estimator",
          "name": "final_estimator",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.final_estimator",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "estimator",
            "default_value": "None",
            "description": "A regressor which will be used to combine the base estimators.\nThe default regressor is a :class:`~sklearn.linear_model.RidgeCV`."
          },
          "type": {
            "kind": "NamedType",
            "name": "estimator"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/cv",
          "name": "cv",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.cv",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, cross-validation generator or an iterable",
            "default_value": "None",
            "description": "Determines the cross-validation splitting strategy used in\n`cross_val_predict` to train `final_estimator`. Possible inputs for\ncv are:\n\n* None, to use the default 5-fold cross validation,\n* integer, to specify the number of folds in a (Stratified) KFold,\n* An object to be used as a cross-validation generator,\n* An iterable yielding train, test splits.\n\nFor integer/None inputs, if the estimator is a classifier and y is\neither binary or multiclass,\n:class:`~sklearn.model_selection.StratifiedKFold` is used.\nIn all other cases, :class:`~sklearn.model_selection.KFold` is used.\nThese splitters are instantiated with `shuffle=False` so the splits\nwill be the same across calls.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here.\n\n.. note::\n   A larger number of split will provide no benefits if the number\n   of training samples is large enough. Indeed, the training time\n   will increase. ``cv`` is not used for model evaluation but for\n   prediction."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "cross-validation generator"
              },
              {
                "kind": "NamedType",
                "name": "an iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel for `fit` of all `estimators`.\n`None` means 1 unless in a `joblib.parallel_backend` context. -1 means\nusing all processors. See Glossary for more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/passthrough",
          "name": "passthrough",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.passthrough",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "When False, only the predictions of estimators will be used as\ntraining data for `final_estimator`. When True, the\n`final_estimator` is trained on the predictions as well as the\noriginal training data."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Verbosity level."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Stack of estimators with a final regressor.\n\nStacked generalization consists in stacking the output of individual\nestimator and use a regressor to compute the final prediction. Stacking\nallows to use the strength of each individual estimator by using their\noutput as input of a final estimator.\n\nNote that `estimators_` are fitted on the full `X` while `final_estimator_`\nis trained using cross-validated predictions of the base estimators using\n`cross_val_predict`.\n\nRead more in the :ref:`User Guide <stacking>`.\n\n.. versionadded:: 0.22",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimators, final_estimator=None, *, cv=None,\n                 n_jobs=None, passthrough=False, verbose=0):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            stack_method=\"predict\",\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose\n        )"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._stacking.StackingRegressor.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the estimators.",
      "docstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        return super().fit(X, y, sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/transform",
      "name": "transform",
      "qname": "sklearn.ensemble._stacking.StackingRegressor.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/transform/self",
          "name": "self",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._stacking/StackingRegressor/transform/X",
          "name": "X",
          "qname": "sklearn.ensemble._stacking.StackingRegressor.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where `n_samples` is the number of samples and\n`n_features` is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return the predictions for X for each estimator.",
      "docstring": "Return the predictions for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where `n_samples` is the number of samples and\n    `n_features` is the number of features.\n\nReturns\n-------\ny_preds : ndarray of shape (n_samples, n_estimators)\n    Prediction outputs for each estimator.",
      "code": "    def transform(self, X):\n        \"\"\"Return the predictions for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._voting.VotingClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/estimators",
          "name": "estimators",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.estimators",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "list of (str, estimator) tuples",
            "default_value": "",
            "description": "Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\nof those original estimators that will be stored in the class attribute\n``self.estimators_``. An estimator can be set to ``'drop'``\nusing ``set_params``.\n\n.. versionchanged:: 0.21\n    ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n    support was removed in 0.24."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of (str, estimator) tuples"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/voting",
          "name": "voting",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.voting",
          "default_value": "'hard'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'hard', 'soft'}",
            "default_value": "'hard'",
            "description": "If 'hard', uses predicted class labels for majority rule voting.\nElse if 'soft', predicts the class label based on the argmax of\nthe sums of the predicted probabilities, which is recommended for\nan ensemble of well-calibrated classifiers."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "hard",
              "soft"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/weights",
          "name": "weights",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.weights",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_classifiers,)",
            "default_value": "None",
            "description": "Sequence of weights (`float` or `int`) to weight the occurrences of\npredicted class labels (`hard` voting) or class probabilities\nbefore averaging (`soft` voting). Uses uniform weights if `None`."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_classifiers,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel for ``fit``.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/flatten_transform",
          "name": "flatten_transform",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.flatten_transform",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Affects shape of transform output only when voting='soft'\nIf voting='soft' and flatten_transform=True, transform method returns\nmatrix with shape (n_samples, n_classifiers * n_classes). If\nflatten_transform=False, it returns\n(n_classifiers, n_samples, n_classes)."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._voting.VotingClassifier.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the time elapsed while fitting will be printed as it\nis completed.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Soft Voting/Majority Rule classifier for unfitted estimators.\n\nRead more in the :ref:`User Guide <voting_classifier>`.\n\n.. versionadded:: 0.17",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimators, *, voting='hard', weights=None,\n                 n_jobs=None, flatten_transform=True, verbose=False):\n        super().__init__(estimators=estimators)\n        self.voting = voting\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.flatten_transform = flatten_transform\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._voting.VotingClassifier.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingClassifier.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._voting.VotingClassifier.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._voting.VotingClassifier.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._voting.VotingClassifier.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the estimators.",
      "docstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\n    .. versionadded:: 0.18\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n            .. versionadded:: 0.18\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        check_classification_targets(y)\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        transformed_y = self.le_.transform(y)\n\n        return super().fit(X, transformed_y, sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._voting.VotingClassifier.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingClassifier.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._voting.VotingClassifier.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class labels for X.",
      "docstring": "Predict class labels for X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\nmaj : array-like of shape (n_samples,)\n    Predicted class labels.",
      "code": "    def predict(self, X):\n        \"\"\"Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        maj : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        check_is_fitted(self)\n        if self.voting == 'soft':\n            maj = np.argmax(self.predict_proba(X), axis=1)\n\n        else:  # 'hard' voting\n            predictions = self._predict(X)\n            maj = np.apply_along_axis(\n                lambda x: np.argmax(\n                    np.bincount(x, weights=self._weights_not_none)),\n                axis=1, arr=predictions)\n\n        maj = self.le_.inverse_transform(maj)\n\n        return maj"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict_proba@getter",
      "name": "predict_proba",
      "qname": "sklearn.ensemble._voting.VotingClassifier.predict_proba",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/predict_proba@getter/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingClassifier.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute probabilities of possible outcomes for samples in X.",
      "docstring": "Compute probabilities of possible outcomes for samples in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\navg : array-like of shape (n_samples, n_classes)\n    Weighted average probability for each class per sample.",
      "code": "    @property\n    def predict_proba(self):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        avg : array-like of shape (n_samples, n_classes)\n            Weighted average probability for each class per sample.\n        \"\"\"\n        if self.voting == 'hard':\n            raise AttributeError(\"predict_proba is not available when\"\n                                 \" voting=%r\" % self.voting)\n        return self._predict_proba"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/transform",
      "name": "transform",
      "qname": "sklearn.ensemble._voting.VotingClassifier.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/transform/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingClassifier.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingClassifier/transform/X",
          "name": "X",
          "qname": "sklearn.ensemble._voting.VotingClassifier.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return class labels or probabilities for X for each estimator.",
      "docstring": "Return class labels or probabilities for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\nReturns\n-------\nprobabilities_or_labels\n    If `voting='soft'` and `flatten_transform=True`:\n        returns ndarray of shape (n_classifiers, n_samples *\n        n_classes), being class probabilities calculated by each\n        classifier.\n    If `voting='soft' and `flatten_transform=False`:\n        ndarray of shape (n_classifiers, n_samples, n_classes)\n    If `voting='hard'`:\n        ndarray of shape (n_samples, n_classifiers), being\n        class labels predicted by each classifier.",
      "code": "    def transform(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        probabilities_or_labels\n            If `voting='soft'` and `flatten_transform=True`:\n                returns ndarray of shape (n_classifiers, n_samples *\n                n_classes), being class probabilities calculated by each\n                classifier.\n            If `voting='soft' and `flatten_transform=False`:\n                ndarray of shape (n_classifiers, n_samples, n_classes)\n            If `voting='hard'`:\n                ndarray of shape (n_samples, n_classifiers), being\n                class labels predicted by each classifier.\n        \"\"\"\n        check_is_fitted(self)\n\n        if self.voting == 'soft':\n            probas = self._collect_probas(X)\n            if not self.flatten_transform:\n                return probas\n            return np.hstack(probas)\n\n        else:\n            return self._predict(X)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._voting.VotingRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__/estimators",
          "name": "estimators",
          "qname": "sklearn.ensemble._voting.VotingRegressor.__init__.estimators",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "list of (str, estimator) tuples",
            "default_value": "",
            "description": "Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\nof those original estimators that will be stored in the class attribute\n``self.estimators_``. An estimator can be set to ``'drop'`` using\n``set_params``.\n\n.. versionchanged:: 0.21\n    ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n    support was removed in 0.24."
          },
          "type": {
            "kind": "NamedType",
            "name": "list of (str, estimator) tuples"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__/weights",
          "name": "weights",
          "qname": "sklearn.ensemble._voting.VotingRegressor.__init__.weights",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_regressors,)",
            "default_value": "None",
            "description": "Sequence of weights (`float` or `int`) to weight the occurrences of\npredicted values before averaging. Uses uniform weights if `None`."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_regressors,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.ensemble._voting.VotingRegressor.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The number of jobs to run in parallel for ``fit``.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.ensemble._voting.VotingRegressor.__init__.verbose",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the time elapsed while fitting will be printed as it\nis completed.\n\n.. versionadded:: 0.23"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Prediction voting regressor for unfitted estimators.\n\nA voting regressor is an ensemble meta-estimator that fits several base\nregressors, each on the whole dataset. Then it averages the individual\npredictions to form a final prediction.\n\nRead more in the :ref:`User Guide <voting_regressor>`.\n\n.. versionadded:: 0.21",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimators, *, weights=None, n_jobs=None,\n                 verbose=False):\n        super().__init__(estimators=estimators)\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._voting.VotingRegressor.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingRegressor.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._voting.VotingRegressor.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors, where n_samples is the number of samples and\nn_features is the number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._voting.VotingRegressor.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._voting.VotingRegressor.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, then samples are equally weighted.\nNote that this is supported only if all underlying estimators\nsupport sample weights."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the estimators.",
      "docstring": "Fit the estimators.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vectors, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like of shape (n_samples,)\n    Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted.\n    Note that this is supported only if all underlying estimators\n    support sample weights.\n\nReturns\n-------\nself : object\n    Fitted estimator.",
      "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        y = column_or_1d(y, warn=True)\n        return super().fit(X, y, sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._voting.VotingRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._voting.VotingRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.",
      "docstring": "Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        return np.average(self._predict(X), axis=1,\n                          weights=self._weights_not_none)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/transform",
      "name": "transform",
      "qname": "sklearn.ensemble._voting.VotingRegressor.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/transform/self",
          "name": "self",
          "qname": "sklearn.ensemble._voting.VotingRegressor.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._voting/VotingRegressor/transform/X",
          "name": "X",
          "qname": "sklearn.ensemble._voting.VotingRegressor.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return predictions for X for each estimator.",
      "docstring": "Return predictions for X for each estimator.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The input samples.\n\nReturns\n-------\npredictions: ndarray of shape (n_samples, n_classifiers)\n    Values predicted by each regressor.",
      "code": "    def transform(self, X):\n        \"\"\"Return predictions for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        predictions: ndarray of shape (n_samples, n_classifiers)\n            Values predicted by each regressor.\n        \"\"\"\n        check_is_fitted(self)\n        return self._predict(X)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__/base_estimator",
          "name": "base_estimator",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__.base_estimator",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "The base estimator from which the boosted ensemble is built.\nSupport for sample weighting is required, as well as proper\n``classes_`` and ``n_classes_`` attributes. If ``None``, then\nthe base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\ninitialized with `max_depth=1`."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__.n_estimators",
          "default_value": "50",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "50",
            "description": "The maximum number of estimators at which boosting is terminated.\nIn case of perfect fit, the learning procedure is stopped early."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__/learning_rate",
          "name": "learning_rate",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__.learning_rate",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.",
            "description": "Weight applied to each classifier at each boosting iteration. A higher\nlearning rate increases the contribution of each classifier. There is\na trade-off between the `learning_rate` and `n_estimators` parameters."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__/algorithm",
          "name": "algorithm",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__.algorithm",
          "default_value": "'SAMME.R'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'SAMME', 'SAMME.R'}",
            "default_value": "'SAMME.R'",
            "description": "If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n``base_estimator`` must support calculation of class probabilities.\nIf 'SAMME' then use the SAMME discrete boosting algorithm.\nThe SAMME.R algorithm typically converges faster than SAMME,\nachieving a lower test error with fewer boosting iterations."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "SAMME",
              "SAMME.R"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the random seed given at each `base_estimator` at each\nboosting iteration.\nThus, it is only used when `base_estimator` exposes a `random_state`.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An AdaBoost classifier.\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None, *,\n                 n_estimators=50,\n                 learning_rate=1.,\n                 algorithm='SAMME.R',\n                 random_state=None):\n\n        super().__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state)\n\n        self.algorithm = algorithm"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/decision_function",
      "name": "decision_function",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the decision function of ``X``.",
      "docstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\nscore : ndarray of shape of (n_samples, k)\n    The decision function of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.\n    Binary classification is a special cases with ``k == 1``,\n    otherwise ``k==n_classes``. For binary classification,\n    values closer to -1 or 1 mean more like the first or second\n    class in ``classes_``, respectively.",
      "code": "    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        score : ndarray of shape of (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n\n        if self.algorithm == 'SAMME.R':\n            # The weights are all 1. for SAMME.R\n            pred = sum(_samme_proba(estimator, n_classes, X)\n                       for estimator in self.estimators_)\n        else:  # self.algorithm == \"SAMME\"\n            pred = sum((estimator.predict(X) == classes).T * w\n                       for estimator, w in zip(self.estimators_,\n                                               self.estimator_weights_))\n\n        pred /= self.estimator_weights_.sum()\n        if n_classes == 2:\n            pred[:, 0] *= -1\n            return pred.sum(axis=1)\n        return pred"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "The target values (class labels)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, the sample weights are initialized to\n``1 / n_samples``."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Build a boosted classifier from the training set (X, y).",
      "docstring": "Build a boosted classifier from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\ny : array-like of shape (n_samples,)\n    The target values (class labels).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, the sample weights are initialized to\n    ``1 / n_samples``.\n\nReturns\n-------\nself : object\n    Fitted estimator.",
      "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"Build a boosted classifier from the training set (X, y).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        y : array-like of shape (n_samples,)\n            The target values (class labels).\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, the sample weights are initialized to\n            ``1 / n_samples``.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        # Check that algorithm is supported\n        if self.algorithm not in ('SAMME', 'SAMME.R'):\n            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n\n        # Fit\n        return super().fit(X, y, sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict classes for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.",
      "docstring": "Predict classes for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted classes.",
      "code": "    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        X = self._check_X(X)\n\n        pred = self.decision_function(X)\n\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_log_proba",
      "name": "predict_log_proba",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_log_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_log_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_log_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_log_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_log_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe weighted mean predicted class log-probabilities of the classifiers\nin the ensemble.",
      "docstring": "Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe weighted mean predicted class log-probabilities of the classifiers\nin the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.",
      "code": "    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the weighted mean predicted class log-probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        X = self._check_X(X)\n        return np.log(self.predict_proba(X))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_proba",
      "name": "predict_proba",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.",
      "docstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\np : ndarray of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.",
      "code": "    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n\n        if n_classes == 1:\n            return np.ones((_num_samples(X), 1))\n\n        decision = self.decision_function(X)\n        return self._compute_proba_from_decision(decision, n_classes)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_decision_function",
      "name": "staged_decision_function",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_decision_function",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_decision_function/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_decision_function/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute decision function of ``X`` for each boosting iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each boosting iteration.",
      "docstring": "Compute decision function of ``X`` for each boosting iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each boosting iteration.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nYields\n------\nscore : generator of ndarray of shape (n_samples, k)\n    The decision function of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.\n    Binary classification is a special cases with ``k == 1``,\n    otherwise ``k==n_classes``. For binary classification,\n    values closer to -1 or 1 mean more like the first or second\n    class in ``classes_``, respectively.",
      "code": "    def staged_decision_function(self, X):\n        \"\"\"Compute decision function of ``X`` for each boosting iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each boosting iteration.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n            Binary classification is a special cases with ``k == 1``,\n            otherwise ``k==n_classes``. For binary classification,\n            values closer to -1 or 1 mean more like the first or second\n            class in ``classes_``, respectively.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_[:, np.newaxis]\n        pred = None\n        norm = 0.\n\n        for weight, estimator in zip(self.estimator_weights_,\n                                     self.estimators_):\n            norm += weight\n\n            if self.algorithm == 'SAMME.R':\n                # The weights are all 1. for SAMME.R\n                current_pred = _samme_proba(estimator, n_classes, X)\n            else:  # elif self.algorithm == \"SAMME\":\n                current_pred = estimator.predict(X)\n                current_pred = (current_pred == classes).T * weight\n\n            if pred is None:\n                pred = current_pred\n            else:\n                pred += current_pred\n\n            if n_classes == 2:\n                tmp_pred = np.copy(pred)\n                tmp_pred[:, 0] *= -1\n                yield (tmp_pred / norm).sum(axis=1)\n            else:\n                yield pred / norm"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict",
      "name": "staged_predict",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return staged predictions for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.",
      "docstring": "Return staged predictions for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nYields\n------\ny : generator of ndarray of shape (n_samples,)\n    The predicted classes.",
      "code": "    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n        classes = self.classes_\n\n        if n_classes == 2:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(pred > 0, axis=0))\n\n        else:\n            for pred in self.staged_decision_function(X):\n                yield np.array(classes.take(\n                    np.argmax(pred, axis=1), axis=0))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict_proba",
      "name": "staged_predict_proba",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict_proba",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict_proba/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostClassifier/staged_predict_proba/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n\nThis generator method yields the ensemble predicted class probabilities\nafter each iteration of boosting and therefore allows monitoring, such\nas to determine the predicted class probabilities on a test set after\neach boost.",
      "docstring": "Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n\nThis generator method yields the ensemble predicted class probabilities\nafter each iteration of boosting and therefore allows monitoring, such\nas to determine the predicted class probabilities on a test set after\neach boost.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nYields\n-------\np : generator of ndarray of shape (n_samples,)\n    The class probabilities of the input samples. The order of\n    outputs is the same of that of the :term:`classes_` attribute.",
      "code": "    def staged_predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        This generator method yields the ensemble predicted class probabilities\n        after each iteration of boosting and therefore allows monitoring, such\n        as to determine the predicted class probabilities on a test set after\n        each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Yields\n        -------\n        p : generator of ndarray of shape (n_samples,)\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the :term:`classes_` attribute.\n        \"\"\"\n        X = self._check_X(X)\n\n        n_classes = self.n_classes_\n\n        for decision in self.staged_decision_function(X):\n            yield self._compute_proba_from_decision(decision, n_classes)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__",
      "name": "__init__",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__/base_estimator",
          "name": "base_estimator",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__.base_estimator",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "None",
            "description": "The base estimator from which the boosted ensemble is built.\nIf ``None``, then the base estimator is\n:class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n`max_depth=3`."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__/n_estimators",
          "name": "n_estimators",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__.n_estimators",
          "default_value": "50",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "50",
            "description": "The maximum number of estimators at which boosting is terminated.\nIn case of perfect fit, the learning procedure is stopped early."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__/learning_rate",
          "name": "learning_rate",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__.learning_rate",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "1.",
            "description": "Weight applied to each classifier at each boosting iteration. A higher\nlearning rate increases the contribution of each classifier. There is\na trade-off between the `learning_rate` and `n_estimators` parameters."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__/loss",
          "name": "loss",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__.loss",
          "default_value": "'linear'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'linear', 'square', 'exponential'}",
            "default_value": "'linear'",
            "description": "The loss function to use when updating the weights after each\nboosting iteration."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "exponential",
              "linear",
              "square"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Controls the random seed given at each `base_estimator` at each\nboosting iteration.\nThus, it is only used when `base_estimator` exposes a `random_state`.\nIn addition, it controls the bootstrap of the weights used to train the\n`base_estimator` at each boosting iteration.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "An AdaBoost regressor.\n\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost.R2 [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n\n.. versionadded:: 0.14",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self,\n                 base_estimator=None, *,\n                 n_estimators=50,\n                 learning_rate=1.,\n                 loss='linear',\n                 random_state=None):\n\n        super().__init__(\n            base_estimator=base_estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state)\n\n        self.loss = loss\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/fit",
      "name": "fit",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/fit/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/fit/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/fit/y",
          "name": "y",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "The target values (real numbers)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/fit/sample_weight",
          "name": "sample_weight",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.fit.sample_weight",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "Sample weights. If None, the sample weights are initialized to\n1 / n_samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Build a boosted regressor from the training set (X, y).",
      "docstring": "Build a boosted regressor from the training set (X, y).\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\ny : array-like of shape (n_samples,)\n    The target values (real numbers).\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, the sample weights are initialized to\n    1 / n_samples.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"Build a boosted regressor from the training set (X, y).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        y : array-like of shape (n_samples,)\n            The target values (real numbers).\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, the sample weights are initialized to\n            1 / n_samples.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        # Check loss\n        if self.loss not in ('linear', 'square', 'exponential'):\n            raise ValueError(\n                \"loss must be 'linear', 'square', or 'exponential'\")\n\n        # Fit\n        return super().fit(X, y, sample_weight)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/predict",
      "name": "predict",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples. Sparse matrix can be CSC, CSR, COO,\nDOK, or LIL. COO, DOK, and LIL are converted to CSR."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict regression value for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.",
      "docstring": "Predict regression value for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Sparse matrix can be CSC, CSR, COO,\n    DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\nReturns\n-------\ny : ndarray of shape (n_samples,)\n    The predicted regression values.",
      "code": "    def predict(self, X):\n        \"\"\"Predict regression value for X.\n\n        The predicted regression value of an input sample is computed\n        as the weighted median prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted regression values.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        return self._get_median_predict(X, len(self.estimators_))"
    },
    {
      "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/staged_predict",
      "name": "staged_predict",
      "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.staged_predict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/staged_predict/self",
          "name": "self",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.staged_predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble._weight_boosting/AdaBoostRegressor/staged_predict/X",
          "name": "X",
          "qname": "sklearn.ensemble._weight_boosting.AdaBoostRegressor.staged_predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return staged predictions for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.",
      "docstring": "Return staged predictions for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples.\n\nYields\n-------\ny : generator of ndarray of shape (n_samples,)\n    The predicted regression values.",
      "code": "    def staged_predict(self, X):\n        \"\"\"Return staged predictions for X.\n\n        The predicted regression value of an input sample is computed\n        as the weighted median prediction of the classifiers in the ensemble.\n\n        This generator method yields the ensemble prediction after each\n        iteration of boosting and therefore allows monitoring, such as to\n        determine the prediction on a test set after each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        Yields\n        -------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted regression values.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_X(X)\n\n        for i, _ in enumerate(self.estimators_, 1):\n            yield self._get_median_predict(X, limit=i)"
    },
    {
      "id": "scikit-learn/sklearn.ensemble.setup/configuration",
      "name": "configuration",
      "qname": "sklearn.ensemble.setup.configuration",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.ensemble.setup/configuration/parent_package",
          "name": "parent_package",
          "qname": "sklearn.ensemble.setup.configuration.parent_package",
          "default_value": "''",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.ensemble.setup/configuration/top_path",
          "name": "top_path",
          "qname": "sklearn.ensemble.setup.configuration.top_path",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def configuration(parent_package=\"\", top_path=None):\n    config = Configuration(\"ensemble\", parent_package, top_path)\n\n    config.add_extension(\"_gradient_boosting\",\n                         sources=[\"_gradient_boosting.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_subpackage(\"tests\")\n\n    # Histogram-based gradient boosting files\n    config.add_extension(\n        \"_hist_gradient_boosting._gradient_boosting\",\n        sources=[\"_hist_gradient_boosting/_gradient_boosting.pyx\"],\n        include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting.histogram\",\n                         sources=[\"_hist_gradient_boosting/histogram.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting.splitting\",\n                         sources=[\"_hist_gradient_boosting/splitting.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting._binning\",\n                         sources=[\"_hist_gradient_boosting/_binning.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting._predictor\",\n                         sources=[\"_hist_gradient_boosting/_predictor.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting._loss\",\n                         sources=[\"_hist_gradient_boosting/_loss.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting._bitset\",\n                         sources=[\"_hist_gradient_boosting/_bitset.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting.common\",\n                         sources=[\"_hist_gradient_boosting/common.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_extension(\"_hist_gradient_boosting.utils\",\n                         sources=[\"_hist_gradient_boosting/utils.pyx\"],\n                         include_dirs=[numpy.get_include()])\n\n    config.add_subpackage(\"_hist_gradient_boosting.tests\")\n\n    return config"
    },
    {
      "id": "scikit-learn/sklearn.externals.conftest/pytest_ignore_collect",
      "name": "pytest_ignore_collect",
      "qname": "sklearn.externals.conftest.pytest_ignore_collect",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.externals.conftest/pytest_ignore_collect/path",
          "name": "path",
          "qname": "sklearn.externals.conftest.pytest_ignore_collect.path",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.externals.conftest/pytest_ignore_collect/config",
          "name": "config",
          "qname": "sklearn.externals.conftest.pytest_ignore_collect.config",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def pytest_ignore_collect(path, config):\n    return True"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__.dtype",
          "default_value": "np.float64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dtype",
            "default_value": "np.float64",
            "description": "The type of feature values. Passed to Numpy array/scipy.sparse matrix\nconstructors as the dtype argument."
          },
          "type": {
            "kind": "NamedType",
            "name": "dtype"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__/separator",
          "name": "separator",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__.separator",
          "default_value": "'='",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "\"=\"",
            "description": "Separator string used when constructing new features for one-hot\ncoding."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__/sparse",
          "name": "sparse",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__.sparse",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether transform should produce scipy.sparse matrices."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/__init__/sort",
          "name": "sort",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__.sort",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether ``feature_names_`` and ``vocabulary_`` should be\nsorted when fitting."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transforms lists of feature-value mappings to vectors.\n\nThis transformer turns lists of mappings (dict-like objects) of feature\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\nwith scikit-learn estimators.\n\nWhen feature values are strings, this transformer will do a binary one-hot\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\nof the possible string values that the feature can take on. For instance,\na feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\nfeatures in the output, one signifying \"f=ham\", the other \"f=spam\".\n\nIf a feature value is a sequence or set of strings, this transformer\nwill iterate over the values and will count the occurrences of each string\nvalue.\n\nHowever, note that this transformer will only do a binary one-hot encoding\nwhen feature values are of type string. If categorical features are\nrepresented as numeric values such as int or iterables of strings, the\nDictVectorizer can be followed by\n:class:`~sklearn.preprocessing.OneHotEncoder` to complete\nbinary one-hot encoding.\n\nFeatures that do not occur in a sample (mapping) will have a zero value\nin the resulting array/matrix.\n\nRead more in the :ref:`User Guide <dict_feature_extraction>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, dtype=np.float64, separator=\"=\", sparse=True,\n                 sort=True):\n        self.dtype = dtype\n        self.separator = separator\n        self.sparse = sparse\n        self.sort = sort"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit/X",
          "name": "X",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Mapping or iterable over Mappings",
            "default_value": "",
            "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python\nobjects) to feature values (strings or convertible to dtype).\n\n.. versionchanged:: 0.24\n   Accepts multiple string values for one categorical feature."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "Mapping"
              },
              {
                "kind": "NamedType",
                "name": "iterable over Mappings"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "(ignored)",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "(ignored)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn a list of feature name -> indices mappings.",
      "docstring": "Learn a list of feature name -> indices mappings.\n\nParameters\n----------\nX : Mapping or iterable over Mappings\n    Dict(s) or Mapping(s) from feature names (arbitrary Python\n    objects) to feature values (strings or convertible to dtype).\n\n    .. versionchanged:: 0.24\n       Accepts multiple string values for one categorical feature.\n\ny : (ignored)\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Learn a list of feature name -> indices mappings.\n\n        Parameters\n        ----------\n        X : Mapping or iterable over Mappings\n            Dict(s) or Mapping(s) from feature names (arbitrary Python\n            objects) to feature values (strings or convertible to dtype).\n\n            .. versionchanged:: 0.24\n               Accepts multiple string values for one categorical feature.\n\n        y : (ignored)\n\n        Returns\n        -------\n        self\n        \"\"\"\n        feature_names = []\n        vocab = {}\n\n        for x in X:\n            for f, v in x.items():\n                if isinstance(v, str):\n                    feature_name = \"%s%s%s\" % (f, self.separator, v)\n                    v = 1\n                elif isinstance(v, Number) or (v is None):\n                    feature_name = f\n                elif isinstance(v, Mapping):\n                    raise TypeError(f'Unsupported value type {type(v)} '\n                                    f'for {f}: {v}.\\n'\n                                    'Mapping objects are not supported.')\n                elif isinstance(v, Iterable):\n                    feature_name = None\n                    self._add_iterable_element(f, v, feature_names, vocab)\n\n                if feature_name is not None:\n                    if feature_name not in vocab:\n                        vocab[feature_name] = len(feature_names)\n                        feature_names.append(feature_name)\n\n        if self.sort:\n            feature_names.sort()\n            vocab = {f: i for i, f in enumerate(feature_names)}\n\n        self.feature_names_ = feature_names\n        self.vocabulary_ = vocab\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit_transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit_transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Mapping or iterable over Mappings",
            "default_value": "",
            "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python\nobjects) to feature values (strings or convertible to dtype).\n\n.. versionchanged:: 0.24\n   Accepts multiple string values for one categorical feature."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "Mapping"
              },
              {
                "kind": "NamedType",
                "name": "iterable over Mappings"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/fit_transform/y",
          "name": "y",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "(ignored)",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "(ignored)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn a list of feature name -> indices mappings and transform X.\n\nLike fit(X) followed by transform(X), but does not require\nmaterializing X in memory.",
      "docstring": "Learn a list of feature name -> indices mappings and transform X.\n\nLike fit(X) followed by transform(X), but does not require\nmaterializing X in memory.\n\nParameters\n----------\nX : Mapping or iterable over Mappings\n    Dict(s) or Mapping(s) from feature names (arbitrary Python\n    objects) to feature values (strings or convertible to dtype).\n\n    .. versionchanged:: 0.24\n       Accepts multiple string values for one categorical feature.\n\ny : (ignored)\n\nReturns\n-------\nXa : {array, sparse matrix}\n    Feature vectors; always 2-d.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Learn a list of feature name -> indices mappings and transform X.\n\n        Like fit(X) followed by transform(X), but does not require\n        materializing X in memory.\n\n        Parameters\n        ----------\n        X : Mapping or iterable over Mappings\n            Dict(s) or Mapping(s) from feature names (arbitrary Python\n            objects) to feature values (strings or convertible to dtype).\n\n            .. versionchanged:: 0.24\n               Accepts multiple string values for one categorical feature.\n\n        y : (ignored)\n\n        Returns\n        -------\n        Xa : {array, sparse matrix}\n            Feature vectors; always 2-d.\n        \"\"\"\n        return self._transform(X, fitting=True)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/get_feature_names",
      "name": "get_feature_names",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.get_feature_names",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/get_feature_names/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.get_feature_names.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Returns a list of feature names, ordered by their indices.\n\nIf one-of-K coding is applied to categorical features, this will\ninclude the constructed feature names but not the original ones.",
      "docstring": "Returns a list of feature names, ordered by their indices.\n\nIf one-of-K coding is applied to categorical features, this will\ninclude the constructed feature names but not the original ones.",
      "code": "    def get_feature_names(self):\n        \"\"\"Returns a list of feature names, ordered by their indices.\n\n        If one-of-K coding is applied to categorical features, this will\n        include the constructed feature names but not the original ones.\n        \"\"\"\n        return self.feature_names_"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/inverse_transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.inverse_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Sample matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/inverse_transform/dict_type",
          "name": "dict_type",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.inverse_transform.dict_type",
          "default_value": "dict",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "type",
            "default_value": "dict",
            "description": "Constructor for feature mappings. Must conform to the\ncollections.Mapping API."
          },
          "type": {
            "kind": "NamedType",
            "name": "type"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform array or sparse matrix X back to feature mappings.\n\nX must have been produced by this DictVectorizer's transform or\nfit_transform method; it may only have passed through transformers\nthat preserve the number of features and their order.\n\nIn the case of one-hot/one-of-K coding, the constructed feature\nnames and values are returned rather than the original ones.",
      "docstring": "Transform array or sparse matrix X back to feature mappings.\n\nX must have been produced by this DictVectorizer's transform or\nfit_transform method; it may only have passed through transformers\nthat preserve the number of features and their order.\n\nIn the case of one-hot/one-of-K coding, the constructed feature\nnames and values are returned rather than the original ones.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Sample matrix.\ndict_type : type, default=dict\n    Constructor for feature mappings. Must conform to the\n    collections.Mapping API.\n\nReturns\n-------\nD : list of dict_type objects of shape (n_samples,)\n    Feature mappings for the samples in X.",
      "code": "    def inverse_transform(self, X, dict_type=dict):\n        \"\"\"Transform array or sparse matrix X back to feature mappings.\n\n        X must have been produced by this DictVectorizer's transform or\n        fit_transform method; it may only have passed through transformers\n        that preserve the number of features and their order.\n\n        In the case of one-hot/one-of-K coding, the constructed feature\n        names and values are returned rather than the original ones.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Sample matrix.\n        dict_type : type, default=dict\n            Constructor for feature mappings. Must conform to the\n            collections.Mapping API.\n\n        Returns\n        -------\n        D : list of dict_type objects of shape (n_samples,)\n            Feature mappings for the samples in X.\n        \"\"\"\n        # COO matrix is not subscriptable\n        X = check_array(X, accept_sparse=['csr', 'csc'])\n        n_samples = X.shape[0]\n\n        names = self.feature_names_\n        dicts = [dict_type() for _ in range(n_samples)]\n\n        if sp.issparse(X):\n            for i, j in zip(*X.nonzero()):\n                dicts[i][names[j]] = X[i, j]\n        else:\n            for i, d in enumerate(dicts):\n                for j, v in enumerate(X[i, :]):\n                    if v != 0:\n                        d[names[j]] = X[i, j]\n\n        return dicts"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/restrict",
      "name": "restrict",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.restrict",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/restrict/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.restrict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/restrict/support",
          "name": "support",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.restrict.support",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like",
            "default_value": "",
            "description": "Boolean mask or list of indices (as returned by the get_support\nmember of feature selectors)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/restrict/indices",
          "name": "indices",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.restrict.indices",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether support is a list of indices."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Restrict the features to those in support using feature selection.\n\nThis function modifies the estimator in-place.",
      "docstring": "Restrict the features to those in support using feature selection.\n\nThis function modifies the estimator in-place.\n\nParameters\n----------\nsupport : array-like\n    Boolean mask or list of indices (as returned by the get_support\n    member of feature selectors).\nindices : bool, default=False\n    Whether support is a list of indices.\n\nReturns\n-------\nself\n\nExamples\n--------\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> from sklearn.feature_selection import SelectKBest, chi2\n>>> v = DictVectorizer()\n>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n>>> X = v.fit_transform(D)\n>>> support = SelectKBest(chi2, k=2).fit(X, [0, 1])\n>>> v.get_feature_names()\n['bar', 'baz', 'foo']\n>>> v.restrict(support.get_support())\nDictVectorizer()\n>>> v.get_feature_names()\n['bar', 'foo']",
      "code": "    def restrict(self, support, indices=False):\n        \"\"\"Restrict the features to those in support using feature selection.\n\n        This function modifies the estimator in-place.\n\n        Parameters\n        ----------\n        support : array-like\n            Boolean mask or list of indices (as returned by the get_support\n            member of feature selectors).\n        indices : bool, default=False\n            Whether support is a list of indices.\n\n        Returns\n        -------\n        self\n\n        Examples\n        --------\n        >>> from sklearn.feature_extraction import DictVectorizer\n        >>> from sklearn.feature_selection import SelectKBest, chi2\n        >>> v = DictVectorizer()\n        >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n        >>> X = v.fit_transform(D)\n        >>> support = SelectKBest(chi2, k=2).fit(X, [0, 1])\n        >>> v.get_feature_names()\n        ['bar', 'baz', 'foo']\n        >>> v.restrict(support.get_support())\n        DictVectorizer()\n        >>> v.get_feature_names()\n        ['bar', 'foo']\n        \"\"\"\n        if not indices:\n            support = np.where(support)[0]\n\n        names = self.feature_names_\n        new_vocab = {}\n        for i in support:\n            new_vocab[names[i]] = len(new_vocab)\n\n        self.vocabulary_ = new_vocab\n        self.feature_names_ = [f for f, i in sorted(new_vocab.items(),\n                                                    key=itemgetter(1))]\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._dict_vectorizer/DictVectorizer/transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction._dict_vectorizer.DictVectorizer.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "Mapping or iterable over Mappings of shape (n_samples,)",
            "default_value": "",
            "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python\nobjects) to feature values (strings or convertible to dtype)."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "Mapping"
              },
              {
                "kind": "NamedType",
                "name": "iterable over Mappings of shape (n_samples,)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform feature->value dicts to array or sparse matrix.\n\nNamed features not encountered during fit or fit_transform will be\nsilently ignored.",
      "docstring": "Transform feature->value dicts to array or sparse matrix.\n\nNamed features not encountered during fit or fit_transform will be\nsilently ignored.\n\nParameters\n----------\nX : Mapping or iterable over Mappings of shape (n_samples,)\n    Dict(s) or Mapping(s) from feature names (arbitrary Python\n    objects) to feature values (strings or convertible to dtype).\n\nReturns\n-------\nXa : {array, sparse matrix}\n    Feature vectors; always 2-d.",
      "code": "    def transform(self, X):\n        \"\"\"Transform feature->value dicts to array or sparse matrix.\n\n        Named features not encountered during fit or fit_transform will be\n        silently ignored.\n\n        Parameters\n        ----------\n        X : Mapping or iterable over Mappings of shape (n_samples,)\n            Dict(s) or Mapping(s) from feature names (arbitrary Python\n            objects) to feature values (strings or convertible to dtype).\n\n        Returns\n        -------\n        Xa : {array, sparse matrix}\n            Feature vectors; always 2-d.\n        \"\"\"\n        if self.sparse:\n            return self._transform(X, fitting=False)\n\n        else:\n            dtype = self.dtype\n            vocab = self.vocabulary_\n            X = _tosequence(X)\n            Xa = np.zeros((len(X), len(vocab)), dtype=dtype)\n\n            for i, x in enumerate(X):\n                for f, v in x.items():\n                    if isinstance(v, str):\n                        f = \"%s%s%s\" % (f, self.separator, v)\n                        v = 1\n                    try:\n                        Xa[i, vocab[f]] = dtype(v)\n                    except KeyError:\n                        pass\n\n            return Xa"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction._hash.FeatureHasher.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__/n_features",
          "name": "n_features",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.__init__.n_features",
          "default_value": "2**20",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "2**20",
            "description": "The number of features (columns) in the output matrices. Small numbers\nof features are likely to cause hash collisions, but large numbers\nwill cause larger coefficient dimensions in linear learners."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__/input_type",
          "name": "input_type",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.__init__.input_type",
          "default_value": "'dict'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{\"dict\", \"pair\", \"string\"}",
            "default_value": "\"dict\"",
            "description": "Either \"dict\" (the default) to accept dictionaries over\n(feature_name, value); \"pair\" to accept pairs of (feature_name, value);\nor \"string\" to accept single strings.\nfeature_name should be a string, while value should be a number.\nIn the case of \"string\", a value of 1 is implied.\nThe feature_name is hashed to find the appropriate column for the\nfeature. The value's sign might be flipped in the output (but see\nnon_negative, below)."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "dict",
              "pair",
              "string"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.__init__.dtype",
          "default_value": "np.float64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "numpy dtype",
            "default_value": "np.float64",
            "description": "The type of feature values. Passed to scipy.sparse matrix constructors\nas the dtype argument. Do not set this to bool, np.boolean or any\nunsigned integer type."
          },
          "type": {
            "kind": "NamedType",
            "name": "numpy dtype"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/__init__/alternate_sign",
          "name": "alternate_sign",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.__init__.alternate_sign",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "When True, an alternating sign is added to the features as to\napproximately conserve the inner product in the hashed space even for\nsmall n_features. This approach is similar to sparse random projection.\n\n.. versionchanged:: 0.19\n    ``alternate_sign`` replaces the now deprecated ``non_negative``\n    parameter."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Implements feature hashing, aka the hashing trick.\n\nThis class turns sequences of symbolic feature names (strings) into\nscipy.sparse matrices, using a hash function to compute the matrix column\ncorresponding to a name. The hash function employed is the signed 32-bit\nversion of Murmurhash3.\n\nFeature names of type byte string are used as-is. Unicode strings are\nconverted to UTF-8 first, but no Unicode normalization is done.\nFeature values must be (finite) numbers.\n\nThis class is a low-memory alternative to DictVectorizer and\nCountVectorizer, intended for large-scale (online) learning and situations\nwhere memory is tight, e.g. when running prediction code on embedded\ndevices.\n\nRead more in the :ref:`User Guide <feature_hashing>`.\n\n.. versionadded:: 0.13",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, n_features=(2 ** 20), *, input_type=\"dict\",\n                 dtype=np.float64, alternate_sign=True):\n        self._validate_params(n_features, input_type)\n\n        self.dtype = dtype\n        self.input_type = input_type\n        self.n_features = n_features\n        self.alternate_sign = alternate_sign"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction._hash.FeatureHasher.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/fit/X",
          "name": "X",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.fit.X",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "No-op.\n\nThis method doesn't do anything. It exists purely for compatibility\nwith the scikit-learn transformer API.",
      "docstring": "No-op.\n\nThis method doesn't do anything. It exists purely for compatibility\nwith the scikit-learn transformer API.\n\nParameters\n----------\nX : ndarray\n\nReturns\n-------\nself : FeatureHasher",
      "code": "    def fit(self, X=None, y=None):\n        \"\"\"No-op.\n\n        This method doesn't do anything. It exists purely for compatibility\n        with the scikit-learn transformer API.\n\n        Parameters\n        ----------\n        X : ndarray\n\n        Returns\n        -------\n        self : FeatureHasher\n\n        \"\"\"\n        # repeat input validation for grid search (which calls set_params)\n        self._validate_params(self.n_features, self.input_type)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction._hash.FeatureHasher.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction._hash/FeatureHasher/transform/raw_X",
          "name": "raw_X",
          "qname": "sklearn.feature_extraction._hash.FeatureHasher.transform.raw_X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable over iterable over raw features, length = n_samples",
            "default_value": "",
            "description": "Samples. Each sample must be iterable an (e.g., a list or tuple)\ncontaining/generating feature names (and optionally values, see\nthe input_type constructor argument) which will be hashed.\nraw_X need not support the len function, so it can be the result\nof a generator; n_samples is determined on the fly."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "iterable over iterable over raw features"
              },
              {
                "kind": "NamedType",
                "name": "length = n_samples"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform a sequence of instances to a scipy.sparse matrix.",
      "docstring": "Transform a sequence of instances to a scipy.sparse matrix.\n\nParameters\n----------\nraw_X : iterable over iterable over raw features, length = n_samples\n    Samples. Each sample must be iterable an (e.g., a list or tuple)\n    containing/generating feature names (and optionally values, see\n    the input_type constructor argument) which will be hashed.\n    raw_X need not support the len function, so it can be the result\n    of a generator; n_samples is determined on the fly.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Feature matrix, for use with estimators or further transformers.",
      "code": "    def transform(self, raw_X):\n        \"\"\"Transform a sequence of instances to a scipy.sparse matrix.\n\n        Parameters\n        ----------\n        raw_X : iterable over iterable over raw features, length = n_samples\n            Samples. Each sample must be iterable an (e.g., a list or tuple)\n            containing/generating feature names (and optionally values, see\n            the input_type constructor argument) which will be hashed.\n            raw_X need not support the len function, so it can be the result\n            of a generator; n_samples is determined on the fly.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Feature matrix, for use with estimators or further transformers.\n\n        \"\"\"\n        raw_X = iter(raw_X)\n        if self.input_type == \"dict\":\n            raw_X = (_iteritems(d) for d in raw_X)\n        elif self.input_type == \"string\":\n            raw_X = (((f, 1) for f in x) for x in raw_X)\n        indices, indptr, values = \\\n            _hashing_transform(raw_X, self.n_features, self.dtype,\n                               self.alternate_sign, seed=0)\n        n_samples = indptr.shape[0] - 1\n\n        if n_samples == 0:\n            raise ValueError(\"Cannot vectorize empty sequence.\")\n\n        X = sp.csr_matrix((values, indices, indptr), dtype=self.dtype,\n                          shape=(n_samples, self.n_features))\n        X.sum_duplicates()  # also sorts the indices\n\n        return X"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction.image.PatchExtractor.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/__init__/patch_size",
          "name": "patch_size",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.__init__.patch_size",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple of int (patch_height, patch_width)",
            "default_value": "None",
            "description": "The dimensions of one patch."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of int (patch_height, patch_width)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/__init__/max_patches",
          "name": "max_patches",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.__init__.max_patches",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "The maximum number of patches per image to extract. If max_patches is a\nfloat in (0, 1), it is taken to mean a proportion of the total number\nof patches."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/__init__/random_state",
          "name": "random_state",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.__init__.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "Determines the random number generator used for random sampling when\n`max_patches` is not None. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Extracts patches from a collection of images\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\n.. versionadded:: 0.9",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, patch_size=None, max_patches=None,\n                 random_state=None):\n        self.patch_size = patch_size\n        self.max_patches = max_patches\n        self.random_state = random_state"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction.image.PatchExtractor.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/fit/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.",
      "docstring": "Do nothing and return the estimator unchanged.\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training data.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Do nothing and return the estimator unchanged.\n\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        \"\"\"\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction.image.PatchExtractor.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/PatchExtractor/transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.image.PatchExtractor.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples, image_height, image_width) or             (n_samples, image_height, image_width, n_channels)",
            "default_value": "",
            "description": "Array of images from which to extract patches. For color images,\nthe last dimension specifies the channel: a RGB image would have\n`n_channels=3`."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples, image_height, image_width) or (n_samples, image_height, image_width, n_channels)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transforms the image samples in X into a matrix of patch data.",
      "docstring": "Transforms the image samples in X into a matrix of patch data.\n\nParameters\n----------\nX : ndarray of shape (n_samples, image_height, image_width) or             (n_samples, image_height, image_width, n_channels)\n    Array of images from which to extract patches. For color images,\n    the last dimension specifies the channel: a RGB image would have\n    `n_channels=3`.\n\nReturns\n-------\npatches : array of shape (n_patches, patch_height, patch_width) or              (n_patches, patch_height, patch_width, n_channels)\n     The collection of patches extracted from the images, where\n     `n_patches` is either `n_samples * max_patches` or the total\n     number of patches that can be extracted.",
      "code": "    def transform(self, X):\n        \"\"\"Transforms the image samples in X into a matrix of patch data.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, image_height, image_width) or \\\n            (n_samples, image_height, image_width, n_channels)\n            Array of images from which to extract patches. For color images,\n            the last dimension specifies the channel: a RGB image would have\n            `n_channels=3`.\n\n        Returns\n        -------\n        patches : array of shape (n_patches, patch_height, patch_width) or \\\n             (n_patches, patch_height, patch_width, n_channels)\n             The collection of patches extracted from the images, where\n             `n_patches` is either `n_samples * max_patches` or the total\n             number of patches that can be extracted.\n        \"\"\"\n        self.random_state = check_random_state(self.random_state)\n        n_images, i_h, i_w = X.shape[:3]\n        X = np.reshape(X, (n_images, i_h, i_w, -1))\n        n_channels = X.shape[-1]\n        if self.patch_size is None:\n            patch_size = i_h // 10, i_w // 10\n        else:\n            patch_size = self.patch_size\n\n        # compute the dimensions of the patches array\n        p_h, p_w = patch_size\n        n_patches = _compute_n_patches(i_h, i_w, p_h, p_w, self.max_patches)\n        patches_shape = (n_images * n_patches,) + patch_size\n        if n_channels > 1:\n            patches_shape += (n_channels,)\n\n        # extract the patches\n        patches = np.empty(patches_shape)\n        for ii, image in enumerate(X):\n            patches[ii * n_patches:(ii + 1) * n_patches] = extract_patches_2d(\n                image, patch_size, max_patches=self.max_patches,\n                random_state=self.random_state)\n        return patches"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/extract_patches_2d",
      "name": "extract_patches_2d",
      "qname": "sklearn.feature_extraction.image.extract_patches_2d",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/extract_patches_2d/image",
          "name": "image",
          "qname": "sklearn.feature_extraction.image.extract_patches_2d.image",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (image_height, image_width) or         (image_height, image_width, n_channels)",
            "default_value": "",
            "description": "The original image data. For color images, the last dimension specifies\nthe channel: a RGB image would have `n_channels=3`."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (image_height, image_width) or (image_height, image_width, n_channels)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/extract_patches_2d/patch_size",
          "name": "patch_size",
          "qname": "sklearn.feature_extraction.image.extract_patches_2d.patch_size",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "tuple of int (patch_height, patch_width)",
            "default_value": "",
            "description": "The dimensions of one patch."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of int (patch_height, patch_width)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/extract_patches_2d/max_patches",
          "name": "max_patches",
          "qname": "sklearn.feature_extraction.image.extract_patches_2d.max_patches",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "The maximum number of patches to extract. If `max_patches` is a float\nbetween 0 and 1, it is taken to be a proportion of the total number\nof patches."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/extract_patches_2d/random_state",
          "name": "random_state",
          "qname": "sklearn.feature_extraction.image.extract_patches_2d.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance",
            "default_value": "None",
            "description": "Determines the random number generator used for random sampling when\n`max_patches` is not None. Use an int to make the randomness\ndeterministic.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Reshape a 2D image into a collection of patches\n\nThe resulting patches are allocated in a dedicated array.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.",
      "docstring": "Reshape a 2D image into a collection of patches\n\nThe resulting patches are allocated in a dedicated array.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters\n----------\nimage : ndarray of shape (image_height, image_width) or         (image_height, image_width, n_channels)\n    The original image data. For color images, the last dimension specifies\n    the channel: a RGB image would have `n_channels=3`.\n\npatch_size : tuple of int (patch_height, patch_width)\n    The dimensions of one patch.\n\nmax_patches : int or float, default=None\n    The maximum number of patches to extract. If `max_patches` is a float\n    between 0 and 1, it is taken to be a proportion of the total number\n    of patches.\n\nrandom_state : int, RandomState instance, default=None\n    Determines the random number generator used for random sampling when\n    `max_patches` is not None. Use an int to make the randomness\n    deterministic.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\npatches : array of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels)\n    The collection of patches extracted from the image, where `n_patches`\n    is either `max_patches` or the total number of patches that can be\n    extracted.\n\nExamples\n--------\n>>> from sklearn.datasets import load_sample_image\n>>> from sklearn.feature_extraction import image\n>>> # Use the array data from the first image in this dataset:\n>>> one_image = load_sample_image(\"china.jpg\")\n>>> print('Image shape: {}'.format(one_image.shape))\nImage shape: (427, 640, 3)\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\n>>> print('Patches shape: {}'.format(patches.shape))\nPatches shape: (272214, 2, 2, 3)\n>>> # Here are just two of these patches:\n>>> print(patches[1])\n[[[174 201 231]\n  [174 201 231]]\n [[173 200 230]\n  [173 200 230]]]\n>>> print(patches[800])\n[[[187 214 243]\n  [188 215 244]]\n [[187 214 243]\n  [188 215 244]]]",
      "code": "@_deprecate_positional_args\ndef extract_patches_2d(image, patch_size, *, max_patches=None,\n                       random_state=None):\n    \"\"\"Reshape a 2D image into a collection of patches\n\n    The resulting patches are allocated in a dedicated array.\n\n    Read more in the :ref:`User Guide <image_feature_extraction>`.\n\n    Parameters\n    ----------\n    image : ndarray of shape (image_height, image_width) or \\\n        (image_height, image_width, n_channels)\n        The original image data. For color images, the last dimension specifies\n        the channel: a RGB image would have `n_channels=3`.\n\n    patch_size : tuple of int (patch_height, patch_width)\n        The dimensions of one patch.\n\n    max_patches : int or float, default=None\n        The maximum number of patches to extract. If `max_patches` is a float\n        between 0 and 1, it is taken to be a proportion of the total number\n        of patches.\n\n    random_state : int, RandomState instance, default=None\n        Determines the random number generator used for random sampling when\n        `max_patches` is not None. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    patches : array of shape (n_patches, patch_height, patch_width) or \\\n        (n_patches, patch_height, patch_width, n_channels)\n        The collection of patches extracted from the image, where `n_patches`\n        is either `max_patches` or the total number of patches that can be\n        extracted.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_sample_image\n    >>> from sklearn.feature_extraction import image\n    >>> # Use the array data from the first image in this dataset:\n    >>> one_image = load_sample_image(\"china.jpg\")\n    >>> print('Image shape: {}'.format(one_image.shape))\n    Image shape: (427, 640, 3)\n    >>> patches = image.extract_patches_2d(one_image, (2, 2))\n    >>> print('Patches shape: {}'.format(patches.shape))\n    Patches shape: (272214, 2, 2, 3)\n    >>> # Here are just two of these patches:\n    >>> print(patches[1])\n    [[[174 201 231]\n      [174 201 231]]\n     [[173 200 230]\n      [173 200 230]]]\n    >>> print(patches[800])\n    [[[187 214 243]\n      [188 215 244]]\n     [[187 214 243]\n      [188 215 244]]]\n    \"\"\"\n    i_h, i_w = image.shape[:2]\n    p_h, p_w = patch_size\n\n    if p_h > i_h:\n        raise ValueError(\"Height of the patch should be less than the height\"\n                         \" of the image.\")\n\n    if p_w > i_w:\n        raise ValueError(\"Width of the patch should be less than the width\"\n                         \" of the image.\")\n\n    image = check_array(image, allow_nd=True)\n    image = image.reshape((i_h, i_w, -1))\n    n_colors = image.shape[-1]\n\n    extracted_patches = _extract_patches(image,\n                                         patch_shape=(p_h, p_w, n_colors),\n                                         extraction_step=1)\n\n    n_patches = _compute_n_patches(i_h, i_w, p_h, p_w, max_patches)\n    if max_patches:\n        rng = check_random_state(random_state)\n        i_s = rng.randint(i_h - p_h + 1, size=n_patches)\n        j_s = rng.randint(i_w - p_w + 1, size=n_patches)\n        patches = extracted_patches[i_s, j_s, 0]\n    else:\n        patches = extracted_patches\n\n    patches = patches.reshape(-1, p_h, p_w, n_colors)\n    # remove the color dimension if useless\n    if patches.shape[-1] == 1:\n        return patches.reshape((n_patches, p_h, p_w))\n    else:\n        return patches"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph",
      "name": "grid_to_graph",
      "qname": "sklearn.feature_extraction.image.grid_to_graph",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph/n_x",
          "name": "n_x",
          "qname": "sklearn.feature_extraction.image.grid_to_graph.n_x",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Dimension in x axis"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph/n_y",
          "name": "n_y",
          "qname": "sklearn.feature_extraction.image.grid_to_graph.n_y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "",
            "description": "Dimension in y axis"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph/n_z",
          "name": "n_z",
          "qname": "sklearn.feature_extraction.image.grid_to_graph.n_z",
          "default_value": "1",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "Dimension in z axis"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph/mask",
          "name": "mask",
          "qname": "sklearn.feature_extraction.image.grid_to_graph.mask",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_x, n_y, n_z), dtype=bool",
            "default_value": "None",
            "description": "An optional mask of the image, to consider only part of the\npixels."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape (n_x, n_y, n_z)"
              },
              {
                "kind": "NamedType",
                "name": "dtype=bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph/return_as",
          "name": "return_as",
          "qname": "sklearn.feature_extraction.image.grid_to_graph.return_as",
          "default_value": "sparse.coo_matrix",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "np.ndarray or a sparse matrix class",
            "default_value": "sparse.coo_matrix",
            "description": "The class to use to build the returned adjacency matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "np.ndarray"
              },
              {
                "kind": "NamedType",
                "name": "a sparse matrix class"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/grid_to_graph/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction.image.grid_to_graph.dtype",
          "default_value": "int",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dtype",
            "default_value": "int",
            "description": "The data of the returned sparse matrix. By default it is int"
          },
          "type": {
            "kind": "NamedType",
            "name": "dtype"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_extraction"
      ],
      "description": "Graph of the pixel-to-pixel connections\n\nEdges exist if 2 voxels are connected.",
      "docstring": "Graph of the pixel-to-pixel connections\n\nEdges exist if 2 voxels are connected.\n\nParameters\n----------\nn_x : int\n    Dimension in x axis\nn_y : int\n    Dimension in y axis\nn_z : int, default=1\n    Dimension in z axis\nmask : ndarray of shape (n_x, n_y, n_z), dtype=bool, default=None\n    An optional mask of the image, to consider only part of the\n    pixels.\nreturn_as : np.ndarray or a sparse matrix class,             default=sparse.coo_matrix\n    The class to use to build the returned adjacency matrix.\ndtype : dtype, default=int\n    The data of the returned sparse matrix. By default it is int\n\nNotes\n-----\nFor scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was\nhandled by returning a dense np.matrix instance.  Going forward, np.ndarray\nreturns an np.ndarray, as expected.\n\nFor compatibility, user code relying on this method should wrap its\ncalls in ``np.asarray`` to avoid type issues.",
      "code": "@_deprecate_positional_args\ndef grid_to_graph(n_x, n_y, n_z=1, *, mask=None, return_as=sparse.coo_matrix,\n                  dtype=int):\n    \"\"\"Graph of the pixel-to-pixel connections\n\n    Edges exist if 2 voxels are connected.\n\n    Parameters\n    ----------\n    n_x : int\n        Dimension in x axis\n    n_y : int\n        Dimension in y axis\n    n_z : int, default=1\n        Dimension in z axis\n    mask : ndarray of shape (n_x, n_y, n_z), dtype=bool, default=None\n        An optional mask of the image, to consider only part of the\n        pixels.\n    return_as : np.ndarray or a sparse matrix class, \\\n            default=sparse.coo_matrix\n        The class to use to build the returned adjacency matrix.\n    dtype : dtype, default=int\n        The data of the returned sparse matrix. By default it is int\n\n    Notes\n    -----\n    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was\n    handled by returning a dense np.matrix instance.  Going forward, np.ndarray\n    returns an np.ndarray, as expected.\n\n    For compatibility, user code relying on this method should wrap its\n    calls in ``np.asarray`` to avoid type issues.\n    \"\"\"\n    return _to_graph(n_x, n_y, n_z, mask=mask, return_as=return_as,\n                     dtype=dtype)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/img_to_graph",
      "name": "img_to_graph",
      "qname": "sklearn.feature_extraction.image.img_to_graph",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/img_to_graph/img",
          "name": "img",
          "qname": "sklearn.feature_extraction.image.img_to_graph.img",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (height, width) or (height, width, channel)",
            "default_value": "",
            "description": "2D or 3D image."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (height, width) or (height, width, channel)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/img_to_graph/mask",
          "name": "mask",
          "qname": "sklearn.feature_extraction.image.img_to_graph.mask",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (height, width) or             (height, width, channel), dtype=bool",
            "default_value": "None",
            "description": "An optional mask of the image, to consider only part of the\npixels."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape (height, width) or (height, width, channel)"
              },
              {
                "kind": "NamedType",
                "name": "dtype=bool"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/img_to_graph/return_as",
          "name": "return_as",
          "qname": "sklearn.feature_extraction.image.img_to_graph.return_as",
          "default_value": "sparse.coo_matrix",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "np.ndarray or a sparse matrix class",
            "default_value": "sparse.coo_matrix",
            "description": "The class to use to build the returned adjacency matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "np.ndarray"
              },
              {
                "kind": "NamedType",
                "name": "a sparse matrix class"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/img_to_graph/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction.image.img_to_graph.dtype",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dtype",
            "default_value": "None",
            "description": "The data of the returned sparse matrix. By default it is the\ndtype of img"
          },
          "type": {
            "kind": "NamedType",
            "name": "dtype"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_extraction"
      ],
      "description": "Graph of the pixel-to-pixel gradient connections\n\nEdges are weighted with the gradient values.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.",
      "docstring": "Graph of the pixel-to-pixel gradient connections\n\nEdges are weighted with the gradient values.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters\n----------\nimg : ndarray of shape (height, width) or (height, width, channel)\n    2D or 3D image.\nmask : ndarray of shape (height, width) or             (height, width, channel), dtype=bool, default=None\n    An optional mask of the image, to consider only part of the\n    pixels.\nreturn_as : np.ndarray or a sparse matrix class,             default=sparse.coo_matrix\n    The class to use to build the returned adjacency matrix.\ndtype : dtype, default=None\n    The data of the returned sparse matrix. By default it is the\n    dtype of img\n\nNotes\n-----\nFor scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was\nhandled by returning a dense np.matrix instance.  Going forward, np.ndarray\nreturns an np.ndarray, as expected.\n\nFor compatibility, user code relying on this method should wrap its\ncalls in ``np.asarray`` to avoid type issues.",
      "code": "@_deprecate_positional_args\ndef img_to_graph(img, *, mask=None, return_as=sparse.coo_matrix, dtype=None):\n    \"\"\"Graph of the pixel-to-pixel gradient connections\n\n    Edges are weighted with the gradient values.\n\n    Read more in the :ref:`User Guide <image_feature_extraction>`.\n\n    Parameters\n    ----------\n    img : ndarray of shape (height, width) or (height, width, channel)\n        2D or 3D image.\n    mask : ndarray of shape (height, width) or \\\n            (height, width, channel), dtype=bool, default=None\n        An optional mask of the image, to consider only part of the\n        pixels.\n    return_as : np.ndarray or a sparse matrix class, \\\n            default=sparse.coo_matrix\n        The class to use to build the returned adjacency matrix.\n    dtype : dtype, default=None\n        The data of the returned sparse matrix. By default it is the\n        dtype of img\n\n    Notes\n    -----\n    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was\n    handled by returning a dense np.matrix instance.  Going forward, np.ndarray\n    returns an np.ndarray, as expected.\n\n    For compatibility, user code relying on this method should wrap its\n    calls in ``np.asarray`` to avoid type issues.\n    \"\"\"\n    img = np.atleast_3d(img)\n    n_x, n_y, n_z = img.shape\n    return _to_graph(n_x, n_y, n_z, mask, img, return_as, dtype)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.image/reconstruct_from_patches_2d",
      "name": "reconstruct_from_patches_2d",
      "qname": "sklearn.feature_extraction.image.reconstruct_from_patches_2d",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/reconstruct_from_patches_2d/patches",
          "name": "patches",
          "qname": "sklearn.feature_extraction.image.reconstruct_from_patches_2d.patches",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels)",
            "default_value": "",
            "description": "The complete set of patches. If the patches contain colour information,\nchannels are indexed along the last dimension: RGB patches would\nhave `n_channels=3`."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_patches, patch_height, patch_width) or (n_patches, patch_height, patch_width, n_channels)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.image/reconstruct_from_patches_2d/image_size",
          "name": "image_size",
          "qname": "sklearn.feature_extraction.image.reconstruct_from_patches_2d.image_size",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "tuple of int (image_height, image_width) or         (image_height, image_width, n_channels)",
            "default_value": "",
            "description": "The size of the image that will be reconstructed."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple of int (image_height, image_width) or (image_height, image_width, n_channels)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Reconstruct the image from all of its patches.\n\nPatches are assumed to overlap and the image is constructed by filling in\nthe patches from left to right, top to bottom, averaging the overlapping\nregions.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.",
      "docstring": "Reconstruct the image from all of its patches.\n\nPatches are assumed to overlap and the image is constructed by filling in\nthe patches from left to right, top to bottom, averaging the overlapping\nregions.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters\n----------\npatches : ndarray of shape (n_patches, patch_height, patch_width) or         (n_patches, patch_height, patch_width, n_channels)\n    The complete set of patches. If the patches contain colour information,\n    channels are indexed along the last dimension: RGB patches would\n    have `n_channels=3`.\n\nimage_size : tuple of int (image_height, image_width) or         (image_height, image_width, n_channels)\n    The size of the image that will be reconstructed.\n\nReturns\n-------\nimage : ndarray of shape image_size\n    The reconstructed image.",
      "code": "def reconstruct_from_patches_2d(patches, image_size):\n    \"\"\"Reconstruct the image from all of its patches.\n\n    Patches are assumed to overlap and the image is constructed by filling in\n    the patches from left to right, top to bottom, averaging the overlapping\n    regions.\n\n    Read more in the :ref:`User Guide <image_feature_extraction>`.\n\n    Parameters\n    ----------\n    patches : ndarray of shape (n_patches, patch_height, patch_width) or \\\n        (n_patches, patch_height, patch_width, n_channels)\n        The complete set of patches. If the patches contain colour information,\n        channels are indexed along the last dimension: RGB patches would\n        have `n_channels=3`.\n\n    image_size : tuple of int (image_height, image_width) or \\\n        (image_height, image_width, n_channels)\n        The size of the image that will be reconstructed.\n\n    Returns\n    -------\n    image : ndarray of shape image_size\n        The reconstructed image.\n    \"\"\"\n    i_h, i_w = image_size[:2]\n    p_h, p_w = patches.shape[1:3]\n    img = np.zeros(image_size)\n    # compute the dimensions of the patches array\n    n_h = i_h - p_h + 1\n    n_w = i_w - p_w + 1\n    for p, (i, j) in zip(patches, product(range(n_h), range(n_w))):\n        img[i:i + p_h, j:j + p_w] += p\n\n    for i in range(i_h):\n        for j in range(i_w):\n            # divide by the amount of overlap\n            # XXX: is this the most efficient way? memory-wise yes, cpu wise?\n            img[i, j] /= float(min(i + 1, p_h, i_h - i) *\n                               min(j + 1, p_w, i_w - j))\n    return img"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.setup/configuration",
      "name": "configuration",
      "qname": "sklearn.feature_extraction.setup.configuration",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.setup/configuration/parent_package",
          "name": "parent_package",
          "qname": "sklearn.feature_extraction.setup.configuration.parent_package",
          "default_value": "''",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.setup/configuration/top_path",
          "name": "top_path",
          "qname": "sklearn.feature_extraction.setup.configuration.top_path",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "def configuration(parent_package='', top_path=None):\n    import numpy\n    from numpy.distutils.misc_util import Configuration\n\n    config = Configuration('feature_extraction', parent_package, top_path)\n    libraries = []\n    if os.name == 'posix':\n        libraries.append('m')\n\n    if platform.python_implementation() != 'PyPy':\n        config.add_extension('_hashing_fast',\n                             sources=['_hashing_fast.pyx'],\n                             include_dirs=[numpy.get_include()],\n                             libraries=libraries)\n    config.add_subpackage(\"tests\")\n\n    return config"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/input",
          "name": "input",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.input",
          "default_value": "'content'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'filename', 'file', 'content'}",
            "default_value": "'content'",
            "description": "- If `'filename'`, the sequence passed as an argument to fit is\n  expected to be a list of filenames that need reading to fetch\n  the raw content to analyze.\n\n- If `'file'`, the sequence items must have a 'read' method (file-like\n  object) that is called to fetch the bytes in memory.\n\n- If `'content'`, the input is expected to be a sequence of items that\n  can be of type string or byte."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "filename",
              "file",
              "content"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/encoding",
          "name": "encoding",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.encoding",
          "default_value": "'utf-8'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string",
            "default_value": "'utf-8'",
            "description": "If bytes or files are given to analyze, this encoding is used to\ndecode."
          },
          "type": {
            "kind": "NamedType",
            "name": "string"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/decode_error",
          "name": "decode_error",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.decode_error",
          "default_value": "'strict'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'strict', 'ignore', 'replace'}",
            "default_value": "'strict'",
            "description": "Instruction on what to do if a byte sequence is given to analyze that\ncontains characters not of the given `encoding`. By default, it is\n'strict', meaning that a UnicodeDecodeError will be raised. Other\nvalues are 'ignore' and 'replace'."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ignore",
              "replace",
              "strict"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/strip_accents",
          "name": "strip_accents",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.strip_accents",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ascii', 'unicode'}",
            "default_value": "None",
            "description": "Remove accents and perform other character normalization\nduring the preprocessing step.\n'ascii' is a fast method that only works on characters that have\nan direct ASCII mapping.\n'unicode' is a slightly slower method that works on any characters.\nNone (default) does nothing.\n\nBoth 'ascii' and 'unicode' use NFKD normalization from\n:func:`unicodedata.normalize`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ascii",
              "unicode"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/lowercase",
          "name": "lowercase",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.lowercase",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Convert all characters to lowercase before tokenizing."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/preprocessor",
          "name": "preprocessor",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.preprocessor",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Override the preprocessing (strip_accents and lowercase) stage while\npreserving the tokenizing and n-grams generation steps.\nOnly applies if ``analyzer is not callable``."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/tokenizer",
          "name": "tokenizer",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.tokenizer",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Override the string tokenization step while preserving the\npreprocessing and n-grams generation steps.\nOnly applies if ``analyzer == 'word'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/stop_words",
          "name": "stop_words",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.stop_words",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'english'}, list",
            "default_value": "None",
            "description": "If 'english', a built-in stop word list for English is used.\nThere are several known issues with 'english' and you should\nconsider an alternative (see :ref:`stop_words`).\n\nIf a list, that list is assumed to contain stop words, all of which\nwill be removed from the resulting tokens.\nOnly applies if ``analyzer == 'word'``.\n\nIf None, no stop words will be used. max_df can be set to a value\nin the range [0.7, 1.0) to automatically detect and filter stop\nwords based on intra corpus document frequency of terms."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "english"
                ]
              },
              {
                "kind": "NamedType",
                "name": "list"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/token_pattern",
          "name": "token_pattern",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.token_pattern",
          "default_value": "'(?u)\\\\b\\\\w\\\\w+\\\\b'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"",
            "description": "Regular expression denoting what constitutes a \"token\", only used\nif ``analyzer == 'word'``. The default regexp select tokens of 2\nor more alphanumeric characters (punctuation is completely ignored\nand always treated as a token separator).\n\nIf there is a capturing group in token_pattern then the\ncaptured group content, not the entire match, becomes the token.\nAt most one capturing group is permitted."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/ngram_range",
          "name": "ngram_range",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.ngram_range",
          "default_value": "(1, 1)",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple (min_n, max_n)",
            "default_value": "(1, 1)",
            "description": "The lower and upper boundary of the range of n-values for different\nword n-grams or char n-grams to be extracted. All values of n such\nsuch that min_n <= n <= max_n will be used. For example an\n``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\nunigrams and bigrams, and ``(2, 2)`` means only bigrams.\nOnly applies if ``analyzer is not callable``."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple (min_n, max_n)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/analyzer",
          "name": "analyzer",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.analyzer",
          "default_value": "'word'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'word', 'char', 'char_wb'} or callable",
            "default_value": "'word'",
            "description": "Whether the feature should be made of word n-gram or character\nn-grams.\nOption 'char_wb' creates character n-grams only from text inside\nword boundaries; n-grams at the edges of words are padded with space.\n\nIf a callable is passed it is used to extract the sequence of features\nout of the raw, unprocessed input.\n\n.. versionchanged:: 0.21\n\nSince v0.21, if ``input`` is ``filename`` or ``file``, the data is\nfirst read from the file and then passed to the given callable\nanalyzer."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "word",
                  "char_wb",
                  "char"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/max_df",
          "name": "max_df",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.max_df",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float in range [0.0, 1.0] or int",
            "default_value": "1.0",
            "description": "When building the vocabulary ignore terms that have a document\nfrequency strictly higher than the given threshold (corpus-specific\nstop words).\nIf float, the parameter represents a proportion of documents, integer\nabsolute counts.\nThis parameter is ignored if vocabulary is not None."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float in range [0.0"
              },
              {
                "kind": "NamedType",
                "name": "1.0]"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/min_df",
          "name": "min_df",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.min_df",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float in range [0.0, 1.0] or int",
            "default_value": "1",
            "description": "When building the vocabulary ignore terms that have a document\nfrequency strictly lower than the given threshold. This value is also\ncalled cut-off in the literature.\nIf float, the parameter represents a proportion of documents, integer\nabsolute counts.\nThis parameter is ignored if vocabulary is not None."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float in range [0.0"
              },
              {
                "kind": "NamedType",
                "name": "1.0]"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.max_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "If not None, build a vocabulary that only consider the top\nmax_features ordered by term frequency across the corpus.\n\nThis parameter is ignored if vocabulary is not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/vocabulary",
          "name": "vocabulary",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.vocabulary",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "Mapping or iterable",
            "default_value": "None",
            "description": "Either a Mapping (e.g., a dict) where keys are terms and values are\nindices in the feature matrix, or an iterable over terms. If not\ngiven, a vocabulary is determined from the input documents. Indices\nin the mapping should not be repeated and should not have any gap\nbetween 0 and the largest index."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "Mapping"
              },
              {
                "kind": "NamedType",
                "name": "iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/binary",
          "name": "binary",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.binary",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, all non zero counts are set to 1. This is useful for discrete\nprobabilistic models that model binary events rather than integer\ncounts."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/__init__/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.__init__.dtype",
          "default_value": "np.int64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "type",
            "default_value": "np.int64",
            "description": "Type of the matrix returned by fit_transform() or transform()."
          },
          "type": {
            "kind": "NamedType",
            "name": "type"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Convert a collection of text documents to a matrix of token counts\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None,\n                 lowercase=True, preprocessor=None, tokenizer=None,\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), analyzer='word',\n                 max_df=1.0, min_df=1, max_features=None,\n                 vocabulary=None, binary=False, dtype=np.int64):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.max_df = max_df\n        self.min_df = min_df\n        if max_df < 0 or min_df < 0:\n            raise ValueError(\"negative value for max_df or min_df\")\n        self.max_features = max_features\n        if max_features is not None:\n            if (not isinstance(max_features, numbers.Integral) or\n                    max_features <= 0):\n                raise ValueError(\n                    \"max_features=%r, neither a positive integer nor None\"\n                    % max_features)\n        self.ngram_range = ngram_range\n        self.vocabulary = vocabulary\n        self.binary = binary\n        self.dtype = dtype"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction.text.CountVectorizer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit/raw_documents",
          "name": "raw_documents",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.fit.raw_documents",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable",
            "default_value": "",
            "description": "An iterable which yields either str, unicode or file objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn a vocabulary dictionary of all tokens in the raw documents.",
      "docstring": "Learn a vocabulary dictionary of all tokens in the raw documents.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nself",
      "code": "    def fit(self, raw_documents, y=None):\n        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self._warn_for_unused_params()\n        self.fit_transform(raw_documents)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.feature_extraction.text.CountVectorizer.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit_transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit_transform/raw_documents",
          "name": "raw_documents",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.fit_transform.raw_documents",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable",
            "default_value": "",
            "description": "An iterable which yields either str, unicode or file objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/fit_transform/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn the vocabulary dictionary and return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.",
      "docstring": "Learn the vocabulary dictionary and return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nX : array of shape (n_samples, n_features)\n    Document-term matrix.",
      "code": "    def fit_transform(self, raw_documents, y=None):\n        \"\"\"Learn the vocabulary dictionary and return document-term matrix.\n\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : array of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        # We intentionally don't call the transform method to make\n        # fit_transform overridable without unwanted side effects in\n        # TfidfVectorizer.\n        if isinstance(raw_documents, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._validate_params()\n        self._validate_vocabulary()\n        max_df = self.max_df\n        min_df = self.min_df\n        max_features = self.max_features\n\n        vocabulary, X = self._count_vocab(raw_documents,\n                                          self.fixed_vocabulary_)\n\n        if self.binary:\n            X.data.fill(1)\n\n        if not self.fixed_vocabulary_:\n            n_doc = X.shape[0]\n            max_doc_count = (max_df\n                             if isinstance(max_df, numbers.Integral)\n                             else max_df * n_doc)\n            min_doc_count = (min_df\n                             if isinstance(min_df, numbers.Integral)\n                             else min_df * n_doc)\n            if max_doc_count < min_doc_count:\n                raise ValueError(\n                    \"max_df corresponds to < documents than min_df\")\n            if max_features is not None:\n                X = self._sort_features(X, vocabulary)\n            X, self.stop_words_ = self._limit_features(X, vocabulary,\n                                                       max_doc_count,\n                                                       min_doc_count,\n                                                       max_features)\n            if max_features is None:\n                X = self._sort_features(X, vocabulary)\n            self.vocabulary_ = vocabulary\n\n        return X"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/get_feature_names",
      "name": "get_feature_names",
      "qname": "sklearn.feature_extraction.text.CountVectorizer.get_feature_names",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/get_feature_names/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.get_feature_names.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Array mapping from feature integer indices to feature name.",
      "docstring": "Array mapping from feature integer indices to feature name.\n\nReturns\n-------\nfeature_names : list\n    A list of feature names.",
      "code": "    def get_feature_names(self):\n        \"\"\"Array mapping from feature integer indices to feature name.\n\n        Returns\n        -------\n        feature_names : list\n            A list of feature names.\n        \"\"\"\n\n        self._check_vocabulary()\n\n        return [t for t, i in sorted(self.vocabulary_.items(),\n                                     key=itemgetter(1))]"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.feature_extraction.text.CountVectorizer.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/inverse_transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.inverse_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Document-term matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Return terms per document with nonzero entries in X.",
      "docstring": "Return terms per document with nonzero entries in X.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Document-term matrix.\n\nReturns\n-------\nX_inv : list of arrays of shape (n_samples,)\n    List of arrays of terms.",
      "code": "    def inverse_transform(self, X):\n        \"\"\"Return terms per document with nonzero entries in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Document-term matrix.\n\n        Returns\n        -------\n        X_inv : list of arrays of shape (n_samples,)\n            List of arrays of terms.\n        \"\"\"\n        self._check_vocabulary()\n        # We need CSR format for fast row manipulations.\n        X = check_array(X, accept_sparse='csr')\n        n_samples = X.shape[0]\n\n        terms = np.array(list(self.vocabulary_.keys()))\n        indices = np.array(list(self.vocabulary_.values()))\n        inverse_vocabulary = terms[np.argsort(indices)]\n\n        if sp.issparse(X):\n            return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\n                    for i in range(n_samples)]\n        else:\n            return [inverse_vocabulary[np.flatnonzero(X[i, :])].ravel()\n                    for i in range(n_samples)]"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction.text.CountVectorizer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/CountVectorizer/transform/raw_documents",
          "name": "raw_documents",
          "qname": "sklearn.feature_extraction.text.CountVectorizer.transform.raw_documents",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable",
            "default_value": "",
            "description": "An iterable which yields either str, unicode or file objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform documents to document-term matrix.\n\nExtract token counts out of raw text documents using the vocabulary\nfitted with fit or the one provided to the constructor.",
      "docstring": "Transform documents to document-term matrix.\n\nExtract token counts out of raw text documents using the vocabulary\nfitted with fit or the one provided to the constructor.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Document-term matrix.",
      "code": "    def transform(self, raw_documents):\n        \"\"\"Transform documents to document-term matrix.\n\n        Extract token counts out of raw text documents using the vocabulary\n        fitted with fit or the one provided to the constructor.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        if isinstance(raw_documents, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n        self._check_vocabulary()\n\n        # use the same matrix-building strategy as fit_transform\n        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n        if self.binary:\n            X.data.fill(1)\n        return X"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/input",
          "name": "input",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.input",
          "default_value": "'content'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'filename', 'file', 'content'}",
            "default_value": "'content'",
            "description": "- If `'filename'`, the sequence passed as an argument to fit is\n  expected to be a list of filenames that need reading to fetch\n  the raw content to analyze.\n\n- If `'file'`, the sequence items must have a 'read' method (file-like\n  object) that is called to fetch the bytes in memory.\n\n- If `'content'`, the input is expected to be a sequence of items that\n  can be of type string or byte."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "filename",
              "file",
              "content"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/encoding",
          "name": "encoding",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.encoding",
          "default_value": "'utf-8'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string",
            "default_value": "'utf-8'",
            "description": "If bytes or files are given to analyze, this encoding is used to\ndecode."
          },
          "type": {
            "kind": "NamedType",
            "name": "string"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/decode_error",
          "name": "decode_error",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.decode_error",
          "default_value": "'strict'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'strict', 'ignore', 'replace'}",
            "default_value": "'strict'",
            "description": "Instruction on what to do if a byte sequence is given to analyze that\ncontains characters not of the given `encoding`. By default, it is\n'strict', meaning that a UnicodeDecodeError will be raised. Other\nvalues are 'ignore' and 'replace'."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ignore",
              "replace",
              "strict"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/strip_accents",
          "name": "strip_accents",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.strip_accents",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ascii', 'unicode'}",
            "default_value": "None",
            "description": "Remove accents and perform other character normalization\nduring the preprocessing step.\n'ascii' is a fast method that only works on characters that have\nan direct ASCII mapping.\n'unicode' is a slightly slower method that works on any characters.\nNone (default) does nothing.\n\nBoth 'ascii' and 'unicode' use NFKD normalization from\n:func:`unicodedata.normalize`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ascii",
              "unicode"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/lowercase",
          "name": "lowercase",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.lowercase",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Convert all characters to lowercase before tokenizing."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/preprocessor",
          "name": "preprocessor",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.preprocessor",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Override the preprocessing (string transformation) stage while\npreserving the tokenizing and n-grams generation steps.\nOnly applies if ``analyzer is not callable``."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/tokenizer",
          "name": "tokenizer",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.tokenizer",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Override the string tokenization step while preserving the\npreprocessing and n-grams generation steps.\nOnly applies if ``analyzer == 'word'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/stop_words",
          "name": "stop_words",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.stop_words",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'english'}, list",
            "default_value": "None",
            "description": "If 'english', a built-in stop word list for English is used.\nThere are several known issues with 'english' and you should\nconsider an alternative (see :ref:`stop_words`).\n\nIf a list, that list is assumed to contain stop words, all of which\nwill be removed from the resulting tokens.\nOnly applies if ``analyzer == 'word'``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "english"
                ]
              },
              {
                "kind": "NamedType",
                "name": "list"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/token_pattern",
          "name": "token_pattern",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.token_pattern",
          "default_value": "'(?u)\\\\b\\\\w\\\\w+\\\\b'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"",
            "description": "Regular expression denoting what constitutes a \"token\", only used\nif ``analyzer == 'word'``. The default regexp selects tokens of 2\nor more alphanumeric characters (punctuation is completely ignored\nand always treated as a token separator).\n\nIf there is a capturing group in token_pattern then the\ncaptured group content, not the entire match, becomes the token.\nAt most one capturing group is permitted."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/ngram_range",
          "name": "ngram_range",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.ngram_range",
          "default_value": "(1, 1)",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple (min_n, max_n)",
            "default_value": "(1, 1)",
            "description": "The lower and upper boundary of the range of n-values for different\nn-grams to be extracted. All values of n such that min_n <= n <= max_n\nwill be used. For example an ``ngram_range`` of ``(1, 1)`` means only\nunigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\nonly bigrams.\nOnly applies if ``analyzer is not callable``."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple (min_n, max_n)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/analyzer",
          "name": "analyzer",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.analyzer",
          "default_value": "'word'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'word', 'char', 'char_wb'} or callable",
            "default_value": "'word'",
            "description": "Whether the feature should be made of word or character n-grams.\nOption 'char_wb' creates character n-grams only from text inside\nword boundaries; n-grams at the edges of words are padded with space.\n\nIf a callable is passed it is used to extract the sequence of features\nout of the raw, unprocessed input.\n\n.. versionchanged:: 0.21\n    Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n    is first read from the file and then passed to the given callable\n    analyzer."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "word",
                  "char_wb",
                  "char"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/n_features",
          "name": "n_features",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.n_features",
          "default_value": "2**20",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "(2 ** 20)",
            "description": "The number of features (columns) in the output matrices. Small numbers\nof features are likely to cause hash collisions, but large numbers\nwill cause larger coefficient dimensions in linear learners."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/binary",
          "name": "binary",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.binary",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False.",
            "description": "If True, all non zero counts are set to 1. This is useful for discrete\nprobabilistic models that model binary events rather than integer\ncounts."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/norm",
          "name": "norm",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.norm",
          "default_value": "'l2'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'l1', 'l2'}",
            "default_value": "'l2'",
            "description": "Norm used to normalize term vectors. None for no normalization."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "l1",
              "l2"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/alternate_sign",
          "name": "alternate_sign",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.alternate_sign",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "When True, an alternating sign is added to the features as to\napproximately conserve the inner product in the hashed space even for\nsmall n_features. This approach is similar to sparse random projection.\n\n.. versionadded:: 0.19"
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/__init__/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.__init__.dtype",
          "default_value": "np.float64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "type",
            "default_value": "np.float64",
            "description": "Type of the matrix returned by fit_transform() or transform()."
          },
          "type": {
            "kind": "NamedType",
            "name": "type"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Convert a collection of text documents to a matrix of token occurrences\n\nIt turns a collection of text documents into a scipy.sparse matrix holding\ntoken occurrence counts (or binary occurrence information), possibly\nnormalized as token frequencies if norm='l1' or projected on the euclidean\nunit sphere if norm='l2'.\n\nThis text vectorizer implementation uses the hashing trick to find the\ntoken string name to feature integer index mapping.\n\nThis strategy has several advantages:\n\n- it is very low memory scalable to large datasets as there is no need to\n  store a vocabulary dictionary in memory\n\n- it is fast to pickle and un-pickle as it holds no state besides the\n  constructor parameters\n\n- it can be used in a streaming (partial fit) or parallel pipeline as there\n  is no state computed during fit.\n\nThere are also a couple of cons (vs using a CountVectorizer with an\nin-memory vocabulary):\n\n- there is no way to compute the inverse transform (from feature indices to\n  string feature names) which can be a problem when trying to introspect\n  which features are most important to a model.\n\n- there can be collisions: distinct tokens can be mapped to the same\n  feature index. However in practice this is rarely an issue if n_features\n  is large enough (e.g. 2 ** 18 for text classification problems).\n\n- no IDF weighting as this would render the transformer stateful.\n\nThe hash function employed is the signed 32-bit version of Murmurhash3.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None,\n                 lowercase=True, preprocessor=None, tokenizer=None,\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), analyzer='word', n_features=(2 ** 20),\n                 binary=False, norm='l2', alternate_sign=True,\n                 dtype=np.float64):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.n_features = n_features\n        self.ngram_range = ngram_range\n        self.binary = binary\n        self.norm = norm\n        self.alternate_sign = alternate_sign\n        self.dtype = dtype"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape [n_samples, n_features]",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features]"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Does nothing: this transformer is stateless.",
      "docstring": "Does nothing: this transformer is stateless.\n\nParameters\n----------\nX : ndarray of shape [n_samples, n_features]\n    Training data.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Does nothing: this transformer is stateless.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n        \"\"\"\n        # triggers a parameter validation\n        if isinstance(X, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._warn_for_unused_params()\n        self._validate_params()\n\n        self._get_hasher().fit(X, y=y)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit_transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit_transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable over raw text documents, length = n_samples",
            "default_value": "",
            "description": "Samples. Each sample must be a text document (either bytes or\nunicode strings, file name or file object depending on the\nconstructor argument) which will be tokenized and hashed."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "iterable over raw text documents"
              },
              {
                "kind": "NamedType",
                "name": "length = n_samples"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/fit_transform/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "any",
            "default_value": "",
            "description": "Ignored. This parameter exists only for compatibility with\nsklearn.pipeline.Pipeline."
          },
          "type": {
            "kind": "NamedType",
            "name": "any"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform a sequence of documents to a document-term matrix.",
      "docstring": "Transform a sequence of documents to a document-term matrix.\n\nParameters\n----------\nX : iterable over raw text documents, length = n_samples\n    Samples. Each sample must be a text document (either bytes or\n    unicode strings, file name or file object depending on the\n    constructor argument) which will be tokenized and hashed.\ny : any\n    Ignored. This parameter exists only for compatibility with\n    sklearn.pipeline.Pipeline.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Document-term matrix.",
      "code": "    def fit_transform(self, X, y=None):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n        y : any\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        return self.fit(X, y).transform(X)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.feature_extraction.text.HashingVectorizer.partial_fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/partial_fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/partial_fit/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape [n_samples, n_features]",
            "default_value": "",
            "description": "Training data."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "ndarray of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features]"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/partial_fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Does nothing: this transformer is stateless.\n\nThis method is just there to mark the fact that this transformer\ncan work in a streaming setup.",
      "docstring": "Does nothing: this transformer is stateless.\n\nThis method is just there to mark the fact that this transformer\ncan work in a streaming setup.\n\nParameters\n----------\nX : ndarray of shape [n_samples, n_features]\n    Training data.",
      "code": "    def partial_fit(self, X, y=None):\n        \"\"\"Does nothing: this transformer is stateless.\n\n        This method is just there to mark the fact that this transformer\n        can work in a streaming setup.\n\n        Parameters\n        ----------\n        X : ndarray of shape [n_samples, n_features]\n            Training data.\n        \"\"\"\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction.text.HashingVectorizer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/HashingVectorizer/transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.HashingVectorizer.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable over raw text documents, length = n_samples",
            "default_value": "",
            "description": "Samples. Each sample must be a text document (either bytes or\nunicode strings, file name or file object depending on the\nconstructor argument) which will be tokenized and hashed."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "iterable over raw text documents"
              },
              {
                "kind": "NamedType",
                "name": "length = n_samples"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform a sequence of documents to a document-term matrix.",
      "docstring": "Transform a sequence of documents to a document-term matrix.\n\nParameters\n----------\nX : iterable over raw text documents, length = n_samples\n    Samples. Each sample must be a text document (either bytes or\n    unicode strings, file name or file object depending on the\n    constructor argument) which will be tokenized and hashed.\n\nReturns\n-------\nX : sparse matrix of shape (n_samples, n_features)\n    Document-term matrix.",
      "code": "    def transform(self, X):\n        \"\"\"Transform a sequence of documents to a document-term matrix.\n\n        Parameters\n        ----------\n        X : iterable over raw text documents, length = n_samples\n            Samples. Each sample must be a text document (either bytes or\n            unicode strings, file name or file object depending on the\n            constructor argument) which will be tokenized and hashed.\n\n        Returns\n        -------\n        X : sparse matrix of shape (n_samples, n_features)\n            Document-term matrix.\n        \"\"\"\n        if isinstance(X, str):\n            raise ValueError(\n                \"Iterable over raw text documents expected, \"\n                \"string object received.\")\n\n        self._validate_params()\n\n        analyzer = self.build_analyzer()\n        X = self._get_hasher().transform(analyzer(doc) for doc in X)\n        if self.binary:\n            X.data.fill(1)\n        if self.norm is not None:\n            X = normalize(X, norm=self.norm, copy=False)\n        return X"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction.text.TfidfTransformer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__/norm",
          "name": "norm",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.__init__.norm",
          "default_value": "'l2'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'l1', 'l2'}",
            "default_value": "'l2'",
            "description": "Each output row will have unit norm, either:\n* 'l2': Sum of squares of vector elements is 1. The cosine\nsimilarity between two vectors is their dot product when l2 norm has\nbeen applied.\n* 'l1': Sum of absolute values of vector elements is 1.\nSee :func:`preprocessing.normalize`"
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "l1",
              "l2"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__/use_idf",
          "name": "use_idf",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.__init__.use_idf",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Enable inverse-document-frequency reweighting."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__/smooth_idf",
          "name": "smooth_idf",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.__init__.smooth_idf",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Smooth idf weights by adding one to document frequencies, as if an\nextra document was seen containing every term in the collection\nexactly once. Prevents zero divisions."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/__init__/sublinear_tf",
          "name": "sublinear_tf",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.__init__.sublinear_tf",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform a count matrix to a normalized tf or tf-idf representation\n\nTf means term-frequency while tf-idf means term-frequency times inverse\ndocument-frequency. This is a common term weighting scheme in information\nretrieval, that has also found good use in document classification.\n\nThe goal of using tf-idf instead of the raw frequencies of occurrence of a\ntoken in a given document is to scale down the impact of tokens that occur\nvery frequently in a given corpus and that are hence empirically less\ninformative than features that occur in a small fraction of the training\ncorpus.\n\nThe formula that is used to compute the tf-idf for a term t of a document d\nin a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\ncomputed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\nn is the total number of documents in the document set and df(t) is the\ndocument frequency of t; the document frequency is the number of documents\nin the document set that contain the term t. The effect of adding \"1\" to\nthe idf in the equation above is that terms with zero idf, i.e., terms\nthat occur in all documents in a training set, will not be entirely\nignored.\n(Note that the idf formula above differs from the standard textbook\nnotation that defines the idf as\nidf(t) = log [ n / (df(t) + 1) ]).\n\nIf ``smooth_idf=True`` (the default), the constant \"1\" is added to the\nnumerator and denominator of the idf as if an extra document was seen\ncontaining every term in the collection exactly once, which prevents\nzero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n\nFurthermore, the formulas used to compute tf and idf depend\non parameter settings that correspond to the SMART notation used in IR\nas follows:\n\nTf is \"n\" (natural) by default, \"l\" (logarithmic) when\n``sublinear_tf=True``.\nIdf is \"t\" when use_idf is given, \"n\" (none) otherwise.\nNormalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\nwhen ``norm=None``.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, norm='l2', use_idf=True, smooth_idf=True,\n                 sublinear_tf=False):\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction.text.TfidfTransformer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/fit/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "sparse matrix of shape n_samples, n_features)",
            "default_value": "",
            "description": "A matrix of term/token counts."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "sparse matrix of shape n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn the idf vector (global term weights).",
      "docstring": "Learn the idf vector (global term weights).\n\nParameters\n----------\nX : sparse matrix of shape n_samples, n_features)\n    A matrix of term/token counts.",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Learn the idf vector (global term weights).\n\n        Parameters\n        ----------\n        X : sparse matrix of shape n_samples, n_features)\n            A matrix of term/token counts.\n        \"\"\"\n        X = check_array(X, accept_sparse=('csr', 'csc'))\n        if not sp.issparse(X):\n            X = sp.csr_matrix(X)\n        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n\n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            df = df.astype(dtype, **_astype_copy_false(df))\n\n            # perform idf smoothing if required\n            df += int(self.smooth_idf)\n            n_samples += int(self.smooth_idf)\n\n            # log+1 instead of log makes sure terms with zero idf don't get\n            # suppressed entirely.\n            idf = np.log(n_samples / df) + 1\n            self._idf_diag = sp.diags(idf, offsets=0,\n                                      shape=(n_features, n_features),\n                                      format='csr',\n                                      dtype=dtype)\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@getter",
      "name": "idf_",
      "qname": "sklearn.feature_extraction.text.TfidfTransformer.idf_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@getter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.idf_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def idf_(self):\n        # if _idf_diag is not set, this will raise an attribute error,\n        # which means hasattr(self, \"idf_\") is False\n        return np.ravel(self._idf_diag.sum(axis=0))"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@setter",
      "name": "idf_",
      "qname": "sklearn.feature_extraction.text.TfidfTransformer.idf_",
      "decorators": [
        "idf_.setter"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@setter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.idf_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/idf_@setter/value",
          "name": "value",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.idf_.value",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @idf_.setter\n    def idf_(self, value):\n        value = np.asarray(value, dtype=np.float64)\n        n_features = value.shape[0]\n        self._idf_diag = sp.spdiags(value, diags=0, m=n_features,\n                                    n=n_features, format='csr')"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction.text.TfidfTransformer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/transform/X",
          "name": "X",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "sparse matrix of (n_samples, n_features)",
            "default_value": "",
            "description": "a matrix of term/token counts"
          },
          "type": {
            "kind": "NamedType",
            "name": "sparse matrix of (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfTransformer/transform/copy",
          "name": "copy",
          "qname": "sklearn.feature_extraction.text.TfidfTransformer.transform.copy",
          "default_value": "True",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to copy X and operate on the copy or perform in-place\noperations."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform a count matrix to a tf or tf-idf representation",
      "docstring": "Transform a count matrix to a tf or tf-idf representation\n\nParameters\n----------\nX : sparse matrix of (n_samples, n_features)\n    a matrix of term/token counts\n\ncopy : bool, default=True\n    Whether to copy X and operate on the copy or perform in-place\n    operations.\n\nReturns\n-------\nvectors : sparse matrix of shape (n_samples, n_features)",
      "code": "    def transform(self, X, copy=True):\n        \"\"\"Transform a count matrix to a tf or tf-idf representation\n\n        Parameters\n        ----------\n        X : sparse matrix of (n_samples, n_features)\n            a matrix of term/token counts\n\n        copy : bool, default=True\n            Whether to copy X and operate on the copy or perform in-place\n            operations.\n\n        Returns\n        -------\n        vectors : sparse matrix of shape (n_samples, n_features)\n        \"\"\"\n        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n        if not sp.issparse(X):\n            X = sp.csr_matrix(X, dtype=np.float64)\n\n        n_samples, n_features = X.shape\n\n        if self.sublinear_tf:\n            np.log(X.data, X.data)\n            X.data += 1\n\n        if self.use_idf:\n            # idf_ being a property, the automatic attributes detection\n            # does not work as usual and we need to specify the attribute\n            # name:\n            check_is_fitted(self, attributes=[\"idf_\"],\n                            msg='idf vector is not fitted')\n\n            expected_n_features = self._idf_diag.shape[0]\n            if n_features != expected_n_features:\n                raise ValueError(\"Input has n_features=%d while the model\"\n                                 \" has been trained with n_features=%d\" % (\n                                     n_features, expected_n_features))\n            # *= doesn't work\n            X = X * self._idf_diag\n\n        if self.norm:\n            X = normalize(X, norm=self.norm, copy=False)\n\n        return X"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/input",
          "name": "input",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.input",
          "default_value": "'content'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'filename', 'file', 'content'}",
            "default_value": "'content'",
            "description": "- If `'filename'`, the sequence passed as an argument to fit is\n  expected to be a list of filenames that need reading to fetch\n  the raw content to analyze.\n\n- If `'file'`, the sequence items must have a 'read' method (file-like\n  object) that is called to fetch the bytes in memory.\n\n- If `'content'`, the input is expected to be a sequence of items that\n  can be of type string or byte."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "filename",
              "file",
              "content"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/encoding",
          "name": "encoding",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.encoding",
          "default_value": "'utf-8'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "'utf-8'",
            "description": "If bytes or files are given to analyze, this encoding is used to\ndecode."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/decode_error",
          "name": "decode_error",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.decode_error",
          "default_value": "'strict'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'strict', 'ignore', 'replace'}",
            "default_value": "'strict'",
            "description": "Instruction on what to do if a byte sequence is given to analyze that\ncontains characters not of the given `encoding`. By default, it is\n'strict', meaning that a UnicodeDecodeError will be raised. Other\nvalues are 'ignore' and 'replace'."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ignore",
              "replace",
              "strict"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/strip_accents",
          "name": "strip_accents",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.strip_accents",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'ascii', 'unicode'}",
            "default_value": "None",
            "description": "Remove accents and perform other character normalization\nduring the preprocessing step.\n'ascii' is a fast method that only works on characters that have\nan direct ASCII mapping.\n'unicode' is a slightly slower method that works on any characters.\nNone (default) does nothing.\n\nBoth 'ascii' and 'unicode' use NFKD normalization from\n:func:`unicodedata.normalize`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "ascii",
              "unicode"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/lowercase",
          "name": "lowercase",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.lowercase",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Convert all characters to lowercase before tokenizing."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/preprocessor",
          "name": "preprocessor",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.preprocessor",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Override the preprocessing (string transformation) stage while\npreserving the tokenizing and n-grams generation steps.\nOnly applies if ``analyzer is not callable``."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/tokenizer",
          "name": "tokenizer",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.tokenizer",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "None",
            "description": "Override the string tokenization step while preserving the\npreprocessing and n-grams generation steps.\nOnly applies if ``analyzer == 'word'``."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/analyzer",
          "name": "analyzer",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.analyzer",
          "default_value": "'word'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'word', 'char', 'char_wb'} or callable",
            "default_value": "'word'",
            "description": "Whether the feature should be made of word or character n-grams.\nOption 'char_wb' creates character n-grams only from text inside\nword boundaries; n-grams at the edges of words are padded with space.\n\nIf a callable is passed it is used to extract the sequence of features\nout of the raw, unprocessed input.\n\n.. versionchanged:: 0.21\n    Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n    is first read from the file and then passed to the given callable\n    analyzer."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "word",
                  "char_wb",
                  "char"
                ]
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/stop_words",
          "name": "stop_words",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.stop_words",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'english'}, list",
            "default_value": "None",
            "description": "If a string, it is passed to _check_stop_list and the appropriate stop\nlist is returned. 'english' is currently the only supported string\nvalue.\nThere are several known issues with 'english' and you should\nconsider an alternative (see :ref:`stop_words`).\n\nIf a list, that list is assumed to contain stop words, all of which\nwill be removed from the resulting tokens.\nOnly applies if ``analyzer == 'word'``.\n\nIf None, no stop words will be used. max_df can be set to a value\nin the range [0.7, 1.0) to automatically detect and filter stop\nwords based on intra corpus document frequency of terms."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": [
                  "english"
                ]
              },
              {
                "kind": "NamedType",
                "name": "list"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/token_pattern",
          "name": "token_pattern",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.token_pattern",
          "default_value": "'(?u)\\\\b\\\\w\\\\w+\\\\b'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str",
            "default_value": "r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"",
            "description": "Regular expression denoting what constitutes a \"token\", only used\nif ``analyzer == 'word'``. The default regexp selects tokens of 2\nor more alphanumeric characters (punctuation is completely ignored\nand always treated as a token separator).\n\nIf there is a capturing group in token_pattern then the\ncaptured group content, not the entire match, becomes the token.\nAt most one capturing group is permitted."
          },
          "type": {
            "kind": "NamedType",
            "name": "str"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/ngram_range",
          "name": "ngram_range",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.ngram_range",
          "default_value": "(1, 1)",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "tuple (min_n, max_n)",
            "default_value": "(1, 1)",
            "description": "The lower and upper boundary of the range of n-values for different\nn-grams to be extracted. All values of n such that min_n <= n <= max_n\nwill be used. For example an ``ngram_range`` of ``(1, 1)`` means only\nunigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\nonly bigrams.\nOnly applies if ``analyzer is not callable``."
          },
          "type": {
            "kind": "NamedType",
            "name": "tuple (min_n, max_n)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/max_df",
          "name": "max_df",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.max_df",
          "default_value": "1.0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or int",
            "default_value": "1.0",
            "description": "When building the vocabulary ignore terms that have a document\nfrequency strictly higher than the given threshold (corpus-specific\nstop words).\nIf float in range [0.0, 1.0], the parameter represents a proportion of\ndocuments, integer absolute counts.\nThis parameter is ignored if vocabulary is not None."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": true,
                "max_inclusive": true
              },
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/min_df",
          "name": "min_df",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.min_df",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or int",
            "default_value": "1",
            "description": "When building the vocabulary ignore terms that have a document\nfrequency strictly lower than the given threshold. This value is also\ncalled cut-off in the literature.\nIf float in range of [0.0, 1.0], the parameter represents a proportion\nof documents, integer absolute counts.\nThis parameter is ignored if vocabulary is not None."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "BoundaryType",
                "base_type": "float",
                "min": 0.0,
                "max": 1.0,
                "min_inclusive": true,
                "max_inclusive": true
              },
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "int"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.max_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "If not None, build a vocabulary that only consider the top\nmax_features ordered by term frequency across the corpus.\n\nThis parameter is ignored if vocabulary is not None."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/vocabulary",
          "name": "vocabulary",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.vocabulary",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "Mapping or iterable",
            "default_value": "None",
            "description": "Either a Mapping (e.g., a dict) where keys are terms and values are\nindices in the feature matrix, or an iterable over terms. If not\ngiven, a vocabulary is determined from the input documents."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "Mapping"
              },
              {
                "kind": "NamedType",
                "name": "iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/binary",
          "name": "binary",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.binary",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, all non-zero term counts are set to 1. This does not mean\noutputs will have only 0/1 values, only that the tf term in tf-idf\nis binary. (Set idf and normalization to False to get 0/1 outputs)."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/dtype",
          "name": "dtype",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.dtype",
          "default_value": "np.float64",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "dtype",
            "default_value": "float64",
            "description": "Type of the matrix returned by fit_transform() or transform()."
          },
          "type": {
            "kind": "NamedType",
            "name": "dtype"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/norm",
          "name": "norm",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.norm",
          "default_value": "'l2'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'l1', 'l2'}",
            "default_value": "'l2'",
            "description": "Each output row will have unit norm, either:\n* 'l2': Sum of squares of vector elements is 1. The cosine\nsimilarity between two vectors is their dot product when l2 norm has\nbeen applied.\n* 'l1': Sum of absolute values of vector elements is 1.\nSee :func:`preprocessing.normalize`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "l1",
              "l2"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/use_idf",
          "name": "use_idf",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.use_idf",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Enable inverse-document-frequency reweighting."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/smooth_idf",
          "name": "smooth_idf",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.smooth_idf",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Smooth idf weights by adding one to document frequencies, as if an\nextra document was seen containing every term in the collection\nexactly once. Prevents zero divisions."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/__init__/sublinear_tf",
          "name": "sublinear_tf",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.__init__.sublinear_tf",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to :class:`CountVectorizer` followed by\n:class:`TfidfTransformer`.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, *, input='content', encoding='utf-8',\n                 decode_error='strict', strip_accents=None, lowercase=True,\n                 preprocessor=None, tokenizer=None, analyzer='word',\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n                 max_features=None, vocabulary=None, binary=False,\n                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n                 sublinear_tf=False):\n\n        super().__init__(\n            input=input, encoding=encoding, decode_error=decode_error,\n            strip_accents=strip_accents, lowercase=lowercase,\n            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n            stop_words=stop_words, token_pattern=token_pattern,\n            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n            max_features=max_features, vocabulary=vocabulary, binary=binary,\n            dtype=dtype)\n\n        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,\n                                       smooth_idf=smooth_idf,\n                                       sublinear_tf=sublinear_tf)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit",
      "name": "fit",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit/raw_documents",
          "name": "raw_documents",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit.raw_documents",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable",
            "default_value": "",
            "description": "An iterable which yields either str, unicode or file objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "None",
            "default_value": "",
            "description": "This parameter is not needed to compute tfidf."
          },
          "type": {
            "kind": "NamedType",
            "name": "None"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn vocabulary and idf from training set.",
      "docstring": "Learn vocabulary and idf from training set.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\ny : None\n    This parameter is not needed to compute tfidf.\n\nReturns\n-------\nself : object\n    Fitted vectorizer.",
      "code": "    def fit(self, raw_documents, y=None):\n        \"\"\"Learn vocabulary and idf from training set.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n        y : None\n            This parameter is not needed to compute tfidf.\n\n        Returns\n        -------\n        self : object\n            Fitted vectorizer.\n        \"\"\"\n        self._check_params()\n        self._warn_for_unused_params()\n        X = super().fit_transform(raw_documents)\n        self._tfidf.fit(X)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit_transform",
      "name": "fit_transform",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit_transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit_transform/raw_documents",
          "name": "raw_documents",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform.raw_documents",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable",
            "default_value": "",
            "description": "An iterable which yields either str, unicode or file objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/fit_transform/y",
          "name": "y",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "None",
            "default_value": "",
            "description": "This parameter is ignored."
          },
          "type": {
            "kind": "NamedType",
            "name": "None"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn vocabulary and idf, return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.",
      "docstring": "Learn vocabulary and idf, return document-term matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\ny : None\n    This parameter is ignored.\n\nReturns\n-------\nX : sparse matrix of (n_samples, n_features)\n    Tf-idf-weighted document-term matrix.",
      "code": "    def fit_transform(self, raw_documents, y=None):\n        \"\"\"Learn vocabulary and idf, return document-term matrix.\n\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n        y : None\n            This parameter is ignored.\n\n        Returns\n        -------\n        X : sparse matrix of (n_samples, n_features)\n            Tf-idf-weighted document-term matrix.\n        \"\"\"\n        self._check_params()\n        X = super().fit_transform(raw_documents)\n        self._tfidf.fit(X)\n        # X is already a transformed view of raw_documents so\n        # we set copy to False\n        return self._tfidf.transform(X, copy=False)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@getter",
      "name": "idf_",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.idf_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@getter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.idf_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def idf_(self):\n        return self._tfidf.idf_"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@setter",
      "name": "idf_",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.idf_",
      "decorators": [
        "idf_.setter"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@setter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.idf_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/idf_@setter/value",
          "name": "value",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.idf_.value",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @idf_.setter\n    def idf_(self, value):\n        self._validate_vocabulary()\n        if hasattr(self, 'vocabulary_'):\n            if len(self.vocabulary_) != len(value):\n                raise ValueError(\"idf length = %d must be equal \"\n                                 \"to vocabulary size = %d\" %\n                                 (len(value), len(self.vocabulary)))\n        self._tfidf.idf_ = value"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@getter",
      "name": "norm",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.norm",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@getter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.norm.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def norm(self):\n        return self._tfidf.norm"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@setter",
      "name": "norm",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.norm",
      "decorators": [
        "norm.setter"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@setter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.norm.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/norm@setter/value",
          "name": "value",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.norm.value",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @norm.setter\n    def norm(self, value):\n        self._tfidf.norm = value"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@getter",
      "name": "smooth_idf",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@getter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def smooth_idf(self):\n        return self._tfidf.smooth_idf"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@setter",
      "name": "smooth_idf",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf",
      "decorators": [
        "smooth_idf.setter"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@setter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/smooth_idf@setter/value",
          "name": "value",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf.value",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @smooth_idf.setter\n    def smooth_idf(self, value):\n        self._tfidf.smooth_idf = value"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@getter",
      "name": "sublinear_tf",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@getter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def sublinear_tf(self):\n        return self._tfidf.sublinear_tf"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@setter",
      "name": "sublinear_tf",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf",
      "decorators": [
        "sublinear_tf.setter"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@setter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/sublinear_tf@setter/value",
          "name": "value",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf.value",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @sublinear_tf.setter\n    def sublinear_tf(self, value):\n        self._tfidf.sublinear_tf = value"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/transform",
      "name": "transform",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/transform/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/transform/raw_documents",
          "name": "raw_documents",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.transform.raw_documents",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "iterable",
            "default_value": "",
            "description": "An iterable which yields either str, unicode or file objects."
          },
          "type": {
            "kind": "NamedType",
            "name": "iterable"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform documents to document-term matrix.\n\nUses the vocabulary and document frequencies (df) learned by fit (or\nfit_transform).",
      "docstring": "Transform documents to document-term matrix.\n\nUses the vocabulary and document frequencies (df) learned by fit (or\nfit_transform).\n\nParameters\n----------\nraw_documents : iterable\n    An iterable which yields either str, unicode or file objects.\n\nReturns\n-------\nX : sparse matrix of (n_samples, n_features)\n    Tf-idf-weighted document-term matrix.",
      "code": "    def transform(self, raw_documents):\n        \"\"\"Transform documents to document-term matrix.\n\n        Uses the vocabulary and document frequencies (df) learned by fit (or\n        fit_transform).\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix of (n_samples, n_features)\n            Tf-idf-weighted document-term matrix.\n        \"\"\"\n        check_is_fitted(self, msg='The TF-IDF vectorizer is not fitted')\n\n        X = super().transform(raw_documents)\n        return self._tfidf.transform(X, copy=False)"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@getter",
      "name": "use_idf",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.use_idf",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@getter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.use_idf.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def use_idf(self):\n        return self._tfidf.use_idf"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@setter",
      "name": "use_idf",
      "qname": "sklearn.feature_extraction.text.TfidfVectorizer.use_idf",
      "decorators": [
        "use_idf.setter"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@setter/self",
          "name": "self",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.use_idf.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/TfidfVectorizer/use_idf@setter/value",
          "name": "value",
          "qname": "sklearn.feature_extraction.text.TfidfVectorizer.use_idf.value",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @use_idf.setter\n    def use_idf(self, value):\n        self._tfidf.use_idf = value"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/strip_accents_ascii",
      "name": "strip_accents_ascii",
      "qname": "sklearn.feature_extraction.text.strip_accents_ascii",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/strip_accents_ascii/s",
          "name": "s",
          "qname": "sklearn.feature_extraction.text.strip_accents_ascii.s",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "string",
            "default_value": "",
            "description": "The string to strip"
          },
          "type": {
            "kind": "NamedType",
            "name": "string"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform accentuated unicode symbols into ascii or nothing\n\nWarning: this solution is only suited for languages that have a direct\ntransliteration to ASCII symbols.",
      "docstring": "Transform accentuated unicode symbols into ascii or nothing\n\nWarning: this solution is only suited for languages that have a direct\ntransliteration to ASCII symbols.\n\nParameters\n----------\ns : string\n    The string to strip\n\nSee Also\n--------\nstrip_accents_unicode : Remove accentuated char for any unicode symbol.",
      "code": "def strip_accents_ascii(s):\n    \"\"\"Transform accentuated unicode symbols into ascii or nothing\n\n    Warning: this solution is only suited for languages that have a direct\n    transliteration to ASCII symbols.\n\n    Parameters\n    ----------\n    s : string\n        The string to strip\n\n    See Also\n    --------\n    strip_accents_unicode : Remove accentuated char for any unicode symbol.\n    \"\"\"\n    nkfd_form = unicodedata.normalize('NFKD', s)\n    return nkfd_form.encode('ASCII', 'ignore').decode('ASCII')"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/strip_accents_unicode",
      "name": "strip_accents_unicode",
      "qname": "sklearn.feature_extraction.text.strip_accents_unicode",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/strip_accents_unicode/s",
          "name": "s",
          "qname": "sklearn.feature_extraction.text.strip_accents_unicode.s",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "string",
            "default_value": "",
            "description": "The string to strip"
          },
          "type": {
            "kind": "NamedType",
            "name": "string"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transform accentuated unicode symbols into their simple counterpart\n\nWarning: the python-level loop and join operations make this\nimplementation 20 times slower than the strip_accents_ascii basic\nnormalization.",
      "docstring": "Transform accentuated unicode symbols into their simple counterpart\n\nWarning: the python-level loop and join operations make this\nimplementation 20 times slower than the strip_accents_ascii basic\nnormalization.\n\nParameters\n----------\ns : string\n    The string to strip\n\nSee Also\n--------\nstrip_accents_ascii : Remove accentuated char for any unicode symbol that\n    has a direct ASCII equivalent.",
      "code": "def strip_accents_unicode(s):\n    \"\"\"Transform accentuated unicode symbols into their simple counterpart\n\n    Warning: the python-level loop and join operations make this\n    implementation 20 times slower than the strip_accents_ascii basic\n    normalization.\n\n    Parameters\n    ----------\n    s : string\n        The string to strip\n\n    See Also\n    --------\n    strip_accents_ascii : Remove accentuated char for any unicode symbol that\n        has a direct ASCII equivalent.\n    \"\"\"\n    try:\n        # If `s` is ASCII-compatible, then it does not contain any accented\n        # characters and we can avoid an expensive list comprehension\n        s.encode(\"ASCII\", errors=\"strict\")\n        return s\n    except UnicodeEncodeError:\n        normalized = unicodedata.normalize('NFKD', s)\n        return ''.join([c for c in normalized if not unicodedata.combining(c)])"
    },
    {
      "id": "scikit-learn/sklearn.feature_extraction.text/strip_tags",
      "name": "strip_tags",
      "qname": "sklearn.feature_extraction.text.strip_tags",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_extraction.text/strip_tags/s",
          "name": "s",
          "qname": "sklearn.feature_extraction.text.strip_tags.s",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "string",
            "default_value": "",
            "description": "The string to strip"
          },
          "type": {
            "kind": "NamedType",
            "name": "string"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Basic regexp based HTML / XML tag stripper function\n\nFor serious HTML/XML preprocessing you should rather use an external\nlibrary such as lxml or BeautifulSoup.",
      "docstring": "Basic regexp based HTML / XML tag stripper function\n\nFor serious HTML/XML preprocessing you should rather use an external\nlibrary such as lxml or BeautifulSoup.\n\nParameters\n----------\ns : string\n    The string to strip",
      "code": "def strip_tags(s):\n    \"\"\"Basic regexp based HTML / XML tag stripper function\n\n    For serious HTML/XML preprocessing you should rather use an external\n    library such as lxml or BeautifulSoup.\n\n    Parameters\n    ----------\n    s : string\n        The string to strip\n    \"\"\"\n    return re.compile(r\"<([^>]+)>\", flags=re.UNICODE).sub(\" \", s)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/get_support",
      "name": "get_support",
      "qname": "sklearn.feature_selection._base.SelectorMixin.get_support",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/get_support/self",
          "name": "self",
          "qname": "sklearn.feature_selection._base.SelectorMixin.get_support.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/get_support/indices",
          "name": "indices",
          "qname": "sklearn.feature_selection._base.SelectorMixin.get_support.indices",
          "default_value": "False",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "If True, the return value will be an array of integers, rather\nthan a boolean mask."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Get a mask, or integer index, of the features selected",
      "docstring": "Get a mask, or integer index, of the features selected\n\nParameters\n----------\nindices : bool, default=False\n    If True, the return value will be an array of integers, rather\n    than a boolean mask.\n\nReturns\n-------\nsupport : array\n    An index that selects the retained features from a feature vector.\n    If `indices` is False, this is a boolean array of shape\n    [# input features], in which an element is True iff its\n    corresponding feature is selected for retention. If `indices` is\n    True, this is an integer array of shape [# output features] whose\n    values are indices into the input feature vector.",
      "code": "    def get_support(self, indices=False):\n        \"\"\"\n        Get a mask, or integer index, of the features selected\n\n        Parameters\n        ----------\n        indices : bool, default=False\n            If True, the return value will be an array of integers, rather\n            than a boolean mask.\n\n        Returns\n        -------\n        support : array\n            An index that selects the retained features from a feature vector.\n            If `indices` is False, this is a boolean array of shape\n            [# input features], in which an element is True iff its\n            corresponding feature is selected for retention. If `indices` is\n            True, this is an integer array of shape [# output features] whose\n            values are indices into the input feature vector.\n        \"\"\"\n        mask = self._get_support_mask()\n        return mask if not indices else np.where(mask)[0]"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/inverse_transform",
      "name": "inverse_transform",
      "qname": "sklearn.feature_selection._base.SelectorMixin.inverse_transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/inverse_transform/self",
          "name": "self",
          "qname": "sklearn.feature_selection._base.SelectorMixin.inverse_transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/inverse_transform/X",
          "name": "X",
          "qname": "sklearn.feature_selection._base.SelectorMixin.inverse_transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape [n_samples, n_selected_features]",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_selected_features]"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Reverse the transformation operation",
      "docstring": "Reverse the transformation operation\n\nParameters\n----------\nX : array of shape [n_samples, n_selected_features]\n    The input samples.\n\nReturns\n-------\nX_r : array of shape [n_samples, n_original_features]\n    `X` with columns of zeros inserted where features would have\n    been removed by :meth:`transform`.",
      "code": "    def inverse_transform(self, X):\n        \"\"\"\n        Reverse the transformation operation\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_selected_features]\n            The input samples.\n\n        Returns\n        -------\n        X_r : array of shape [n_samples, n_original_features]\n            `X` with columns of zeros inserted where features would have\n            been removed by :meth:`transform`.\n        \"\"\"\n        if issparse(X):\n            X = X.tocsc()\n            # insert additional entries in indptr:\n            # e.g. if transform changed indptr from [0 2 6 7] to [0 2 3]\n            # col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3]\n            it = self.inverse_transform(np.diff(X.indptr).reshape(1, -1))\n            col_nonzeros = it.ravel()\n            indptr = np.concatenate([[0], np.cumsum(col_nonzeros)])\n            Xt = csc_matrix((X.data, X.indices, indptr),\n                            shape=(X.shape[0], len(indptr) - 1), dtype=X.dtype)\n            return Xt\n\n        support = self.get_support()\n        X = check_array(X, dtype=None)\n        if support.sum() != X.shape[1]:\n            raise ValueError(\"X has a different shape than during fitting.\")\n\n        if X.ndim == 1:\n            X = X[None, :]\n        Xt = np.zeros((X.shape[0], support.size), dtype=X.dtype)\n        Xt[:, support] = X\n        return Xt"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/transform",
      "name": "transform",
      "qname": "sklearn.feature_selection._base.SelectorMixin.transform",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/transform/self",
          "name": "self",
          "qname": "sklearn.feature_selection._base.SelectorMixin.transform.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._base/SelectorMixin/transform/X",
          "name": "X",
          "qname": "sklearn.feature_selection._base.SelectorMixin.transform.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape [n_samples, n_features]",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features]"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Reduce X to the selected features.",
      "docstring": "Reduce X to the selected features.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\nReturns\n-------\nX_r : array of shape [n_samples, n_selected_features]\n    The input samples with only the selected features.",
      "code": "    def transform(self, X):\n        \"\"\"Reduce X to the selected features.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        X_r : array of shape [n_samples, n_selected_features]\n            The input samples with only the selected features.\n        \"\"\"\n        # note: we use _safe_tags instead of _get_tags because this is a\n        # public Mixin.\n        X = check_array(\n            X,\n            dtype=None,\n            accept_sparse=\"csr\",\n            force_all_finite=not _safe_tags(self, key=\"allow_nan\"),\n        )\n        mask = self.get_support()\n        if not mask.any():\n            warn(\"No features were selected: either the data is\"\n                 \" too noisy or the selection test too strict.\",\n                 UserWarning)\n            return np.empty(0).reshape((X.shape[0], 0))\n        if len(mask) != X.shape[1]:\n            raise ValueError(\"X has a different shape than during fitting.\")\n        return X[:, safe_mask(X, mask)]"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/estimator",
          "name": "estimator",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.estimator",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "object",
            "default_value": "",
            "description": "The base estimator from which the transformer is built.\nThis can be both a fitted (if ``prefit`` is set to True)\nor a non-fitted estimator. The estimator should have a\n``feature_importances_`` or ``coef_`` attribute after fitting.\nOtherwise, the ``importance_getter`` parameter should be used."
          },
          "type": {
            "kind": "NamedType",
            "name": "object"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/threshold",
          "name": "threshold",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.threshold",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string or float",
            "default_value": "None",
            "description": "The threshold value to use for feature selection. Features whose\nimportance is greater or equal are kept while the others are\ndiscarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\nthe median (resp. the mean) of the feature importances. A scaling\nfactor (e.g., \"1.25*mean\") may also be used. If None and if the\nestimator has a parameter penalty set to l1, either explicitly\nor implicitly (e.g, Lasso), the threshold used is 1e-5.\nOtherwise, \"mean\" is used by default."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "string"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/prefit",
          "name": "prefit",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.prefit",
          "default_value": "False",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "False",
            "description": "Whether a prefit model is expected to be passed into the constructor\ndirectly or not. If True, ``transform`` must be called directly\nand SelectFromModel cannot be used with ``cross_val_score``,\n``GridSearchCV`` and similar utilities that clone the estimator.\nOtherwise train the model using ``fit`` and then ``transform`` to do\nfeature selection."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/norm_order",
          "name": "norm_order",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.norm_order",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "non-zero int, inf, -inf",
            "default_value": "1",
            "description": "Order of the norm used to filter the vectors of coefficients below\n``threshold`` in the case where the ``coef_`` attribute of the\nestimator is of dimension 2."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "non-zero int"
              },
              {
                "kind": "NamedType",
                "name": "inf"
              },
              {
                "kind": "NamedType",
                "name": "-inf"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/max_features",
          "name": "max_features",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.max_features",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "The maximum number of features to select.\nTo only select based on ``max_features``, set ``threshold=-np.inf``.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/__init__/importance_getter",
          "name": "importance_getter",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.__init__.importance_getter",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'auto'",
            "description": "If 'auto', uses the feature importance either through a ``coef_``\nattribute or ``feature_importances_`` attribute of estimator.\n\nAlso accepts a string that specifies an attribute name/path\nfor extracting feature importance (implemented with `attrgetter`).\nFor example, give `regressor_.coef_` in case of\n:class:`~sklearn.compose.TransformedTargetRegressor`  or\n`named_steps.clf.feature_importances_` in case of\n:class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\nIf `callable`, overrides the default feature importance getter.\nThe callable is passed with the fitted estimator and it should\nreturn importance for each feature.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <select_from_model>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimator, *, threshold=None, prefit=False,\n                 norm_order=1, max_features=None,\n                 importance_getter='auto'):\n        self.estimator = estimator\n        self.threshold = threshold\n        self.prefit = prefit\n        self.importance_getter = importance_getter\n        self.norm_order = norm_order\n        self.max_features = max_features"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/fit",
      "name": "fit",
      "qname": "sklearn.feature_selection._from_model.SelectFromModel.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/fit/self",
          "name": "self",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/fit/X",
          "name": "X",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/fit/y",
          "name": "y",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The target values (integers that correspond to classes in\nclassification, real numbers in regression)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/fit/fit_params",
          "name": "fit_params",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.fit.fit_params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "Other estimator specific parameters",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Other estimator specific parameters"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the SelectFromModel meta-transformer.",
      "docstring": "Fit the SelectFromModel meta-transformer.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The training input samples.\n\ny : array-like of shape (n_samples,), default=None\n    The target values (integers that correspond to classes in\n    classification, real numbers in regression).\n\n**fit_params : Other estimator specific parameters\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y=None, **fit_params):\n        \"\"\"Fit the SelectFromModel meta-transformer.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,), default=None\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        **fit_params : Other estimator specific parameters\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        if self.max_features is not None:\n            if not isinstance(self.max_features, numbers.Integral):\n                raise TypeError(\"'max_features' should be an integer between\"\n                                \" 0 and {} features. Got {!r} instead.\"\n                                .format(X.shape[1], self.max_features))\n            elif self.max_features < 0 or self.max_features > X.shape[1]:\n                raise ValueError(\"'max_features' should be 0 and {} features.\"\n                                 \"Got {} instead.\"\n                                 .format(X.shape[1], self.max_features))\n\n        if self.prefit:\n            raise NotFittedError(\n                \"Since 'prefit=True', call transform directly\")\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(X, y, **fit_params)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/n_features_in_@getter",
      "name": "n_features_in_",
      "qname": "sklearn.feature_selection._from_model.SelectFromModel.n_features_in_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/n_features_in_@getter/self",
          "name": "self",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.n_features_in_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def n_features_in_(self):\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() fails if the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\"\n                .format(self.__class__.__name__)\n            ) from nfe\n\n        return self.estimator_.n_features_in_"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/partial_fit",
      "name": "partial_fit",
      "qname": "sklearn.feature_selection._from_model.SelectFromModel.partial_fit",
      "decorators": [
        "if_delegate_has_method('estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/partial_fit/self",
          "name": "self",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.partial_fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/partial_fit/X",
          "name": "X",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.partial_fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/partial_fit/y",
          "name": "y",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.partial_fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "None",
            "description": "The target values (integers that correspond to classes in\nclassification, real numbers in regression)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/partial_fit/fit_params",
          "name": "fit_params",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.partial_fit.fit_params",
          "default_value": null,
          "assigned_by": "NAMED_VARARG",
          "is_public": true,
          "docstring": {
            "type": "Other estimator specific parameters",
            "default_value": "",
            "description": ""
          },
          "type": {
            "kind": "NamedType",
            "name": "Other estimator specific parameters"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the SelectFromModel meta-transformer only once.",
      "docstring": "Fit the SelectFromModel meta-transformer only once.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The training input samples.\n\ny : array-like of shape (n_samples,), default=None\n    The target values (integers that correspond to classes in\n    classification, real numbers in regression).\n\n**fit_params : Other estimator specific parameters\n\nReturns\n-------\nself : object",
      "code": "    @if_delegate_has_method('estimator')\n    def partial_fit(self, X, y=None, **fit_params):\n        \"\"\"Fit the SelectFromModel meta-transformer only once.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,), default=None\n            The target values (integers that correspond to classes in\n            classification, real numbers in regression).\n\n        **fit_params : Other estimator specific parameters\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        if self.prefit:\n            raise NotFittedError(\n                \"Since 'prefit=True', call transform directly\")\n        if not hasattr(self, \"estimator_\"):\n            self.estimator_ = clone(self.estimator)\n        self.estimator_.partial_fit(X, y, **fit_params)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/threshold_@getter",
      "name": "threshold_",
      "qname": "sklearn.feature_selection._from_model.SelectFromModel.threshold_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._from_model/SelectFromModel/threshold_@getter/self",
          "name": "self",
          "qname": "sklearn.feature_selection._from_model.SelectFromModel.threshold_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def threshold_(self):\n        scores = _get_feature_importances(estimator=self.estimator_,\n                                          getter=self.importance_getter,\n                                          transform_func='norm',\n                                          norm_order=self.norm_order)\n        return _calculate_threshold(self.estimator, scores, self.threshold)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif",
      "name": "mutual_info_classif",
      "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif/X",
          "name": "X",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like or sparse matrix, shape (n_samples, n_features)",
            "default_value": "",
            "description": "Feature matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "sparse matrix"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif/y",
          "name": "y",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif/discrete_features",
          "name": "discrete_features",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif.discrete_features",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', bool, array-like}",
            "default_value": "'auto'",
            "description": "If bool, then determines whether to consider all features discrete\nor continuous. If array, then it should be either a boolean mask\nwith shape (n_features,) or array with indices of discrete features.\nIf 'auto', it is assigned to False for dense `X` and to True for\nsparse `X`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif/n_neighbors",
          "name": "n_neighbors",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif.n_neighbors",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of neighbors to use for MI estimation for continuous variables,\nsee [2]_ and [3]_. Higher values reduce variance of the estimation, but\ncould introduce a bias."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif/copy",
          "name": "copy",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to make a copy of the given data. If set to False, the initial\ndata will be overwritten."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_classif/random_state",
          "name": "random_state",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_classif.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for adding small noise to\ncontinuous variables in order to remove repeated values.\nPass an int for reproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Feature matrix.\n\ny : array-like of shape (n_samples,)\n    Target vector.\n\ndiscrete_features : {'auto', bool, array-like}, default='auto'\n    If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`.\n\nn_neighbors : int, default=3\n    Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias.\n\ncopy : bool, default=True\n    Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nmi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target.\n\nNotes\n-----\n1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16",
      "code": "@_deprecate_positional_args\ndef mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3,\n                        copy=True, random_state=None):\n    \"\"\"Estimate mutual information for a discrete target variable.\n\n    Mutual information (MI) [1]_ between two random variables is a non-negative\n    value, which measures the dependency between the variables. It is equal\n    to zero if and only if two random variables are independent, and higher\n    values mean higher dependency.\n\n    The function relies on nonparametric methods based on entropy estimation\n    from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n    methods are based on the idea originally proposed in [4]_.\n\n    It can be used for univariate features selection, read more in the\n    :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array-like of shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array-like}, default='auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    n_neighbors : int, default=3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n        could introduce a bias.\n\n    copy : bool, default=True\n        Whether to make a copy of the given data. If set to False, the initial\n        data will be overwritten.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for adding small noise to\n        continuous variables in order to remove repeated values.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    mi : ndarray, shape (n_features,)\n        Estimated mutual information between each feature and the target.\n\n    Notes\n    -----\n    1. The term \"discrete features\" is used instead of naming them\n       \"categorical\", because it describes the essence more accurately.\n       For example, pixel intensities of an image are discrete features\n       (but hardly categorical) and you will get better results if mark them\n       as such. Also note, that treating a continuous variable as discrete and\n       vice versa will usually give incorrect results, so be attentive about\n       that.\n    2. True mutual information can't be negative. If its estimate turns out\n       to be negative, it is replaced by zero.\n\n    References\n    ----------\n    .. [1] `Mutual Information\n           <https://en.wikipedia.org/wiki/Mutual_information>`_\n           on Wikipedia.\n    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n           Data Sets\". PLoS ONE 9(2), 2014.\n    .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n           of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\n    \"\"\"\n    check_classification_targets(y)\n    return _estimate_mi(X, y, discrete_features, True, n_neighbors,\n                        copy, random_state)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression",
      "name": "mutual_info_regression",
      "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression/X",
          "name": "X",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like or sparse matrix, shape (n_samples, n_features)",
            "default_value": "",
            "description": "Feature matrix."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like"
              },
              {
                "kind": "NamedType",
                "name": "sparse matrix"
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression/y",
          "name": "y",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression/discrete_features",
          "name": "discrete_features",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression.discrete_features",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'auto', bool, array-like}",
            "default_value": "'auto'",
            "description": "If bool, then determines whether to consider all features discrete\nor continuous. If array, then it should be either a boolean mask\nwith shape (n_features,) or array with indices of discrete features.\nIf 'auto', it is assigned to False for dense `X` and to True for\nsparse `X`."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "auto"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression/n_neighbors",
          "name": "n_neighbors",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression.n_neighbors",
          "default_value": "3",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "3",
            "description": "Number of neighbors to use for MI estimation for continuous variables,\nsee [2]_ and [3]_. Higher values reduce variance of the estimation, but\ncould introduce a bias."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression/copy",
          "name": "copy",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression.copy",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "Whether to make a copy of the given data. If set to False, the initial\ndata will be overwritten."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._mutual_info/mutual_info_regression/random_state",
          "name": "random_state",
          "qname": "sklearn.feature_selection._mutual_info.mutual_info_regression.random_state",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, RandomState instance or None",
            "default_value": "None",
            "description": "Determines random number generation for adding small noise to\ncontinuous variables in order to remove repeated values.\nPass an int for reproducible results across multiple function calls.\nSee :term:`Glossary <random_state>`."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "RandomState instance"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Estimate mutual information for a continuous target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Estimate mutual information for a continuous target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : array-like or sparse matrix, shape (n_samples, n_features)\n    Feature matrix.\n\ny : array-like of shape (n_samples,)\n    Target vector.\n\ndiscrete_features : {'auto', bool, array-like}, default='auto'\n    If bool, then determines whether to consider all features discrete\n    or continuous. If array, then it should be either a boolean mask\n    with shape (n_features,) or array with indices of discrete features.\n    If 'auto', it is assigned to False for dense `X` and to True for\n    sparse `X`.\n\nn_neighbors : int, default=3\n    Number of neighbors to use for MI estimation for continuous variables,\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n    could introduce a bias.\n\ncopy : bool, default=True\n    Whether to make a copy of the given data. If set to False, the initial\n    data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines random number generation for adding small noise to\n    continuous variables in order to remove repeated values.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nmi : ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target.\n\nNotes\n-----\n1. The term \"discrete features\" is used instead of naming them\n   \"categorical\", because it describes the essence more accurately.\n   For example, pixel intensities of an image are discrete features\n   (but hardly categorical) and you will get better results if mark them\n   as such. Also note, that treating a continuous variable as discrete and\n   vice versa will usually give incorrect results, so be attentive about\n   that.\n2. True mutual information can't be negative. If its estimate turns out\n   to be negative, it is replaced by zero.\n\nReferences\n----------\n.. [1] `Mutual Information\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n       information\". Phys. Rev. E 69, 2004.\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16",
      "code": "@_deprecate_positional_args\ndef mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3,\n                           copy=True, random_state=None):\n    \"\"\"Estimate mutual information for a continuous target variable.\n\n    Mutual information (MI) [1]_ between two random variables is a non-negative\n    value, which measures the dependency between the variables. It is equal\n    to zero if and only if two random variables are independent, and higher\n    values mean higher dependency.\n\n    The function relies on nonparametric methods based on entropy estimation\n    from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n    methods are based on the idea originally proposed in [4]_.\n\n    It can be used for univariate features selection, read more in the\n    :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array-like of shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array-like}, default='auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    n_neighbors : int, default=3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n        could introduce a bias.\n\n    copy : bool, default=True\n        Whether to make a copy of the given data. If set to False, the initial\n        data will be overwritten.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for adding small noise to\n        continuous variables in order to remove repeated values.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    mi : ndarray, shape (n_features,)\n        Estimated mutual information between each feature and the target.\n\n    Notes\n    -----\n    1. The term \"discrete features\" is used instead of naming them\n       \"categorical\", because it describes the essence more accurately.\n       For example, pixel intensities of an image are discrete features\n       (but hardly categorical) and you will get better results if mark them\n       as such. Also note, that treating a continuous variable as discrete and\n       vice versa will usually give incorrect results, so be attentive about\n       that.\n    2. True mutual information can't be negative. If its estimate turns out\n       to be negative, it is replaced by zero.\n\n    References\n    ----------\n    .. [1] `Mutual Information\n           <https://en.wikipedia.org/wiki/Mutual_information>`_\n           on Wikipedia.\n    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n           Data Sets\". PLoS ONE 9(2), 2014.\n    .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n           of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\n    \"\"\"\n    return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n                        copy, random_state)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._rfe.RFE.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__/estimator",
          "name": "estimator",
          "qname": "sklearn.feature_selection._rfe.RFE.__init__.estimator",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "``Estimator`` instance",
            "default_value": "",
            "description": "A supervised learning estimator with a ``fit`` method that provides\ninformation about feature importance\n(e.g. `coef_`, `feature_importances_`)."
          },
          "type": {
            "kind": "NamedType",
            "name": "``Estimator`` instance"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__/n_features_to_select",
          "name": "n_features_to_select",
          "qname": "sklearn.feature_selection._rfe.RFE.__init__.n_features_to_select",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "The number of features to select. If `None`, half of the features are\nselected. If integer, the parameter is the absolute number of features\nto select. If float between 0 and 1, it is the fraction of features to\nselect.\n\n.. versionchanged:: 0.24\n   Added float values for fractions."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__/step",
          "name": "step",
          "qname": "sklearn.feature_selection._rfe.RFE.__init__.step",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "If greater than or equal to 1, then ``step`` corresponds to the\n(integer) number of features to remove at each iteration.\nIf within (0.0, 1.0), then ``step`` corresponds to the percentage\n(rounded down) of features to remove at each iteration."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.feature_selection._rfe.RFE.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls verbosity of output."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/__init__/importance_getter",
          "name": "importance_getter",
          "qname": "sklearn.feature_selection._rfe.RFE.__init__.importance_getter",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'auto'",
            "description": "If 'auto', uses the feature importance either through a `coef_`\nor `feature_importances_` attributes of estimator.\n\nAlso accepts a string that specifies an attribute name/path\nfor extracting feature importance (implemented with `attrgetter`).\nFor example, give `regressor_.coef_` in case of\n:class:`~sklearn.compose.TransformedTargetRegressor`  or\n`named_steps.clf.feature_importances_` in case of\nclass:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\nIf `callable`, overrides the default feature importance getter.\nThe callable is passed with the fitted estimator and it should\nreturn importance for each feature.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through\nany specific attribute or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimator, *, n_features_to_select=None, step=1,\n                 verbose=0, importance_getter='auto'):\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.step = step\n        self.importance_getter = importance_getter\n        self.verbose = verbose"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/classes_@getter",
      "name": "classes_",
      "qname": "sklearn.feature_selection._rfe.RFE.classes_",
      "decorators": [
        "property"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/classes_@getter/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.classes_.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "",
      "docstring": "",
      "code": "    @property\n    def classes_(self):\n        return self.estimator_.classes_"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/decision_function",
      "name": "decision_function",
      "qname": "sklearn.feature_selection._rfe.RFE.decision_function",
      "decorators": [
        "if_delegate_has_method(delegate='estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/decision_function/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.decision_function.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/decision_function/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFE.decision_function.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like or sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Compute the decision function of ``X``.",
      "docstring": "Compute the decision function of ``X``.\n\nParameters\n----------\nX : {array-like or sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\nscore : array, shape = [n_samples, n_classes] or [n_samples]\n    The decision function of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.\n    Regression and binary classification produce an array of shape\n    [n_samples].",
      "code": "    @if_delegate_has_method(delegate='estimator')\n    def decision_function(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : array, shape = [n_samples, n_classes] or [n_samples]\n            The decision function of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification produce an array of shape\n            [n_samples].\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.decision_function(self.transform(X))"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/fit",
      "name": "fit",
      "qname": "sklearn.feature_selection._rfe.RFE.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/fit/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/fit/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFE.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The training input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/fit/y",
          "name": "y",
          "qname": "sklearn.feature_selection._rfe.RFE.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "The target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the RFE model and then the underlying estimator on the selected\n   features.",
      "docstring": "Fit the RFE model and then the underlying estimator on the selected\n   features.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples.\n\ny : array-like of shape (n_samples,)\n    The target values.",
      "code": "    def fit(self, X, y):\n        \"\"\"Fit the RFE model and then the underlying estimator on the selected\n           features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples.\n\n        y : array-like of shape (n_samples,)\n            The target values.\n        \"\"\"\n        return self._fit(X, y)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict",
      "name": "predict",
      "qname": "sklearn.feature_selection._rfe.RFE.predict",
      "decorators": [
        "if_delegate_has_method(delegate='estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.predict.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFE.predict.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape [n_samples, n_features]",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features]"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Reduce X to the selected features and then predict using the\n   underlying estimator.",
      "docstring": "Reduce X to the selected features and then predict using the\n   underlying estimator.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\nReturns\n-------\ny : array of shape [n_samples]\n    The predicted target values.",
      "code": "    @if_delegate_has_method(delegate='estimator')\n    def predict(self, X):\n        \"\"\"Reduce X to the selected features and then predict using the\n           underlying estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        y : array of shape [n_samples]\n            The predicted target values.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict(self.transform(X))"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_log_proba",
      "name": "predict_log_proba",
      "qname": "sklearn.feature_selection._rfe.RFE.predict_log_proba",
      "decorators": [
        "if_delegate_has_method(delegate='estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_log_proba/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.predict_log_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_log_proba/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFE.predict_log_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape [n_samples, n_features]",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features]"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class log-probabilities for X.",
      "docstring": "Predict class log-probabilities for X.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\nReturns\n-------\np : array of shape (n_samples, n_classes)\n    The class log-probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    @if_delegate_has_method(delegate='estimator')\n    def predict_log_proba(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_log_proba(self.transform(X))"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_proba",
      "name": "predict_proba",
      "qname": "sklearn.feature_selection._rfe.RFE.predict_proba",
      "decorators": [
        "if_delegate_has_method(delegate='estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_proba/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.predict_proba.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/predict_proba/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFE.predict_proba.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like or sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The input samples. Internally, it will be converted to\n``dtype=np.float32`` and if a sparse matrix is provided\nto a sparse ``csr_matrix``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Predict class probabilities for X.",
      "docstring": "Predict class probabilities for X.\n\nParameters\n----------\nX : {array-like or sparse matrix} of shape (n_samples, n_features)\n    The input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csr_matrix``.\n\nReturns\n-------\np : array of shape (n_samples, n_classes)\n    The class probabilities of the input samples. The order of the\n    classes corresponds to that in the attribute :term:`classes_`.",
      "code": "    @if_delegate_has_method(delegate='estimator')\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like or sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.predict_proba(self.transform(X))"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/score",
      "name": "score",
      "qname": "sklearn.feature_selection._rfe.RFE.score",
      "decorators": [
        "if_delegate_has_method(delegate='estimator')"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/score/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFE.score.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/score/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFE.score.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape [n_samples, n_features]",
            "default_value": "",
            "description": "The input samples."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array of shape [n_samples"
              },
              {
                "kind": "NamedType",
                "name": "n_features]"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFE/score/y",
          "name": "y",
          "qname": "sklearn.feature_selection._rfe.RFE.score.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape [n_samples]",
            "default_value": "",
            "description": "The target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape [n_samples]"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Reduce X to the selected features and then return the score of the\n   underlying estimator.",
      "docstring": "Reduce X to the selected features and then return the score of the\n   underlying estimator.\n\nParameters\n----------\nX : array of shape [n_samples, n_features]\n    The input samples.\n\ny : array of shape [n_samples]\n    The target values.",
      "code": "    @if_delegate_has_method(delegate='estimator')\n    def score(self, X, y):\n        \"\"\"Reduce X to the selected features and then return the score of the\n           underlying estimator.\n\n        Parameters\n        ----------\n        X : array of shape [n_samples, n_features]\n            The input samples.\n\n        y : array of shape [n_samples]\n            The target values.\n        \"\"\"\n        check_is_fitted(self)\n        return self.estimator_.score(self.transform(X), y)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._rfe.RFECV.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/estimator",
          "name": "estimator",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.estimator",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "``Estimator`` instance",
            "default_value": "",
            "description": "A supervised learning estimator with a ``fit`` method that provides\ninformation about feature importance either through a ``coef_``\nattribute or through a ``feature_importances_`` attribute."
          },
          "type": {
            "kind": "NamedType",
            "name": "``Estimator`` instance"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/step",
          "name": "step",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.step",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "1",
            "description": "If greater than or equal to 1, then ``step`` corresponds to the\n(integer) number of features to remove at each iteration.\nIf within (0.0, 1.0), then ``step`` corresponds to the percentage\n(rounded down) of features to remove at each iteration.\nNote that the last iteration may remove fewer than ``step`` features in\norder to reach ``min_features_to_select``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/min_features_to_select",
          "name": "min_features_to_select",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.min_features_to_select",
          "default_value": "1",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "1",
            "description": "The minimum number of features to be selected. This number of features\nwill always be scored, even if the difference between the original\nfeature count and ``min_features_to_select`` isn't divisible by\n``step``.\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/cv",
          "name": "cv",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.cv",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, cross-validation generator or an iterable",
            "default_value": "None",
            "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\n- None, to use the default 5-fold cross-validation,\n- integer, to specify the number of folds.\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, if ``y`` is binary or multiclass,\n:class:`~sklearn.model_selection.StratifiedKFold` is used. If the\nestimator is a classifier or if ``y`` is neither binary nor multiclass,\n:class:`~sklearn.model_selection.KFold` is used.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here.\n\n.. versionchanged:: 0.22\n    ``cv`` default value of None changed from 3-fold to 5-fold."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "cross-validation generator"
              },
              {
                "kind": "NamedType",
                "name": "an iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/scoring",
          "name": "scoring",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.scoring",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "string, callable or None",
            "default_value": "None",
            "description": "A string (see model evaluation documentation) or\na scorer callable object / function with signature\n``scorer(estimator, X, y)``."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "string"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/verbose",
          "name": "verbose",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.verbose",
          "default_value": "0",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "0",
            "description": "Controls verbosity of output."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or None",
            "default_value": "None",
            "description": "Number of cores to run in parallel while fitting across folds.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/__init__/importance_getter",
          "name": "importance_getter",
          "qname": "sklearn.feature_selection._rfe.RFECV.__init__.importance_getter",
          "default_value": "'auto'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str or callable",
            "default_value": "'auto'",
            "description": "If 'auto', uses the feature importance either through a `coef_`\nor `feature_importances_` attributes of estimator.\n\nAlso accepts a string that specifies an attribute name/path\nfor extracting feature importance.\nFor example, give `regressor_.coef_` in case of\n:class:`~sklearn.compose.TransformedTargetRegressor`  or\n`named_steps.clf.feature_importances_` in case of\n:class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n\nIf `callable`, overrides the default feature importance getter.\nThe callable is passed with the fitted estimator and it should\nreturn importance for each feature.\n\n.. versionadded:: 0.24"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Feature ranking with recursive feature elimination and cross-validated\nselection of the best number of features.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nRead more in the :ref:`User Guide <rfe>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, estimator, *, step=1, min_features_to_select=1,\n                 cv=None, scoring=None, verbose=0, n_jobs=None,\n                 importance_getter='auto'):\n        self.estimator = estimator\n        self.step = step\n        self.importance_getter = importance_getter\n        self.cv = cv\n        self.scoring = scoring\n        self.verbose = verbose\n        self.n_jobs = n_jobs\n        self.min_features_to_select = min_features_to_select"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/fit",
      "name": "fit",
      "qname": "sklearn.feature_selection._rfe.RFECV.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/fit/self",
          "name": "self",
          "qname": "sklearn.feature_selection._rfe.RFECV.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/fit/X",
          "name": "X",
          "qname": "sklearn.feature_selection._rfe.RFECV.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vector, where `n_samples` is the number of samples and\n`n_features` is the total number of features."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/fit/y",
          "name": "y",
          "qname": "sklearn.feature_selection._rfe.RFECV.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values (integers for classification, real numbers for\nregression)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._rfe/RFECV/fit/groups",
          "name": "groups",
          "qname": "sklearn.feature_selection._rfe.RFECV.fit.groups",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,) or None",
            "default_value": "None",
            "description": "Group labels for the samples used while splitting the dataset into\ntrain/test set. Only used in conjunction with a \"Group\" :term:`cv`\ninstance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n.. versionadded:: 0.20"
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "array-like of shape (n_samples,)"
              },
              {
                "kind": "NamedType",
                "name": "None"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Fit the RFE model and automatically tune the number of selected\n   features.",
      "docstring": "Fit the RFE model and automatically tune the number of selected\n   features.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training vector, where `n_samples` is the number of samples and\n    `n_features` is the total number of features.\n\ny : array-like of shape (n_samples,)\n    Target values (integers for classification, real numbers for\n    regression).\n\ngroups : array-like of shape (n_samples,) or None, default=None\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n    .. versionadded:: 0.20",
      "code": "    def fit(self, X, y, groups=None):\n        \"\"\"Fit the RFE model and automatically tune the number of selected\n           features.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the total number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values (integers for classification, real numbers for\n            regression).\n\n        groups : array-like of shape (n_samples,) or None, default=None\n            Group labels for the samples used while splitting the dataset into\n            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n            instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n\n            .. versionadded:: 0.20\n        \"\"\"\n        tags = self._get_tags()\n        X, y = self._validate_data(\n            X, y, accept_sparse=\"csr\", ensure_min_features=2,\n            force_all_finite=not tags.get('allow_nan', True),\n            multi_output=True\n        )\n\n        # Initialization\n        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n        scorer = check_scoring(self.estimator, scoring=self.scoring)\n        n_features = X.shape[1]\n\n        if 0.0 < self.step < 1.0:\n            step = int(max(1, self.step * n_features))\n        else:\n            step = int(self.step)\n        if step <= 0:\n            raise ValueError(\"Step must be >0\")\n\n        # Build an RFE object, which will evaluate and score each possible\n        # feature count, down to self.min_features_to_select\n        rfe = RFE(estimator=self.estimator,\n                  n_features_to_select=self.min_features_to_select,\n                  importance_getter=self.importance_getter,\n                  step=self.step, verbose=self.verbose)\n\n        # Determine the number of subsets of features by fitting across\n        # the train folds and choosing the \"features_to_select\" parameter\n        # that gives the least averaged error across all folds.\n\n        # Note that joblib raises a non-picklable error for bound methods\n        # even if n_jobs is set to 1 with the default multiprocessing\n        # backend.\n        # This branching is done so that to\n        # make sure that user code that sets n_jobs to 1\n        # and provides bound methods as scorers is not broken with the\n        # addition of n_jobs parameter in version 0.18.\n\n        if effective_n_jobs(self.n_jobs) == 1:\n            parallel, func = list, _rfe_single_fit\n        else:\n            parallel = Parallel(n_jobs=self.n_jobs)\n            func = delayed(_rfe_single_fit)\n\n        scores = parallel(\n            func(rfe, self.estimator, X, y, train, test, scorer)\n            for train, test in cv.split(X, y, groups))\n\n        scores = np.sum(scores, axis=0)\n        scores_rev = scores[::-1]\n        argmax_idx = len(scores) - np.argmax(scores_rev) - 1\n        n_features_to_select = max(\n            n_features - (argmax_idx * step),\n            self.min_features_to_select)\n\n        # Re-execute an elimination with best_k over the whole set\n        rfe = RFE(estimator=self.estimator,\n                  n_features_to_select=n_features_to_select, step=self.step,\n                  importance_getter=self.importance_getter,\n                  verbose=self.verbose)\n\n        rfe.fit(X, y)\n\n        # Set final attributes\n        self.support_ = rfe.support_\n        self.n_features_ = rfe.n_features_\n        self.ranking_ = rfe.ranking_\n        self.estimator_ = clone(self.estimator)\n        self.estimator_.fit(self.transform(X), y)\n\n        # Fixing a normalization error, n is equal to get_n_splits(X, y) - 1\n        # here, the scores are normalized by get_n_splits(X, y)\n        self.grid_scores_ = scores[::-1] / cv.get_n_splits(X, y, groups)\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/estimator",
          "name": "estimator",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.estimator",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "estimator instance",
            "default_value": "",
            "description": "An unfitted estimator."
          },
          "type": {
            "kind": "NamedType",
            "name": "estimator instance"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/n_features_to_select",
          "name": "n_features_to_select",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.n_features_to_select",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or float",
            "default_value": "None",
            "description": "The number of features to select. If `None`, half of the features are\nselected. If integer, the parameter is the absolute number of features\nto select. If float between 0 and 1, it is the fraction of features to\nselect."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "float"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/direction",
          "name": "direction",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.direction",
          "default_value": "'forward'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'forward', 'backward'}",
            "default_value": "'forward'",
            "description": "Whether to perform forward selection or backward selection."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "forward",
              "backward"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/scoring",
          "name": "scoring",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.scoring",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "str, callable, list/tuple or dict",
            "default_value": "None",
            "description": "A single str (see :ref:`scoring_parameter`) or a callable\n(see :ref:`scoring`) to evaluate the predictions on the test set.\n\nNOTE that when using custom scorers, each scorer should return a single\nvalue. Metric functions returning a list/array of values can be wrapped\ninto multiple scorers that return one value each.\n\nIf None, the estimator's score method is used."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "str"
              },
              {
                "kind": "NamedType",
                "name": "callable"
              },
              {
                "kind": "NamedType",
                "name": "list/tuple"
              },
              {
                "kind": "NamedType",
                "name": "dict"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/cv",
          "name": "cv",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.cv",
          "default_value": "5",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int, cross-validation generator or an iterable",
            "default_value": "None",
            "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\n- None, to use the default 5-fold cross validation,\n- integer, to specify the number of folds in a `(Stratified)KFold`,\n- :term:`CV splitter`,\n- An iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, if the estimator is a classifier and ``y`` is\neither binary or multiclass, :class:`StratifiedKFold` is used. In all\nother cases, :class:`KFold` is used. These splitters are instantiated\nwith `shuffle=False` so the splits will be the same across calls.\n\nRefer :ref:`User Guide <cross_validation>` for the various\ncross-validation strategies that can be used here."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "cross-validation generator"
              },
              {
                "kind": "NamedType",
                "name": "an iterable"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/__init__/n_jobs",
          "name": "n_jobs",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__.n_jobs",
          "default_value": "None",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "None",
            "description": "Number of jobs to run in parallel. When evaluating a new feature to\nadd or remove, the cross-validation procedure is parallel over the\nfolds.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary <n_jobs>`\nfor more details."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Transformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or\nremoves (backward selection) features to form a feature subset in a\ngreedy fashion. At each stage, this estimator chooses the best feature to\nadd or remove based on the cross-validation score of an estimator.\n\nRead more in the :ref:`User Guide <sequential_feature_selection>`.\n\n.. versionadded:: 0.24",
      "docstring": "",
      "code": "    def __init__(self, estimator, *, n_features_to_select=None,\n                 direction='forward', scoring=None, cv=5, n_jobs=None):\n\n        self.estimator = estimator\n        self.n_features_to_select = n_features_to_select\n        self.direction = direction\n        self.scoring = scoring\n        self.cv = cv\n        self.n_jobs = n_jobs"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/fit",
      "name": "fit",
      "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/fit/self",
          "name": "self",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/fit/X",
          "name": "X",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Training vectors."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples, n_features)"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._sequential/SequentialFeatureSelector/fit/y",
          "name": "y",
          "qname": "sklearn.feature_selection._sequential.SequentialFeatureSelector.fit.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target values."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn the features to select.",
      "docstring": "Learn the features to select.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Training vectors.\ny : array-like of shape (n_samples,)\n    Target values.\n\nReturns\n-------\nself : object",
      "code": "    def fit(self, X, y):\n        \"\"\"Learn the features to select.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vectors.\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        tags = self._get_tags()\n        X, y = self._validate_data(\n            X, y, accept_sparse=\"csc\",\n            ensure_min_features=2,\n            force_all_finite=not tags.get(\"allow_nan\", True),\n            multi_output=True\n        )\n        n_features = X.shape[1]\n\n        error_msg = (\"n_features_to_select must be either None, an \"\n                     \"integer in [1, n_features - 1] \"\n                     \"representing the absolute \"\n                     \"number of features, or a float in (0, 1] \"\n                     \"representing a percentage of features to \"\n                     f\"select. Got {self.n_features_to_select}\")\n        if self.n_features_to_select is None:\n            self.n_features_to_select_ = n_features // 2\n        elif isinstance(self.n_features_to_select, numbers.Integral):\n            if not 0 < self.n_features_to_select < n_features:\n                raise ValueError(error_msg)\n            self.n_features_to_select_ = self.n_features_to_select\n        elif isinstance(self.n_features_to_select, numbers.Real):\n            if not 0 < self.n_features_to_select <= 1:\n                raise ValueError(error_msg)\n            self.n_features_to_select_ = int(n_features *\n                                             self.n_features_to_select)\n        else:\n            raise ValueError(error_msg)\n\n        if self.direction not in ('forward', 'backward'):\n            raise ValueError(\n                \"direction must be either 'forward' or 'backward'. \"\n                f\"Got {self.direction}.\"\n            )\n\n        cloned_estimator = clone(self.estimator)\n\n        # the current mask corresponds to the set of features:\n        # - that we have already *selected* if we do forward selection\n        # - that we have already *excluded* if we do backward selection\n        current_mask = np.zeros(shape=n_features, dtype=bool)\n        n_iterations = (\n            self.n_features_to_select_ if self.direction == 'forward'\n            else n_features - self.n_features_to_select_\n        )\n        for _ in range(n_iterations):\n            new_feature_idx = self._get_best_new_feature(cloned_estimator, X,\n                                                         y, current_mask)\n            current_mask[new_feature_idx] = True\n\n        if self.direction == 'backward':\n            current_mask = ~current_mask\n        self.support_ = current_mask\n\n        return self"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._univariate_selection.GenericUnivariateSelect.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._univariate_selection.GenericUnivariateSelect.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/__init__/score_func",
          "name": "score_func",
          "qname": "sklearn.feature_selection._univariate_selection.GenericUnivariateSelect.__init__.score_func",
          "default_value": "f_classif",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n(scores, pvalues). For modes 'percentile' or 'kbest' it can return\na single array scores."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/__init__/mode",
          "name": "mode",
          "qname": "sklearn.feature_selection._univariate_selection.GenericUnivariateSelect.__init__.mode",
          "default_value": "'percentile'",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "{'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}",
            "default_value": "'percentile'",
            "description": "Feature selection mode."
          },
          "type": {
            "kind": "EnumType",
            "values": [
              "fpr",
              "fwe",
              "fdr",
              "percentile",
              "k_best"
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/GenericUnivariateSelect/__init__/param",
          "name": "param",
          "qname": "sklearn.feature_selection._univariate_selection.GenericUnivariateSelect.__init__.param",
          "default_value": "1e-05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float or int depending on the feature selection mode",
            "default_value": "1e-5",
            "description": "Parameter of the corresponding mode."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "float"
              },
              {
                "kind": "NamedType",
                "name": "int depending on the feature selection mode"
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, mode='percentile', param=1e-5):\n        super().__init__(score_func=score_func)\n        self.mode = mode\n        self.param = param"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._univariate_selection.SelectFdr.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFdr.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr/__init__/score_func",
          "name": "score_func",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFdr.__init__.score_func",
          "default_value": "f_classif",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n(scores, pvalues).\nDefault is f_classif (see below \"See Also\"). The default function only\nworks with classification tasks."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFdr/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFdr.__init__.alpha",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "5e-2",
            "description": "The highest uncorrected p-value for features to keep."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Filter: Select the p-values for an estimated false discovery rate\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._univariate_selection.SelectFpr.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFpr.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr/__init__/score_func",
          "name": "score_func",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFpr.__init__.score_func",
          "default_value": "f_classif",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n(scores, pvalues).\nDefault is f_classif (see below \"See Also\"). The default function only\nworks with classification tasks."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFpr/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFpr.__init__.alpha",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "5e-2",
            "description": "The highest p-value for features to be kept."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._univariate_selection.SelectFwe.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFwe.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe/__init__/score_func",
          "name": "score_func",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFwe.__init__.score_func",
          "default_value": "f_classif",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n(scores, pvalues).\nDefault is f_classif (see below \"See Also\"). The default function only\nworks with classification tasks."
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectFwe/__init__/alpha",
          "name": "alpha",
          "qname": "sklearn.feature_selection._univariate_selection.SelectFwe.__init__.alpha",
          "default_value": "0.05",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "5e-2",
            "description": "The highest uncorrected p-value for features to keep."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Filter: Select the p-values corresponding to Family-wise error rate\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, alpha=5e-2):\n        super().__init__(score_func=score_func)\n        self.alpha = alpha"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._univariate_selection.SelectKBest.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._univariate_selection.SelectKBest.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/__init__/score_func",
          "name": "score_func",
          "qname": "sklearn.feature_selection._univariate_selection.SelectKBest.__init__.score_func",
          "default_value": "f_classif",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n(scores, pvalues) or a single array with scores.\nDefault is f_classif (see below \"See Also\"). The default function only\nworks with classification tasks.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectKBest/__init__/k",
          "name": "k",
          "qname": "sklearn.feature_selection._univariate_selection.SelectKBest.__init__.k",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int or \"all\"",
            "default_value": "10",
            "description": "Number of top features to select.\nThe \"all\" option bypasses selection, for use in a parameter search."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "NamedType",
                "name": "int"
              },
              {
                "kind": "NamedType",
                "name": "\"all\""
              }
            ]
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, k=10):\n        super().__init__(score_func=score_func)\n        self.k = k"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._univariate_selection.SelectPercentile.__init__",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._univariate_selection.SelectPercentile.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/__init__/score_func",
          "name": "score_func",
          "qname": "sklearn.feature_selection._univariate_selection.SelectPercentile.__init__.score_func",
          "default_value": "f_classif",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "callable",
            "default_value": "f_classif",
            "description": "Function taking two arrays X and y, and returning a pair of arrays\n(scores, pvalues) or a single array with scores.\nDefault is f_classif (see below \"See Also\"). The default function only\nworks with classification tasks.\n\n.. versionadded:: 0.18"
          },
          "type": {
            "kind": "NamedType",
            "name": "callable"
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/SelectPercentile/__init__/percentile",
          "name": "percentile",
          "qname": "sklearn.feature_selection._univariate_selection.SelectPercentile.__init__.percentile",
          "default_value": "10",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "int",
            "default_value": "10",
            "description": "Percent of features to keep."
          },
          "type": {
            "kind": "NamedType",
            "name": "int"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "",
      "code": "    @_deprecate_positional_args\n    def __init__(self, score_func=f_classif, *, percentile=10):\n        super().__init__(score_func=score_func)\n        self.percentile = percentile"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/chi2",
      "name": "chi2",
      "qname": "sklearn.feature_selection._univariate_selection.chi2",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/chi2/X",
          "name": "X",
          "qname": "sklearn.feature_selection._univariate_selection.chi2.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "Sample vectors."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/chi2/y",
          "name": "y",
          "qname": "sklearn.feature_selection._univariate_selection.chi2.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array-like of shape (n_samples,)",
            "default_value": "",
            "description": "Target vector (class labels)."
          },
          "type": {
            "kind": "NamedType",
            "name": "array-like of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Compute chi-squared stats between each non-negative feature and class.\n\nThis score can be used to select the n_features features with the\nhighest values for the test chi-squared statistic from X, which must\ncontain only non-negative features such as booleans or frequencies\n(e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic\nvariables, so using this function \"weeds out\" the features that are the\nmost likely to be independent of class and therefore irrelevant for\nclassification.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Compute chi-squared stats between each non-negative feature and class.\n\nThis score can be used to select the n_features features with the\nhighest values for the test chi-squared statistic from X, which must\ncontain only non-negative features such as booleans or frequencies\n(e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic\nvariables, so using this function \"weeds out\" the features that are the\nmost likely to be independent of class and therefore irrelevant for\nclassification.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Sample vectors.\n\ny : array-like of shape (n_samples,)\n    Target vector (class labels).\n\nReturns\n-------\nchi2 : ndarray of shape (n_features,)\n    Chi2 statistics for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values for each feature.\n\nNotes\n-----\nComplexity of this algorithm is O(n_classes * n_features).\n\nSee Also\n--------\nf_classif : ANOVA F-value between label/feature for classification tasks.\nf_regression : F-value between label/feature for regression tasks.",
      "code": "def chi2(X, y):\n    \"\"\"Compute chi-squared stats between each non-negative feature and class.\n\n    This score can be used to select the n_features features with the\n    highest values for the test chi-squared statistic from X, which must\n    contain only non-negative features such as booleans or frequencies\n    (e.g., term counts in document classification), relative to the classes.\n\n    Recall that the chi-square test measures dependence between stochastic\n    variables, so using this function \"weeds out\" the features that are the\n    most likely to be independent of class and therefore irrelevant for\n    classification.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Sample vectors.\n\n    y : array-like of shape (n_samples,)\n        Target vector (class labels).\n\n    Returns\n    -------\n    chi2 : ndarray of shape (n_features,)\n        Chi2 statistics for each feature.\n\n    p_values : ndarray of shape (n_features,)\n        P-values for each feature.\n\n    Notes\n    -----\n    Complexity of this algorithm is O(n_classes * n_features).\n\n    See Also\n    --------\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    \"\"\"\n\n    # XXX: we might want to do some of the following in logspace instead for\n    # numerical stability.\n    X = check_array(X, accept_sparse='csr')\n    if np.any((X.data if issparse(X) else X) < 0):\n        raise ValueError(\"Input X must be non-negative.\")\n\n    Y = LabelBinarizer().fit_transform(y)\n    if Y.shape[1] == 1:\n        Y = np.append(1 - Y, Y, axis=1)\n\n    observed = safe_sparse_dot(Y.T, X)          # n_classes * n_features\n\n    feature_count = X.sum(axis=0).reshape(1, -1)\n    class_prob = Y.mean(axis=0).reshape(1, -1)\n    expected = np.dot(class_prob.T, feature_count)\n\n    return _chisquare(observed, expected)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_classif",
      "name": "f_classif",
      "qname": "sklearn.feature_selection._univariate_selection.f_classif",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_classif/X",
          "name": "X",
          "qname": "sklearn.feature_selection._univariate_selection.f_classif.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix} of shape (n_samples, n_features)",
            "default_value": "",
            "description": "The set of regressors that will be tested sequentially."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "of shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_classif/y",
          "name": "y",
          "qname": "sklearn.feature_selection._univariate_selection.f_classif.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "ndarray of shape (n_samples,)",
            "default_value": "",
            "description": "The target vector."
          },
          "type": {
            "kind": "NamedType",
            "name": "ndarray of shape (n_samples,)"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Compute the ANOVA F-value for the provided sample.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Compute the ANOVA F-value for the provided sample.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The set of regressors that will be tested sequentially.\n\ny : ndarray of shape (n_samples,)\n    The target vector.\n\nReturns\n-------\nf_statistic : ndarray of shape (n_features,)\n    F-statistic for each feature.\n\np_values : ndarray of shape (n_features,)\n    P-values associated with the F-statistic.\n\nSee Also\n--------\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nf_regression : F-value between label/feature for regression tasks.",
      "code": "def f_classif(X, y):\n    \"\"\"Compute the ANOVA F-value for the provided sample.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The set of regressors that will be tested sequentially.\n\n    y : ndarray of shape (n_samples,)\n        The target vector.\n\n    Returns\n    -------\n    f_statistic : ndarray of shape (n_features,)\n        F-statistic for each feature.\n\n    p_values : ndarray of shape (n_features,)\n        P-values associated with the F-statistic.\n\n    See Also\n    --------\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    f_regression : F-value between label/feature for regression tasks.\n    \"\"\"\n    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])\n    args = [X[safe_mask(X, y == k)] for k in np.unique(y)]\n    return f_oneway(*args)"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_oneway",
      "name": "f_oneway",
      "qname": "sklearn.feature_selection._univariate_selection.f_oneway",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_oneway/args",
          "name": "args",
          "qname": "sklearn.feature_selection._univariate_selection.f_oneway.args",
          "default_value": null,
          "assigned_by": "POSITIONAL_VARARG",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix}",
            "default_value": "",
            "description": "sample1, sample2... The sample measurements should be given as\narguments."
          },
          "type": {
            "kind": "EnumType",
            "values": []
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Performs a 1-way ANOVA.\n\nThe one-way ANOVA tests the null hypothesis that 2 or more groups have\nthe same population mean. The test is applied to samples from two or\nmore groups, possibly with differing sizes.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Performs a 1-way ANOVA.\n\nThe one-way ANOVA tests the null hypothesis that 2 or more groups have\nthe same population mean. The test is applied to samples from two or\nmore groups, possibly with differing sizes.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\n*args : {array-like, sparse matrix}\n    sample1, sample2... The sample measurements should be given as\n    arguments.\n\nReturns\n-------\nf_statistic : float\n    The computed F-value of the test.\np_value : float\n    The associated p-value from the F-distribution.\n\nNotes\n-----\nThe ANOVA test has important assumptions that must be satisfied in order\nfor the associated p-value to be valid.\n\n1. The samples are independent\n2. Each sample is from a normally distributed population\n3. The population standard deviations of the groups are all equal. This\n   property is known as homoscedasticity.\n\nIf these assumptions are not true for a given set of data, it may still be\npossible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although\nwith some loss of power.\n\nThe algorithm is from Heiman[2], pp.394-7.\n\nSee ``scipy.stats.f_oneway`` that should give the same results while\nbeing less efficient.\n\nReferences\n----------\n\n.. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n       Statistics\". Chapter 14.\n       http://faculty.vassar.edu/lowry/ch14pt1.html\n\n.. [2] Heiman, G.W.  Research Methods in Statistics. 2002.",
      "code": "def f_oneway(*args):\n    \"\"\"Performs a 1-way ANOVA.\n\n    The one-way ANOVA tests the null hypothesis that 2 or more groups have\n    the same population mean. The test is applied to samples from two or\n    more groups, possibly with differing sizes.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    *args : {array-like, sparse matrix}\n        sample1, sample2... The sample measurements should be given as\n        arguments.\n\n    Returns\n    -------\n    f_statistic : float\n        The computed F-value of the test.\n    p_value : float\n        The associated p-value from the F-distribution.\n\n    Notes\n    -----\n    The ANOVA test has important assumptions that must be satisfied in order\n    for the associated p-value to be valid.\n\n    1. The samples are independent\n    2. Each sample is from a normally distributed population\n    3. The population standard deviations of the groups are all equal. This\n       property is known as homoscedasticity.\n\n    If these assumptions are not true for a given set of data, it may still be\n    possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although\n    with some loss of power.\n\n    The algorithm is from Heiman[2], pp.394-7.\n\n    See ``scipy.stats.f_oneway`` that should give the same results while\n    being less efficient.\n\n    References\n    ----------\n\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 14.\n           http://faculty.vassar.edu/lowry/ch14pt1.html\n\n    .. [2] Heiman, G.W.  Research Methods in Statistics. 2002.\n\n    \"\"\"\n    n_classes = len(args)\n    args = [as_float_array(a) for a in args]\n    n_samples_per_class = np.array([a.shape[0] for a in args])\n    n_samples = np.sum(n_samples_per_class)\n    ss_alldata = sum(safe_sqr(a).sum(axis=0) for a in args)\n    sums_args = [np.asarray(a.sum(axis=0)) for a in args]\n    square_of_sums_alldata = sum(sums_args) ** 2\n    square_of_sums_args = [s ** 2 for s in sums_args]\n    sstot = ss_alldata - square_of_sums_alldata / float(n_samples)\n    ssbn = 0.\n    for k, _ in enumerate(args):\n        ssbn += square_of_sums_args[k] / n_samples_per_class[k]\n    ssbn -= square_of_sums_alldata / float(n_samples)\n    sswn = sstot - ssbn\n    dfbn = n_classes - 1\n    dfwn = n_samples - n_classes\n    msb = ssbn / float(dfbn)\n    msw = sswn / float(dfwn)\n    constant_features_idx = np.where(msw == 0.)[0]\n    if (np.nonzero(msb)[0].size != msb.size and constant_features_idx.size):\n        warnings.warn(\"Features %s are constant.\" % constant_features_idx,\n                      UserWarning)\n    f = msb / msw\n    # flatten matrix to vector in sparse case\n    f = np.asarray(f).ravel()\n    prob = special.fdtrc(dfbn, dfwn, f)\n    return f, prob"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_regression",
      "name": "f_regression",
      "qname": "sklearn.feature_selection._univariate_selection.f_regression",
      "decorators": [
        "_deprecate_positional_args"
      ],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_regression/X",
          "name": "X",
          "qname": "sklearn.feature_selection._univariate_selection.f_regression.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix}  shape = (n_samples, n_features)",
            "default_value": "",
            "description": "The set of regressors that will be tested sequentially."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "shape = (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_regression/y",
          "name": "y",
          "qname": "sklearn.feature_selection._univariate_selection.f_regression.y",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "array of shape(n_samples).",
            "default_value": "",
            "description": "The data matrix"
          },
          "type": {
            "kind": "NamedType",
            "name": "array of shape(n_samples)."
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._univariate_selection/f_regression/center",
          "name": "center",
          "qname": "sklearn.feature_selection._univariate_selection.f_regression.center",
          "default_value": "True",
          "assigned_by": "NAME_ONLY",
          "is_public": true,
          "docstring": {
            "type": "bool",
            "default_value": "True",
            "description": "If true, X and y will be centered."
          },
          "type": {
            "kind": "NamedType",
            "name": "bool"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [
        "scikit-learn/sklearn.feature_selection"
      ],
      "description": "Univariate linear regression tests.\n\nLinear model for testing the individual effect of each of many regressors.\nThis is a scoring function to be used in a feature selection procedure, not\na free standing feature selection procedure.\n\nThis is done in 2 steps:\n\n1. The correlation between each regressor and the target is computed,\n   that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *\n   std(y)).\n2. It is converted to an F score then to a p-value.\n\nFor more on usage see the :ref:`User Guide <univariate_feature_selection>`.",
      "docstring": "Univariate linear regression tests.\n\nLinear model for testing the individual effect of each of many regressors.\nThis is a scoring function to be used in a feature selection procedure, not\na free standing feature selection procedure.\n\nThis is done in 2 steps:\n\n1. The correlation between each regressor and the target is computed,\n   that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *\n   std(y)).\n2. It is converted to an F score then to a p-value.\n\nFor more on usage see the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters\n----------\nX : {array-like, sparse matrix}  shape = (n_samples, n_features)\n    The set of regressors that will be tested sequentially.\n\ny : array of shape(n_samples).\n    The data matrix\n\ncenter : bool, default=True\n    If true, X and y will be centered.\n\nReturns\n-------\nF : array, shape=(n_features,)\n    F values of features.\n\npval : array, shape=(n_features,)\n    p-values of F-scores.\n\nSee Also\n--------\nmutual_info_regression : Mutual information for a continuous target.\nf_classif : ANOVA F-value between label/feature for classification tasks.\nchi2 : Chi-squared stats of non-negative features for classification tasks.\nSelectKBest : Select features based on the k highest scores.\nSelectFpr : Select features based on a false positive rate test.\nSelectFdr : Select features based on an estimated false discovery rate.\nSelectFwe : Select features based on family-wise error rate.\nSelectPercentile : Select features based on percentile of the highest\n    scores.",
      "code": "@_deprecate_positional_args\ndef f_regression(X, y, *, center=True):\n    \"\"\"Univariate linear regression tests.\n\n    Linear model for testing the individual effect of each of many regressors.\n    This is a scoring function to be used in a feature selection procedure, not\n    a free standing feature selection procedure.\n\n    This is done in 2 steps:\n\n    1. The correlation between each regressor and the target is computed,\n       that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *\n       std(y)).\n    2. It is converted to an F score then to a p-value.\n\n    For more on usage see the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}  shape = (n_samples, n_features)\n        The set of regressors that will be tested sequentially.\n\n    y : array of shape(n_samples).\n        The data matrix\n\n    center : bool, default=True\n        If true, X and y will be centered.\n\n    Returns\n    -------\n    F : array, shape=(n_features,)\n        F values of features.\n\n    pval : array, shape=(n_features,)\n        p-values of F-scores.\n\n    See Also\n    --------\n    mutual_info_regression : Mutual information for a continuous target.\n    f_classif : ANOVA F-value between label/feature for classification tasks.\n    chi2 : Chi-squared stats of non-negative features for classification tasks.\n    SelectKBest : Select features based on the k highest scores.\n    SelectFpr : Select features based on a false positive rate test.\n    SelectFdr : Select features based on an estimated false discovery rate.\n    SelectFwe : Select features based on family-wise error rate.\n    SelectPercentile : Select features based on percentile of the highest\n        scores.\n    \"\"\"\n    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n                     dtype=np.float64)\n    n_samples = X.shape[0]\n\n    # compute centered values\n    # note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we\n    # need not center X\n    if center:\n        y = y - np.mean(y)\n        if issparse(X):\n            X_means = X.mean(axis=0).getA1()\n        else:\n            X_means = X.mean(axis=0)\n        # compute the scaled standard deviations via moments\n        X_norms = np.sqrt(row_norms(X.T, squared=True) -\n                          n_samples * X_means ** 2)\n    else:\n        X_norms = row_norms(X.T)\n\n    # compute the correlation\n    corr = safe_sparse_dot(y, X)\n    corr /= X_norms\n    corr /= np.linalg.norm(y)\n\n    # convert to p-value\n    degrees_of_freedom = y.size - (2 if center else 1)\n    F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n    pv = stats.f.sf(F, 1, degrees_of_freedom)\n    return F, pv"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/__init__",
      "name": "__init__",
      "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.__init__",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/__init__/self",
          "name": "self",
          "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.__init__.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/__init__/threshold",
          "name": "threshold",
          "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.__init__.threshold",
          "default_value": "0.0",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "float",
            "default_value": "0",
            "description": "Features with a training-set variance lower than this threshold will\nbe removed. The default is to keep all features with non-zero variance,\ni.e. remove the features that have the same value in all samples."
          },
          "type": {
            "kind": "NamedType",
            "name": "float"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.",
      "docstring": "",
      "code": "    def __init__(self, threshold=0.):\n        self.threshold = threshold"
    },
    {
      "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/fit",
      "name": "fit",
      "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.fit",
      "decorators": [],
      "parameters": [
        {
          "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/fit/self",
          "name": "self",
          "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.fit.self",
          "default_value": null,
          "assigned_by": "IMPLICIT",
          "is_public": true,
          "docstring": {
            "type": "",
            "default_value": "",
            "description": ""
          },
          "type": {}
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/fit/X",
          "name": "X",
          "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.fit.X",
          "default_value": null,
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "{array-like, sparse matrix}, shape (n_samples, n_features)",
            "default_value": "",
            "description": "Sample vectors from which to compute variances."
          },
          "type": {
            "kind": "UnionType",
            "types": [
              {
                "kind": "EnumType",
                "values": []
              },
              {
                "kind": "NamedType",
                "name": "shape (n_samples, n_features)"
              }
            ]
          }
        },
        {
          "id": "scikit-learn/sklearn.feature_selection._variance_threshold/VarianceThreshold/fit/y",
          "name": "y",
          "qname": "sklearn.feature_selection._variance_threshold.VarianceThreshold.fit.y",
          "default_value": "None",
          "assigned_by": "POSITION_OR_NAME",
          "is_public": true,
          "docstring": {
            "type": "any",
            "default_value": "None",
            "description": "Ignored. This parameter exists only for compatibility with\nsklearn.pipeline.Pipeline."
          },
          "type": {
            "kind": "NamedType",
            "name": "any"
          }
        }
      ],
      "results": [],
      "is_public": true,
      "reexported_by": [],
      "description": "Learn empirical variances from X.",
      "docstring": "Learn empirical variances from X.\n\nParameters\n----------\nX : {array-like, sparse matrix}, shape (n_samples, n_features)\n    Sample vectors from which to compute variances.\n\ny : any, default=None\n    Ignored. This parameter exists only for compatibility with\n    sklearn.pipeline.Pipeline.\n\nReturns\n-------\nself",
      "code": "    def fit(self, X, y=None):\n        \"\"\"Learn empirical variances from X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Sample vectors from which to compute variances.\n\n        y : any, default=None\n            Ignored. This parameter exists only for compatibility with\n            sklearn.pipeline.Pipeline.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n                                dtype=np.float64,\n                                force_all_finite='allow-nan')\n\n        if hasattr(X, \"toarray\"):   # sparse matrix\n            _, self.variances_ = mean_variance_axis(X, axis=0)\n            if self.threshold == 0:\n                mins, maxes = min_max_axis(X, axis=0)\n                peak_to_peaks = maxes - mins\n        else:\n            self.variances_ = np.nanvar(X, axis=0)\n            if self.threshold == 0:\n                peak_to_peaks = np.ptp(X, axis=0)\n\n        if self.threshold == 0:\n            # Use peak-to-peak to avoid numeric precision issues\n            # for constant features\n            compare_arr = np.array([self.variances_, peak_to_peaks])\n            self.variances_ = np.nanmin(compare_arr, axis=0)\n\n        if np.all(~np.isfinite(self.variances_) |\n                  (self.variances_ <= self.threshold)):\n            msg = \"No feature in X meets the variance threshold {0:.5f}\"\n            if X.shape[0] == 1:\n                msg += \" (X contains only one sample)\"\n            raise ValueError(msg.format(self.threshold))\n\n        return self"
    }
  ]
}